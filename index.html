<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-16T00:00:00Z">2022-12-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-<span class="highlight-title">Prompt</span>ing Large Language Models for Open-Domain QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlong Li, Zhuosheng Zhang, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Domain Question Answering (ODQA) requires models to answer factoid
questions with no context given. The common way for this task is to train
models on a large-scale annotated dataset to retrieve related documents and
generate answers based on these documents. In this paper, we show that the ODQA
architecture can be dramatically simplified by treating Large Language Models
(LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to
perform ODQA so as to eliminate the need for training data and external
knowledge corpus. Concretely, we firstly generate multiple pseudo QA pairs with
background passages and one-sentence explanations for these QAs by prompting
LLMs step by step and then leverage the generated QA pairs for in-context
learning. Experimental results show our method surpasses previous
state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA
datasets, and even achieves comparable performance with several
retrieval-augmented fine-tuned models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multi-modal and Multi-hop Question Answering via Structured
  Knowledge and Unified Retrieval-Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Yang, Qian Chen, Wen Wang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal and multi-hop question answering aims to answer a question based
on multiple input sources from different modalities. Previous methods retrieve
the evidence separately and feed the retrieved evidence to a language model to
generate the corresponding answer. However, these methods fail to build
connections between candidates and thus cannot model the inter-dependent
relation during retrieval. Moreover, the reasoning process over multi-modality
candidates can be unbalanced without building alignments between different
modalities. To address this limitation, we propose a Structured Knowledge and
Unified Retrieval Generation based method (SKURG). We align the sources from
different modalities via the shared entities and map them into a shared
semantic space via structured knowledge. Then, we utilize a unified
retrieval-generation decoder to integrate intermediate retrieval results for
answer generation and adaptively determine the number of retrieval steps. We
perform experiments on two multi-modal and multi-hop datasets: WebQA and
MultimodalQA. The results demonstrate that SKURG achieves state-of-the-art
performance on both retrieval and answer generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POTATO: The Portable Text Annotation Tool <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Jackson Sargent, Apostolos Dedeloudis, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present POTATO, the Portable text annotation tool, a free, fully
open-sourced annotation system that 1) supports labeling many types of text and
multimodal data; 2) offers easy-to-configure features to maximize the
productivity of both deployers and annotators (convenient templates for common
ML/NLP tasks, active learning, keypress shortcuts, keyword highlights,
tooltips); and 3) supports a high degree of customization (editable UI,
inserting pre-screening questions, attention and qualification tests).
Experiments over two annotation tasks suggest that POTATO improves labeling
speed through its specially-designed productivity features, especially for long
documents and complex tasks. POTATO is available at
https://github.com/davidjurgens/potato and will continue to be updated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022 DEMO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planting and Mitigating Memorized Content in Predictive-Text Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. M. Downey, Wei Dai, Huseyin A. Inan, Kim Laine, Saurabh Naik, Tomasz Religa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are widely deployed to provide automatic text completion
services in user products. However, recent research has revealed that language
models (especially large ones) bear considerable risk of memorizing private
training data, which is then vulnerable to leakage and extraction by
adversaries. In this study, we test the efficacy of a range of
privacy-preserving techniques to mitigate unintended memorization of sensitive
user text, while varying other factors such as model size and adversarial
conditions. We test both "heuristic" mitigations (those without formal privacy
guarantees) and Differentially Private training, which provides provable levels
of privacy at the cost of some model performance. Our experiments show that
(with the exception of L2 regularization), heuristic mitigations are largely
ineffective in preventing memorization in our test suite, possibly because they
make too strong of assumptions about the characteristics that define
"sensitive" or "private" text. In contrast, Differential Privacy reliably
prevents memorization in our experiments, despite its computational and
model-performance costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting large language models has enabled significant recent progress in
multi-step reasoning over text. However, when applied to text generation from
semi-structured data (e.g., graphs or tables), these methods typically suffer
from low semantic coverage, hallucination, and logical inconsistency. We
propose MURMUR, a neuro-symbolic modular approach to text generation from
semi-structured data with multi-step reasoning. MURMUR is a best-first search
method that generates reasoning paths using: (1) neural and symbolic modules
with specific linguistic and logical skills, (2) a grammar whose production
rules define valid compositions of modules, and (3) value functions that assess
the quality of each reasoning step. We conduct experiments on two diverse
data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in
their data representations (graphs and tables) and span multiple linguistic and
logical skills. MURMUR obtains significant improvements over recent few-shot
baselines like direct prompting and chain-of-thought prompting, while also
achieving comparable performance to fine-tuned GPT-2 on out-of-domain data.
Moreover, human evaluation shows that MURMUR generates highly faithful and
correct reasoning paths that lead to 26% more logically consistent summaries on
LogicNLG, compared to direct prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (9 figures, 18 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Mitigating Hallucinations in Machine Translation: Model
  Internal Workings Alone Do Well, Sentence Similarity Even Better 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Dale, Elena Voita, Loïc Barrault, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the problem of hallucinations in neural machine translation has long
been recognized, so far the progress on its alleviation is very little. Indeed,
recently it turned out that without artificially encouraging models to
hallucinate, previously existing methods fall short and even the standard
sequence log-probability is more informative. It means that characteristics
internal to the model can give much more information than we expect, and before
using external models and measures, we first need to ask: how far can we go if
we use nothing but the translation model itself ? We propose to use a method
that evaluates the percentage of the source contribution to a generated
translation. Intuitively, hallucinations are translations "detached" from the
source, hence they can be identified by low source contribution. This method
improves detection accuracy for the most severe hallucinations by a factor of 2
and is able to alleviate hallucinations at test time on par with the previous
best approach that relies on external models. Next, if we move away from
internal model characteristics and allow external tools, we show that using
sentence similarity from cross-lingual embeddings further improves these
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Czech News Article <span class="highlight-title">Dataset</span>: An Interdisciplinary Approach
  to Trustworthiness Analysis <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matyáš Boháček, Michal Bravanský, Filip Trhlík, Václav Moravec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Verifee Dataset: a novel dataset of news articles with
fine-grained trustworthiness annotations. We develop a detailed methodology
that assesses the texts based on their parameters encompassing editorial
transparency, journalist conventions, and objective reporting while penalizing
manipulative techniques. We bring aboard a diverse set of researchers from
social, media, and computer sciences to overcome barriers and limited framing
of this interdisciplinary problem. We collect over $10,000$ unique articles
from almost $60$ Czech online news sources. These are categorized into one of
the $4$ classes across the credibility spectrum we propose, raging from
entirely trustworthy articles all the way to the manipulative ones. We produce
detailed statistics and study trends emerging throughout the set. Lastly, we
fine-tune multiple popular sequence-to-sequence language models using our
dataset on the trustworthiness classification task and report the best testing
F-1 score of $0.52$. We open-source the dataset, annotation methodology, and
annotators' instructions in full length at https://verifee.ai/research to
enable easy build-up work. We believe similar methods can help prevent
disinformation and educate in the realm of media literacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures; to be published at the Second Workshop on
  Multimodal Fact-Checking and Hate Speech Detection (DEFACTIFY 2023) at the
  AAAI 2023 Conference, February 14, 2023, Washington, D.C</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-aware Fine-tuning of <span class="highlight-title">Self-supervised</span> Speech Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suwon Shon, Felix Wu, Kwangyoun Kim, Prashant Sridhar, Karen Livescu, Shinji Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-trained transformers have improved the state of the art
on a variety of speech tasks. Due to the quadratic time and space complexity of
self-attention, they usually operate at the level of relatively short (e.g.,
utterance) segments. In this paper, we study the use of context, i.e.,
surrounding segments, during fine-tuning and propose a new approach called
context-aware fine-tuning. We attach a context module on top of the last layer
of a pre-trained model to encode the whole segment into a context embedding
vector which is then used as an additional feature for the final prediction.
During the fine-tuning stage, we introduce an auxiliary loss that encourages
this context embedding vector to be similar to context vectors of surrounding
segments. This allows the model to make predictions without access to these
surrounding segments at inference time and requires only a tiny overhead
compared to standard fine-tuned models. We evaluate the proposed approach using
the SLUE and Librilight benchmarks for several downstream tasks: Automatic
speech recognition (ASR), named entity recognition (NER), and sentiment
analysis (SA). The results show that context-aware fine-tuning not only
outperforms a standard fine-tuning baseline but also rivals a strong context
injection baseline that uses neighboring speech segments during inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Check-worthy Claim Detection across Topics for Automated Fact-checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amani S. Abumansour, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important component of an automated fact-checking system is the claim
check-worthiness detection system, which ranks sentences by prioritising them
based on their need to be checked. Despite a body of research tackling the
task, previous research has overlooked the challenging nature of identifying
check-worthy claims across different topics. In this paper, we assess and
quantify the challenge of detecting check-worthy claims for new, unseen topics.
After highlighting the problem, we propose the AraCWA model to mitigate the
performance deterioration when detecting check-worthy claims across topics. The
AraCWA model enables boosting the performance for new topics by incorporating
two components for few-shot learning and data augmentation. Using a publicly
available dataset of Arabic tweets consisting of 14 different topics, we
demonstrate that our proposed data augmentation strategy achieves substantial
improvements across topics overall, where the extent of the improvement varies
across topics. Further, we analyse the semantic similarities between topics,
suggesting that the similarity metric could be used as a proxy to determine the
difficulty level of an unseen topic prior to undertaking the task of labelling
the underlying sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effectiveness of Text, Acoustic, and Lattice-based representations in
  Spoken Language Understanding tasks <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esaú Villatoro-Tello, Srikanth Madikeri, Juan Zuluaga-Gomez, Bidisha Sharma, Seyyed Saeed Sarfjoo, Iuliia Nigmatulina, Petr Motlicek, Alexei V. Ivanov, Aravind Ganapathiraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we perform an exhaustive evaluation of different
representations to address the intent classification problem in a Spoken
Language Understanding (SLU) setup. We benchmark three types of systems to
perform the SLU intent detection task: 1) text-based, 2) lattice-based, and a
novel 3) multimodal approach. Our work provides a comprehensive analysis of
what could be the achievable performance of different state-of-the-art SLU
systems under different circumstances, e.g., automatically- vs.
manually-generated transcripts. We evaluate the systems on the publicly
available SLURP spoken language resource corpus. Our results indicate that
using richer forms of Automatic Speech Recognition (ASR) outputs allows SLU
systems to improve in comparison to the 1-best setup (4% relative improvement).
However, crossmodal approaches, i.e., learning from acoustic and text
embeddings, obtains performance similar to the oracle setup, and a relative
improvement of 18% over the 1-best configuration. Thus, crossmodal
architectures represent a good alternative to overcome the limitations of
working purely automatically generated textual data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023 (Under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingda Chen, Paul-Ambroise Duquenne, Pierre Andrews, Justine Kao, Alexandre Mourachko, Holger Schwenk, Marta R. Costa-jussà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-End speech-to-speech translation (S2ST) is generally evaluated with
text-based metrics. This means that generated speech has to be automatically
transcribed, making the evaluation dependent on the availability and quality of
automatic speech recognition (ASR) systems. In this paper, we propose a
text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the
dependency on ASR systems. BLASER leverages a multilingual multimodal encoder
to directly encode the speech segments for source input, translation output and
reference into a shared embedding space and computes a score of the translation
quality that can be used as a proxy to human evaluation. To evaluate our
approach, we construct training and evaluation sets from more than 40k human
annotations covering seven language directions. The best results of BLASER are
achieved by training with supervision from human rating scores. We show that
when evaluated at the sentence level, BLASER correlates significantly better
with human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in
all translation directions and ASR-COMET in five of them. Our analysis shows
combining speech and text as inputs to BLASER does not increase the correlation
with human scores, but best correlations are achieved when using speech, which
motivates the goal of our research. Moreover, we show that using ASR for
references is detrimental for text-based metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of general formal translators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iosif Iulian Petrila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The general translator formalism and computing specific implementations are
proposed. The implementation of specific elements necessary to process the
source and destination information within the translators are presented. Some
common directives or instructions, such as classes and procedures, were unified
and generalized in order to allow general translations implementations. In
order to cover general cases, two levels of processing are required, related to
the source and destination information appropriate transformations, with the
related control and processing instructions. The proposed general translator
elements are useful for processing natural or artificial information described
through any types of languages or systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experiments on Generalizability of <span class="highlight-title">BERT</span>opic on Multi-Domain Short Text <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muriël de Groot, Mohammad Aliannejadi, Marcel R. Haas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is widely used for analytically evaluating large collections
of textual data. One of the most popular topic techniques is Latent Dirichlet
Allocation (LDA), which is flexible and adaptive, but not optimal for e.g.
short texts from various domains. We explore how the state-of-the-art BERTopic
algorithm performs on short multi-domain text and find that it generalizes
better than LDA in terms of topic coherence and diversity. We further analyze
the performance of the HDBSCAN clustering algorithm utilized by BERTopic and
find that it classifies a majority of the documents as outliers. This crucial,
yet overseen problem excludes too many documents from further analysis. When we
replace HDBSCAN with k-Means, we achieve similar performance, but without
outliers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted poster presentation at WiNLP 2022, as a part of EMNLP 2022,
  2 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Rule-Based Decoding: Revisiting Syntactic Rules in Neural
  Constituency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Shi, Zhicheng Wang, Liyin Xiao, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent studies on neural constituency parsing focus on encoder
structures, while few developments are devoted to decoders. Previous research
has demonstrated that probabilistic statistical methods based on syntactic
rules are particularly effective in constituency parsing, whereas syntactic
rules are not used during the training of neural models in prior work probably
due to their enormous computation requirements. In this paper, we first
implement a fast CKY decoding procedure harnessing GPU acceleration, based on
which we further derive a syntactic rule-based (rule-constrained) CKY decoding.
In the experiments, our method obtains 95.89 and 92.52 F1 on the datasets of
PTB and CTB respectively, which shows significant improvements compared with
previous approaches. Besides, our parser achieves strong and competitive
cross-domain performance in zero-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Small Language Models to Reason 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of thought prompting successfully improves the reasoning capabilities
of large language models, achieving state of the art results on a range of
datasets. However, these reasoning capabilities only appear to emerge in models
with a size of over 100 billion parameters. In this paper, we explore the
transfer of such reasoning capabilities to models with less than 100 billion
parameters via knowledge distillation. Specifically, we finetune a student
model on the chain of thought outputs generated by a larger teacher model. Our
experiments show that the proposed method improves task performance across
arithmetic, commonsense and symbolic reasoning datasets. For example, the
accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on
PaLM-540B generated chains of thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoder Tuning: Efficient Language Understanding as Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganqu Cui, Wentao Li, Ning Ding, Longtao Huang, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the evergrowing sizes of pre-trained models (PTMs), it has been an
emerging practice to only provide the inference APIs for users, namely
model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,
most current approaches focus on the input side, seeking for powerful prompts
to stimulate models for correct answers. However, we argue that input-side
adaptation could be arduous due to the lack of gradient signals and they
usually require thousands of API queries, resulting in high computation and
time costs. In light of this, we present Decoder Tuning (DecT), which in
contrast optimizes task-specific decoder networks on the output side.
Specifically, DecT first extracts prompt-stimulated output scores for initial
predictions. On top of that, we train an additional decoder network on the
output representations to incorporate posterior data knowledge. By
gradient-based optimization, DecT can be trained within several seconds and
requires only one PTM query per sample. Empirically, we conduct extensive
natural language understanding experiments and show that DecT significantly
outperforms state-of-the-art algorithms with a $10^3\times$ speed-up.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing distil<span class="highlight-title">Bert</span> <span class="highlight-title">transformer</span> model for sentiment classification of
  COVID-19's Persian open-text responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Sadat Masoumi, Mohammad Bahrani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has caused drastic alternations in human life in all
aspects. The government's laws in this regard affected the lifestyle of all
people. Due to this fact studying the sentiment of individuals is essential to
be aware of the future impacts of the coming pandemics. To contribute to this
aim, we proposed an NLP (Natural Language Processing) model to analyze
open-text answers in a survey in Persian and detect positive and negative
feelings of the people in Iran. In this study, a distilBert transformer model
was applied to take on this task. We deployed three approaches to perform the
comparison, and our best model could gain accuracy: 0.824, Precision: 0.824,
Recall: 0.798, and F1 score: 0.804.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted at The 7th International Conference on Science
  and Technology of Electrical, Computer, and Mechanical Engineering of Iran</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metaphorical Polysemy Detection: Conventional Metaphor meets Word Sense
  Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Hall Maudslay, Simone Teufel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linguists distinguish between novel and conventional metaphor, a distinction
which the metaphor detection task in NLP does not take into account. Instead,
metaphoricity is formulated as a property of a token in a sentence, regardless
of metaphor type. In this paper, we investigate the limitations of treating
conventional metaphors in this way, and advocate for an alternative which we
name 'metaphorical polysemy detection' (MPD). In MPD, only conventional
metaphoricity is treated, and it is formulated as a property of word senses in
a lexicon. We develop the first MPD model, which learns to identify
conventional metaphors in the English WordNet. To train it, we present a novel
training procedure that combines metaphor detection with word sense
disambiguation (WSD). For evaluation, we manually annotate metaphor in two
subsets of WordNet. Our model significantly outperforms a strong baseline based
on a state-of-the-art metaphor detection model, attaining an ROC-AUC score of
.78 (compared to .65) on one of the sets. Additionally, when paired with a WSD
model, our approach outperforms a state-of-the-art metaphor detection model at
identifying conventional metaphors in text (.659 F1 compared to .626).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lessons learned from the evaluation of Spanish Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Agerri, Eneko Agirre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the impact of language models on the field of Natural Language
Processing, a number of Spanish encoder-only masked language models (aka BERTs)
have been trained and released. These models were developed either within large
projects using very large private corpora or by means of smaller scale academic
efforts leveraging freely available data. In this paper we present a
comprehensive head-to-head comparison of language models for Spanish with the
following results: (i) Previously ignored multilingual models from large
companies fare better than monolingual models, substantially changing the
evaluation landscape of language models in Spanish; (ii) Results across the
monolingual models are not conclusive, with supposedly smaller and inferior
models performing competitively. Based on these empirical results, we argue for
the need of more research to understand the factors underlying them. In this
sense, the effect of corpus size, quality and pre-training techniques need to
be further investigated to be able to obtain Spanish monolingual models
significantly better than the multilingual ones released by large private
companies, specially in the face of rapid ongoing progress in the field. The
recent activity in the development of language technology for Spanish is to be
welcomed, but our results show that building language models remains an open,
resource-heavy problem which requires to marry resources (monetary and/or
computational) with the best research expertise and practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, three tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homonymy Information for English WordNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Hall Maudslay, Simone Teufel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A widely acknowledged shortcoming of WordNet is that it lacks a distinction
between word meanings which are systematically related (polysemy), and those
which are coincidental (homonymy). Several previous works have attempted to
fill this gap, by inferring this information using computational methods. We
revisit this task, and exploit recent advances in language modelling to
synthesise homonymy annotation for Princeton WordNet. Previous approaches treat
the problem using clustering methods; by contrast, our method works by linking
WordNet to the Oxford English Dictionary, which contains the information we
need. To perform this alignment, we pair definitions based on their proximity
in an embedding space produced by a Transformer model. Despite the simplicity
of this approach, our best model attains an F1 of .97 on an evaluation set that
we annotate. The outcome of our work is a high-quality homonymy annotation
layer for Princeton WordNet, which we release.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilong Dong, Xinwei Wu, Junzhuo Li, Shuangzhi Wu, Chao Bian, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massively multi-task learning with large language models has recently made
substantial progress on few-shot generalization. However, this is usually
performed in a centralized learning fashion, ignoring the privacy sensitivity
issue of (annotated) data used in multiple tasks. To mitigate this issue, we
propose FewFedWeight, a few-shot federated learning framework across multiple
tasks, to achieve the best of both worlds: privacy preservation and cross-task
generalization. FewFedWeight trains client models in isolated devices without
sharing data. It broadcasts the global model in the server to each client and
produces pseudo data for clients so that knowledge from the global model can be
explored to enhance few-shot learning of each client model. An energy-based
algorithm is further proposed to weight pseudo samples in order to reduce the
negative impact of noise from the generated pseudo data. Adaptive model weights
of client models are also tuned according to their performance. We use these
model weights to dynamically aggregate client models to update the global
model. Experiments on 118 NLP tasks show that FewFedWeight can significantly
improve the performance of client models on 61% tasks with an average
performance improvement rate of 30.5% over the baseline and substantially
outperform FedAvg and other decentralized learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to disagree well: Investigating the dispute tactics used on
  Wikipedia <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine de Kock, Tom Stafford, Andreas Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disagreements are frequently studied from the perspective of either detecting
toxicity or analysing argument structure. We propose a framework of dispute
tactics that unifies these two perspectives, as well as other dialogue acts
which play a role in resolving disputes, such as asking questions and providing
clarification. This framework includes a preferential ordering among
rebuttal-type tactics, ranging from ad hominem attacks to refuting the central
argument. Using this framework, we annotate 213 disagreements (3,865
utterances) from Wikipedia Talk pages. This allows us to investigate research
questions around the tactics used in disagreements; for instance, we provide
empirical validation of the approach to disagreement recommended by Wikipedia.
We develop models for multilabel prediction of dispute tactics in an utterance,
achieving the best performance with a transformer-based label powerset model.
Adding an auxiliary task to incorporate the ordering of rebuttal tactics
further yields a statistically significant increase. Finally, we show that
these annotations can be used to provide useful additional signals to improve
performance on the task of predicting escalation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2022 (Long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Law to Binary Tree -- An Formal Interpretation of Legal Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha-Thanh Nguyen, Vu Tran, Ngoc-Cam Le, Thi-Thuy Le, Quang-Huy Nguyen, Le-Minh Nguyen, Ken Satoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge representation and reasoning in law are essential to facilitate the
automation of legal analysis and decision-making tasks. In this paper, we
propose a new approach based on legal science, specifically legal taxonomy, for
representing and reasoning with legal documents. Our approach interprets the
regulations in legal documents as binary trees, which facilitates legal
reasoning systems to make decisions and resolve logical contradictions. The
advantages of this approach are twofold. First, legal reasoning can be
performed on the basis of the binary tree representation of the regulations.
Second, the binary tree representation of the regulations is more
understandable than the existing sentence-based representations. We provide an
example of how our approach can be used to interpret the regulations in a legal
document.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LN2FR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-to-speech synthesis based on latent variable conversion using
  diffusion probabilistic model and variational autoencoder <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Yasuda, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of
the factors that have been driving TTS are the advancements of probabilistic
models and latent representation learning. We propose a TTS method based on
latent variable conversion using a diffusion probabilistic model and the
variational autoencoder (VAE). In our TTS method, we use a waveform model based
on VAE, a diffusion model that predicts the distribution of latent variables in
the waveform model from texts, and an alignment model that learns alignments
between the text and speech latent sequences. Our method integrates diffusion
with VAE by modeling both mean and variance parameters with diffusion, where
the target distribution is determined by approximation from VAE. This latent
variable conversion framework potentially enables us to flexibly incorporate
various latent feature extractors. Our experiments show that our method is
robust to linguistic labels with poor orthography and alignment errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolution-enhanced Evolving Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, Xiangtai Li, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based neural networks, such as Transformers, have become ubiquitous
in numerous applications, including computer vision, natural language
processing, and time-series analysis. In all kinds of attention networks, the
attention maps are crucial as they encode semantic dependencies between input
tokens. However, most existing attention networks perform modeling or reasoning
based on representations, wherein the attention maps of different layers are
learned separately without explicit interactions. In this paper, we propose a
novel and generic evolving attention mechanism, which directly models the
evolution of inter-token relationships through a chain of residual
convolutional modules. The major motivations are twofold. On the one hand, the
attention maps in different layers share transferable knowledge, thus adding a
residual connection can facilitate the information flow of inter-token
relationships across layers. On the other hand, there is naturally an
evolutionary trend among attention maps at different abstraction levels, so it
is beneficial to exploit a dedicated convolution-based module to capture this
process. Equipped with the proposed mechanism, the convolution-enhanced
evolving attention networks achieve superior performance in various
applications, including time-series representation, natural language
understanding, machine translation, and image classification. Especially on
time-series representation tasks, Evolving Attention-enhanced Dilated
Convolutional (EA-DC-) Transformer outperforms state-of-the-art models
significantly, achieving an average of 17% improvement compared to the best
SOTA. To the best of our knowledge, this is the first work that explicitly
models the layer-wise evolution of attention maps. Our implementation is
available at https://github.com/pkuyym/EvolvingAttention
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of the previous work (arXiv:2102.12895). arXiv admin note:
  text overlap with arXiv:2102.12895</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent
  Neural Networks <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Xiong, Xiao Ding, Zhongyang Li, Li Du, Bing Qin, Yi Zheng, Baoxing Huai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal chain reasoning (CCR) is an essential ability for many decision-making
AI systems, which requires the model to build reliable causal chains by
connecting causal pairs. However, CCR suffers from two main transitive
problems: threshold effect and scene drift. In other words, the causal pairs to
be spliced may have a conflicting threshold boundary or scenario. To address
these issues, we propose a novel Reliable Causal chain reasoning
framework~(ReCo), which introduces exogenous variables to represent the
threshold and scene factors of each causal pair within the causal chain, and
estimates the threshold and scene contradictions across exogenous variables via
structural causal recurrent neural networks~(SRNN). Experiments show that ReCo
outperforms a series of strong baselines on both Chinese and English CCR
datasets. Moreover, by injecting reliable causal chain knowledge distilled by
ReCo, BERT can achieve better performances on four downstream causal-related
tasks than BERT models enhanced by other kinds of knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigation of Japanese PnG <span class="highlight-title">BERT</span> language model in text-to-speech
  synthesis for pitch accent language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Yasuda, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end text-to-speech synthesis (TTS) can generate highly natural
synthetic speech from raw text. However, rendering the correct pitch accents is
still a challenging problem for end-to-end TTS. To tackle the challenge of
rendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a
self-supervised pretrained model in the character and phoneme domain for TTS.
We investigate the effects of features captured by PnG~BERT on Japanese TTS by
modifying the fine-tuning condition to determine the conditions helpful
inferring pitch accents. We manipulate content of PnG~BERT features from being
text-oriented to speech-oriented by changing the number of fine-tuned layers
during TTS. In addition, we teach PnG~BERT pitch accent information by
fine-tuning with tone prediction as an additional downstream task. Our
experimental results show that the features of PnG~BERT captured by pretraining
contain information helpful inferring pitch accent, and PnG~BERT outperforms
baseline Tacotron on accent correctness in a listening test.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Text Generation via Probability Density Estimation in the
  Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on controllable text generation has explored the idea of
control from the latent space, such as optimizing a representation with
attribute-related classifiers or sampling a representation from relevant
discrete samples. However, they are not effective enough in modeling both the
latent space and the control, leaving controlled text with low quality and
diversity. In this work, we propose a novel control framework using probability
density estimation in the latent space. Our method utilizes an invertible
transformation function, the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaussian distributions in the prior
space. Thus, we can perform sophisticated and flexible control in the prior
space and feed the control effects back into the latent space owing to the
one-one-mapping property of invertible transformations. Experiments on
single-attribute controls and multi-attribute control reveal that our method
outperforms several strong baselines on attribute relevance and text quality
and achieves the SOTA. Further analysis of control strength adjustment
demonstrates the flexibility of our control strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rich Event Modeling for Script Event Prediction <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Bai, Saiping Guan, Zixuan Li, Jiafeng Guo, Xiaolong Jin, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Script is a kind of structured knowledge extracted from texts, which contains
a sequence of events. Based on such knowledge, script event prediction aims to
predict the subsequent event. To do so, two aspects should be considered for
events, namely, event description (i.e., what the events should contain) and
event encoding (i.e., how they should be encoded). Most existing methods
describe an event by a verb together with only a few core arguments (i.e.,
subject, object, and indirect object), which are not precise. In addition,
existing event encoders are limited to a fixed number of arguments, which are
not flexible to deal with extra information. Thus, in this paper, we propose
the Rich Event Prediction (REP) framework for script event prediction.
Fundamentally, it is based on the proposed rich event description, which
enriches the existing ones with three kinds of important information, namely,
the senses of verbs, extra semantic roles, and types of participants. REP
contains an event extractor to extract such information from texts. Based on
the extracted rich information, a predictor then selects the most probable
subsequent event. The core component of the predictor is a transformer-based
event encoder to flexibly deal with an arbitrary number of arguments.
Experimental results on the widely used Gigaword Corpus show the effectiveness
of the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALERT: Adapting Language Models to Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, Asli Celikyilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current large language models can perform reasonably well on complex tasks
that require step-by-step reasoning with few-shot learning. Are these models
applying reasoning skills they have learnt during pre-training and reason
outside of their training context, or are they simply memorizing their training
corpus at finer granularity and have learnt to better understand their context?
To tease apart these possibilities, we introduce ALERT, a benchmark and suite
of analyses for assessing language models' reasoning ability comparing
pre-trained and finetuned models on complex tasks that require reasoning skills
to solve. ALERT provides a test bed to asses any language model on fine-grained
reasoning skills, which spans over 20 datasets and covers 10 different
reasoning skills. We leverage ALERT to further investigate the role of
finetuning. With extensive empirical analysis we find that language models
learn more reasoning skills such as textual entailment, abductive reasoning,
and analogical reasoning during finetuning stage compared to pretraining state.
We also find that when language models are finetuned they tend to overfit to
the prompt template, which hurts the robustness of models causing
generalization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneGATE: Scene-Graph based co-Attention networks for TExt visual
  question answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwen Luo, Feiqi Cao, Felipe Nunez, Zean Wen, Josiah Poon, Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most TextVQA approaches focus on the integration of objects, scene texts and
question words by a simple transformer encoder. But this fails to capture the
semantic relations between different modalities. The paper proposes a Scene
Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the
semantic relations among the objects, Optical Character Recognition (OCR)
tokens and the question words. It is achieved by a TextVQA-based scene graph
that discovers the underlying semantics of an image. We created a
guided-attention module to capture the intra-modal interplay between the
language and the vision as a guidance for inter-modal interactions. To make
explicit teaching of the relations between the two modalities, we proposed and
integrated two attention modules, namely a scene graph-based semantic
relation-aware attention and a positional relation-aware attention. We
conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.
It is shown that our SceneGATE method outperformed existing ones because of the
scene graph and its attention modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Werewolf Among Us: A Multimodal <span class="highlight-title">Dataset</span> for Modeling Persuasion
  Behaviors in Social Deduction Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persuasion modeling is a key building block for conversational agents.
Existing works in this direction are limited to analyzing textual dialogue
corpus. We argue that visual signals also play an important role in
understanding human persuasive behaviors. In this paper, we introduce the first
multimodal dataset for modeling persuasion behaviors. Our dataset includes 199
dialogue transcriptions and videos captured in a multi-player social deduction
game setting, 26,647 utterance level annotations of persuasion strategy, and
game level annotations of deduction game outcomes. We provide extensive
experiments to show how dialogue context and visual signals benefit persuasion
strategy prediction. We also explore the generalization ability of language
models for persuasion modeling and the role of persuasion strategies in
predicting social deduction game outcomes. Our dataset, code, and models can be
found at https://persuasion-deductiongame.socialai-data.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Azimuth: Systematic Error Analysis for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Gauthier-Melançon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler, Frédéric Branchaud-Charron, Joseph Marinier, Karine Grande, Di Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Azimuth, an open-source and easy-to-use tool to perform error
analysis for text classification. Compared to other stages of the ML
development cycle, such as model training and hyper-parameter tuning, the
process and tooling for the error analysis stage are less mature. However, this
stage is critical for the development of reliable and trustworthy AI systems.
To make error analysis more systematic, we propose an approach comprising
dataset analysis and model quality assessment, which Azimuth facilitates. We
aim to help AI practitioners discover and address areas where the model does
not generalize by leveraging and integrating a range of ML techniques, such as
saliency maps, similarity, uncertainty, and behavioral analyses, all in one
tool. Our code and documentation are available at
github.com/servicenow/azimuth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing: System Demonstrations. 13 pages and
  14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meeting Summarization: A <span class="highlight-title">Survey</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Prasanna Kumar, Arman Kabiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information overloading requires the need for summarizers to extract salient
information from the text. Currently, there is an overload of dialogue data due
to the rise of virtual communication platforms. The rise of Covid-19 has led
people to rely on online communication platforms like Zoom, Slack, Microsoft
Teams, Discord, etc. to conduct their company meetings. Instead of going
through the entire meeting transcripts, people can use meeting summarizers to
select useful data. Nevertheless, there is a lack of comprehensive surveys in
the field of meeting summarizers. In this survey, we aim to cover recent
meeting summarization techniques. Our survey offers a general overview of text
summarization along with datasets and evaluation metrics for meeting
summarization. We also provide the performance of each summarizer on a
leaderboard. We conclude our survey with different challenges in this domain
and potential research opportunities for future researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A unified information-theoretic model of EEG signatures of human
  language processing <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Li, Richard Futrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We advance an information-theoretic model of human language processing in the
brain, in which incoming linguistic input is processed at two levels, in terms
of a heuristic interpretation and in terms of error correction. We propose that
these two kinds of information processing have distinct electroencephalographic
signatures, corresponding to the well-documented N400 and P600 components of
language-related event-related potentials (ERPs). Formally, we show that the
information content (surprisal) of a word in context can be decomposed into two
quantities: (A) heuristic surprise, which signals processing difficulty of word
given its inferred context, and corresponds with the N400 signal; and (B)
discrepancy signal, which reflects divergence between the true context and the
inferred context, and corresponds to the P600 signal. Both of these quantities
can be estimated using modern NLP techniques. We validate our theory by
successfully simulating ERP patterns elicited by a variety of linguistic
manipulations in previously-reported experimental data from Ryskin et al.
(2021). Our theory is in principle compatible with traditional cognitive
theories assuming a `good-enough' heuristic interpretation stage, but with
precise information-theoretic formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, accepted InfoCog workshop at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text
  Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Yuchen Zhang, Zhe Chen, Josie Li, Melanie Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Natural Language Processing (NLP) to specialized domains,
such as the law, has recently received a surge of interest. As many legal
services rely on processing and analyzing large collections of documents,
automating such tasks with NLP tools emerges as a key challenge. Many popular
language models, such as BERT or RoBERTa, are general-purpose models, which
have limitations on processing specialized legal terminology and syntax. In
addition, legal documents may contain specialized vocabulary from other
domains, such as medical terminology in personal injury text. Here, we propose
LegalRelectra, a legal-domain language model that is trained on mixed-domain
legal and medical corpora. We show that our model improves over general-domain
and single-domain medical and legal language models when processing
mixed-domain (personal injury) text. Our training architecture implements the
Electra framework, but utilizes Reformer instead of BERT for its generator and
discriminator. We show that this improves the model's performance on processing
long passages and results in better long-range text comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Mechanism of Evolution Shared by Proteins and Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.14309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.14309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Min Wang, Hsing-Yi Lai, Sun-Ting Tsai, Chen Siang Ng, Shan-Jyun Wu, Meng-Xue Tsai, Yi-Ching Su, Daw-Wei Wang, Tzay-Ming Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex systems, such as life and languages, are governed by principles of
evolution. The analogy and comparison between biology and
linguistics\cite{alphafold2, RoseTTAFold, lang_virus, cell language, faculty1,
language of gene, Protein linguistics, dictionary, Grammar of pro_dom,
complexity, genomics_nlp, InterPro, language modeling, Protein language
modeling} provide a computational foundation for characterizing and analyzing
protein sequences, human corpora, and their evolution. However, no general
mathematical formula has been proposed so far to illuminate the origin of
quantitative hallmarks shared by life and language. Here we show several new
statistical relationships shared by proteins and words, which inspire us to
establish a general mechanism of evolution with explicit formulations that can
incorporate both old and new characteristics. We found natural selection can be
quantified via the entropic formulation by the principle of least effort to
determine the sequence variation that survives in evolution. Besides, the
origin of power law behavior and how changes in the environment stimulate the
emergence of new proteins and words can also be explained via the introduction
of function connection network. Our results demonstrate not only the
correspondence between genetics and linguistics over their different
hierarchies but also new fundamental physical properties for the evolution of
complex adaptive systems. We anticipate our statistical tests can function as
quantitative criteria to examine whether an evolution theory of sequence is
consistent with the regularity of real data. In the meantime, their
correspondence broadens the bridge to exchange existing knowledge, spurs new
interpretations, and opens Pandora's box to release several potentially
revolutionary challenges. For example, does linguistic arbitrariness conflict
with the dogma that structure determines function?
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Linguistically Informed Multi-Objective <span class="highlight-title">Pre-Train</span>ing for Natural
  Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Pielka, Svetlana Schmidt, Lisa Pucknat, Rafet Sifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Philine Bouscasse, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AUC Maximization for Low-Resource Named Entity Recognition <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngoc Dang Nguyen, Wei Tan, Wray Buntine, Richard Beare, Changyou Chen, Lan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, AAAI 2023 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attentive Mask CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Weiquan Huang, Yixuan Wei, Houwen Peng, Xinyang Jiang, Huiqiang Jiang, Fangyun Wei, Yin Wang, Han Hu, Lili Qiu, Yuqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image token removal is an efficient augmentation strategy for reducing the
cost of computing image features. However, this efficient augmentation strategy
has been found to adversely affect the accuracy of CLIP-based training. We
hypothesize that removing a large portion of image tokens may improperly
discard the semantic content associated with a given text description, thus
constituting an incorrect pairing target in CLIP training. To address this
issue, we propose an attentive token removal approach for CLIP training, which
retains tokens with a high semantic correlation to the text description. The
correlation scores are computed in an online fashion using the EMA version of
the visual encoder. Our experiments show that the proposed attentive masking
approach performs better than the previous method of random token removal for
CLIP training. The approach also makes it efficient to apply multiple
augmentation views to the image, as well as introducing instance contrastive
learning tasks between these views into the CLIP framework. Compared to other
CLIP improvements that combine different pre-training targets such as SLIP and
MaskCLIP, our method is not only more effective, but also much more efficient.
Specifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\%$
top-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$
and $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are
$+1.1\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being
$2.30\times$ faster. An efficient version of our approach running $1.16\times$
faster than the plain CLIP model achieves significant gains of $+5.3\%$,
$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Human Visual Contrast Sensitivity and Machine Vision Robustness: A
  Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Chang Chiu, Yingfei Wang, Derrick Eui Gyu Kim, Pin-Yu Chen, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well established in neuroscience that color vision plays an essential
part in the human visual perception system. Meanwhile, many novel designs for
computer vision inspired by human vision have achieved success in a wide range
of tasks and applications. Nonetheless, how color differences affect machine
vision has not been well explored. Our work tries to bridge this gap between
the human color vision aspect of visual recognition and that of the machine. To
achieve this, we curate two datasets: CIFAR10-F and CIFAR100-F, which are based
on the foreground colors of the popular CIFAR datasets. Together with CIFAR10-B
and CIFAR100-B, the existing counterpart datasets with information on the
background colors of CIFAR test sets, we assign each image based on its color
contrast level per its foreground and background color labels and use this as a
proxy to study how color contrast affects machine vision. We first conduct a
proof-of-concept study, showing the effect of color difference and validate our
datasets. Furthermore, on a broader level, an important characteristic of human
vision is its robustness against ambient changes; therefore, drawing
inspirations from ophthalmology and the robustness literature, we analogize
contrast sensitivity from the human visual aspect to machine vision and
complement the current robustness study using corrupted images with our
CIFAR-CoCo datasets. In summary, motivated by neuroscience and equipped with
the datasets we curate, we devise a new framework in two dimensions to perform
extensive analyses on the effect of color contrast and corrupted images: (1)
model architecture, (2) model size, to measure the perception ability of
machine vision beyond total accuracy. We also explore how task complexity and
data augmentation play a role in this setup. Our results call attention to new
evaluation approaches for human-like machine perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better May Not Be Fairer: Can Data Augmentation Mitigate Subgroup
  Degradation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Chang Chiu, Pin-Yu Chen, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is no secret that deep learning models exhibit undesirable behaviors such
as learning spurious correlations instead of learning correct relationships
between input/output pairs. Prior works on robustness study datasets that mix
low-level features to quantify how spurious correlations affect predictions
instead of considering natural semantic factors due to limitations in accessing
realistic datasets for comprehensive evaluation. To bridge this gap, in this
paper we first investigate how natural background colors play a role as
spurious features in image classification tasks by manually splitting the test
sets of CIFAR10 and CIFAR100 into subgroups based on the background color of
each image. We name our datasets CIFAR10-B and CIFAR100-B. We find that while
standard CNNs achieve human-level accuracy, the subgroup performances are not
consistent, and the phenomenon remains even after data augmentation (DA). To
alleviate this issue, we propose FlowAug, a semantic DA method that leverages
the decoupled semantic representations captured by a pre-trained generative
flow. Experimental results show that FlowAug achieves more consistent results
across subgroups than other types of DA methods on CIFAR10 and CIFAR100.
Additionally, it shows better generalization performance. Furthermore, we
propose a generic metric for studying model robustness to spurious
correlations, where we take a macro average on the weighted standard deviations
across different classes. Per our metric, FlowAug demonstrates less reliance on
spurious correlations. Although this metric is proposed to study our curated
datasets, it applies to all datasets that have subgroups or subclasses. Lastly,
aside from less dependence on spurious correlations and better generalization
on in-distribution test sets, we also show superior out-of-distribution results
on CIFAR10.1 and competitive performances on CIFAR10-C and CIFAR100-C.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GFPose: Learning 3D Human Pose Prior with Gradient Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao Dong, Fangwei Zhong, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning 3D human pose prior is essential to human-centered AI. Here, we
present GFPose, a versatile framework to model plausible 3D human poses for
various applications. At the core of GFPose is a time-dependent score network,
which estimates the gradient on each body joint and progressively denoises the
perturbed 3D human pose to match a given task specification. During the
denoising process, GFPose implicitly incorporates pose priors in gradients and
unifies various discriminative and generative tasks in an elegant framework.
Despite the simplicity, GFPose demonstrates great potential in several
downstream tasks. Our experiments empirically show that 1) as a
multi-hypothesis pose estimator, GFPose outperforms existing SOTAs by 20% on
Human3.6M dataset. 2) as a single-hypothesis pose estimator, GFPose achieves
comparable results to deterministic SOTAs, even with a vanilla backbone. 3)
GFPose is able to produce diverse and realistic samples in pose denoising,
completion and generation tasks. Project page
https://sites.google.com/view/gfpose/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An annotated instance segmentation XXL-CT <span class="highlight-title">dataset</span> from a historic
  airplane 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roland Gruber, Nils Reims, Andreas Hempfer, Stefan Gerth, Michael Salamon, Thomas Wittenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Me 163 was a Second World War fighter airplane and a result of the German
air force secret developments. One of these airplanes is currently owned and
displayed in the historic aircraft exhibition of the Deutsches Museum in
Munich, Germany. To gain insights with respect to its history, design and state
of preservation, a complete CT scan was obtained using an industrial
XXL-computer tomography scanner.
  Using the CT data from the Me 163, all its details can visually be examined
at various levels, ranging from the complete hull down to single sprockets and
rivets. However, while a trained human observer can identify and interpret the
volumetric data with all its parts and connections, a virtual dissection of the
airplane and all its different parts would be quite desirable. Nevertheless,
this means, that an instance segmentation of all components and objects of
interest into disjoint entities from the CT data is necessary.
  As of currently, no adequate computer-assisted tools for automated or
semi-automated segmentation of such XXL-airplane data are available, in a first
step, an interactive data annotation and object labeling process has been
established. So far, seven 512 x 512 x 512 voxel sub-volumes from the Me 163
airplane have been annotated and labeled, whose results can potentially be used
for various new applications in the field of digital heritage, non-destructive
testing, or machine-learning.
  This work describes the data acquisition process of the airplane using an
industrial XXL-CT scanner, outlines the interactive segmentation and labeling
scheme to annotate sub-volumes of the airplane's CT data, describes and
discusses various challenges with respect to interpreting and handling the
annotated and labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multi-modal and Multi-hop Question Answering via Structured
  Knowledge and Unified Retrieval-Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Yang, Qian Chen, Wen Wang, Baotian Hu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal and multi-hop question answering aims to answer a question based
on multiple input sources from different modalities. Previous methods retrieve
the evidence separately and feed the retrieved evidence to a language model to
generate the corresponding answer. However, these methods fail to build
connections between candidates and thus cannot model the inter-dependent
relation during retrieval. Moreover, the reasoning process over multi-modality
candidates can be unbalanced without building alignments between different
modalities. To address this limitation, we propose a Structured Knowledge and
Unified Retrieval Generation based method (SKURG). We align the sources from
different modalities via the shared entities and map them into a shared
semantic space via structured knowledge. Then, we utilize a unified
retrieval-generation decoder to integrate intermediate retrieval results for
answer generation and adaptively determine the number of retrieval steps. We
perform experiments on two multi-modal and multi-hop datasets: WebQA and
MultimodalQA. The results demonstrate that SKURG achieves state-of-the-art
performance on both retrieval and answer generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of A Real-time POCUS Image Quality Assessment and
  Acquisition Guidance System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenge Jia, Yiyu Shi, Jingtong Hu, Lei Yang, Benjamin Nti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-of-care ultrasound (POCUS) is one of the most commonly applied tools
for cardiac function imaging in the clinical routine of the emergency
department and pediatric intensive care unit. The prior studies demonstrate
that AI-assisted software can guide nurses or novices without prior sonography
experience to acquire POCUS by recognizing the interest region, assessing image
quality, and providing instructions. However, these AI algorithms cannot simply
replace the role of skilled sonographers in acquiring diagnostic-quality POCUS.
Unlike chest X-ray, CT, and MRI, which have standardized imaging protocols,
POCUS can be acquired with high inter-observer variability. Though being with
variability, they are usually all clinically acceptable and interpretable. In
challenging clinical environments, sonographers employ novel heuristics to
acquire POCUS in complex scenarios. To help novice learners to expedite the
training process while reducing the dependency on experienced sonographers in
the curriculum implementation, We will develop a framework to perform real-time
AI-assisted quality assessment and probe position guidance to provide training
process for novice learners with less manual intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atrous Space Bender U-Net (ASBU-Net/LogiNet) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Bansal, Oleg Ostap, Miguel Maestre Trueba, Kristopher Perry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $ $With recent advances in CNNs, exceptional improvements have been made in
semantic segmentation of high resolution images in terms of accuracy and
latency. However, challenges still remain in detecting objects in crowded
scenes, large scale variations, partial occlusion, and distortions, while still
maintaining mobility and latency. We introduce a fast and efficient
convolutional neural network, ASBU-Net, for semantic segmentation of high
resolution images that addresses these problems and uses no novelty layers for
ease of quantization and embedded hardware support. ASBU-Net is based on a new
feature extraction module, atrous space bender layer (ASBL), which is efficient
in terms of computation and memory. The ASB layers form a building block that
is used to make ASBNet. Since this network does not use any special layers it
can be easily implemented, quantized and deployed on FPGAs and other hardware
with limited memory. We present experiments on resource and accuracy trade-offs
and show strong performance compared to other popular models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Huruf: An Application for Arabic Handwritten Character Recognition Using
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhaz Kamal, Fairuz Shaiara, Chowdhury Mohammad Abdullah, Sabbir Ahmed, Tasnim Ahmed, Md. Hasanul Kabir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwriting Recognition has been a field of great interest in the Artificial
Intelligence domain. Due to its broad use cases in real life, research has been
conducted widely on it. Prominent work has been done in this field focusing
mainly on Latin characters. However, the domain of Arabic handwritten character
recognition is still relatively unexplored. The inherent cursive nature of the
Arabic characters and variations in writing styles across individuals makes the
task even more challenging. We identified some probable reasons behind this and
proposed a lightweight Convolutional Neural Network-based architecture for
recognizing Arabic characters and digits. The proposed pipeline consists of a
total of 18 layers containing four layers each for convolution, pooling, batch
normalization, dropout, and finally one Global average pooling and a Dense
layer. Furthermore, we thoroughly investigated the different choices of
hyperparameters such as the choice of the optimizer, kernel initializer,
activation function, etc. Evaluating the proposed architecture on the publicly
available 'Arabic Handwritten Character Dataset (AHCD)' and 'Modified Arabic
handwritten digits Database (MadBase)' datasets, the proposed model
respectively achieved an accuracy of 96.93% and 99.35% which is comparable to
the state-of-the-art and makes it a suitable solution for real-life end-level
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 25th ICCIT (6 pages, 4 tables, 4 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-risking Carbon Capture and Sequestration with Explainable CO2 Leakage
  Detection in Time-lapse Seismic Monitoring Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huseyin Tuna Erdinc, Abhinav Prakash Gahlot, Ziyi Yin, Mathias Louboutin, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing global deployment of carbon capture and sequestration
technology to combat climate change, monitoring and detection of potential CO2
leakage through existing or storage induced faults are critical to the safe and
long-term viability of the technology. Recent work on time-lapse seismic
monitoring of CO2 storage has shown promising results in its ability to monitor
the growth of the CO2 plume from surface recorded seismic data. However, due to
the low sensitivity of seismic imaging to CO2 concentration, additional
developments are required to efficiently interpret the seismic images for
leakage. In this work, we introduce a binary classification of time-lapse
seismic images to delineate CO2 plumes (leakage) using state-of-the-art deep
learning models. Additionally, we localize the leakage region of CO2 plumes by
leveraging Class Activation Mapping methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Cooking State Recognition with Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akib Mohammed Khan, Alif Ashrafee, Reeshoon Sayera, Shahriar Ivan, Sabbir Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure proper knowledge representation of the kitchen environment, it is
vital for kitchen robots to recognize the states of the food items that are
being cooked. Although the domain of object detection and recognition has been
extensively studied, the task of object state classification has remained
relatively unexplored. The high intra-class similarity of ingredients during
different states of cooking makes the task even more challenging. Researchers
have proposed adopting Deep Learning based strategies in recent times, however,
they are yet to achieve high performance. In this study, we utilized the
self-attention mechanism of the Vision Transformer (ViT) architecture for the
Cooking State Recognition task. The proposed approach encapsulates the globally
salient features from images, while also exploiting the weights learned from a
larger dataset. This global attention allows the model to withstand the
similarities between samples of different cooking objects, while the employment
of transfer learning helps to overcome the lack of inductive bias by utilizing
pretrained weights. To improve recognition accuracy, several augmentation
techniques have been employed as well. Evaluation of our proposed framework on
the `Cooking State Recognition Challenge Dataset' has achieved an accuracy of
94.3%, which significantly outperforms the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 25th ICCIT (6 pages, 5 Figures, 5 Tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Siamese Network for Robust Change Detection Across Different
  Domains with Applications to 3D Printing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushuo Niu, Ethan Chadwick, Anson W. K. Ma, Qian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic defect detection for 3D printing processes, which shares many
characteristics with change detection problems, is a vital step for quality
control of 3D printed products. However, there are some critical challenges in
the current state of practice. First, existing methods for computer
vision-based process monitoring typically work well only under specific camera
viewpoints and lighting situations, requiring expensive pre-processing,
alignment, and camera setups. Second, many defect detection techniques are
specific to pre-defined defect patterns and/or print schematics. In this work,
we approach the automatic defect detection problem differently using a novel
Semi-Siamese deep learning model that directly compares a reference schematic
of the desired print and a camera image of the achieved print. The model then
solves an image segmentation problem, identifying the locations of defects with
respect to the reference frame. Unlike most change detection problems, our
model is specially developed to handle images coming from different domains and
is robust against perturbations in the imaging setup such as camera angle and
illumination. Defect localization predictions were made in 2.75 seconds per
layer using a standard MacBookPro, which is comparable to the typical tens of
seconds or less for printing a single layer on an inkjet-based 3D printer,
while achieving an F1-score of more than 0.9.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biomedical image analysis competitions: The state of current
  participation practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Carlos Ortiz de Solórzano, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Kimberlin van Wijnen, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng, Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Marcela de Oliveira, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Francesco La Rosa, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of international benchmarking competitions is steadily increasing
in various fields of machine learning (ML) research and practice. So far,
however, little is known about the common practice as well as bottlenecks faced
by the community in tackling the research questions posed. To shed light on the
status quo of algorithm development in the specific field of biomedical imaging
analysis, we designed an international survey that was issued to all
participants of challenges conducted in conjunction with the IEEE ISBI 2021 and
MICCAI 2021 conferences (80 competitions in total). The survey covered
participants' expertise and working environments, their chosen strategies, as
well as algorithm characteristics. A median of 72% challenge participants took
part in the survey. According to our results, knowledge exchange was the
primary incentive (70%) for participation, while the reception of prize money
played only a minor role (16%). While a median of 80 working hours was spent on
method development, a large portion of participants stated that they did not
have enough time for method development (32%). 25% perceived the infrastructure
to be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of
these, 84% were based on standard architectures. 43% of the respondents
reported that the data samples (e.g., images) were too large to be processed at
once. This was most commonly addressed by patch-based training (69%),
downsampling (37%), and solving 3D analysis tasks as a series of 2D tasks.
K-fold cross-validation on the training set was performed by only 37% of the
participants and only 50% of the participants performed ensembling based on
multiple identical models (61%) or heterogeneous models (39%). 48% of the
respondents applied postprocessing steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Road Spray Effects in Automotive Lidar Sensor Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemens Linnhoff, Dominik Scheuble, Mario Bijelic, Lukas Elster, Philipp Rosenberger, Werner Ritter, Dengxin Dai, Hermann Winner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling perception sensors is key for simulation based testing of automated
driving functions. Beyond weather conditions themselves, sensors are also
subjected to object dependent environmental influences like tire spray caused
by vehicles moving on wet pavement. In this work, a novel modeling approach for
spray in lidar data is introduced. The model conforms to the Open Simulation
Interface (OSI) standard and is based on the formation of detection clusters
within a spray plume. The detections are rendered with a simple custom ray
casting algorithm without the need of a fluid dynamics simulation or physics
engine. The model is subsequently used to generate training data for object
detection algorithms. It is shown that the model helps to improve detection in
real-world spray scenarios significantly. Furthermore, a systematic real-world
data set is recorded and published for analysis, model calibration and
validation of spray effects in active perception sensors. Experiments are
conducted on a test track by driving over artificially watered pavement with
varying vehicle speeds, vehicle types and levels of pavement wetness. All
models and data of this work are available open source.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Sensors Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection-aware multi-object tracking evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. SanMiguel, Jorge Muñoz, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How would you fairly evaluate two multi-object tracking algorithms (i.e.
trackers), each one employing a different object detector? Detectors keep
improving, thus trackers can make less effort to estimate object states over
time. Is it then fair to compare a new tracker employing a new detector with
another tracker using an old detector? In this paper, we propose a novel
performance measure, named Tracking Effort Measure (TEM), to evaluate trackers
that use different detectors. TEM estimates the improvement that the tracker
does with respect to its input data (i.e. detections) at frame level
(intra-frame complexity) and sequence level (inter-frame complexity). We
evaluate TEM over well-known datasets, four trackers and eight detection sets.
Results show that, unlike conventional tracking evaluation measures, TEM can
quantify the effort done by the tracker with a reduced correlation on the input
detections. Its implementation is publicly available online at
https://github.com/vpulab/MOT-evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at IEEE International Conference on Advanced
  Video and Signal Based Surveillance (AVSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Human Motion Synthesis and Style Transfer with Denoising
  Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chang, Edmund J. C. Findlay, Haozheng Zhang, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic motions for digital humans is a core but challenging
part of computer animations and games, as human motions are both diverse in
content and rich in styles. While the latest deep learning approaches have made
significant advancements in this domain, they mostly consider motion synthesis
and style manipulation as two separate problems. This is mainly due to the
challenge of learning both motion contents that account for the inter-class
behaviour and styles that account for the intra-class behaviour effectively in
a common representation. To tackle this challenge, we propose a denoising
diffusion probabilistic model solution for styled motion synthesis. As
diffusion models have a high capacity brought by the injection of
stochasticity, we can represent both inter-class motion content and intra-class
style behaviour in the same latent. This results in an integrated, end-to-end
trained pipeline that facilitates the generation of optimal motion and
exploration of content-style coupled latent space. To achieve high-quality
results, we design a multi-task architecture of diffusion model that
strategically generates aspects of human motions for local guidance. We also
design adversarial and physical regulations for global guidance. We demonstrate
superior performance with quantitative and qualitative results and validate the
effectiveness of our multi-task architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Road Detection in Snowy Forest Environment using RGB Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirawich Vachmanus, Takanori Emaru, Ankit A. Ravankar, Yukinori Kobayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated driving technology has gained a lot of momentum in the last few
years. For the exploration field, navigation is the important key for
autonomous operation. In difficult scenarios such as snowy environment, the
road is covered with snow and road detection is impossible in this situation
using only basic techniques. This paper introduces detection of snowy road in
forest environment using RGB camera. The method combines noise filtering
technique with morphological operation to classify the image component. By
using the assumption that all road is covered by snow and the snow part is
defined as road area. From the perspective image of road, the vanishing point
of road is one of factor to scope the region of road. This vanishing point is
found with fitting triangle technique. The performance of algorithm is
evaluated by two error value: False Negative Rate and False Positive Rate. The
error shows that the method has high efficiency for detect road with straight
road but low performance for curved road. This road region will be applied with
depth information from camera to detect for obstacle in the future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference proceeding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Supervised Video Anomaly Detection Based on Cross-Batch
  Clustering Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Congqi Cao, Xin Zhang, Shizhou Zhang, Peng Wang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised video anomaly detection (WSVAD) is a challenging task since
only video-level labels are available for training. In previous studies, the
discriminative power of the learned features is not strong enough, and the data
imbalance resulting from the mini-batch training strategy is ignored. To
address these two issues, we propose a novel WSVAD method based on cross-batch
clustering guidance. To enhance the discriminative power of features, we
propose a batch clustering based loss to encourage a clustering branch to
generate distinct normal and abnormal clusters based on a batch of data.
Meanwhile, we design a cross-batch learning strategy by introducing clustering
results from previous mini-batches to reduce the impact of data imbalance. In
addition, we propose to generate more accurate segment-level anomaly scores
based on batch clustering guidance further improving the performance of WSVAD.
Extensive experiments on two public datasets demonstrate the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEDCNet: A Lightweight and Efficient Semantic Segmentation Algorithm
  Using Dual Context Module for Extracting Ground Objects from UAV Aerial
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiang Han, Yiman Liu, Gang Liu, Qiaohong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of UAV aerial remote sensing images provides a more
efficient and convenient surveying and mapping method for traditional surveying
and mapping. In order to make the model lightweight and improve a certain
accuracy, this research developed a new lightweight and efficient network for
the extraction of ground features from UAV aerial remote sensing images, called
LDMCNet. Meanwhile, this research develops a powerful lightweight backbone
network for the proposed semantic segmentation model. It is called LDCNet, and
it is hoped that it can become the backbone network of a new generation of
lightweight semantic segmentation algorithms. The proposed model uses dual
multi-scale context modules, namely the Atrous Space Pyramid Pooling module
(ASPP) and the Object Context Representation module (OCR). In addition, this
research constructs a private dataset for semantic segmentation of aerial
remote sensing images from drones. This data set contains 2431 training sets,
945 validation sets, and 475 test sets. The proposed model performs well on
this dataset, with only 1.4M parameters and 5.48G floating-point operations
(FLOPs), achieving an average intersection-over-union ratio (mIoU) of 71.12%.
7.88% higher than the baseline model. In order to verify the effectiveness of
the proposed model, training on the public datasets "LoveDA" and "CITY-OSM"
also achieved excellent results, achieving mIoU of 65.27% and 74.39%,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Huang, Hongwei Li, Gastao Cruz, Jiazhen Pan, Daniel Rueckert, Kerstin Hammernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel image reconstruction framework that directly
learns a neural implicit representation in k-space for ECG-triggered
non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods
bin acquired data from neighboring time points to reconstruct one phase of the
cardiac motion, our framework allows for a continuous, binning-free, and
subject-specific k-space representation.We assign a unique coordinate that
consists of time, coil index, and frequency domain location to each sampled
k-space point. We then learn the subject-specific mapping from these unique
coordinates to k-space intensities using a multi-layer perceptron with
frequency domain regularization. During inference, we obtain a complete k-space
for Cartesian coordinates and an arbitrary temporal resolution. A simple
inverse Fourier transform recovers the image, eliminating the need for density
compensation and costly non-uniform Fourier transforms for non-Cartesian data.
This novel imaging framework was tested on 42 radially sampled datasets from 6
subjects. The proposed method outperforms other techniques qualitatively and
quantitatively using data from four and one heartbeat(s) and 30 cardiac phases.
Our results for one heartbeat reconstruction of 50 cardiac phases show improved
artifact removal and spatio-temporal resolution, leveraging the potential for
real-time CMR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Stage Cascade Refinement Networks for Infrared Small Target
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimian Dai, Xiang Li, Fei Zhou, Yulei Qian, Yaohong Chen, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-frame InfraRed Small Target (SIRST) detection has been a challenging
task due to a lack of inherent characteristics, imprecise bounding box
regression, a scarcity of real-world datasets, and sensitive localization
evaluation. In this paper, we propose a comprehensive solution to these
challenges. First, we find that the existing anchor-free label assignment
method is prone to mislabeling small targets as background, leading to their
omission by detectors. To overcome this issue, we propose an all-scale
pseudo-box-based label assignment scheme that relaxes the constraints on scale
and decouples the spatial assignment from the size of the ground-truth target.
Second, motivated by the structured prior of feature pyramids, we introduce the
one-stage cascade refinement network (OSCAR), which uses the high-level head as
soft proposals for the low-level refinement head. This allows OSCAR to process
the same target in a cascade coarse-to-fine manner. Finally, we present a new
research benchmark for infrared small target detection, consisting of the
SIRST-V2 dataset of real-world, high-resolution single-frame targets, the
normalized contrast evaluation metric, and the DeepInfrared toolkit for
detection. We conduct extensive ablation studies to evaluate the components of
OSCAR and compare its performance to state-of-the-art model-driven and
data-driven methods on the SIRST-V2 benchmark. Our results demonstrate that a
top-down cascade refinement framework can improve the accuracy of infrared
small target detection without sacrificing efficiency. The DeepInfrared
toolkit, dataset, and trained models are available at
https://github.com/YimianDai/open-deepinfrared to advance further research in
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TGRS, Major Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free-form 3D Scene Inpainting with Dual-stream GAN <span class="chip">BMVC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ru-Fen Jheng, Tsung-Han Wu, Jia-Fong Yeh, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the need for user editing in a 3D scene has rapidly increased due
to the development of AR and VR technology. However, the existing 3D scene
completion task (and datasets) cannot suit the need because the missing regions
in scenes are generated by the sensor limitation or object occlusion. Thus, we
present a novel task named free-form 3D scene inpainting. Unlike scenes in
previous 3D completion datasets preserving most of the main structures and
hints of detailed shapes around missing regions, the proposed inpainting
dataset, FF-Matterport, contains large and diverse missing regions formed by
our free-form 3D mask generation algorithm that can mimic human drawing
trajectories in 3D space. Moreover, prior 3D completion methods cannot perform
well on this challenging yet practical task, simply interpolating nearby
geometry and color context. Thus, a tailored dual-stream GAN method is
proposed. First, our dual-stream generator, fusing both geometry and color
information, produces distinct semantic boundaries and solves the interpolation
issue. To further enhance the details, our lightweight dual-stream
discriminator regularizes the geometry and color edges of the predicted scenes
to be realistic and sharp. We conducted experiments with the proposed
FF-Matterport dataset. Qualitative and quantitative results validate the
superiority of our approach over existing scene completion methods and the
efficacy of all proposed components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Xception to NEXcepTion: New Design Decisions and Neural
  Architecture Search <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadar Shavit, Filip Jatelnicki, Pol Mor-Puigventós, Wojtek Kowalczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a modified Xception architecture, the NEXcepTion
network. Our network has significantly better performance than the original
Xception, achieving top-1 accuracy of 81.5% on the ImageNet validation dataset
(an improvement of 2.5%) as well as a 28% higher throughput. Another variant of
our model, NEXcepTion-TP, reaches 81.8% top-1 accuracy, similar to ConvNeXt
(82.1%), while having a 27% higher throughput. Our model is the result of
applying improved training procedures and new design decisions combined with an
application of Neural Architecture Search (NAS) on a smaller dataset. These
findings call for revisiting older architectures and reassessing their
potential when combined with the latest enhancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPRAM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Label Learning: Improving Background Class Representations in
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Konstantinos Kamnitsas, Cheng Ouyang, Chen Chen, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background samples provide key contextual information for segmenting regions
of interest (ROIs). However, they always cover a diverse set of structures,
causing difficulties for the segmentation model to learn good decision
boundaries with high sensitivity and precision. The issue concerns the highly
heterogeneous nature of the background class, resulting in multi-modal
distributions. Empirically, we find that neural networks trained with
heterogeneous background struggle to map the corresponding contextual samples
to compact clusters in feature space. As a result, the distribution over
background logit activations may shift across the decision boundary, leading to
systematic over-segmentation across different datasets and tasks. In this
study, we propose context label learning (CoLab) to improve the context
representations by decomposing the background class into several subclasses.
Specifically, we train an auxiliary network as a task generator, along with the
primary segmentation model, to automatically generate context labels that
positively affect the ROI segmentation accuracy. Extensive experiments are
conducted on several challenging segmentation tasks and datasets. The results
demonstrate that CoLab can guide the segmentation model to map the logits of
background samples away from the decision boundary, resulting in significantly
improved segmentation accuracy. Code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Provisionally accepted to IEEE Transactions on Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake it till you make it: Learning(s) from a synthetic ImageNet clone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale image generation models such as Stable Diffusion have
exhibited an impressive ability to generate fairly realistic images starting
from a very simple text prompt. Could such models render real images obsolete
for training image prediction models? In this paper, we answer part of this
provocative question by questioning the need for real images when training
models for ImageNet classification. More precisely, provided only with the
class names that have been used to build the dataset, we explore the ability of
Stable Diffusion to generate synthetic clones of ImageNet and measure how
useful they are for training classification models from scratch. We show that
with minimal and class-agnostic prompt engineering those ImageNet clones we
denote as ImageNet-SD are able to close a large part of the gap between models
produced by synthetic images and models trained with real images for the
several standard classification benchmarks that we consider in this study. More
importantly, we show that models trained on synthetic images exhibit strong
generalization properties and perform on par with models trained on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Person Detection Using an Ultra Low-resolution Thermal Imager on a
  Low-cost MCU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Vandersteegen, Wouter Reusen, Kristof Van Beeck, Toon Goedemé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting persons in images or video with neural networks is a well-studied
subject in literature. However, such works usually assume the availability of a
camera of decent resolution and a high-performance processor or GPU to run the
detection algorithm, which significantly increases the cost of a complete
detection system. However, many applications require low-cost solutions,
composed of cheap sensors and simple microcontrollers. In this paper, we
demonstrate that even on such hardware we are not condemned to simple classic
image processing techniques. We propose a novel ultra-lightweight CNN-based
person detector that processes thermal video from a low-cost 32x24 pixel static
imager. Trained and compressed on our own recorded dataset, our model achieves
up to 91.62% accuracy (F1-score), has less than 10k parameters, and runs as
fast as 87ms and 46ms on low-cost microcontrollers STM32F407 and STM32F746,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Methods for Calibrated Photometric Stereo and Beyond: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Ju, Kin-Man Lam, Wuyuan Xie, Huiyu Zhou, Junyu Dong, Boxin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photometric stereo recovers the surface normals of an object from multiple
images with varying shading cues, i.e., modeling the relationship between
surface orientation and intensity at each pixel. Photometric stereo prevails in
superior per-pixel resolution and fine reconstruction details. However, it is a
complicated problem because of the non-linear relationship caused by
non-Lambertian surface reflectance. Recently, various deep learning methods
have shown a powerful ability in the context of photometric stereo against
non-Lambertian surfaces. This paper provides a comprehensive review of existing
deep learning-based calibrated photometric stereo methods. We first analyze
these methods from different perspectives, including input processing,
supervision, and network architecture. We summarize the performance of deep
learning photometric stereo models on the most widely-used benchmark data set.
This demonstrates the advanced performance of deep learning-based photometric
stereo methods. Finally, we give suggestions and propose future research trends
based on the limitations of existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic sign detection and recognition using event camera image
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Jeziorek, Tomasz Kryjak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for detection and recognition of traffic signs
based on information extracted from an event camera. The solution used a
FireNet deep convolutional neural network to reconstruct events into greyscale
frames. Two YOLOv4 network models were trained, one based on greyscale images
and the other on colour images. The best result was achieved for the model
trained on the basis of greyscale images, achieving an efficiency of 87.03%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for publication in: Zeszyty Studenckiego Towarzystwa
  Naukowego, 59. Hutnicza Konferencja Studenckich Kol Naukowych AGH,ISSN
  1732-0925, 2022 nr 38, pp. 127-134. (original manuscript in Polish)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast-moving object counting with an event camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Bialik, Marcin Kowalczyk, Krzysztof Blachut, Tomasz Kryjak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the use of an event camera as a component of a vision
system that enables counting of fast-moving objects - in this case, falling
corn grains. These type of cameras transmit information about the change in
brightness of individual pixels and are characterised by low latency, no motion
blur, correct operation in different lighting conditions, as well as very low
power consumption. The proposed counting algorithm processes events in real
time. The operation of the solution was demonstrated on a stand consisting of a
chute with a vibrating feeder, which allowed the number of grains falling to be
adjusted. The objective of the control system with a PID controller was to
maintain a constant average number of falling objects. The proposed solution
was subjected to a series of tests to determine the correctness of the
developed method operation. On their basis, the validity of using an event
camera to count small, fast-moving objects and the associated wide range of
potential industrial applications can be confirmed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for the Automation 2023 (7-9 March 2023, Warsaw,
  Poland) conference and PAR journal (original manuscript in Polish)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-specific Label Distribution Regularization for Learning with
  Label Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Liao, Shishuai Hu, Yutong Xie, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling noise transition matrix is a kind of promising method for learning
with label noise. Based on the estimated noise transition matrix and the noisy
posterior probabilities, the clean posterior probabilities, which are jointly
called Label Distribution (LD) in this paper, can be calculated as the
supervision. To reliably estimate the noise transition matrix, some methods
assume that anchor points are available during training. Nonetheless, if anchor
points are invalid, the noise transition matrix might be poorly learned,
resulting in poor performance. Consequently, other methods treat reliable data
points, extracted from training data, as pseudo anchor points. However, from a
statistical point of view, the noise transition matrix can be inferred from
data with noisy labels under the clean-label-domination assumption. Therefore,
we aim to estimate the noise transition matrix without (pseudo) anchor points.
There is evidence showing that samples are more likely to be mislabeled as
other similar class labels, which means the mislabeling probability is highly
correlated with the inter-class correlation. Inspired by this observation, we
propose an instance-specific Label Distribution Regularization (LDR), in which
the instance-specific LD is estimated as the supervision, to prevent DCNNs from
memorizing noisy labels. Specifically, we estimate the noisy posterior under
the supervision of noisy labels, and approximate the batch-level noise
transition matrix by estimating the inter-class correlation matrix with neither
anchor points nor pseudo anchor points. Experimental results on two synthetic
noisy datasets and two real-world noisy datasets demonstrate that our LDR
outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Dropout: Revisiting the Role of Augmentations in Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What role do augmentations play in contrastive learning? Recent work suggests
that good augmentations are label-preserving with respect to a specific
downstream task. We complicate this picture by showing that label-destroying
augmentations can be useful in the foundation model setting, where the goal is
to learn diverse, general-purpose representations for multiple downstream
tasks. We perform contrastive learning experiments on a range of image and
audio datasets with multiple downstream tasks (e.g. for digits superimposed on
photographs, predicting the class of one vs. the other). We find that Viewmaker
Networks, a recently proposed model for learning augmentations for contrastive
learning, produce label-destroying augmentations that stochastically destroy
features needed for different downstream tasks. These augmentations are
interpretable (e.g. altering shapes, digits, or letters added to images) and
surprisingly often result in better performance compared to expert-designed
augmentations, despite not preserving label information. To support our
empirical results, we theoretically analyze a simple contrastive learning
setting with a linear model. In this setting, label-destroying augmentations
are crucial for preventing one set of features from suppressing the learning of
features useful for another downstream task. Our results highlight the need for
analyzing the interaction between multiple downstream tasks when trying to
explain the success of foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PointAvatar: Deformable Point-based Head Avatars from Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, Otmar Hilliges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to create realistic, animatable and relightable head avatars from
casual video sequences would open up wide ranging applications in communication
and entertainment. Current methods either build on explicit 3D morphable meshes
(3DMM) or exploit neural implicit representations. The former are limited by
fixed topology, while the latter are non-trivial to deform and inefficient to
render. Furthermore, existing approaches entangle lighting in the color
estimation, thus they are limited in re-rendering the avatar in new
environments. In contrast, we propose PointAvatar, a deformable point-based
representation that disentangles the source color into intrinsic albedo and
normal-dependent shading. We demonstrate that PointAvatar bridges the gap
between existing mesh- and implicit representations, combining high-quality
geometry and appearance with topological flexibility, ease of deformation and
rendering efficiency. We show that our method is able to generate animatable 3D
avatars using monocular videos from multiple sources including hand-held
smartphones, laptop webcams and internet videos, achieving state-of-the-art
quality in challenging cases where previous methods fail, e.g., thin hair
strands, while being significantly more efficient in training than competing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zhengyuf.github.io/pointavatar/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Rectification of Creased Document Images based on Isometric
  Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Luo, Pengbo Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric rectification of images of distorted documents finds wide
applications in document digitization and Optical Character Recognition (OCR).
Although smoothly curved deformations have been widely investigated by many
works, the most challenging distortions, e.g. complex creases and large
foldings, have not been studied in particular. The performance of existing
approaches, when applied to largely creased or folded documents, is far from
satisfying, leaving substantial room for improvement. To tackle this task,
knowledge about document rectification should be incorporated into the
computation, among which the developability of 3D document models and
particular textural features in the images, such as straight lines, are the
most essential ones. For this purpose, we propose a general framework of
document image rectification in which a computational isometric mapping model
is utilized for expressing a 3D document model and its flattening in the plane.
Based on this framework, both model developability and textural features are
considered in the computation. The experiments and comparisons to the
state-of-the-art approaches demonstrated the effectiveness and outstanding
performance of the proposed method. Our method is also flexible in that the
rectification results can be enhanced by any other methods that extract
high-quality feature lines in the images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages,17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Learning of Dynamic Hand Gesture Recognition with Few-Shot Learning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels Schlüsener, Michael Bücker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop Few-Shot Learning models trained to recognize five or ten
different dynamic hand gestures, respectively, which are arbitrarily
interchangeable by providing the model with one, two, or five examples per hand
gesture. All models were built in the Few-Shot Learning architecture of the
Relation Network (RN), in which Long-Short-Term Memory cells form the backbone.
The models use hand reference points extracted from RGB-video sequences of the
Jester dataset which was modified to contain 190 different types of hand
gestures. Result show accuracy of up to 88.8% for recognition of five and up to
81.2% for ten dynamic hand gestures. The research also sheds light on the
potential effort savings of using a Few-Shot Learning approach instead of a
traditional Deep Learning approach to detect dynamic hand gestures. Savings
were defined as the number of additional observations required when a Deep
Learning model is trained on new hand gestures instead of a Few Shot Learning
model. The difference with respect to the total number of observations required
to achieve approximately the same accuracy indicates potential savings of up to
630 observations for five and up to 1260 observations for ten hand gestures to
be recognized. Since labeling video recordings of hand gestures implies
significant effort, these savings can be considered substantial.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CD-TTA: Compound Domain Test-time Adaptation for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junha Song, Kwanyong Park, Inkyu Shin, Sanghyun Woo, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) has attracted significant attention due to its
practical properties which enable the adaptation of a pre-trained model to a
new domain with only target dataset during the inference stage. Prior works on
TTA assume that the target dataset comes from the same distribution and thus
constitutes a single homogeneous domain. In practice, however, the target
domain can contain multiple homogeneous domains which are sufficiently
distinctive from each other and those multiple domains might occur cyclically.
Our preliminary investigation shows that domain-specific TTA outperforms
vanilla TTA treating compound domain (CD) as a single one. However, domain
labels are not available for CD, which makes domain-specific TTA not
practicable. To this end, we propose an online clustering algorithm for finding
pseudo-domain labels to obtain similar benefits as domain-specific
configuration and accumulating knowledge of cyclic domains effectively.
Moreover, we observe that there is a significant discrepancy in terms of
prediction quality among samples, especially in the CD context. This further
motivates us to boost its performance with gradient denoising by considering
the image-wise similarity with the source distribution. Overall, the key
contribution of our work lies in proposing a highly significant new task
compound domain test-time adaptation (CD-TTA) on semantic segmentation as well
as providing a strong baseline to facilitate future works to benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Classifiers of Prototypes and Reciprocal Points for Universal
  Domain Adaptation <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungsu Hur, Inkyu Shin, Kwanyong Park, Sanghyun Woo, In So Kweon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal Domain Adaptation aims to transfer the knowledge between the
datasets by handling two shifts: domain-shift and category-shift. The main
challenge is correctly distinguishing the unknown target samples while adapting
the distribution of known class knowledge from source to target. Most existing
methods approach this problem by first training the target adapted known
classifier and then relying on the single threshold to distinguish unknown
target samples. However, this simple threshold-based approach prevents the
model from considering the underlying complexities existing between the known
and unknown samples in the high-dimensional feature space. In this paper, we
propose a new approach in which we use two sets of feature points, namely dual
Classifiers for Prototypes and Reciprocals (CPR). Our key idea is to associate
each prototype with corresponding known class features while pushing the
reciprocals apart from these prototypes to locate them in the potential unknown
feature space. The target samples are then classified as unknown if they fall
near any reciprocals at test time. To successfully train our framework, we
collect the partial, confident target samples that are classified as known or
unknown through on our proposed multi-criteria selection. We then additionally
apply the entropy loss regularization to them. For further adaptation, we also
apply standard consistency regularization that matches the predictions of two
different views of the input to make more compact target feature space. We
evaluate our proposal, CPR, on three standard benchmarks and achieve comparable
or new state-of-the-art results. We also provide extensive ablation experiments
to verify our main design choices in our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Example Defense via Perturbation Grading Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Zhu, Wanli Lyu, Bin Li, Zhaoxia Yin, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks have been widely used in many fields. However, studies
have shown that DNNs are easily attacked by adversarial examples, which have
tiny perturbations and greatly mislead the correct judgment of DNNs.
Furthermore, even if malicious attackers cannot obtain all the underlying model
parameters, they can use adversarial examples to attack various DNN-based task
systems. Researchers have proposed various defense methods to protect DNNs,
such as reducing the aggressiveness of adversarial examples by preprocessing or
improving the robustness of the model by adding modules. However, some defense
methods are only effective for small-scale examples or small perturbations but
have limited defense effects for adversarial examples with large perturbations.
This paper assigns different defense strategies to adversarial perturbations of
different strengths by grading the perturbations on the input examples.
Experimental results show that the proposed method effectively improves defense
performance. In addition, the proposed method does not modify any task model,
which can be used as a preprocessing module, which significantly reduces the
deployment cost in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Enhanced Belief Propagation for Multiobject Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchao Liang, Florian Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic solutions for multi-object tracking (MOT) are a key enabler for
applications in autonomous navigation and applied ocean sciences.
State-of-the-art MOT methods fully rely on a statistical model and typically
use preprocessed sensor data as measurements. In particular, measurements are
produced by a detector that extracts potential object locations from the raw
sensor data collected for a discrete time step. This preparatory processing
step reduces data flow and computational complexity but may result in a loss of
information. State-of-the-art Bayesian MOT methods that are based on belief
propagation (BP) systematically exploit graph structures of the statistical
model to reduce computational complexity and improve scalability. However, as a
fully model-based approach, BP can only provide suboptimal estimates when there
is a mismatch between the statistical model and the true data-generating
process. Existing BP-based MOT methods can further only make use of
preprocessed measurements. In this paper, we introduce a variant of BP that
combines model-based with data-driven MOT. The proposed neural enhanced belief
propagation (NEBP) method complements the statistical model of BP by
information learned from raw sensor data. This approach conjectures that the
learned information can reduce model mismatch and thus improve data association
and false alarm rejection. Our NEBP method improves tracking performance
compared to model-based methods. At the same time, it inherits the advantages
of BP-based MOT, i.e., it scales only quadratically in the number of objects,
and it can thus generate and maintain a large number of object tracks. We
evaluate the performance of our NEBP approach for MOT on the nuScenes
autonomous driving dataset and demonstrate that it has state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight integration of 3D features to improve 2D image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Pradelle, Raphaelle Chaine, David Wendland, Julie Digne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene understanding is a major challenge of today's computer vision. Center
to this task is image segmentation, since scenes are often provided as a set of
pictures. Nowadays, many such datasets also provide 3D geometry information
given as a 3D point cloud acquired by a laser scanner or a depth camera. To
exploit this geometric information, many current approaches rely on both a 2D
loss and 3D loss, requiring not only 2D per pixel labels but also 3D per point
labels. However obtaining a 3D groundtruth is challenging, time-consuming and
error-prone. In this paper, we show that image segmentation can benefit from 3D
geometric information without requiring any 3D groundtruth, by training the
geometric feature extraction with a 2D segmentation loss in an end-to-end
fashion. Our method starts by extracting a map of 3D features directly from the
point cloud by using a lightweight and simple 3D encoder neural network. The 3D
feature map is then used as an additional input to a classical image
segmentation network. During training, the 3D features extraction is optimized
for the segmentation task by back-propagation through the entire pipeline. Our
method exhibits state-of-the-art performance with much lighter input dataset
requirements, since no 3D groundtruth is required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main : 7 pages, 4 figures; supplementary : 4 pages, 3 figures;
  submitted to CVIU</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolution-enhanced Evolving Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, Xiangtai Li, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based neural networks, such as Transformers, have become ubiquitous
in numerous applications, including computer vision, natural language
processing, and time-series analysis. In all kinds of attention networks, the
attention maps are crucial as they encode semantic dependencies between input
tokens. However, most existing attention networks perform modeling or reasoning
based on representations, wherein the attention maps of different layers are
learned separately without explicit interactions. In this paper, we propose a
novel and generic evolving attention mechanism, which directly models the
evolution of inter-token relationships through a chain of residual
convolutional modules. The major motivations are twofold. On the one hand, the
attention maps in different layers share transferable knowledge, thus adding a
residual connection can facilitate the information flow of inter-token
relationships across layers. On the other hand, there is naturally an
evolutionary trend among attention maps at different abstraction levels, so it
is beneficial to exploit a dedicated convolution-based module to capture this
process. Equipped with the proposed mechanism, the convolution-enhanced
evolving attention networks achieve superior performance in various
applications, including time-series representation, natural language
understanding, machine translation, and image classification. Especially on
time-series representation tasks, Evolving Attention-enhanced Dilated
Convolutional (EA-DC-) Transformer outperforms state-of-the-art models
significantly, achieving an average of 17% improvement compared to the best
SOTA. To the best of our knowledge, this is the first work that explicitly
models the layer-wise evolution of attention maps. Our implementation is
available at https://github.com/pkuyym/EvolvingAttention
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of the previous work (arXiv:2102.12895). arXiv admin note:
  text overlap with arXiv:2102.12895</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Chung, Kanggeon Lee, Sungyong Baik, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hinged on the representation power of neural networks, neural radiance fields
(NeRF) have recently emerged as one of the promising and widely applicable
methods for 3D object and scene representation. However, NeRF faces challenges
in practical applications, such as large-scale scenes and edge devices with a
limited amount of memory, where data needs to be processed sequentially. Under
such incremental learning scenarios, neural networks are known to suffer
catastrophic forgetting: easily forgetting previously seen data after training
with new data. We observe that previous incremental learning algorithms are
limited by either low performance or memory scalability issues. As such, we
develop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).
MEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve
as a memory that provides the pixel RGB values, given rays as queries. Upon the
motivation, our framework learns which rays to query NeRF to extract previous
pixel values. The extracted pixel values are then used to train NeRF in a
self-distillation manner to prevent catastrophic forgetting. As a result,
MEIL-NeRF demonstrates constant memory consumption and competitive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavEnhancer: Unifying Wavelet and <span class="highlight-title">Transformer</span> for Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zinuo Li, Xuhang Chen, Chi-Man Pun, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image enhancement is a technique that frequently utilized in digital image
processing. In recent years, the popularity of learning-based techniques for
enhancing the aesthetic performance of photographs has increased. However, the
majority of current works do not optimize an image from different frequency
domains and typically focus on either pixel-level or global-level enhancements.
In this paper, we propose a transformer-based model in the wavelet domain to
refine different frequency bands of an image. Our method focuses both on local
details and high-level features for enhancement, which can generate superior
results. On the basis of comprehensive benchmark evaluations, our method
outperforms the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autoencoders as Cross-Modal Teachers: Can <span class="highlight-title">Pretrain</span>ed 2D Image
  <span class="highlight-title">Transformer</span>s Help 3D Representation Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, Kaisheng Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep learning heavily relies on large-scale data with
comprehensive labels, which is more expensive and time-consuming to fetch in 3D
compared to 2D images or natural languages. This promotes the potential of
utilizing models pretrained with data more than 3D as teachers for cross-modal
knowledge transferring. In this paper, we revisit masked modeling in a unified
fashion of knowledge distillation, and we show that foundational Transformers
pretrained with 2D images or natural languages can help self-supervised 3D
representation learning through training Autoencoders as Cross-Modal Teachers
(ACT). The pretrained Transformers are transferred as cross-modal 3D teachers
using discrete variational autoencoding self-supervision, during which the
Transformers are frozen with prompt tuning for better knowledge inheritance.
The latent features encoded by the 3D teachers are used as the target of masked
point modeling, wherein the dark knowledge is distilled to the 3D Transformer
students as foundational geometry understanding. Our ACT pretrained 3D learner
achieves state-of-the-art generalization capacity across various downstream
benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes will be
released at https://github.com/RunpeiDong/ACT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Find Strong Lottery Tickets in Generative Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangyeop Yeo, Yoojin Jang, Jy-yong Sohn, Dongyoon Han, Jaejun Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yes. In this paper, we investigate strong lottery tickets in generative
models, the subnetworks that achieve good generative performance without any
weight update. Neural network pruning is considered the main cornerstone of
model compression for reducing the costs of computation and memory.
Unfortunately, pruning a generative model has not been extensively explored,
and all existing pruning algorithms suffer from excessive weight-training
costs, performance degradation, limited generalizability, or complicated
training. To address these problems, we propose to find a strong lottery ticket
via moment-matching scores. Our experimental results show that the discovered
subnetwork can perform similarly or better than the trained dense model even
when only 10% of the weights remain. To the best of our knowledge, we are the
first to show the existence of strong lottery tickets in generative models and
provide an algorithm to find it stably. Our code and supplementary materials
are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DQnet: Cross-Model Detail Querying for Camouflaged Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Sun, Chengao Liu, Linyan Zhang, Yu Li, Pengxu Wei, Chang Liu, Jialing Zou, Jianbin Jiao, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged objects are seamlessly blended in with their surroundings, which
brings a challenging detection task in computer vision. Optimizing a
convolutional neural network (CNN) for camouflaged object detection (COD) tends
to activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue which inevitably leads to missing or
redundant regions of objects. In this paper, we argue that partial activation
is caused by the intrinsic characteristics of CNN, where the convolution
operations produce local receptive fields and experience difficulty to capture
long-range feature dependency among image regions. In order to obtain feature
maps that could activate full object extent, keeping the segmental results from
being overwhelmed by noisy features, a novel framework termed Cross-Model
Detail Querying network (DQnet) is proposed. It reasons the relations between
long-range-aware representations and multi-scale local details to make the
enhanced representation fully highlight the object regions and eliminate noise
on non-object regions. Specifically, a vanilla ViT pretrained with
self-supervised learning (SSL) is employed to model long-range dependencies
among image regions. A ResNet is employed to enable learning fine-grained
spatial local details in multiple scales. Then, to effectively retrieve
object-related details, a Relation-Based Querying (RBQ) module is proposed to
explore window-based interactions between the global representations and the
multi-scale local details. Extensive experiments are conducted on the widely
used COD datasets and show that our DQnet outperforms the current
state-of-the-arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Learning Protocol for Federated Tumor Segmentation Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambrish Rawat, Giulio Zizzo, Swanand Kadhe, Jonathan P. Epperlein, Stefano Braghin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we devise robust and efficient learning protocols for
orchestrating a Federated Learning (FL) process for the Federated Tumor
Segmentation Challenge (FeTS 2022). Enabling FL for FeTS setup is challenging
mainly due to data heterogeneity among collaborators and communication cost of
training. To tackle these challenges, we propose Robust Learning Protocol
(RoLePRO) which is a combination of server-side adaptive optimisation (e.g.,
server-side Adam) and judicious parameter (weights) aggregation schemes (e.g.,
adaptive weighted aggregation). RoLePRO takes a two-phase approach, where the
first phase consists of vanilla Federated Averaging, while the second phase
consists of a judicious aggregation scheme that uses a sophisticated
reweighting, all in the presence of an adaptive optimisation algorithm at the
server. We draw insights from extensive experimentation to tune learning rates
for the two phases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneGATE: Scene-Graph based co-Attention networks for TExt visual
  question answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siwen Luo, Feiqi Cao, Felipe Nunez, Zean Wen, Josiah Poon, Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most TextVQA approaches focus on the integration of objects, scene texts and
question words by a simple transformer encoder. But this fails to capture the
semantic relations between different modalities. The paper proposes a Scene
Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the
semantic relations among the objects, Optical Character Recognition (OCR)
tokens and the question words. It is achieved by a TextVQA-based scene graph
that discovers the underlying semantics of an image. We created a
guided-attention module to capture the intra-modal interplay between the
language and the vision as a guidance for inter-modal interactions. To make
explicit teaching of the relations between the two modalities, we proposed and
integrated two attention modules, namely a scene graph-based semantic
relation-aware attention and a positional relation-aware attention. We
conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.
It is shown that our SceneGATE method outperformed existing ones because of the
scene graph and its attention modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGAN: Hierarchical Graph Alignment Network for Image-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Guo, Meiting Wang, Yan Zhou, Bin Song, Yuhao Chi, Wei Fan, Jianglong Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text retrieval (ITR) is a challenging task in the field of multimodal
information processing due to the semantic gap between different modalities. In
recent years, researchers have made great progress in exploring the accurate
alignment between image and text. However, existing works mainly focus on the
fine-grained alignment between image regions and sentence fragments, which
ignores the guiding significance of context background information. Actually,
integrating the local fine-grained information and global context background
information can provide more semantic clues for retrieval. In this paper, we
propose a novel Hierarchical Graph Alignment Network (HGAN) for image-text
retrieval. First, to capture the comprehensive multimodal features, we
construct the feature graphs for the image and text modality respectively.
Then, a multi-granularity shared space is established with a designed
Multi-granularity Feature Aggregation and Rearrangement (MFAR) module, which
enhances the semantic corresponding relations between the local and global
information, and obtains more accurate feature representations for the image
and text modalities. Finally, the ultimate image and text features are further
refined through three-level similarity functions to achieve the hierarchical
alignment. To justify the proposed model, we perform extensive experiments on
MS-COCO and Flickr30K datasets. Experimental results show that the proposed
HGAN outperforms the state-of-the-art methods on both datasets, which
demonstrates the effectiveness and superiority of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Werewolf Among Us: A Multimodal <span class="highlight-title">Dataset</span> for Modeling Persuasion
  Behaviors in Social Deduction Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persuasion modeling is a key building block for conversational agents.
Existing works in this direction are limited to analyzing textual dialogue
corpus. We argue that visual signals also play an important role in
understanding human persuasive behaviors. In this paper, we introduce the first
multimodal dataset for modeling persuasion behaviors. Our dataset includes 199
dialogue transcriptions and videos captured in a multi-player social deduction
game setting, 26,647 utterance level annotations of persuasion strategy, and
game level annotations of deduction game outcomes. We provide extensive
experiments to show how dialogue context and visual signals benefit persuasion
strategy prediction. We also explore the generalization ability of language
models for persuasion modeling and the role of persuasion strategies in
predicting social deduction game outcomes. Our dataset, code, and models can be
found at https://persuasion-deductiongame.socialai-data.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">self-supervised</span> representation learning via sequential
  adversarial masking <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, Min Bai, Tristan McKinney, Li Erran Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent methods in self-supervised learning have demonstrated that
masking-based pretext tasks extend beyond NLP, serving as useful pretraining
objectives in computer vision. However, existing approaches apply random or ad
hoc masking strategies that limit the difficulty of the reconstruction task
and, consequently, the strength of the learnt representations. We improve upon
current state-of-the-art work in learning adversarial masks by proposing a new
framework that generates masks in a sequential fashion with different
constraints on the adversary. This leads to improvements in performance on
various downstream tasks, such as classification on ImageNet100, STL10, and
CIFAR10/100 and segmentation on Pascal VOC. Our results further demonstrate the
promising capabilities of masking-based approaches for SSL in computer vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, Presented at NeurIPS 2022 SSL: Theory and
  Practice Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Li, Runsheng Xu, Xinyu Liu, Jin Ma, Zicheng Chi, Jiaqi Ma, Hongkai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been widely used in the perception (e.g., 3D object
detection) of intelligent vehicle driving. Due to the beneficial
Vehicle-to-Vehicle (V2V) communication, the deep learning based features from
other agents can be shared to the ego vehicle so as to improve the perception
of the ego vehicle. It is named as Cooperative Perception in the V2V research,
whose algorithms have been dramatically advanced recently. However, all the
existing cooperative perception algorithms assume the ideal V2V communication
without considering the possible lossy shared features because of the Lossy
Communication (LC) which is common in the complex real-world driving scenarios.
In this paper, we first study the side effect (e.g., detection performance
drop) by the lossy communication in the V2V Cooperative Perception, and then we
propose a novel intermediate LC-aware feature fusion method to relieve the side
effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance
the interaction between the ego vehicle and other vehicles by a specially
designed V2V Attention Module (V2VAM) including intra-vehicle attention of ego
vehicle and uncertainty-aware inter-vehicle attention. The extensive experiment
on the public cooperative perception dataset OPV2V (based on digital-twin CARLA
simulator) demonstrates that the proposed method is quite effective for the
cooperative point cloud based 3D object detection under lossy V2V
communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepQ-ViT: Scale Reparameterization for Post-Training Quantization of
  Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Li, Junrui Xiao, Lianwei Yang, Qingyi Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ), which only requires a tiny dataset for
calibration without end-to-end retraining, is a light and practical model
compression technique. Recently, several PTQ schemes for vision transformers
(ViTs) have been presented; unfortunately, they typically suffer from
non-trivial accuracy degradation, especially in low-bit cases. In this paper,
we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale
reparameterization, to address the above issues. RepQ-ViT decouples the
quantization and inference processes, where the former employs complex
quantizers and the latter employs scale-reparameterized simplified quantizers.
This ensures both accurate quantization and efficient inference, which
distinguishes it from existing approaches that sacrifice quantization
performance to meet the target hardware. More specifically, we focus on two
components with extreme distributions: post-LayerNorm activations with severe
inter-channel variation and post-Softmax activations with power-law features,
and initially apply channel-wise quantization and log$\sqrt{2}$ quantization,
respectively. Then, we reparameterize the scales to hardware-friendly
layer-wise quantization and log2 quantization for inference, with only slight
accuracy or computational costs. Extensive experiments are conducted on
multiple vision tasks with different model variants, proving that RepQ-ViT,
without hyperparameters and expensive reconstruction procedures, can outperform
existing strong baselines and encouragingly improve the accuracy of 4-bit PTQ
of ViTs to a usable level.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Saliency Guidance for Data-free Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xialei Liu, Jiang-Tian Zhai, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-Free Class Incremental Learning (DFCIL) aims to sequentially learn tasks
with access only to data from the current one. DFCIL is of interest because it
mitigates concerns about privacy and long-term storage of data, while at the
same time alleviating the problem of catastrophic forgetting in incremental
learning. In this work, we introduce robust saliency guidance for DFCIL and
propose a new framework, which we call RObust Saliency Supervision (ROSS), for
mitigating the negative effect of saliency drift. Firstly, we use a
teacher-student architecture leveraging low-level tasks to supervise the model
with global saliency. We also apply boundary-guided saliency to protect it from
drifting across object boundaries at intermediate layers. Finally, we introduce
a module for injecting and recovering saliency noise to increase robustness of
saliency preservation. Our experiments demonstrate that our method can retain
better saliency maps across tasks and achieve state-of-the-art results on the
CIFAR-100, Tiny-ImageNet and ImageNet-Subset DFCIL benchmarks. Code will be
made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning for Visual Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Shah, Arjun Bhorkar, Hrish Leen, Ilya Kostrikov, Nick Rhinehart, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning can enable robots to navigate to distant goals while
optimizing user-specified reward functions, including preferences for following
lanes, staying on paved paths, or avoiding freshly mowed grass. However, online
learning from trial-and-error for real-world robots is logistically
challenging, and methods that instead can utilize existing datasets of robotic
navigation data could be significantly more scalable and enable broader
generalization. In this paper, we present ReViND, the first offline RL system
for robotic navigation that can leverage previously collected data to optimize
user-specified reward functions in the real-world. We evaluate our system for
off-road navigation without any additional data collection or fine-tuning, and
show that it can navigate to distant goals using only offline training from
this dataset, and exhibit behaviors that qualitatively differ based on the
user-specified reward function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://sites.google.com/view/revind/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jee Seok Yoon, Chenghao Zhang, Heung-Il Suk, Jia Guo, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human organs constantly undergo anatomical changes due to a complex mix of
short-term (e.g., heartbeat) and long-term (e.g., aging) factors. Evidently,
prior knowledge of these factors will be beneficial when modeling their future
state, i.e., via image generation. However, most of the medical image
generation tasks only rely on the input from a single image, thus ignoring the
sequential dependency even when longitudinal data is available. Sequence-aware
deep generative models, where model input is a sequence of ordered and
timestamped images, are still underexplored in the medical imaging domain that
is featured by several unique challenges: 1) Sequences with various lengths; 2)
Missing data or frame, and 3) High dimensionality. To this end, we propose a
sequence-aware diffusion model (SADM) for the generation of longitudinal
medical images. Recently, diffusion models have shown promising results on
high-fidelity image generation. Our method extends this new technique by
introducing a sequence-aware transformer as the conditional module in a
diffusion model. The novel design enables learning longitudinal dependency even
with missing data during training and allows autoregressive generation of a
sequence of images during inference. Our extensive experiments on 3D
longitudinal medical images demonstrate the effectiveness of SADM compared with
baselines and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Location-aware Adaptive Denormalization: A Deep Learning Approach For
  Wildfire Danger Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Hakam Shams Eddin, Ribana Roscher, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change is expected to intensify and increase extreme events in the
weather cycle. Since this has a significant impact on various sectors of our
life, recent works are concerned with identifying and predicting such extreme
events from Earth observations. This paper proposes a 2D/3D two-branch
convolutional neural network (CNN) for wildfire danger forecasting. To use a
unified framework, previous approaches duplicate static variables along the
time dimension and neglect the intrinsic differences between static and dynamic
variables. Furthermore, most existing multi-branch architectures lose the
interconnections between the branches during the feature learning stage. To
address these issues, we propose a two-branch architecture with a
Location-aware Adaptive Denormalization layer (LOADE). Using LOADE as a
building block, we can modulate the dynamic features conditional on their
geographical location. Thus, our approach considers feature properties as a
unified yet compound 2D/3D model. Besides, we propose using an absolute
temporal encoding for time-related forecasting problems. Our experimental
results show a better performance of our approach than other baselines on the
challenging FireCube dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Realistic Underwater <span class="highlight-title">Dataset</span> Generation and Color Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neham Jain, Gopi Matta, Kaushik Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovery of true color from underwater images is an ill-posed problem. This
is because the wide-band attenuation coefficients for the RGB color channels
depend on object range, reflectance, etc. which are difficult to model. Also,
there is backscattering due to suspended particles in water. Thus, most
existing deep-learning based color restoration methods, which are trained on
synthetic underwater datasets, do not perform well on real underwater data.
This can be attributed to the fact that synthetic data cannot accurately
represent real conditions. To address this issue, we use an image to image
translation network to bridge the gap between the synthetic and real domains by
translating images from synthetic underwater domain to real underwater domain.
Using this multimodal domain adaptation technique, we create a dataset that can
capture a diverse array of underwater conditions. We then train a simple but
effective CNN based network on our domain adapted dataset to perform color
restoration. Code and pre-trained models can be accessed at
https://github.com/nehamjain10/TRUDGCR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at The Indian Conference on Computer Vision, Graphics and
  Image Processing (ICVGIP) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORL: Compositional Representation Learning for Few-Shot Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.11878v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.11878v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju He, Adam Kortylewski, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image classification consists of two consecutive learning processes:
1) In the meta-learning stage, the model acquires a knowledge base from a set
of training classes. 2) During meta-testing, the acquired knowledge is used to
recognize unseen classes from very few examples. Inspired by the compositional
representation of objects in humans, we train a neural network architecture
that explicitly represents objects as a dictionary of shared components and
their spatial composition. In particular, during meta-learning, we train a
knowledge base that consists of a dictionary of component representations and a
dictionary of component activation maps that encode common spatial activation
patterns of components. The elements of both dictionaries are shared among the
training classes. During meta-testing, the representation of unseen classes is
learned using the component representations and the component activation maps
from the knowledge base. Finally, an attention mechanism is used to strengthen
those components that are most important for each category. We demonstrate the
value of our interpretable compositional learning framework for a few-shot
classification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where
we achieve comparable performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we make virtual characters in a scene interact with their surrounding
objects through simple instructions? Is it possible to synthesize such motion
plausibly with a diverse set of objects and instructions? Inspired by these
questions, we present the first framework to synthesize the full-body motion of
virtual human characters performing specified actions with 3D objects placed
within their reach. Our system takes as input textual instructions specifying
the objects and the associated intentions of the virtual characters and outputs
diverse sequences of full-body motions. This is in contrast to existing work,
where full-body action synthesis methods generally do not consider object
interactions, and human-object interaction methods focus mainly on synthesizing
hand or finger movements for grasping objects. We accomplish our objective by
designing an intent-driven full-body motion generator, which uses a pair of
decoupled conditional variational autoencoders (CVAE) to learn the motion of
the body parts in an autoregressive manner. We also optimize for the positions
of the objects with six degrees of freedom (6DoF) such that they plausibly fit
within the hands of the synthesized characters. We compare our proposed method
with the existing methods of motion synthesis and establish a new and stronger
state-of-the-art for the task of intent-driven motion synthesis. Through a user
study, we further show that our synthesized full-body motions appear more
realistic to the participants in more than 80% of scenarios compared to the
current state-of-the-art methods, and are perceived to be as good as the ground
truth on several occasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic progress in computer vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Erdil, Tamay Besiroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Image Synthesis and <span class="highlight-title">Self-supervised</span> Feature Adaptation for
  Cross-Modality Biomedical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrong Hu, Corey Wang, Yiyu Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a novel framework CISFA (Contrastive Image synthesis and
Self-supervised Feature Adaptation)that builds on image domain translation and
unsupervised feature adaptation for cross-modality biomedical image
segmentation. Different from existing works, we use a one-sided generative
model and add a weighted patch-wise contrastive loss between sampled patches of
the input image and the corresponding synthetic image, which serves as shape
constraints. Moreover, we notice that the generated images and input images
share similar structural information but are in different modalities. As such,
we enforce contrastive losses on the generated images and the input images to
train the encoder of a segmentation model to minimize the discrepancy between
paired images in the learned embedding space. Compared with existing works that
rely on adversarial learning for feature adaptation, such a method enables the
encoder to learn domain-independent features in a more explicit way. We
extensively evaluate our methods on segmentation tasks containing CT and MRI
images for abdominal cavities and whole hearts. Experimental results show that
the proposed framework not only outputs synthetic images with less distortion
of organ shapes, but also outperforms state-of-the-art domain adaptation
methods by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Channel Selection in <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Krishna, Ayush K. Rai, Yasser A. D. Djilali, Alan F. Smeaton, Kevin McGuinness, Noel E. O'Connor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whilst computer vision models built using self-supervised approaches are now
commonplace, some important questions remain. Do self-supervised models learn
highly redundant channel features? What if a self-supervised network could
dynamically select the important channels and get rid of the unnecessary ones?
Currently, convnets pre-trained with self-supervision have obtained comparable
performance on downstream tasks in comparison to their supervised counterparts
in computer vision. However, there are drawbacks to self-supervised models
including their large numbers of parameters, computationally expensive training
strategies and a clear need for faster inference on downstream tasks. In this
work, our goal is to address the latter by studying how a standard channel
selection method developed for supervised learning can be applied to networks
trained with self-supervision. We validate our findings on a range of target
budgets $t_{d}$ for channel computation on image classification task across
different datasets, specifically CIFAR-10, CIFAR-100, and ImageNet-100,
obtaining comparable performance to that of the original network when selecting
all channels but at a significant reduction in computation reported in terms of
FLOPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Irish Machine Vision and Image Processing Conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COVID-19 Monitoring System using Social Distancing and Face Mask
  Detection on Surveillance video <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahana Srinivasan, Rujula Singh R, Ruchita R Biradar, Revathi SA
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current times, the fear and danger of COVID-19 virus still stands
large. Manual monitoring of social distancing norms is impractical with a large
population moving about and with insufficient task force and resources to
administer them. There is a need for a lightweight, robust and 24X7
video-monitoring system that automates this process. This paper proposes a
comprehensive and effective solution to perform person detection, social
distancing violation detection, face detection and face mask classification
using object detection, clustering and Convolution Neural Network (CNN) based
binary classifier. For this, YOLOv3, Density-based spatial clustering of
applications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and
MobileNetV2 based binary classifier have been employed on surveillance video
datasets. This paper also provides a comparative study of different face
detection and face mask classification models. Finally, a video dataset
labelling method is proposed along with the labelled video dataset to
compensate for the lack of dataset in the community and is used for evaluation
of the system. The system performance is evaluated in terms of accuracy, F1
score as well as the prediction time, which has to be low for practical
applicability. The system performs with an accuracy of 91.2% and F1 score of
90.79% on the labelled video dataset and has an average prediction time of 7.12
seconds for 78 frames of a video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I, Rujula Singh R, would like to apologize to the research community
  for the confusion caused by the inconsistency in author lists between
  multiple versions of this paper. I take full responsibility for this error
  and will be more diligent in the future to ensure the accuracy and
  consistency of our research publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artifact Removal in Histopathology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cameron Dahan, Stergios Christodoulidis, Maria Vakalopoulou, Joseph Boyd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the clinical setting of histopathology, whole-slide image (WSI) artifacts
frequently arise, distorting regions of interest, and having a pernicious
impact on WSI analysis. Image-to-image translation networks such as CycleGANs
are in principle capable of learning an artifact removal function from unpaired
data. However, we identify a surjection problem with artifact removal, and
propose an weakly-supervised extension to CycleGAN to address this. We assemble
a pan-cancer dataset comprising artifact and clean tiles from the TCGA
database. Promising results highlight the soundness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected typos, small modification of Figure 1 (+ reflected in
  Section 2.1), results unchanged</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation Contracts for Outlier-Robust Geometric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Carlone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outlier-robust estimation is a fundamental problem and has been extensively
investigated by statisticians and practitioners. The last few years have seen a
convergence across research fields towards "algorithmic robust statistics",
which focuses on developing tractable outlier-robust techniques for
high-dimensional estimation problems. Despite this convergence, research
efforts across fields have been mostly disconnected from one another. This
monograph bridges recent work on certifiable outlier-robust estimation for
geometric perception in robotics and computer vision with parallel work in
robust statistics. In particular, we adapt and extend recent results on robust
linear regression (applicable to the low-outlier regime with << 50% outliers)
and list-decodable regression (applicable to the high-outlier regime with >>
50% outliers) to the setup commonly found in robotics and vision, where (i)
variables (e.g., rotations, poses) belong to a non-convex domain, (ii)
measurements are vector-valued, and (iii) the number of outliers is not known a
priori. The emphasis here is on performance guarantees: rather than proposing
radically new algorithms, we provide conditions on the input measurements under
which modern estimation algorithms (possibly after small modifications) are
guaranteed to recover an estimate close to the ground truth in the presence of
outliers. These conditions are what we call an "estimation contract". Besides
the proposed extensions of existing results, we believe the main contributions
of this monograph are (i) to unify parallel research lines by pointing out
commonalities and differences, (ii) to introduce advanced material (e.g.,
sum-of-squares proofs) in an accessible and self-contained presentation for the
practitioner, and (iii) to point out a few immediate opportunities and open
questions in outlier-robust geometric perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>95 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVSFormer: Multi-View Stereo by Learning Robust Image Features and
  Temperature-based Depth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjie Cao, Xinlin Ren, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature representation learning is the key recipe for learning-based
Multi-View Stereo (MVS). As the common feature extractor of learning-based MVS,
vanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature
representations for reflection and texture-less areas, which limits the
generalization of MVS. Even FPNs worked with pre-trained Convolutional Neural
Networks (CNNs) fail to tackle these issues. On the other hand, Vision
Transformers (ViTs) have achieved prominent success in many 2D vision tasks.
Thus we ask whether ViTs can facilitate feature learning in MVS? In this paper,
we propose a pre-trained ViT enhanced MVS network called MVSFormer, which can
learn more reliable feature representations benefited by informative priors
from ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention
mechanisms can achieve prominent improvement based on FPNs. Besides, the
alternative MVSFormer with frozen ViT weights is further proposed. This largely
alleviates the training cost with competitive performance strengthened by the
attention map from the self-distillation pre-training. MVSFormer can be
generalized to various input resolutions with efficient multi-scale training
strengthened by gradient accumulation. Moreover, we discuss the merits and
drawbacks of classification and regression-based MVS methods, and further
propose to unify them with a temperature-based strategy. MVSFormer achieves
state-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks
as Top-1 on both intermediate and advanced sets of the highly competitive
Tanks-and-Temples leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROIFormer: Semantic-Aware Region of Interest <span class="highlight-title">Transformer</span> for Efficient
  <span class="highlight-title">Self-Supervised</span> Monocular Depth Estimation <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daitao Xing, Jinglin Shen, Chiuman Ho, Anthony Tzes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exploration of mutual-benefit cross-domains has shown great potential
toward accurate self-supervised depth estimation. In this work, we revisit
feature fusion between depth and semantic information and propose an efficient
local adaptive attention method for geometric aware representation enhancement.
Instead of building global connections or deforming attention across the
feature space without restraint, we bound the spatial interaction within a
learnable region of interest. In particular, we leverage geometric cues from
semantic information to learn local adaptive bounding boxes to guide
unsupervised feature aggregation. The local areas preclude most irrelevant
reference points from attention space, yielding more selective feature learning
and faster convergence. We naturally extend the paradigm into a multi-head and
hierarchic way to enable the information distillation in different semantic
levels and improve the feature discriminative ability for fine-grained depth
estimation. Extensive experiments on the KITTI dataset show that our proposed
method establishes a new state-of-the-art in self-supervised monocular depth
estimation task, demonstrating the effectiveness of our approach over former
Transformer variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Decomposition of Joint Low Rankness and Local Smoothness Plus
  Sparse Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangjun Peng, Yao Wang, Hongying Zhang, Jianjun Wang, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is known that the decomposition in low-rank and sparse matrices
(\textbf{L+S} for short) can be achieved by several Robust PCA techniques.
Besides the low rankness, the local smoothness (\textbf{LSS}) is a vitally
essential prior for many real-world matrix data such as hyperspectral images
and surveillance videos, which makes such matrices have low-rankness and local
smoothness properties at the same time. This poses an interesting question: Can
we make a matrix decomposition in terms of \textbf{L\&LSS +S } form exactly? To
address this issue, we propose in this paper a new RPCA model based on
three-dimensional correlated total variation regularization (3DCTV-RPCA for
short) by fully exploiting and encoding the prior expression underlying such
joint low-rank and local smoothness matrices. Specifically, using a
modification of Golfing scheme, we prove that under some mild assumptions, the
proposed 3DCTV-RPCA model can decompose both components exactly, which should
be the first theoretical guarantee among all such related methods combining low
rankness and local smoothness. In addition, by utilizing Fast Fourier Transform
(FFT), we propose an efficient ADMM algorithm with a solid convergence
guarantee for solving the resulting optimization problem. Finally, a series of
experiments on both simulations and real applications are carried out to
demonstrate the general validity of the proposed 3DCTV-RPCA model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intention-Conditioned Long-Term Human Egocentric Action Forecasting <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12080v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12080v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esteve Valls Mascaro, Hyemin Ahn, Dongheui Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To anticipate how a human would act in the future, it is essential to
understand the human intention since it guides the human towards a certain
goal. In this paper, we propose a hierarchical architecture which assumes a
sequence of human action (low-level) can be driven from the human intention
(high-level). Based on this, we deal with Long-Term Action Anticipation task in
egocentric videos. Our framework first extracts two level of human information
over the N observed videos human actions through a Hierarchical Multi-task MLP
Mixer (H3M). Then, we condition the uncertainty of the future through an
Intention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stable
predictions of the next Z=20 actions that the observed human might perform. By
leveraging human intention as high-level information, we claim that our model
is able to anticipate more time-consistent actions in the long-term, thus
improving the results over baseline methods in EGO4D Challenge. This work
ranked first in both CVPR@2022 and ECVV@2022 EGO4D LTA Challenge by providing
more plausible anticipated sequences, improving the anticipation of nouns and
overall actions. The code is available at
https://github.com/Evm7/ego4dlta-icvae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Validation report Winner of CVPR@2022 and ECCV@2022 EGO4D LTA
  Challenge Accepted in IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV), 2023 More info
  https://sites.google.com/view/estevevallsmascaro/publications/wacv2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object
  Detection System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Yang, Jianan Liu, Tao Huang, Qing-Long Han, Gang Ma, Bing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving systems, LiDAR and radar play important roles in the
perception of the surrounding environment.LiDAR provides accurate 3D spatial
sensing information but cannot work in adverse weather like fog. On the other
hand, the radar signal can be diffracted when encountering raindrops or mist
particles thanks to its wavelength, but it suffers from large noise. Recent
state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust
detection in adverse weather. The existing works adopt convolutional neural
network architecture to extract features from each sensor data stream, then
align and aggregate the two branch features to predict object detection
results. However, these methods have low accuracy of bounding box estimations
due to a simple design of label assignment and fusion strategies. In this
paper, we propose a bird's-eye view fusion learning-based anchor box-free
object detection system, which fuses the feature derived from the radar
range-azimuth heatmap and the LiDAR point cloud to estimate the possible
objects. Different label assignment strategies have been designed to facilitate
the consistency between the classification of foreground or background anchor
points and the corresponding bounding box regressions. In addition, the
performance of the proposed object detector is further enhanced by employing a
novel interactive transformer module. The superior performance of the proposed
methods in this paper has been demonstrated using the recently published Oxford
radar robotCar dataset, showing that the average precision of our system
significantly outperforms the best state-of-the-art method by 14.4% and 20.5%
at IoU equals 0.8 in clear and foggy weather testing, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multimodal VAEs through Mutual Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Joy, Yuge Shi, Philip H. S. Torr, Tom Rainforth, Sebastian M. Schmon, N. Siddharth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal VAEs seek to model the joint distribution over heterogeneous data
(e.g.\ vision, language), whilst also capturing a shared representation across
such modalities. Prior work has typically combined information from the
modalities by reconciling idiosyncratic representations directly in the
recognition model through explicit products, mixtures, or other such
factorisations. Here we introduce a novel alternative, the MEME, that avoids
such explicit combinations by repurposing semi-supervised VAEs to combine
information between modalities implicitly through mutual supervision. This
formulation naturally allows learning from partially-observed data where some
modalities can be entirely missing -- something that most existing approaches
either cannot handle, or do so to a limited extent. We demonstrate that MEME
outperforms baselines on standard metrics across both partial and complete
observation schemes on the MNIST-SVHN (image-image) and CUB (image-text)
datasets. We also contrast the quality of the representations learnt by mutual
supervision against standard approaches and observe interesting trends in its
ability to capture relatedness between data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CgAT: Center-Guided Adversarial Training for Deep Hashing-Based
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10779v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10779v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Yinqun Lin, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. However, deep hashing models are
vulnerable to adversarial examples, making it essential to develop adversarial
defense methods for image retrieval. Existing solutions achieved limited
defense performance because of using weak adversarial samples for training and
lacking discriminative optimization objectives to learn robust features. In
this paper, we present a min-max based Center-guided Adversarial Training,
namely CgAT, to improve the robustness of deep hashing networks through worst
adversarial examples. Specifically, we first formulate the center code as a
semantically-discriminative representative of the input image content, which
preserves the semantic similarity with positive samples and dissimilarity with
negative examples. We prove that a mathematical formula can calculate the
center code immediately. After obtaining the center codes in each optimization
iteration of the deep hashing network, they are adopted to guide the
adversarial training process. On the one hand, CgAT generates the worst
adversarial examples as augmented data by maximizing the Hamming distance
between the hash codes of the adversarial examples and the center codes. On the
other hand, CgAT learns to mitigate the effects of adversarial samples by
minimizing the Hamming distance to the center codes. Extensive experiments on
the benchmark datasets demonstrate the effectiveness of our adversarial
training algorithm in defending against adversarial attacks for deep
hashing-based retrieval. Compared with the current state-of-the-art defense
method, we significantly improve the defense performance by an average of
18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SL3D: <span class="highlight-title">Self-supervised</span>-Self-labeled 3D Recognition <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16810v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16810v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Julio Cendra, Lan Ma, Jiajun Shen, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has attained remarkable success in many 3D visual recognition
tasks, including shape classification, object detection, and semantic
segmentation. However, many of these results rely on manually collecting
densely annotated real-world 3D data, which is highly time-consuming and
expensive to obtain, limiting the scalability of 3D recognition tasks. Thus, we
study unsupervised 3D recognition and propose a Self-supervised-Self-Labeled 3D
Recognition (SL3D) framework. SL3D simultaneously solves two coupled
objectives, i.e., clustering and learning feature representation to generate
pseudo-labeled data for unsupervised 3D recognition. SL3D is a generic
framework and can be applied to solve different 3D recognition tasks, including
classification, object detection, and semantic segmentation. Extensive
experiments demonstrate its effectiveness. Code is available at
https://github.com/fcendra/sl3d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has already been accepted by Neural Information Processing
  Systems (NeurIPS 2022) Workshop on Self-Supervised Learning: Theory and
  Practice</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relightable Neural Human Assets from Multi-view Gradient Illuminations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human modeling and relighting are two fundamental problems in computer vision
and graphics, where high-quality datasets can largely facilitate related
research. However, most existing human datasets only provide multi-view human
images captured under the same illumination. Although valuable for modeling
tasks, they are not readily used in relighting problems. To promote research in
both fields, in this paper, we present UltraStage, a new 3D human dataset that
contains more than 2K high-quality human assets captured under both multi-view
and multi-illumination settings. Specifically, for each example, we provide 32
surrounding views illuminated with one white light and two gradient
illuminations. In addition to regular multi-view images, gradient illuminations
help recover detailed surface normal and spatially-varying material maps,
enabling various relighting applications. Inspired by recent advances in neural
representation, we further interpret each example into a neural human asset
which allows novel view synthesis under arbitrary lighting conditions. We show
our neural human assets can achieve extremely high capture performance and are
capable of representing fine details such as facial wrinkles and cloth folds.
We also validate UltraStage in single image relighting tasks, training neural
networks with virtual relighted data from neural assets and demonstrating
realistic rendering improvements over prior arts. UltraStage will be publicly
available to the community to stimulate significant future developments in
various human modeling and rendering tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclical Self-Supervision for Semi-Supervised Ejection Fraction
  Prediction from Echocardiogram Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Dai, Xiaomeng Li, Xinpeng Ding, Kwang-Ting Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Left-ventricular ejection fraction (LVEF) is an important indicator of heart
failure. Existing methods for LVEF estimation from video require large amounts
of annotated data to achieve high performance, e.g. using 10,030 labeled
echocardiogram videos to achieve mean absolute error (MAE) of 4.10. Labeling
these videos is time-consuming however and limits potential downstream
applications to other heart diseases. This paper presents the first
semi-supervised approach for LVEF prediction. Unlike general video prediction
tasks, LVEF prediction is specifically related to changes in the left ventricle
(LV) in echocardiogram videos. By incorporating knowledge learned from
predicting LV segmentations into LVEF regression, we can provide additional
context to the model for better predictions. To this end, we propose a novel
Cyclical Self-Supervision (CSS) method for learning video-based LV
segmentation, which is motivated by the observation that the heartbeat is a
cyclical process with temporal repetition. Prediction masks from our
segmentation model can then be used as additional input for LVEF regression to
provide spatial context for the LV region. We also introduce teacher-student
distillation to distill the information from LV segmentation masks into an
end-to-end LVEF regression model that only requires video inputs. Results show
our method outperforms alternative semi-supervised methods and can achieve MAE
of 4.17, which is competitive with state-of-the-art supervised performance,
using half the number of labels. Validation on an external dataset also shows
improved generalization ability from using our method. Our code is available at
https://github.com/xmed-lab/CSS-SemiVideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>-based Cross-Modal Recipe Embeddings with Large Batch
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Junwen Chen, Keiji Yanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a cross-modal recipe retrieval framework,
Transformer-based Network for Large Batch Training (TNLBT), which is inspired
by ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).
TNLBT aims to accomplish retrieval tasks while generating images from recipe
embeddings. We apply the Hierarchical Transformer-based recipe text encoder,
the Vision Transformer~(ViT)-based recipe image encoder, and an adversarial
network architecture to enable better cross-modal embedding learning for recipe
texts and images. In addition, we use self-supervised learning to exploit the
rich information in the recipe texts having no corresponding images. Since
contrastive learning could benefit from a larger batch size according to the
recent literature on self-supervised learning, we adopt a large batch size
during training and have validated its effectiveness. In the experiments, the
proposed framework significantly outperformed the current state-of-the-art
frameworks in both cross-modal recipe retrieval and image generation tasks on
the benchmark Recipe1M. This is the first work which confirmed the
effectiveness of large batch training on cross-modal recipe embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MMM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Candidate Rectification for Weakly Supervised Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised semantic segmentation is typically inspired by class
activation maps, which serve as pseudo masks with class-discriminative regions
highlighted. Although tremendous efforts have been made to recall precise and
complete locations for each class, existing methods still commonly suffer from
the unsolicited Out-of-Candidate (OC) error predictions that not belongs to the
label candidates, which could be avoidable since the contradiction with
image-level class tags is easy to be detected. In this paper, we develop a
group ranking-based Out-of-Candidate Rectification (OCR) mechanism in a
plug-and-play fashion. Firstly, we adaptively split the semantic categories
into In-Candidate (IC) and OC groups for each OC pixel according to their prior
annotation correlation and posterior prediction correlation. Then, we derive a
differentiable rectification loss to force OC pixels to shift to the IC group.
Incorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM,
MCTformer), we can achieve remarkable performance gains on both Pascal VOC
(+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with
negligible extra training overhead, which justifies the effectiveness and
generality of our OCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Decorrelation with Potential Energy Ranking <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12194v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12194v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Pei, Jiaxi Sun, Richard Yi Da Xu, Shiming Xiang, Gaofeng Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning systems, especially the methods based on deep learning,
enjoy great success in modern computer vision tasks under experimental
settings. Generally, these classic deep learning methods are built on the
\emph{i.i.d.} assumption, supposing the training and test data are drawn from a
similar distribution independently and identically. However, the aforementioned
\emph{i.i.d.} assumption is in general unavailable in the real-world scenario,
and as a result, leads to sharp performance decay of deep learning algorithms.
Behind this, domain shift is one of the primary factors to be blamed. In order
to tackle this problem, we propose using \textbf{Po}tential \textbf{E}nergy
\textbf{R}anking (PoER) to decouple the object feature and the domain feature
(\emph{i.e.,} appearance feature) in given images, promoting the learning of
label-discriminative features while filtering out the irrelevant correlations
between the objects and the background. PoER helps the neural networks to
capture label-related features which contain the domain information first in
shallow layers and then distills the label-discriminative representations out
progressively, enforcing the neural networks to be aware of the characteristic
of objects and background which is vital to the generation of domain-invariant
features. PoER reports superior performance on domain generalization
benchmarks, improving the average top-1 accuracy by at least 1.20\% compared to
the existing methods. Moreover, we use PoER in the ECCV 2022 NICO
Challenge\footnote{https://nicochallenge.com}, achieving top place with only a
vanilla ResNet-18. The code has been made available at
https://github.com/ForeverPs/PoER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 ECCV jury award, accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Prototype Encoding for Incremental Video Highlights Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05166v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05166v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Pei, Shixiong Xu, Ye Yuan, Jiashi Feng, Xiaohui Shen, Xiaojie Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video highlights detection has been long researched as a topic in computer
vision tasks, digging the user-appealing clips out given unexposed raw video
inputs. However, in most case, the mainstream methods in this line of research
are built on the closed world assumption, where a fixed number of highlight
categories is defined properly in advance and need all training data to be
available at the same time, and as a result, leads to poor scalability with
respect to both the highlight categories and the size of the dataset. To tackle
the problem mentioned above, we propose a video highlights detector that is
able to learn incrementally, namely \textbf{G}lobal \textbf{P}rototype
\textbf{E}ncoding (GPE), capturing newly defined video highlights in the
extended dataset via their corresponding prototypes. Alongside, we present a
well annotated and costly dataset termed \emph{ByteFood}, including more than
5.1k gourmet videos belongs to four different domains which are \emph{cooking},
\emph{eating}, \emph{food material}, and \emph{presentation} respectively. To
the best of our knowledge, this is the first time the incremental learning
settings are introduced to video highlights detection, which in turn relieves
the burden of training video inputs and promotes the scalability of
conventional neural networks in proportion to both the size of the dataset and
the quantity of domains. Moreover, the proposed GPE surpasses current
incremental learning methods on \emph{ByteFood}, reporting an improvement of
1.57\% mAP at least. The code and dataset will be made available sooner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Amodal Completion: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Ao, Qiuhong Ke, Krista A. Ehinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript is under consideration at Computer Vision and Image
  Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Semi-Supervised Semantic Segmentation with Probabilistic
  Representations <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14670v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14670v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Xie, Changqi Wang, Mingkai Zheng, Minjing Dong, Shan You, Chong Fu, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in semi-supervised semantic segmentation have been
developed through contrastive learning. In prevalent pixel-wise contrastive
learning solutions, the model maps pixels to deterministic representations and
regularizes them in the latent space. However, there exist inaccurate
pseudo-labels which map the ambiguous representations of pixels to the wrong
classes due to the limited cognitive ability of the model. In this paper, we
define pixel-wise representations from a new perspective of probability theory
and propose a Probabilistic Representation Contrastive Learning (PRCL)
framework that improves representation quality by taking its probability into
consideration. Through modelling the mapping from pixels to representations as
the probability via multivariate Gaussian distributions, we can tune the
contribution of the ambiguous representations to tolerate the risk of
inaccurate pseudo-labels. Furthermore, we define prototypes in the form of
distributions, which indicates the confidence of a class, while the point
prototype cannot. Moreover, we propose to regularize the distribution variance
to enhance the reliability of representations. Taking advantage of these
benefits, high-quality feature representations can be derived in the latent
space, thereby the performance of semantic segmentation can be further
improved. We conduct sufficient experiment to evaluate PRCL on Pascal VOC and
CityScapes to demonstrate its superiority. The code is available at
https://github.com/Haoyu-Xie/PRCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointConvFormer: Revenge of the Point-based Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wu, Qi Shan, Li Fuxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PointConvFormer, a novel building block for point cloud based
deep network architectures. Inspired by generalization theory, PointConvFormer
combines ideas from point convolution, where filter weights are only based on
relative position, and Transformers which utilize feature-based attention. In
PointConvFormer, attention computed from feature difference between neighboring
points is used to modify the convolutional weights at each point. Hence,
invariances from point convolution are preserved, whereas attention helps to
select relevant points in the neighborhood. PointConvFormer is suitable for
multiple tasks that require details at the point level, such as segmentation
and scene flow estimation tasks. We experiment on both tasks with multiple
datasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our
results show that PointConvFormer substantially outperforms classic
convolutions, regular transformers, and voxelized sparse convolution approaches
with much smaller and faster networks. Visualizations show that PointConvFormer
performs similarly to convolution on flat areas, whereas the neighborhood
selection effect is stronger on object boundaries, showing that it has got the
best of both worlds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modality Neuroimage Synthesis: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06997v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06997v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Xie, Jinbao Wang, Yawen Huang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is expensive or even
impractical, since the practical difficulties may include high cost, long time
acquisition, image corruption, and privacy issues. A realistic solution is to
explore either an unsupervised learning or a semi-supervised learning to
synthesize the absent neuroimaging data. In this paper, we are the first one to
comprehensively approach cross-modality neuroimage synthesis task from
different perspectives, which include the level of the supervision (especially
for weakly-supervised and unsupervised), loss function, evaluation metrics, the
range of modality synthesis, datasets (aligned, private and public) and the
synthesis-based downstream tasks. To begin with, we highlight several opening
challenges for cross-modality neuroimage sysnthesis. Then we summarize the
architecture of cross-modality synthesis under various of supervision level. In
addition, we provide in-depth analysis of how cross-modality neuroimage
synthesis can improve the performance of different downstream tasks. Finally,
we re-evaluate the open challenges and point out the future directions for the
remaining challenges. All resources are available at
https://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Sequence Better: Time Interval Aware Data Augmentation for
  Sequential Recommendation <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Dang, Enneng Yang, Guibing Guo, Linying Jiang, Xingwei Wang, Xiaoxiao Xu, Qinghui Sun, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation is an important task to predict the next-item to
access based on a sequence of interacted items. Most existing works learn user
preference as the transition pattern from the previous item to the next one,
ignoring the time interval between these two items. However, we observe that
the time interval in a sequence may vary significantly different, and thus
result in the ineffectiveness of user modeling due to the issue of
\emph{preference drift}. In fact, we conducted an empirical study to validate
this observation, and found that a sequence with uniformly distributed time
interval (denoted as uniform sequence) is more beneficial for performance
improvement than that with greatly varying time interval. Therefore, we propose
to augment sequence data from the perspective of time interval, which is not
studied in the literature. Specifically, we design five operators (Ti-Crop,
Ti-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original
non-uniform sequence to uniform sequence with the consideration of variance of
time intervals. Then, we devise a control strategy to execute data augmentation
on item sequences in different lengths. Finally, we implement these
improvements on a state-of-the-art model CoSeRec and validate our approach on
four real datasets. The experimental results show that our approach reaches
significantly better performance than the other 11 competing methods. Our
implementation is available: https://github.com/KingGugu/TiCoSeRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, AAAI-2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connecting Permutation Equivariant Neural Networks and Partition
  Diagrams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Pearce-Crump
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how the Schur-Weyl duality that exists between the partition algebra
and the symmetric group results in a stronger theoretical foundation for
characterising all of the possible permutation equivariant neural networks
whose layers are some tensor power of the permutation representation $M_n$ of
the symmetric group $S_n$. In doing so, we unify two separate bodies of
literature, and we correct some of the major results that are now widely quoted
by the machine learning community. In particular, we find a basis of matrices
for the learnable, linear, permutation equivariant layer functions between such
tensor power spaces in the standard basis of $M_n$ by using an elegant
graphical representation of a basis of set partitions for the partition algebra
and its related vector spaces. Also, we show how we can calculate the number of
weights that must appear in these layer functions by looking at certain paths
through the McKay quiver for $M_n$. Finally, we describe how our approach
generalises to the construction of neural networks that are equivariant to
local symmetries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Conditionally Invariant Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, Arthur Gretton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Conditional Independence Regression CovariancE (CIRCE), a
measure of conditional independence for multivariate continuous-valued
variables. CIRCE applies as a regularizer in settings where we wish to learn
neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being
conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are
assumed to be continuous-valued but relatively low dimensional, whereas $X$ and
its features may be complex and high dimensional. Relevant settings include
domain-invariant learning, fairness, and causal learning. The procedure
requires just a single ridge regression from $Y$ to kernelized features of $Z$,
which can be done in advance. It is then only necessary to enforce independence
of $\varphi(X)$ from residuals of this regression, which is possible with
attractive estimation properties and consistency guarantees. By contrast,
earlier measures of conditional feature dependence require multiple regressions
for each step of feature learning, resulting in more severe bias and variance,
and greater computational cost. When sufficiently rich features are used, we
establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z
\mid Y$. In experiments, we show superior performance to previous methods on
challenging benchmarks, including learning conditionally invariant image
features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brauer's Group Equivariant Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Pearce-Crump
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a full characterisation of all of the possible group equivariant
neural networks whose layers are some tensor power of $\mathbb{R}^{n}$ for
three symmetry groups that are missing from the machine learning literature:
$O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and
$Sp(n)$, the symplectic group. In particular, we find a spanning set of
matrices for the learnable, linear, equivariant layer functions between such
tensor power spaces in the standard basis of $\mathbb{R}^{n}$ when the group is
$O(n)$ or $SO(n)$, and in the symplectic basis of $\mathbb{R}^{n}$ when the
group is $Sp(n)$. The neural networks that we characterise are simple to
implement since our method circumvents the typical requirement when building
group equivariant neural networks of having to decompose the tensor power
spaces of $\mathbb{R}^{n}$ into irreducible representations. We also describe
how our approach generalises to the construction of neural networks that are
equivariant to local symmetries.
  The theoretical background for our results comes from the Schur-Weyl
dualities that were established by Brauer in his 1937 paper "On Algebras Which
are Connected with the Semisimple Continuous Groups" for each of the three
groups in question. We suggest that Schur-Weyl duality is a powerful
mathematical concept that could be used to understand the structure of neural
networks that are equivariant to groups beyond those considered in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development of A Real-time POCUS Image Quality Assessment and
  Acquisition Guidance System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenge Jia, Yiyu Shi, Jingtong Hu, Lei Yang, Benjamin Nti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-of-care ultrasound (POCUS) is one of the most commonly applied tools
for cardiac function imaging in the clinical routine of the emergency
department and pediatric intensive care unit. The prior studies demonstrate
that AI-assisted software can guide nurses or novices without prior sonography
experience to acquire POCUS by recognizing the interest region, assessing image
quality, and providing instructions. However, these AI algorithms cannot simply
replace the role of skilled sonographers in acquiring diagnostic-quality POCUS.
Unlike chest X-ray, CT, and MRI, which have standardized imaging protocols,
POCUS can be acquired with high inter-observer variability. Though being with
variability, they are usually all clinically acceptable and interpretable. In
challenging clinical environments, sonographers employ novel heuristics to
acquire POCUS in complex scenarios. To help novice learners to expedite the
training process while reducing the dependency on experienced sonographers in
the curriculum implementation, We will develop a framework to perform real-time
AI-assisted quality assessment and probe position guidance to provide training
process for novice learners with less manual intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POTATO: The Portable Text Annotation Tool <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Pei, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Jackson Sargent, Apostolos Dedeloudis, David Jurgens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present POTATO, the Portable text annotation tool, a free, fully
open-sourced annotation system that 1) supports labeling many types of text and
multimodal data; 2) offers easy-to-configure features to maximize the
productivity of both deployers and annotators (convenient templates for common
ML/NLP tasks, active learning, keypress shortcuts, keyword highlights,
tooltips); and 3) supports a high degree of customization (editable UI,
inserting pre-screening questions, attention and qualification tests).
Experiments over two annotation tasks suggest that POTATO improves labeling
speed through its specially-designed productivity features, especially for long
documents and complex tasks. POTATO is available at
https://github.com/davidjurgens/potato and will continue to be updated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022 DEMO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting large language models has enabled significant recent progress in
multi-step reasoning over text. However, when applied to text generation from
semi-structured data (e.g., graphs or tables), these methods typically suffer
from low semantic coverage, hallucination, and logical inconsistency. We
propose MURMUR, a neuro-symbolic modular approach to text generation from
semi-structured data with multi-step reasoning. MURMUR is a best-first search
method that generates reasoning paths using: (1) neural and symbolic modules
with specific linguistic and logical skills, (2) a grammar whose production
rules define valid compositions of modules, and (3) value functions that assess
the quality of each reasoning step. We conduct experiments on two diverse
data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in
their data representations (graphs and tables) and span multiple linguistic and
logical skills. MURMUR obtains significant improvements over recent few-shot
baselines like direct prompting and chain-of-thought prompting, while also
achieving comparable performance to fine-tuned GPT-2 on out-of-domain data.
Moreover, human evaluation shows that MURMUR generates highly faithful and
correct reasoning paths that lead to 26% more logically consistent summaries on
LogicNLG, compared to direct prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (9 figures, 18 tables)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning Visual-Tactile Precision Grasps via Complementary Use of Vision
  and Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Matak, Tucker Hermans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliably planning fingertip grasps for multi-fingered hands lies as a key
challenge for many tasks including tool use, insertion, and dexterous in-hand
manipulation. This task becomes even more difficult when the robot lacks an
accurate model of the object to be grasped. Tactile sensing offers a promising
approach to account for uncertainties in object shape. However, current robotic
hands tend to lack full tactile coverage. As such, a problem arises of how to
plan and execute grasps for multi-fingered hands such that contact is made with
the area covered by the tactile sensors. To address this issue, we propose an
approach to grasp planning that explicitly reasons about where the fingertips
should contact the estimated object surface while maximizing the probability of
grasp success. Key to our method's success is the use of visual surface
estimation for initial planning to encode the contact constraint. The robot
then executes this plan using a tactile-feedback controller that enables the
robot to adapt to online estimates of the object's surface to correct for
errors in the initial plan. Importantly, the robot never explicitly integrates
object pose or surface estimates between visual and tactile sensing, instead it
uses the two modalities in complementary ways. Vision guides the robots motion
prior to contact; touch updates the plan when contact occurs differently than
predicted from vision. We show that our method successfully synthesises and
executes precision grasps for previously unseen objects using surface estimates
from a single camera view. Further, our approach outperforms a state of the art
multi-fingered grasp planner, while also beating several baselines we propose.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Fairness for Neural Network Models using Formal Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgian Borca-Tasciuc, Xingzhi Guo, Stanley Bak, Steven Skiena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are increasingly deployed for critical
decision-making tasks, making it important to verify that they do not contain
gender or racial biases picked up from training data. Typical approaches to
achieve fairness revolve around efforts to clean or curate training data, with
post-hoc statistical evaluation of the fairness of the model on evaluation
data. In contrast, we propose techniques to \emph{prove} fairness using
recently developed formal methods that verify properties of neural network
models.Beyond the strength of guarantee implied by a formal proof, our methods
have the advantage that we do not need explicit training or evaluation data
(which is often proprietary) in order to analyze a given trained model. In
experiments on two familiar datasets in the fairness literature (COMPAS and
ADULTS), we show that through proper training, we can reduce unfairness by an
average of 65.4\% at a cost of less than 1\% in AUC score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biomedical image analysis competitions: The state of current
  participation practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Eisenmann, Annika Reinke, Vivienn Weru, Minu Dietlinde Tizabi, Fabian Isensee, Tim J. Adler, Patrick Godau, Veronika Cheplygina, Michal Kozubek, Sharib Ali, Anubha Gupta, Jan Kybic, Alison Noble, Carlos Ortiz de Solórzano, Samiksha Pachade, Caroline Petitjean, Daniel Sage, Donglai Wei, Elizabeth Wilden, Deepak Alapatt, Vincent Andrearczyk, Ujjwal Baid, Spyridon Bakas, Niranjan Balu, Sophia Bano, Vivek Singh Bawa, Jorge Bernal, Sebastian Bodenstedt, Alessandro Casella, Jinwook Choi, Olivier Commowick, Marie Daum, Adrien Depeursinge, Reuben Dorent, Jan Egger, Hannah Eichhorn, Sandy Engelhardt, Melanie Ganz, Gabriel Girard, Lasse Hansen, Mattias Heinrich, Nicholas Heller, Alessa Hering, Arnaud Huaulmé, Hyunjeong Kim, Bennett Landman, Hongwei Bran Li, Jianning Li, Jun Ma, Anne Martel, Carlos Martín-Isla, Bjoern Menze, Chinedu Innocent Nwoye, Valentin Oreiller, Nicolas Padoy, Sarthak Pati, Kelly Payette, Carole Sudre, Kimberlin van Wijnen, Armine Vardazaryan, Tom Vercauteren, Martin Wagner, Chuanbo Wang, Moi Hoon Yap, Zeyun Yu, Chun Yuan, Maximilian Zenk, Aneeq Zia, David Zimmerer, Rina Bao, Chanyeol Choi, Andrew Cohen, Oleh Dzyubachyk, Adrian Galdran, Tianyuan Gan, Tianqi Guo, Pradyumna Gupta, Mahmood Haithami, Edward Ho, Ikbeom Jang, Zhili Li, Zhengbo Luo, Filip Lux, Sokratis Makrogiannis, Dominik Müller, Young-tack Oh, Subeen Pang, Constantin Pape, Gorkem Polat, Charlotte Rosalie Reed, Kanghyun Ryu, Tim Scherr, Vajira Thambawita, Haoyu Wang, Xinliang Wang, Kele Xu, Hung Yeh, Doyeob Yeo, Yixuan Yuan, Yan Zeng, Xin Zhao, Julian Abbing, Jannes Adam, Nagesh Adluru, Niklas Agethen, Salman Ahmed, Yasmina Al Khalil, Mireia Alenyà, Esa Alhoniemi, Chengyang An, Talha Anwar, Tewodros Weldebirhan Arega, Netanell Avisdris, Dogu Baran Aydogan, Yingbin Bai, Maria Baldeon Calisto, Berke Doga Basaran, Marcel Beetz, Cheng Bian, Hao Bian, Kevin Blansit, Louise Bloch, Robert Bohnsack, Sara Bosticardo, Jack Breen, Mikael Brudfors, Raphael Brüngel, Mariano Cabezas, Alberto Cacciola, Zhiwei Chen, Yucong Chen, Daniel Tianming Chen, Minjeong Cho, Min-Kook Choi, Chuantao Xie Chuantao Xie, Dana Cobzas, Julien Cohen-Adad, Jorge Corral Acero, Sujit Kumar Das, Marcela de Oliveira, Hanqiu Deng, Guiming Dong, Lars Doorenbos, Cory Efird, Di Fan, Mehdi Fatan Serj, Alexandre Fenneteau, Lucas Fidon, Patryk Filipiak, René Finzel, Nuno R. Freitas, Christoph M. Friedrich, Mitchell Fulton, Finn Gaida, Francesco Galati, Christoforos Galazis, Chang Hee Gan, Zheyao Gao, Shengbo Gao, Matej Gazda, Beerend Gerats, Neil Getty, Adam Gibicar, Ryan Gifford, Sajan Gohil, Maria Grammatikopoulou, Daniel Grzech, Orhun Güley, Timo Günnemann, Chunxu Guo, Sylvain Guy, Heonjin Ha, Luyi Han, Il Song Han, Ali Hatamizadeh, Tian He, Jimin Heo, Sebastian Hitziger, SeulGi Hong, SeungBum Hong, Rian Huang, Ziyan Huang, Markus Huellebrand, Stephan Huschauer, Mustaffa Hussain, Tomoo Inubushi, Ece Isik Polat, Mojtaba Jafaritadi, SeongHun Jeong, Bailiang Jian, Yuanhong Jiang, Zhifan Jiang, Yueming Jin, Smriti Joshi, Abdolrahim Kadkhodamohammadi, Reda Abdellah Kamraoui, Inha Kang, Junghwa Kang, Davood Karimi, April Khademi, Muhammad Irfan Khan, Suleiman A. Khan, Rishab Khantwal, Kwang-Ju Kim, Timothy Kline, Satoshi Kondo, Elina Kontio, Adrian Krenzer, Artem Kroviakov, Hugo Kuijf, Satyadwyoom Kumar, Francesco La Rosa, Abhi Lad, Doohee Lee, Minho Lee, Chiara Lena, Hao Li, Ling Li, Xingyu Li, Fuyuan Liao, KuanLun Liao, Arlindo Limede Oliveira, Chaonan Lin, Shan Lin, Akis Linardos, Marius George Linguraru, Han Liu, Tao Liu, Di Liu, Yanling Liu, João Lourenço-Silva, Jingpei Lu, Jiangshan Lu, Imanol Luengo, Christina B. Lund, Huan Minh Luu, Yi Lv, Yi Lv, Uzay Macar, Leon Maechler, Sina Mansour L., Kenji Marshall, Moona Mazher, Richard McKinley, Alfonso Medela, Felix Meissen, Mingyuan Meng, Dylan Miller, Seyed Hossein Mirjahanmardi, Arnab Mishra, Samir Mitha, Hassan Mohy-ud-Din, Tony Chi Wing Mok, Gowtham Krishnan Murugesan, Enamundram Naga Karthik, Sahil Nalawade, Jakub Nalepa, Mohamed Naser, Ramin Nateghi, Hammad Naveed, Quang-Minh Nguyen, Cuong Nguyen Quoc, Brennan Nichyporuk, Bruno Oliveira, David Owen, Jimut Bahan Pal, Junwen Pan, Wentao Pan, Winnie Pang, Bogyu Park, Vivek Pawar, Kamlesh Pawar, Michael Peven, Lena Philipp, Tomasz Pieciak, Szymon Plotka, Marcel Plutat, Fattaneh Pourakpour, Domen Preložnik, Kumaradevan Punithakumar, Abdul Qayyum, Sandro Queirós, Arman Rahmim, Salar Razavi, Jintao Ren, Mina Rezaei, Jonathan Adam Rico, ZunHyan Rieu, Markus Rink, Johannes Roth, Yusely Ruiz-Gonzalez, Numan Saeed, Anindo Saha, Mostafa Salem, Ricardo Sanchez-Matilla, Kurt Schilling, Wei Shao, Zhiqiang Shen, Ruize Shi, Pengcheng Shi, Daniel Sobotka, Théodore Soulier, Bella Specktor Fadida, Danail Stoyanov, Timothy Sum Hon Mun, Xiaowu Sun, Rong Tao, Franz Thaler, Antoine Théberge, Felix Thielke, Helena Torres, Kareem A. Wahid, Jiacheng Wang, YiFei Wang, Wei Wang, Xiong Wang, Jianhui Wen, Ning Wen, Marek Wodzinski, Ye Wu, Fangfang Xia, Tianqi Xiang, Chen Xiaofei, Lizhan Xu, Tingting Xue, Yuxuan Yang, Lin Yang, Kai Yao, Huifeng Yao, Amirsaeed Yazdani, Michael Yip, Hwanseung Yoo, Fereshteh Yousefirizi, Shunkai Yu, Lei Yu, Jonathan Zamora, Ramy Ashraf Zeineldin, Dewen Zeng, Jianpeng Zhang, Bokai Zhang, Jiapeng Zhang, Fan Zhang, Huahong Zhang, Zhongchen Zhao, Zixuan Zhao, Jiachen Zhao, Can Zhao, Qingshuo Zheng, Yuheng Zhi, Ziqi Zhou, Baosheng Zou, Klaus Maier-Hein, Paul F. Jäger, Annette Kopp-Schneider, Lena Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of international benchmarking competitions is steadily increasing
in various fields of machine learning (ML) research and practice. So far,
however, little is known about the common practice as well as bottlenecks faced
by the community in tackling the research questions posed. To shed light on the
status quo of algorithm development in the specific field of biomedical imaging
analysis, we designed an international survey that was issued to all
participants of challenges conducted in conjunction with the IEEE ISBI 2021 and
MICCAI 2021 conferences (80 competitions in total). The survey covered
participants' expertise and working environments, their chosen strategies, as
well as algorithm characteristics. A median of 72% challenge participants took
part in the survey. According to our results, knowledge exchange was the
primary incentive (70%) for participation, while the reception of prize money
played only a minor role (16%). While a median of 80 working hours was spent on
method development, a large portion of participants stated that they did not
have enough time for method development (32%). 25% perceived the infrastructure
to be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of
these, 84% were based on standard architectures. 43% of the respondents
reported that the data samples (e.g., images) were too large to be processed at
once. This was most commonly addressed by patch-based training (69%),
downsampling (37%), and solving 3D analysis tasks as a series of 2D tasks.
K-fold cross-validation on the training set was performed by only 37% of the
participants and only 50% of the participants performed ensembling based on
multiple identical models (61%) or heterogeneous models (39%). 48% of the
respondents applied postprocessing steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An automated parameter domain decomposition approach for gravitational
  wave surrogates using hp-greedy refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franco Cerino, J. Andrés Diaz-Pace, Manuel Tiglio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce hp-greedy, a refinement approach for building gravitational wave
surrogates as an extension of the standard reduced basis framework. Our
proposal is data-driven, with a domain decomposition of the parameter space,
local reduced basis, and a binary tree as the resulting structure, which are
obtained in an automated way. When compared to the standard global reduced
basis approach, the numerical simulations of our proposal show three salient
features: i) representations of lower dimension with no loss of accuracy, ii) a
significantly higher accuracy for a fixed maximum dimensionality of the basis,
in some cases by orders of magnitude, and iii) results that depend on the
reduced basis seed choice used by the refinement algorithm. We first illustrate
the key parts of our approach with a toy model and then present a more
realistic use case of gravitational waves emitted by the collision of two
spinning, non-precessing black holes. We discuss performance aspects of
hp-greedy, such as overfitting with respect to the depth of the tree structure,
and other hyperparameter dependences. As two direct applications of the
proposed hp-greedy refinement, we envision: i) a further acceleration of
statistical inference, which might be complementary to focused reduced-order
quadratures, and ii) the search of gravitational waves through clustering and
nearest neighbors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, code available from authors upon request</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating truncation effects of quantum bosonic systems using sampling
  algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanori Hanada, Junyu Liu, Enrico Rinaldi, Masaki Tezuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To simulate bosons on a qubit- or qudit-based quantum computer, one has to
regularize the theory by truncating infinite-dimensional local Hilbert spaces
to finite dimensions. In the search for practical quantum applications, it is
important to know how big the truncation errors can be. In general, it is not
easy to estimate errors unless we have a good quantum computer. In this paper
we show that traditional sampling methods on classical devices, specifically
Markov Chain Monte Carlo, can address this issue with a reasonable amount of
computational resources available today. As a demonstration, we apply this idea
to the scalar field theory on a two-dimensional lattice, with a size that goes
beyond what is achievable using exact diagonalization methods. This method can
be used to estimate the resources needed for realistic quantum simulations of
bosonic theories, and also, to check the validity of the results of the
corresponding quantum simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnable Commutative Monoids for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euan Ong, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have been shown to be highly sensitive to the
choice of aggregation function. While summing over a node's neighbours can
approximate any permutation-invariant function over discrete inputs,
Cohen-Karlik et al. [2020] proved there are set-aggregation problems for which
summing cannot generalise to unbounded inputs, proposing recurrent neural
networks regularised towards permutation-invariance as a more expressive
aggregator. We show that these results carry over to the graph domain: GNNs
equipped with recurrent aggregators are competitive with state-of-the-art
permutation-invariant aggregators, on both synthetic benchmarks and real-world
problems. However, despite the benefits of recurrent aggregators, their $O(V)$
depth makes them both difficult to parallelise and harder to train on large
graphs. Inspired by the observation that a well-behaved aggregator for a GNN is
a commutative monoid over its latent space, we propose a framework for
constructing learnable, commutative, associative binary operators. And with
this, we construct an aggregator of $O(\log V)$ depth, yielding exponential
improvements for both parallelism and dependency length while achieving
performance competitive with recurrent aggregators. Based on our empirical
observations, our proposed learnable commutative monoid (LCM) aggregator
represents a favourable tradeoff between efficient and expressive aggregators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the proceedings of the First Learning on Graphs
  Conference (LoG 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Not Trust a Model Because It is Confident: Uncovering and
  Characterizing Unknown Unknowns to Student Success Predictors in Online-Based
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberta Galici, Tanja Käser, Gianni Fenu, Mirko Marras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Student success models might be prone to develop weak spots, i.e., examples
hard to accurately classify due to insufficient representation during model
creation. This weakness is one of the main factors undermining users' trust,
since model predictions could for instance lead an instructor to not intervene
on a student in need. In this paper, we unveil the need of detecting and
characterizing unknown unknowns in student success prediction in order to
better understand when models may fail. Unknown unknowns include the students
for which the model is highly confident in its predictions, but is actually
wrong. Therefore, we cannot solely rely on the model's confidence when
evaluating the predictions quality. We first introduce a framework for the
identification and characterization of unknown unknowns. We then assess its
informativeness on log data collected from flipped courses and online courses
using quantitative analyses and interviews with instructors. Our results show
that unknown unknowns are a critical issue in this domain and that our
framework can be applied to support their detection. The source code is
available at https://github.com/epfl-ml4ed/unknown-unknowns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at the International Conference on Learning
  Analytics & Knowledge (LAK23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Explanation Constraints for Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Wicker, Juyeon Heo, Luca Costabello, Adrian Weller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-hoc explanation methods are used with the intent of providing insights
about neural networks and are sometimes said to help engender trust in their
outputs. However, popular explanations methods have been found to be fragile to
minor perturbations of input features or model parameters. Relying on
constraint relaxation techniques from non-convex optimization, we develop a
method that upper-bounds the largest change an adversary can make to a
gradient-based explanation via bounded manipulation of either the input
features or model parameters. By propagating a compact input or parameter set
as symbolic intervals through the forwards and backwards computations of the
neural network we can formally certify the robustness of gradient-based
explanations. Our bounds are differentiable, hence we can incorporate provable
explanation robustness into neural network training. Empirically, our method
surpasses the robustness provided by previous heuristic approaches. We find
that our training method is the only method able to learn neural networks with
certificates of explanation robustness across all six datasets tested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning with Flexible Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqiang Wang, Jake Perazzone, Mingyue Ji, Kevin S. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables distributed model training from local data
collected by users. In distributed systems with constrained resources and
potentially high dynamics, e.g., mobile edge networks, the efficiency of FL is
an important problem. Existing works have separately considered different
configurations to make FL more efficient, such as infrequent transmission of
model updates, client subsampling, and compression of update vectors. However,
an important open problem is how to jointly apply and tune these control knobs
in a single FL algorithm, to achieve the best performance by allowing a high
degree of freedom in control decisions. In this paper, we address this problem
and propose FlexFL - an FL algorithm with multiple options that can be adjusted
flexibly. Our FlexFL algorithm allows both arbitrary rates of local computation
at clients and arbitrary amounts of communication between clients and the
server, making both the computation and communication resource consumption
adjustable. We prove a convergence upper bound of this algorithm. Based on this
result, we further propose a stochastic optimization formulation and algorithm
to determine the control decisions that (approximately) minimize the
convergence bound, while conforming to constraints related to resource
consumption. The advantage of our approach is also verified using experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE INFOCOM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Huang, Hongwei Li, Gastao Cruz, Jiazhen Pan, Daniel Rueckert, Kerstin Hammernik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel image reconstruction framework that directly
learns a neural implicit representation in k-space for ECG-triggered
non-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods
bin acquired data from neighboring time points to reconstruct one phase of the
cardiac motion, our framework allows for a continuous, binning-free, and
subject-specific k-space representation.We assign a unique coordinate that
consists of time, coil index, and frequency domain location to each sampled
k-space point. We then learn the subject-specific mapping from these unique
coordinates to k-space intensities using a multi-layer perceptron with
frequency domain regularization. During inference, we obtain a complete k-space
for Cartesian coordinates and an arbitrary temporal resolution. A simple
inverse Fourier transform recovers the image, eliminating the need for density
compensation and costly non-uniform Fourier transforms for non-Cartesian data.
This novel imaging framework was tested on 42 radially sampled datasets from 6
subjects. The proposed method outperforms other techniques qualitatively and
quantitatively using data from four and one heartbeat(s) and 30 cardiac phases.
Our results for one heartbeat reconstruction of 50 cardiac phases show improved
artifact removal and spatio-temporal resolution, leveraging the potential for
real-time CMR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Rule-Based Decoding: Revisiting Syntactic Rules in Neural
  Constituency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Shi, Zhicheng Wang, Liyin Xiao, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent studies on neural constituency parsing focus on encoder
structures, while few developments are devoted to decoders. Previous research
has demonstrated that probabilistic statistical methods based on syntactic
rules are particularly effective in constituency parsing, whereas syntactic
rules are not used during the training of neural models in prior work probably
due to their enormous computation requirements. In this paper, we first
implement a fast CKY decoding procedure harnessing GPU acceleration, based on
which we further derive a syntactic rule-based (rule-constrained) CKY decoding.
In the experiments, our method obtains 95.89 and 92.52 F1 on the datasets of
PTB and CTB respectively, which shows significant improvements compared with
previous approaches. Besides, our parser achieves strong and competitive
cross-domain performance in zero-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Generative Adversarial Network for keystroke presentation
  attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idoia Eizaguirre-Peral, Lander Segurola-Gil, Francesco Zola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cybersecurity is a crucial step in data protection to ensure user security
and personal data privacy. In this sense, many companies have started to
control and restrict access to their data using authentication systems.
However, these traditional authentication methods, are not enough for ensuring
data protection, and for this reason, behavioral biometrics have gained
importance. Despite their promising results and the wide range of applications,
biometric systems have shown to be vulnerable to malicious attacks, such as
Presentation Attacks. For this reason, in this work, we propose to study a new
approach aiming to deploy a presentation attack towards a keystroke
authentication system. Our idea is to use Conditional Generative Adversarial
Networks (cGAN) for generating synthetic keystroke data that can be used for
impersonating an authorized user. These synthetic data are generated following
two different real use cases, one in which the order of the typed words is
known (ordered dynamic) and the other in which this order is unknown
(no-ordered dynamic). Finally, both keystroke dynamics (ordered and no-ordered)
are validated using an external keystroke authentication system. Results
indicate that the cGAN can effectively generate keystroke dynamics patterns
that can be used for deceiving keystroke authentication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Label Learning: Improving Background Class Representations in
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Konstantinos Kamnitsas, Cheng Ouyang, Chen Chen, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background samples provide key contextual information for segmenting regions
of interest (ROIs). However, they always cover a diverse set of structures,
causing difficulties for the segmentation model to learn good decision
boundaries with high sensitivity and precision. The issue concerns the highly
heterogeneous nature of the background class, resulting in multi-modal
distributions. Empirically, we find that neural networks trained with
heterogeneous background struggle to map the corresponding contextual samples
to compact clusters in feature space. As a result, the distribution over
background logit activations may shift across the decision boundary, leading to
systematic over-segmentation across different datasets and tasks. In this
study, we propose context label learning (CoLab) to improve the context
representations by decomposing the background class into several subclasses.
Specifically, we train an auxiliary network as a task generator, along with the
primary segmentation model, to automatically generate context labels that
positively affect the ROI segmentation accuracy. Extensive experiments are
conducted on several challenging segmentation tasks and datasets. The results
demonstrate that CoLab can guide the segmentation model to map the logits of
background samples away from the decision boundary, resulting in significantly
improved segmentation accuracy. Code is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Provisionally accepted to IEEE Transactions on Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake it till you make it: Learning(s) from a synthetic ImageNet clone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large-scale image generation models such as Stable Diffusion have
exhibited an impressive ability to generate fairly realistic images starting
from a very simple text prompt. Could such models render real images obsolete
for training image prediction models? In this paper, we answer part of this
provocative question by questioning the need for real images when training
models for ImageNet classification. More precisely, provided only with the
class names that have been used to build the dataset, we explore the ability of
Stable Diffusion to generate synthetic clones of ImageNet and measure how
useful they are for training classification models from scratch. We show that
with minimal and class-agnostic prompt engineering those ImageNet clones we
denote as ImageNet-SD are able to close a large part of the gap between models
produced by synthetic images and models trained with real images for the
several standard classification benchmarks that we consider in this study. More
importantly, we show that models trained on synthetic images exhibit strong
generalization properties and perform on par with models trained on real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Person Detection Using an Ultra Low-resolution Thermal Imager on a
  Low-cost MCU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Vandersteegen, Wouter Reusen, Kristof Van Beeck, Toon Goedemé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting persons in images or video with neural networks is a well-studied
subject in literature. However, such works usually assume the availability of a
camera of decent resolution and a high-performance processor or GPU to run the
detection algorithm, which significantly increases the cost of a complete
detection system. However, many applications require low-cost solutions,
composed of cheap sensors and simple microcontrollers. In this paper, we
demonstrate that even on such hardware we are not condemned to simple classic
image processing techniques. We propose a novel ultra-lightweight CNN-based
person detector that processes thermal video from a low-cost 32x24 pixel static
imager. Trained and compressed on our own recorded dataset, our model achieves
up to 91.62% accuracy (F1-score), has less than 10k parameters, and runs as
fast as 87ms and 46ms on low-cost microcontrollers STM32F407 and STM32F746,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Small Language Models to Reason 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of thought prompting successfully improves the reasoning capabilities
of large language models, achieving state of the art results on a range of
datasets. However, these reasoning capabilities only appear to emerge in models
with a size of over 100 billion parameters. In this paper, we explore the
transfer of such reasoning capabilities to models with less than 100 billion
parameters via knowledge distillation. Specifically, we finetune a student
model on the chain of thought outputs generated by a larger teacher model. Our
experiments show that the proposed method improves task performance across
arithmetic, commonsense and symbolic reasoning datasets. For example, the
accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on
PaLM-540B generated chains of thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoder Tuning: Efficient Language Understanding as Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganqu Cui, Wentao Li, Ning Ding, Longtao Huang, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the evergrowing sizes of pre-trained models (PTMs), it has been an
emerging practice to only provide the inference APIs for users, namely
model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,
most current approaches focus on the input side, seeking for powerful prompts
to stimulate models for correct answers. However, we argue that input-side
adaptation could be arduous due to the lack of gradient signals and they
usually require thousands of API queries, resulting in high computation and
time costs. In light of this, we present Decoder Tuning (DecT), which in
contrast optimizes task-specific decoder networks on the output side.
Specifically, DecT first extracts prompt-stimulated output scores for initial
predictions. On top of that, we train an additional decoder network on the
output representations to incorporate posterior data knowledge. By
gradient-based optimization, DecT can be trained within several seconds and
requires only one PTM query per sample. Empirically, we conduct extensive
natural language understanding experiments and show that DecT significantly
outperforms state-of-the-art algorithms with a $10^3\times$ speed-up.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiFe-net: Data-driven Modelling of Time-dependent Temperatures and
  Charging Statistics Of Tesla's LiFePo4 EV Battery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeyhun Rustamov, Luisa Fennert, Nico Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modelling the temperature of Electric Vehicle (EV) batteries is a fundamental
task of EV manufacturing. Extreme temperatures in the battery packs can affect
their longevity and power output. Although theoretical models exist for
describing heat transfer in battery packs, they are computationally expensive
to simulate. Furthermore, it is difficult to acquire data measurements from
within the battery cell. In this work, we propose a data-driven surrogate model
(LiFe-net) that uses readily accessible driving diagnostics for battery
temperature estimation to overcome these limitations. This model incorporates
Neural Operators with a traditional numerical integration scheme to estimate
the temperature evolution. Moreover, we propose two further variations of the
baseline model: LiFe-net trained with a regulariser and LiFe-net trained with
time stability loss. We compared these models in terms of generalization error
on test data. The results showed that LiFe-net trained with time stability loss
outperforms the other two models and can estimate the temperature evolution on
unseen data with a relative error of 2.77 % on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Sequence Length Learning Impacts on <span class="highlight-title">Transformer</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Thomas Baillargeon, Luc Lamontagne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification algorithms using Transformer architectures can be affected by
the sequence length learning problem whenever observations from different
classes have a different length distribution. This problem brings models to use
sequence length as a predictive feature instead of relying on important textual
information. Even if most public datasets are not affected by this problem,
privately corpora for fields such as medicine and insurance may carry this data
bias. This poses challenges throughout the value chain given their usage in a
machine learning application. In this paper, we empirically expose this problem
and present approaches to minimize its impacts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 content - 2 appendix, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeneFormer: Learned Gene Compression using <span class="highlight-title">Transformer</span>-based Context
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanbei Cui, Yu Liao, Tongda Xu, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of gene sequencing technology, an explosive growth of
gene data has been witnessed. And the storage of gene data has become an
important issue. Traditional gene data compression methods rely on general
software like G-zip, which fails to utilize the interrelation of nucleotide
sequence. Recently, many researchers begin to investigate deep learning based
gene data compression method. In this paper, we propose a transformer-based
gene compression method named GeneFormer. Specifically, we first introduce a
modified transformer structure to fully explore the nucleotide sequence
dependency. Then, we propose fixed-length parallel grouping to accelerate the
decoding speed of our autoregressive model. Experimental results on real-world
datasets show that our method saves 29.7% bit rate compared with the
state-of-the-art method, and the decoding speed is significantly faster than
all existing learning-based gene compression methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Dropout: Revisiting the Role of Augmentations in Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Tamkin, Margalit Glasgow, Xiluo He, Noah Goodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What role do augmentations play in contrastive learning? Recent work suggests
that good augmentations are label-preserving with respect to a specific
downstream task. We complicate this picture by showing that label-destroying
augmentations can be useful in the foundation model setting, where the goal is
to learn diverse, general-purpose representations for multiple downstream
tasks. We perform contrastive learning experiments on a range of image and
audio datasets with multiple downstream tasks (e.g. for digits superimposed on
photographs, predicting the class of one vs. the other). We find that Viewmaker
Networks, a recently proposed model for learning augmentations for contrastive
learning, produce label-destroying augmentations that stochastically destroy
features needed for different downstream tasks. These augmentations are
interpretable (e.g. altering shapes, digits, or letters added to images) and
surprisingly often result in better performance compared to expert-designed
augmentations, despite not preserving label information. To support our
empirical results, we theoretically analyze a simple contrastive learning
setting with a linear model. In this setting, label-destroying augmentations
are crucial for preventing one set of features from suppressing the learning of
features useful for another downstream task. Our results highlight the need for
analyzing the interaction between multiple downstream tasks when trying to
explain the success of foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shapley variable importance cloud for machine learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Ning, Mingxuan Liu, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current practice in interpretable machine learning often focuses on
explaining the final model trained from data, e.g., by using the Shapley
additive explanations (SHAP) method. The recently developed Shapley variable
importance cloud (ShapleyVIC) extends the current practice to a group of
"nearly optimal models" to provide comprehensive and robust variable importance
assessments, with estimated uncertainty intervals for a more complete
understanding of variable contributions to predictions. ShapleyVIC was
initially developed for applications with traditional regression models, and
the benefits of ShapleyVIC inference have been demonstrated in real-life
prediction tasks using the logistic regression model. However, as a
model-agnostic approach, ShapleyVIC application is not limited to such
scenarios. In this work, we extend ShapleyVIC implementation for machine
learning models to enable wider applications, and propose it as a useful
complement to the current SHAP analysis to enable more trustworthy applications
of these black-box models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swing Distillation: A Privacy-Preserving Knowledge Distillation
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhuo Li, Xinwei Wu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has been widely used for model compression and
knowledge transfer. Typically, a big teacher model trained on sufficient data
transfers knowledge to a small student model. However, despite the success of
KD, little effort has been made to study whether KD leaks the training data of
the teacher model. In this paper, we experimentally reveal that KD suffers from
the risk of privacy leakage. To alleviate this issue, we propose a novel
knowledge distillation method, swing distillation, which can effectively
protect the private information of the teacher model from flowing to the
student model. In our framework, the temperature coefficient is dynamically and
adaptively adjusted according to the degree of private information contained in
the data, rather than a predefined constant hyperparameter. It assigns
different temperatures to tokens according to the likelihood that a token in a
position contains private information. In addition, we inject noise into soft
targets provided to the student model, in order to avoid unshielded knowledge
transfer. Experiments on multiple datasets and tasks demonstrate that the
proposed swing distillation can significantly reduce (by over 80% in terms of
canary exposure) the risk of privacy leakage in comparison to KD with
competitive or better performance. Furthermore, swing distillation is robust
against the increasing privacy budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplitGP: Achieving Both Generalization and Personalization in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Jun Han, Do-Yeon Kim, Minseok Choi, Christopher G. Brinton, Jaekyun Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental challenge to providing edge-AI services is the need for a
machine learning (ML) model that achieves personalization (i.e., to individual
clients) and generalization (i.e., to unseen data) properties concurrently.
Existing techniques in federated learning (FL) have encountered a steep
tradeoff between these objectives and impose large computational requirements
on edge devices during training and inference. In this paper, we propose
SplitGP, a new split learning solution that can simultaneously capture
generalization and personalization capabilities for efficient inference across
resource-constrained clients (e.g., mobile/IoT devices). Our key idea is to
split the full ML model into client-side and server-side components, and impose
different roles to them: the client-side model is trained to have strong
personalization capability optimized to each client's main task, while the
server-side model is trained to have strong generalization capability for
handling all clients' out-of-distribution tasks. We analytically characterize
the convergence behavior of SplitGP, revealing that all client models approach
stationary points asymptotically. Further, we analyze the inference time in
SplitGP and provide bounds for determining model split ratios. Experimental
results show that SplitGP outperforms existing baselines by wide margins in
inference time and test accuracy for varying amounts of out-of-distribution
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Example Defense via Perturbation Grading Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Zhu, Wanli Lyu, Bin Li, Zhaoxia Yin, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks have been widely used in many fields. However, studies
have shown that DNNs are easily attacked by adversarial examples, which have
tiny perturbations and greatly mislead the correct judgment of DNNs.
Furthermore, even if malicious attackers cannot obtain all the underlying model
parameters, they can use adversarial examples to attack various DNN-based task
systems. Researchers have proposed various defense methods to protect DNNs,
such as reducing the aggressiveness of adversarial examples by preprocessing or
improving the robustness of the model by adding modules. However, some defense
methods are only effective for small-scale examples or small perturbations but
have limited defense effects for adversarial examples with large perturbations.
This paper assigns different defense strategies to adversarial perturbations of
different strengths by grading the perturbations on the input examples.
Experimental results show that the proposed method effectively improves defense
performance. In addition, the proposed method does not modify any task model,
which can be used as a preprocessing module, which significantly reduces the
deployment cost in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Enhanced Belief Propagation for Multiobject Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchao Liang, Florian Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic solutions for multi-object tracking (MOT) are a key enabler for
applications in autonomous navigation and applied ocean sciences.
State-of-the-art MOT methods fully rely on a statistical model and typically
use preprocessed sensor data as measurements. In particular, measurements are
produced by a detector that extracts potential object locations from the raw
sensor data collected for a discrete time step. This preparatory processing
step reduces data flow and computational complexity but may result in a loss of
information. State-of-the-art Bayesian MOT methods that are based on belief
propagation (BP) systematically exploit graph structures of the statistical
model to reduce computational complexity and improve scalability. However, as a
fully model-based approach, BP can only provide suboptimal estimates when there
is a mismatch between the statistical model and the true data-generating
process. Existing BP-based MOT methods can further only make use of
preprocessed measurements. In this paper, we introduce a variant of BP that
combines model-based with data-driven MOT. The proposed neural enhanced belief
propagation (NEBP) method complements the statistical model of BP by
information learned from raw sensor data. This approach conjectures that the
learned information can reduce model mismatch and thus improve data association
and false alarm rejection. Our NEBP method improves tracking performance
compared to model-based methods. At the same time, it inherits the advantages
of BP-based MOT, i.e., it scales only quadratically in the number of objects,
and it can thus generate and maintain a large number of object tracks. We
evaluate the performance of our NEBP approach for MOT on the nuScenes
autonomous driving dataset and demonstrate that it has state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Bounds for Inductive Matrix Completion in Low-noise
  Settings <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Ledent, Rodrigo Alves, Yunwen Lei, Yann Guermeur, Marius Kloft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study inductive matrix completion (matrix completion with side
information) under an i.i.d. subgaussian noise assumption at a low noise
regime, with uniform sampling of the entries. We obtain for the first time
generalization bounds with the following three properties: (1) they scale like
the standard deviation of the noise and in particular approach zero in the
exact recovery case; (2) even in the presence of noise, they converge to zero
when the sample size approaches infinity; and (3) for a fixed dimension of the
side information, they only have a logarithmic dependence on the size of the
matrix. Differently from many works in approximate recovery, we present results
both for bounded Lipschitz losses and for the absolute loss, with the latter
relying on Talagrand-type inequalities. The proofs create a bridge between two
approaches to the theoretical analysis of matrix completion, since they consist
in a combination of techniques from both the exact recovery literature and the
approximate recovery literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 Pages, 1 figure; Accepted for publication at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolution-enhanced Evolving Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Wang, Yaming Yang, Zhuo Li, Jiangang Bai, Mingliang Zhang, Xiangtai Li, Jing Yu, Ce Zhang, Gao Huang, Yunhai Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based neural networks, such as Transformers, have become ubiquitous
in numerous applications, including computer vision, natural language
processing, and time-series analysis. In all kinds of attention networks, the
attention maps are crucial as they encode semantic dependencies between input
tokens. However, most existing attention networks perform modeling or reasoning
based on representations, wherein the attention maps of different layers are
learned separately without explicit interactions. In this paper, we propose a
novel and generic evolving attention mechanism, which directly models the
evolution of inter-token relationships through a chain of residual
convolutional modules. The major motivations are twofold. On the one hand, the
attention maps in different layers share transferable knowledge, thus adding a
residual connection can facilitate the information flow of inter-token
relationships across layers. On the other hand, there is naturally an
evolutionary trend among attention maps at different abstraction levels, so it
is beneficial to exploit a dedicated convolution-based module to capture this
process. Equipped with the proposed mechanism, the convolution-enhanced
evolving attention networks achieve superior performance in various
applications, including time-series representation, natural language
understanding, machine translation, and image classification. Especially on
time-series representation tasks, Evolving Attention-enhanced Dilated
Convolutional (EA-DC-) Transformer outperforms state-of-the-art models
significantly, achieving an average of 17% improvement compared to the best
SOTA. To the best of our knowledge, this is the first work that explicitly
models the layer-wise evolution of attention maps. Our implementation is
available at https://github.com/pkuyym/EvolvingAttention
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of the previous work (arXiv:2102.12895). arXiv admin note:
  text overlap with arXiv:2102.12895</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile Augmented Reality with Federated Learning in the Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhou, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Metaverse is deemed the next evolution of the Internet and has received
much attention recently. Metaverse applications via mobile augmented reality
(MAR) require rapid and accurate object detection to mix digital data with the
real world. As mobile devices evolve, they become more potent in computing.
Hence, their computational resources can be leveraged to train machine learning
models. In light of the increasing concerns of user privacy and data security,
federated learning (FL) has become a promising distributed learning framework
for privacy-preserving analytics. In this article, FL and MAR are brought
together in the Metaverse. We discuss the necessity and rationality of the
combination of FL and MAR. The prospective technologies that power FL and MAR
in the Metaverse are also identified. In addition, existing challenges that
prevent the fulfilment of FL and MAR in the Metaverse and several application
scenarios are presented. Finally, two case studies of Metaverse FL-MAR systems
are demonstrated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ensemble neural network approach to forecast Dengue outbreak based on
  climatic condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhurima Panja, Tanujit Chakraborty, Sk Shahid Nadim, Indrajit Ghosh, Uttam Kumar, Nan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dengue fever is a virulent disease spreading over 100 tropical and
subtropical countries in Africa, the Americas, and Asia. This arboviral disease
affects around 400 million people globally, severely distressing the healthcare
systems. The unavailability of a specific drug and ready-to-use vaccine makes
the situation worse. Hence, policymakers must rely on early warning systems to
control intervention-related decisions. Forecasts routinely provide critical
information for dangerous epidemic events. However, the available forecasting
models (e.g., weather-driven mechanistic, statistical time series, and machine
learning models) lack a clear understanding of different components to improve
prediction accuracy and often provide unstable and unreliable forecasts. This
study proposes an ensemble wavelet neural network with exogenous factor(s)
(XEWNet) model that can produce reliable estimates for dengue outbreak
prediction for three geographical regions, namely San Juan, Iquitos, and
Ahmedabad. The proposed XEWNet model is flexible and can easily incorporate
exogenous climate variable(s) confirmed by statistical causality tests in its
scalable framework. The proposed model is an integrated approach that uses
wavelet transformation into an ensemble neural network framework that helps in
generating more reliable long-term forecasts. The proposed XEWNet allows
complex non-linear relationships between the dengue incidence cases and
rainfall; however, mathematically interpretable, fast in execution, and easily
comprehensible. The proposal's competitiveness is measured using computational
experiments based on various statistical metrics and several statistical
comparison tests. In comparison with statistical, machine learning, and deep
learning methods, our proposed XEWNet performs better in 75% of the cases for
short-term and long-term forecasting of dengue incidence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Framework for Monitoring Subgroup Performance of Machine
  Learning Systems <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huong Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring machine learning systems post deployment is critical to ensure the
reliability of the systems. Particularly importance is the problem of
monitoring the performance of machine learning systems across all the data
subgroups (subpopulations). In practice, this process could be prohibitively
expensive as the number of data subgroups grows exponentially with the number
of input features, and the process of labelling data to evaluate each
subgroup's performance is costly. In this paper, we propose an efficient
framework for monitoring subgroup performance of machine learning systems.
Specifically, we aim to find the data subgroup with the worst performance using
a limited number of labeled data. We mathematically formulate this problem as
an optimization problem with an expensive black-box objective function, and
then suggest to use Bayesian optimization to solve this problem. Our
experimental results on various real-world datasets and machine learning
systems show that our proposed framework can retrieve the worst-performing data
subgroup effectively and efficiently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ML Safety Workshop at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Find Strong Lottery Tickets in Generative Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangyeop Yeo, Yoojin Jang, Jy-yong Sohn, Dongyoon Han, Jaejun Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yes. In this paper, we investigate strong lottery tickets in generative
models, the subnetworks that achieve good generative performance without any
weight update. Neural network pruning is considered the main cornerstone of
model compression for reducing the costs of computation and memory.
Unfortunately, pruning a generative model has not been extensively explored,
and all existing pruning algorithms suffer from excessive weight-training
costs, performance degradation, limited generalizability, or complicated
training. To address these problems, we propose to find a strong lottery ticket
via moment-matching scores. Our experimental results show that the discovered
subnetwork can perform similarly or better than the trained dense model even
when only 10% of the weights remain. To the best of our knowledge, we are the
first to show the existence of strong lottery tickets in generative models and
provide an algorithm to find it stably. Our code and supplementary materials
are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Evaluation For Offline Learning: Are We Ready To Deploy? <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hager Radi, Josiah P. Hanna, Peter Stone, Matthew E. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world currently offers an abundance of data in multiple domains, from
which we can learn reinforcement learning (RL) policies without further
interaction with the environment. RL agents learning offline from such data is
possible but deploying them while learning might be dangerous in domains where
safety is critical. Therefore, it is essential to find a way to estimate how a
newly-learned agent will perform if deployed in the target environment before
actually deploying it and without the risk of overestimating its true
performance. To achieve this, we introduce a framework for safe evaluation of
offline learning using approximate high-confidence off-policy evaluation
(HCOPE) to estimate the performance of offline policies during learning. In our
setting, we assume a source of data, which we split into a train-set, to learn
an offline policy, and a test-set, to estimate a lower-bound on the offline
policy using off-policy evaluation with bootstrapping. A lower-bound estimate
tells us how good a newly-learned target policy would perform before it is
deployed in the real environment, and therefore allows us to decide when to
deploy our learned policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021 Workshop on Deployable Decision Making in Embodied
  Systems [Spotlight]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metaheuristic for Hub-Spoke Facility Location Problem: Application to
  Indian E-commerce Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Sachdeva, Bhupinder Singh, Rahul Prasad, Nakshatra Goel, Ronit Mondal, Jatin Munjal, Abhishek Bhatnagar, Manjeet Dahiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indian e-commerce industry has evolved over the last decade and is expected
to grow over the next few years. The focus has now shifted to turnaround time
(TAT) due to the emergence of many third-party logistics providers and higher
customer expectations. The key consideration for delivery providers is to
balance their overall operating costs while meeting the promised TAT to their
customers. E-commerce delivery partners operate through a network of facilities
whose strategic locations help to run the operations efficiently. In this work,
we identify the locations of hubs throughout the country and their
corresponding mapping with the distribution centers. The objective is to
minimize the total network costs with TAT adherence. We use Genetic Algorithm
and leverage business constraints to reduce the solution search space and hence
the solution time. The results indicate an improvement of 9.73% in TAT
compliance compared with the current scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning on Persistence Diagrams as Radon Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Elchesen, Iryna Hartsock, Jose A. Perea, Tatum Rask
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistence diagrams are common descriptors of the topological structure of
data appearing in various classification and regression tasks. They can be
generalized to Radon measures supported on the birth-death plane and endowed
with an optimal transport distance. Examples of such measures are expectations
of probability distributions on the space of persistence diagrams. In this
paper, we develop methods for approximating continuous functions on the space
of Radon measures supported on the birth-death plane, as well as their
utilization in supervised learning tasks. Indeed, we show that any continuous
function defined on a compact subset of the space of such measures (e.g., a
classifier or regressor) can be approximated arbitrarily well by polynomial
combinations of features computed using a continuous compactly supported
function on the birth-death plane (a template). We provide insights into the
structure of relatively compact subsets of the space of Radon measures, and
test our approximation methodology on various data sets and supervised learning
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Learning Protocol for Federated Tumor Segmentation Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambrish Rawat, Giulio Zizzo, Swanand Kadhe, Jonathan P. Epperlein, Stefano Braghin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we devise robust and efficient learning protocols for
orchestrating a Federated Learning (FL) process for the Federated Tumor
Segmentation Challenge (FeTS 2022). Enabling FL for FeTS setup is challenging
mainly due to data heterogeneity among collaborators and communication cost of
training. To tackle these challenges, we propose Robust Learning Protocol
(RoLePRO) which is a combination of server-side adaptive optimisation (e.g.,
server-side Adam) and judicious parameter (weights) aggregation schemes (e.g.,
adaptive weighted aggregation). RoLePRO takes a two-phase approach, where the
first phase consists of vanilla Federated Averaging, while the second phase
consists of a judicious aggregation scheme that uses a sophisticated
reweighting, all in the presence of an adaptive optimisation algorithm at the
server. We draw insights from extensive experimentation to tune learning rates
for the two phases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Werewolf Among Us: A Multimodal <span class="highlight-title">Dataset</span> for Modeling Persuasion
  Behaviors in Social Deduction Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolin Lai, Hongxin Zhang, Miao Liu, Aryan Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persuasion modeling is a key building block for conversational agents.
Existing works in this direction are limited to analyzing textual dialogue
corpus. We argue that visual signals also play an important role in
understanding human persuasive behaviors. In this paper, we introduce the first
multimodal dataset for modeling persuasion behaviors. Our dataset includes 199
dialogue transcriptions and videos captured in a multi-player social deduction
game setting, 26,647 utterance level annotations of persuasion strategy, and
game level annotations of deduction game outcomes. We provide extensive
experiments to show how dialogue context and visual signals benefit persuasion
strategy prediction. We also explore the generalization ability of language
models for persuasion modeling and the role of persuasion strategies in
predicting social deduction game outcomes. Our dataset, code, and models can be
found at https://persuasion-deductiongame.socialai-data.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">self-supervised</span> representation learning via sequential
  adversarial masking <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, Min Bai, Tristan McKinney, Li Erran Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent methods in self-supervised learning have demonstrated that
masking-based pretext tasks extend beyond NLP, serving as useful pretraining
objectives in computer vision. However, existing approaches apply random or ad
hoc masking strategies that limit the difficulty of the reconstruction task
and, consequently, the strength of the learnt representations. We improve upon
current state-of-the-art work in learning adversarial masks by proposing a new
framework that generates masks in a sequential fashion with different
constraints on the adversary. This leads to improvements in performance on
various downstream tasks, such as classification on ImageNet100, STL10, and
CIFAR10/100 and segmentation on Pascal VOC. Our results further demonstrate the
promising capabilities of masking-based approaches for SSL in computer vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, Presented at NeurIPS 2022 SSL: Theory and
  Practice Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing RNN from Using Sequence Length as a Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Thomas Baillargeon, Hélène Cossette, Luc Lamontagne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks are deep learning topologies that can be trained to
classify long documents. However, in our recent work, we found a critical
problem with these cells: they can use the length differences between texts of
different classes as a prominent classification feature. This has the effect of
producing models that are brittle and fragile to concept drift, can provide
misleading performances and are trivially explainable regardless of text
content. This paper illustrates the problem using synthetic and real-world data
and provides a simple solution using weight decay regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, but my overleaf generrates 5 pages. I have no error, the
  font size seems different</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Li, Runsheng Xu, Xinyu Liu, Jin Ma, Zicheng Chi, Jiaqi Ma, Hongkai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been widely used in the perception (e.g., 3D object
detection) of intelligent vehicle driving. Due to the beneficial
Vehicle-to-Vehicle (V2V) communication, the deep learning based features from
other agents can be shared to the ego vehicle so as to improve the perception
of the ego vehicle. It is named as Cooperative Perception in the V2V research,
whose algorithms have been dramatically advanced recently. However, all the
existing cooperative perception algorithms assume the ideal V2V communication
without considering the possible lossy shared features because of the Lossy
Communication (LC) which is common in the complex real-world driving scenarios.
In this paper, we first study the side effect (e.g., detection performance
drop) by the lossy communication in the V2V Cooperative Perception, and then we
propose a novel intermediate LC-aware feature fusion method to relieve the side
effect of lossy communication by a LC-aware Repair Network (LCRN) and enhance
the interaction between the ego vehicle and other vehicles by a specially
designed V2V Attention Module (V2VAM) including intra-vehicle attention of ego
vehicle and uncertainty-aware inter-vehicle attention. The extensive experiment
on the public cooperative perception dataset OPV2V (based on digital-twin CARLA
simulator) demonstrates that the proposed method is quite effective for the
cooperative point cloud based 3D object detection under lossy V2V
communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Sequence Better: Time Interval Aware Data Augmentation for
  Sequential Recommendation <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Dang, Enneng Yang, Guibing Guo, Linying Jiang, Xingwei Wang, Xiaoxiao Xu, Qinghui Sun, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation is an important task to predict the next-item to
access based on a sequence of interacted items. Most existing works learn user
preference as the transition pattern from the previous item to the next one,
ignoring the time interval between these two items. However, we observe that
the time interval in a sequence may vary significantly different, and thus
result in the ineffectiveness of user modeling due to the issue of
\emph{preference drift}. In fact, we conducted an empirical study to validate
this observation, and found that a sequence with uniformly distributed time
interval (denoted as uniform sequence) is more beneficial for performance
improvement than that with greatly varying time interval. Therefore, we propose
to augment sequence data from the perspective of time interval, which is not
studied in the literature. Specifically, we design five operators (Ti-Crop,
Ti-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original
non-uniform sequence to uniform sequence with the consideration of variance of
time intervals. Then, we devise a control strategy to execute data augmentation
on item sequences in different lengths. Finally, we implement these
improvements on a state-of-the-art model CoSeRec and validate our approach on
four real datasets. The experimental results show that our approach reaches
significantly better performance than the other 11 competing methods. Our
implementation is available: https://github.com/KingGugu/TiCoSeRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, AAAI-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepQ-ViT: Scale Reparameterization for Post-Training Quantization of
  Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Li, Junrui Xiao, Lianwei Yang, Qingyi Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ), which only requires a tiny dataset for
calibration without end-to-end retraining, is a light and practical model
compression technique. Recently, several PTQ schemes for vision transformers
(ViTs) have been presented; unfortunately, they typically suffer from
non-trivial accuracy degradation, especially in low-bit cases. In this paper,
we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale
reparameterization, to address the above issues. RepQ-ViT decouples the
quantization and inference processes, where the former employs complex
quantizers and the latter employs scale-reparameterized simplified quantizers.
This ensures both accurate quantization and efficient inference, which
distinguishes it from existing approaches that sacrifice quantization
performance to meet the target hardware. More specifically, we focus on two
components with extreme distributions: post-LayerNorm activations with severe
inter-channel variation and post-Softmax activations with power-law features,
and initially apply channel-wise quantization and log$\sqrt{2}$ quantization,
respectively. Then, we reparameterize the scales to hardware-friendly
layer-wise quantization and log2 quantization for inference, with only slight
accuracy or computational costs. Extensive experiments are conducted on
multiple vision tasks with different model variants, proving that RepQ-ViT,
without hyperparameters and expensive reconstruction procedures, can outperform
existing strong baselines and encouragingly improve the accuracy of 4-bit PTQ
of ViTs to a usable level.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Reinforcement Learning for Visual Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Shah, Arjun Bhorkar, Hrish Leen, Ilya Kostrikov, Nick Rhinehart, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning can enable robots to navigate to distant goals while
optimizing user-specified reward functions, including preferences for following
lanes, staying on paved paths, or avoiding freshly mowed grass. However, online
learning from trial-and-error for real-world robots is logistically
challenging, and methods that instead can utilize existing datasets of robotic
navigation data could be significantly more scalable and enable broader
generalization. In this paper, we present ReViND, the first offline RL system
for robotic navigation that can leverage previously collected data to optimize
user-specified reward functions in the real-world. We evaluate our system for
off-road navigation without any additional data collection or fine-tuning, and
show that it can navigate to distant goals using only offline training from
this dataset, and exhibit behaviors that qualitatively differ based on the
user-specified reward function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://sites.google.com/view/revind/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Decentralized Cross-Entropy Method <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhang, Jun Jin, Martin Jagersand, Jun Luo, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Entropy Method (CEM) is commonly used for planning in model-based
reinforcement learning (MBRL) where a centralized approach is typically
utilized to update the sampling distribution based on only the top-$k$
operation's results on samples. In this paper, we show that such a centralized
approach makes CEM vulnerable to local optima, thus impairing its sample
efficiency. To tackle this issue, we propose Decentralized CEM (DecentCEM), a
simple but effective improvement over classical CEM, by using an ensemble of
CEM instances running independently from one another, and each performing a
local improvement of its own sampling distribution. We provide both theoretical
and empirical analysis to demonstrate the effectiveness of this simple
decentralized approach. We empirically show that, compared to the classical
centralized approach using either a single or even a mixture of Gaussian
distributions, our DecentCEM finds the global optimum much more consistently
thus improves the sample efficiency. Furthermore, we plug in our DecentCEM in
the planning problem of MBRL, and evaluate our approach in several continuous
control environments, with comparison to the state-of-art CEM based MBRL
approaches (PETS and POPLIN). Results show sample efficiency improvement by
simply replacing the classical CEM module with our DecentCEM module, while only
sacrificing a reasonable amount of computational cost. Lastly, we conduct
ablation studies for more in-depth analysis. Code is available at
https://github.com/vincentzhang/decentCEM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022. The last two authors advised equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-aware Autoregressive Models for Calorimeter Shower Simulations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junze Liu, Aishik Ghosh, Dylan Smith, Pierre Baldi, Daniel Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calorimeter shower simulations are often the bottleneck in simulation time
for particle physics detectors. A lot of effort is currently spent on
optimizing generative architectures for specific detector geometries, which
generalize poorly. We develop a geometry-aware autoregressive model on a range
of calorimeter geometries such that the model learns to adapt its energy
deposition depending on the size and position of the cells. This is a key
proof-of-concept step towards building a model that can generalize to new
unseen calorimeter geometries with little to no additional training. Such a
model can replace the hundreds of generative models used for calorimeter
simulation in a Large Hadron Collider experiment. For the study of future
detectors, such a model will dramatically reduce the large upfront investment
usually needed to generate simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to NeurIPS Machine Learning and the Physical
  Sciences Workshop 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Robot Reinforcement Learning with Uncertainty-Guided Human
  Expert Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Kumar, Ilya Kuzovkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in batch (offline) reinforcement learning have shown
promising results in learning from available offline data and proved offline
reinforcement learning to be an essential toolkit in learning control policies
in a model-free setting. An offline reinforcement learning algorithm applied to
a dataset collected by a suboptimal non-learning-based algorithm can result in
a policy that outperforms the behavior agent used to collect the data. Such a
scenario is frequent in robotics, where existing automation is collecting
operational data. Although offline learning techniques can learn from data
generated by a sub-optimal behavior agent, there is still an opportunity to
improve the sample complexity of existing offline reinforcement learning
algorithms by strategically introducing human demonstration data into the
training process. To this end, we propose a novel approach that uses
uncertainty estimation to trigger the injection of human demonstration data and
guide policy training towards optimal behavior while reducing overall sample
complexity. Our experiments show that this approach is more sample efficient
when compared to a naive way of combining expert data with data collected from
a sub-optimal agent. We augmented an existing offline reinforcement learning
algorithm Conservative Q-Learning with our approach and performed experiments
on data collected from MuJoCo and OffWorld Gym learning environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Patrolling with Battery Constraints through Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Tong, Aaron Harwood, Maria A. Rodriguez, Richard O. Sinnott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown and evolving environmental factors. Secondly, autonomous vehicles can
have failures or hardware constraints such as limited battery lives.
Importantly, patrolling large areas often requires multiple agents that need to
collectively coordinate their actions. In this work, we consider these
limitations and propose an approach based on a distributed, model-free deep
reinforcement learning based multi-agent patrolling strategy. In this approach,
agents make decisions locally based on their own environmental observations and
on shared information. In addition, agents are trained to automatically
recharge themselves when required to support continuous collective patrolling.
A homogeneous multi-agent architecture is proposed, where all patrolling agents
have an identical policy. This architecture provides a robust patrolling system
that can tolerate agent failures and allow supplementary agents to be added to
replace failed agents or to increase the overall patrol performance. This
performance is validated through experiments from multiple perspectives,
including the overall patrol performance, the efficiency of the battery
recharging strategy, the overall robustness of the system, and the agents'
ability to adapt to environment dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jee Seok Yoon, Chenghao Zhang, Heung-Il Suk, Jia Guo, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human organs constantly undergo anatomical changes due to a complex mix of
short-term (e.g., heartbeat) and long-term (e.g., aging) factors. Evidently,
prior knowledge of these factors will be beneficial when modeling their future
state, i.e., via image generation. However, most of the medical image
generation tasks only rely on the input from a single image, thus ignoring the
sequential dependency even when longitudinal data is available. Sequence-aware
deep generative models, where model input is a sequence of ordered and
timestamped images, are still underexplored in the medical imaging domain that
is featured by several unique challenges: 1) Sequences with various lengths; 2)
Missing data or frame, and 3) High dimensionality. To this end, we propose a
sequence-aware diffusion model (SADM) for the generation of longitudinal
medical images. Recently, diffusion models have shown promising results on
high-fidelity image generation. Our method extends this new technique by
introducing a sequence-aware transformer as the conditional module in a
diffusion model. The novel design enables learning longitudinal dependency even
with missing data during training and allows autoregressive generation of a
sequence of images during inference. Our extensive experiments on 3D
longitudinal medical images demonstrate the effectiveness of SADM compared with
baselines and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Materials Discovery using Max K-Armed Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nobuaki Kikkawa, Hiroshi Ohno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search algorithms for the bandit problems are applicable in materials
discovery. However, the objectives of the conventional bandit problem are
different from those of materials discovery. The conventional bandit problem
aims to maximize the total rewards, whereas materials discovery aims to achieve
breakthroughs in material properties. The max K-armed bandit (MKB) problem,
which aims to acquire the single best reward, matches with the discovery tasks
better than the conventional bandit. Thus, here, we propose a search algorithm
for materials discovery based on the MKB problem using a pseudo-value of the
upper confidence bound of expected improvement of the best reward. This
approach is pseudo-guaranteed to be asymptotic oracles that do not depends on
the time horizon. In addition, compared with other MKB algorithms, the proposed
algorithm has only one hyperparameter, which is advantageous in materials
discovery. We applied the proposed algorithm to synthetic problems and
molecular-design demonstrations using a Monte Carlo tree search. According to
the results, the proposed algorithm stably outperformed other bandit algorithms
in the late stage of the search process when the optimal arm of the MKB could
not be determined based on its expectation reward.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Improved Generalization: Meta Transfer of <span class="highlight-title">Self-supervised</span>
  Knowledge on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Cui, Haleh Akrami, Anand A. Joshi, Richard M. Leahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success achieved by graph convolutional networks for
functional brain activity analysis, the heterogeneity of functional patterns
and the scarcity of imaging data still pose challenges in many tasks.
Transferring knowledge from a source domain with abundant training data to a
target domain is effective for improving representation learning on scarce
training data. However, traditional transfer learning methods often fail to
generalize the pre-trained knowledge to the target task due to domain
discrepancy. Self-supervised learning on graphs can increase the
generalizability of graph features since self-supervision concentrates on
inherent graph properties that are not limited to a particular supervised task.
We propose a novel knowledge transfer strategy by integrating meta-learning
with self-supervised learning to deal with the heterogeneity and scarcity of
fMRI data. Specifically, we perform a self-supervised task on the source domain
and apply meta-learning, which strongly improves the generalizability of the
model using the bi-level optimization, to transfer the self-supervised
knowledge to the target domain. Through experiments on a neurological disorder
classification task, we demonstrate that the proposed strategy significantly
improves target task performance by increasing the generalizability and
transferability of graph-based knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Azimuth: Systematic Error Analysis for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabrielle Gauthier-Melançon, Orlando Marquez Ayala, Lindsay Brin, Chris Tyler, Frédéric Branchaud-Charron, Joseph Marinier, Karine Grande, Di Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Azimuth, an open-source and easy-to-use tool to perform error
analysis for text classification. Compared to other stages of the ML
development cycle, such as model training and hyper-parameter tuning, the
process and tooling for the error analysis stage are less mature. However, this
stage is critical for the development of reliable and trustworthy AI systems.
To make error analysis more systematic, we propose an approach comprising
dataset analysis and model quality assessment, which Azimuth facilitates. We
aim to help AI practitioners discover and address areas where the model does
not generalize by leveraging and integrating a range of ML techniques, such as
saliency maps, similarity, uncertainty, and behavioral analyses, all in one
tool. Our code and documentation are available at
github.com/servicenow/azimuth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing: System Demonstrations. 13 pages and
  14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The alignment problem from a deep learning perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00626v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00626v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ngo, Lawrence Chan, Sören Mindermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the coming decades, artificial general intelligence (AGI) may surpass
human capabilities at a wide range of important tasks. We outline a case for
expecting that, without substantial effort to prevent it, AGIs could learn to
pursue goals which are very undesirable (in other words, misaligned) from a
human perspective. We argue that AGIs trained in similar ways as today's most
capable models could learn to act deceptively to receive higher reward; learn
internally-represented goals which generalize beyond their training
distributions; and pursue those goals using power-seeking strategies. We
outline how the deployment of misaligned AGIs might irreversibly undermine
human control over the world, and briefly review research directions aimed at
preventing these problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we make virtual characters in a scene interact with their surrounding
objects through simple instructions? Is it possible to synthesize such motion
plausibly with a diverse set of objects and instructions? Inspired by these
questions, we present the first framework to synthesize the full-body motion of
virtual human characters performing specified actions with 3D objects placed
within their reach. Our system takes as input textual instructions specifying
the objects and the associated intentions of the virtual characters and outputs
diverse sequences of full-body motions. This is in contrast to existing work,
where full-body action synthesis methods generally do not consider object
interactions, and human-object interaction methods focus mainly on synthesizing
hand or finger movements for grasping objects. We accomplish our objective by
designing an intent-driven full-body motion generator, which uses a pair of
decoupled conditional variational autoencoders (CVAE) to learn the motion of
the body parts in an autoregressive manner. We also optimize for the positions
of the objects with six degrees of freedom (6DoF) such that they plausibly fit
within the hands of the synthesized characters. We compare our proposed method
with the existing methods of motion synthesis and establish a new and stronger
state-of-the-art for the task of intent-driven motion synthesis. Through a user
study, we further show that our synthesized full-body motions appear more
realistic to the participants in more than 80% of scenarios compared to the
current state-of-the-art methods, and are perceived to be as good as the ground
truth on several occasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDL: A Defense for Label-Based Membership Inference Attacks <span class="chip">CCS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arezoo Rajabi, Dinuka Sahabandu, Luyao Niu, Bhaskar Ramasubramanian, Radha Poovendran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data used to train deep neural network (DNN) models in applications such
as healthcare and finance typically contain sensitive information. A DNN model
may suffer from overfitting. Overfitted models have been shown to be
susceptible to query-based attacks such as membership inference attacks (MIAs).
MIAs aim to determine whether a sample belongs to the dataset used to train a
classifier (members) or not (nonmembers). Recently, a new class of label based
MIAs (LAB MIAs) was proposed, where an adversary was only required to have
knowledge of predicted labels of samples. Developing a defense against an
adversary carrying out a LAB MIA on DNN models that cannot be retrained remains
an open problem. We present LDL, a light weight defense against LAB MIAs. LDL
works by constructing a high-dimensional sphere around queried samples such
that the model decision is unchanged for (noisy) variants of the sample within
the sphere. This sphere of label-invariance creates ambiguity and prevents a
querying adversary from correctly determining whether a sample is a member or a
nonmember. We analytically characterize the success rate of an adversary
carrying out a LAB MIA when LDL is deployed, and show that the formulation is
consistent with experimental observations. We evaluate LDL on seven datasets --
CIFAR-10, CIFAR-100, GTSRB, Face, Purchase, Location, and Texas -- with varying
sizes of training data. All of these datasets have been used by SOTA LAB MIAs.
Our experiments demonstrate that LDL reduces the success rate of an adversary
carrying out a LAB MIA in each case. We empirically compare LDL with defenses
against LAB MIAs that require retraining of DNN models, and show that LDL
performs favorably despite not needing to retrain the DNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in ACM ASIA Conference on Computer and Communications
  Security (ACM ASIACCS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic progress in computer vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege Erdil, Tamay Besiroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate algorithmic progress in image classification on ImageNet,
perhaps the most well-known test bed for computer vision. We estimate a model,
informed by work on neural scaling laws, and infer a decomposition of progress
into the scaling of compute, data, and algorithms. Using Shapley values to
attribute performance improvements, we find that algorithmic improvements have
been roughly as important as the scaling of compute for progress computer
vision. Our estimates indicate that algorithmic innovations mostly take the
form of compute-augmenting algorithmic advances (which enable researchers to
get better performance from less compute), not data-augmenting algorithmic
advances. We find that compute-augmenting algorithmic advances are made at a
pace more than twice as fast as the rate usually associated with Moore's law.
In particular, we estimate that compute-augmenting innovations halve compute
requirements every nine months (95\% confidence interval: 4 to 25 months).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Filtering in Simplicial Complexes <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvin Isufi, Maosheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes convolutional filtering for data whose structure can be
modeled by a simplicial complex (SC). SCs are mathematical tools that not only
capture pairwise relationships as graphs but account also for higher-order
network structures. These filters are built by following the shift-and-sum
principle of the convolution operation and rely on the Hodge-Laplacians to
shift the signal within the simplex. But since in SCs we have also
inter-simplex coupling, we use the incidence matrices to transfer the signal in
adjacent simplices and build a filter bank to jointly filter signals from
different levels. We prove some interesting properties for the proposed filter
bank, including permutation and orientation equivariance, a computational
complexity that is linear in the SC dimension, and a spectral interpretation
using the simplicial Fourier transform. We illustrate the proposed approach
with numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, accepted in ICASSP 2022 (The first version has
  some errors and we fixed them in the second version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FIS-GAN: GAN with Flow-based Importance Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1910.02519v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1910.02519v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Yi, Donglin Zhan, Wenqing Zhang, Denglin Jiang, Kang An, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GAN) training process, in most cases, apply
Uniform or Gaussian sampling methods in the latent space, which probably spends
most of the computation on examples that can be properly handled and easy to
generate. Theoretically, importance sampling speeds up stochastic optimization
in supervised learning by prioritizing training examples. In this paper, we
explore the possibility of adapting importance sampling into adversarial
learning. We use importance sampling to replace Uniform and Gaussian sampling
methods in the latent space and employ normalizing flow to approximate latent
space posterior distribution by density estimation. Empirically, results on
MNIST and Fashion-MNIST demonstrate that our method significantly accelerates
GAN's optimization while retaining visual fidelity in generated samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COVID-19 Monitoring System using Social Distancing and Face Mask
  Detection on Surveillance video <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahana Srinivasan, Rujula Singh R, Ruchita R Biradar, Revathi SA
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current times, the fear and danger of COVID-19 virus still stands
large. Manual monitoring of social distancing norms is impractical with a large
population moving about and with insufficient task force and resources to
administer them. There is a need for a lightweight, robust and 24X7
video-monitoring system that automates this process. This paper proposes a
comprehensive and effective solution to perform person detection, social
distancing violation detection, face detection and face mask classification
using object detection, clustering and Convolution Neural Network (CNN) based
binary classifier. For this, YOLOv3, Density-based spatial clustering of
applications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and
MobileNetV2 based binary classifier have been employed on surveillance video
datasets. This paper also provides a comparative study of different face
detection and face mask classification models. Finally, a video dataset
labelling method is proposed along with the labelled video dataset to
compensate for the lack of dataset in the community and is used for evaluation
of the system. The system performance is evaluated in terms of accuracy, F1
score as well as the prediction time, which has to be low for practical
applicability. The system performs with an accuracy of 91.2% and F1 score of
90.79% on the labelled video dataset and has an average prediction time of 7.12
seconds for 78 frames of a video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I, Rujula Singh R, would like to apologize to the research community
  for the confusion caused by the inconsistency in author lists between
  multiple versions of this paper. I take full responsibility for this error
  and will be more diligent in the future to ensure the accuracy and
  consistency of our research publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Linguistically Informed Multi-Objective <span class="highlight-title">Pre-Train</span>ing for Natural
  Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Pielka, Svetlana Schmidt, Lisa Pucknat, Rafet Sifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Robust is Unsupervised Representation Learning to Distribution
  Shift? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuge Shi, Imant Daunhawer, Julia E. Vogt, Philip H. S. Torr, Amartya Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of machine learning algorithms to distributions shift is
primarily discussed in the context of supervised learning (SL). As such, there
is a lack of insight on the robustness of the representations learned from
unsupervised methods, such as self-supervised learning (SSL) and auto-encoder
based algorithms (AE), to distribution shift. We posit that the input-driven
objectives of unsupervised algorithms lead to representations that are more
robust to distribution shift than the target-driven objective of SL. We verify
this by extensively evaluating the performance of SSL and AE on both synthetic
and realistic distribution shift datasets. Following observations that the
linear layer used for classification itself can be susceptible to spurious
correlations, we evaluate the representations using a linear head trained on a
small amount of out-of-distribution (OOD) data, to isolate the robustness of
the learned representations from that of the linear head. We also develop
"controllable" versions of existing realistic domain generalisation datasets
with adjustable degrees of distribution shifts. This allows us to study the
robustness of different learning algorithms under versatile yet realistic
distribution shift conditions. Our experiments show that representations
learned from unsupervised learning algorithms generalise better than SL under a
wide variety of extreme as well as realistic distribution shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directional Direct Feedback Alignment: Estimating Backpropagation Paths
  for Efficient Learning on Neural Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bacho, Dominique Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The error Backpropagation algorithm (BP) is a key method for training deep
neural networks. While performant, it is also resource-demanding in terms of
computation, memory usage and energy. This makes it unsuitable for online
learning on edge devices that require a high processing rate and low energy
consumption. More importantly, BP does not take advantage of the parallelism
and local characteristics offered by dedicated neural processors. There is
therefore a demand for alternative algorithms to BP that could improve the
latency, memory requirements, and energy footprint of neural networks on
hardware. In this work, we propose a novel method based on Direct Feedback
Alignment (DFA) which uses Forward-Mode Automatic Differentiation to estimate
backpropagation paths and learn feedback connections in an online manner. We
experimentally show that Directional DFA achieves performances that are closer
to BP than other feedback methods on several benchmark datasets and
architectures while benefiting from the locality and parallelization
characteristics of DFA. Moreover, we show that, unlike other feedback learning
algorithms, our method provides stable learning for convolution layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Inter-Group Link Injection Degrades the Fairness of Graph
  Neural Networks <span class="chip">ICDM 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussain Hussain, Meng Cao, Sandipan Sikdar, Denis Helic, Elisabeth Lex, Markus Strohmaier, Roman Kern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present evidence for the existence and effectiveness of adversarial
attacks on graph neural networks (GNNs) that aim to degrade fairness. These
attacks can disadvantage a particular subgroup of nodes in GNN-based node
classification, where nodes of the underlying network have sensitive
attributes, such as race or gender. We conduct qualitative and experimental
analyses explaining how adversarial link injection impairs the fairness of GNN
predictions. For example, an attacker can compromise the fairness of GNN-based
node classification by injecting adversarial links between nodes belonging to
opposite subgroups and opposite class labels. Our experiments on empirical
datasets demonstrate that adversarial fairness attacks can significantly
degrade the fairness of GNN predictions (attacks are effective) with a low
perturbation rate (attacks are efficient) and without a significant drop in
accuracy (attacks are deceptive). This work demonstrates the vulnerability of
GNN models to adversarial fairness attacks. We hope our findings raise
awareness about this issue in our community and lay a foundation for the future
development of GNN models that are more robust to such attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shorter version of this work has been accepted by IEEE ICDM 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variable-Based Calibration for Machine Learning Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markelle Kelly, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of machine learning classifiers in high-stakes domains
requires well-calibrated confidence scores for model predictions. In this paper
we introduce the notion of variable-based calibration to characterize
calibration properties of a model with respect to a variable of interest,
generalizing traditional score-based calibration and metrics such as expected
calibration error (ECE). In particular, we find that models with near-perfect
ECE can exhibit significant variable-based calibration error as a function of
features of the data. We demonstrate this phenomenon both theoretically and in
practice on multiple well-known datasets, and show that it can persist after
the application of existing recalibration methods. To mitigate this issue, we
propose strategies for detection, visualization, and quantification of
variable-based calibration error. We then examine the limitations of current
score-based recalibration methods and explore potential modifications. Finally,
we discuss the implications of these findings, emphasizing that an
understanding of calibration beyond simple aggregate measures is crucial for
endeavors such as fairness and model interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Graph Generator for Multi-View Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianpeng Chen, Yawen Ling, Jie Xu, Yazhou Ren, Shudong Huang, Xiaorong Pu, Zhifeng Hao, Philip S. Yu, Lifang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view graph clustering (MGC) methods are increasingly being studied due
to the explosion of multi-view data with graph structural information. The
critical point of MGC is to better utilize the view-specific and view-common
information in features and graphs of multiple views. However, existing works
have an inherent limitation that they are unable to concurrently utilize the
consensus graph information across multiple graphs and the view-specific
feature information. To address this issue, we propose Variational Graph
Generator for Multi-View Graph Clustering (VGMGC). Specifically, a novel
variational graph generator is proposed to extract common information among
multiple graphs. This generator infers a reliable variational consensus graph
based on a priori assumption over multiple graphs. Then a simple yet effective
graph encoder in conjunction with the multi-view clustering objective is
presented to learn the desired graph embeddings for clustering, which embeds
the inferred view-common graph and view-specific graphs together with features.
Finally, theoretical results illustrate the rationality of VGMGC by analyzing
the uncertainty of the inferred consensus graph with information bottleneck
principle. Extensive experiments demonstrate the superior performance of our
VGMGC over SOTAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gibbs-Helmholtz Graph Neural Network: capturing the temperature
  dependency of activity coefficients at infinite dilution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01199v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01199v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Ivan Sanchez Medina, Steffen Linke, Martin Stoll, Kai Sundmacher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate prediction of physicochemical properties of chemical compounds
in mixtures (such as the activity coefficient at infinite dilution
$\gamma_{ij}^\infty$) is essential for developing novel and more sustainable
chemical processes. In this work, we analyze the performance of
previously-proposed GNN-based models for the prediction of
$\gamma_{ij}^\infty$, and compare them with several mechanistic models in a
series of 9 isothermal studies. Moreover, we develop the Gibbs-Helmholtz Graph
Neural Network (GH-GNN) model for predicting $\ln \gamma_{ij}^\infty$ of
molecular systems at different temperatures. Our method combines the simplicity
of a Gibbs-Helmholtz-derived expression with a series of graph neural networks
that incorporate explicit molecular and intermolecular descriptors for
capturing dispersion and hydrogen bonding effects. We have trained this model
using experimentally determined $\ln \gamma_{ij}^\infty$ data of 40,219
binary-systems involving 1032 solutes and 866 solvents, overall showing
superior performance compared to the popular UNIFAC-Dortmund model. We analyze
the performance of GH-GNN for continuous and discrete inter/extrapolation and
give indications for the model's applicability domain and expected accuracy. In
general, GH-GNN is able to produce accurate predictions for extrapolated
binary-systems if at least 25 systems with the same combination of
solute-solvent chemical classes are contained in the training set and a
similarity indicator above 0.35 is also present. This model and its
applicability domain recommendations have been made open-source at
https://github.com/edgarsmdn/GH-GNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/edgarsmdn/GH-GNN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimation Contracts for Outlier-Robust Geometric Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Carlone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outlier-robust estimation is a fundamental problem and has been extensively
investigated by statisticians and practitioners. The last few years have seen a
convergence across research fields towards "algorithmic robust statistics",
which focuses on developing tractable outlier-robust techniques for
high-dimensional estimation problems. Despite this convergence, research
efforts across fields have been mostly disconnected from one another. This
monograph bridges recent work on certifiable outlier-robust estimation for
geometric perception in robotics and computer vision with parallel work in
robust statistics. In particular, we adapt and extend recent results on robust
linear regression (applicable to the low-outlier regime with << 50% outliers)
and list-decodable regression (applicable to the high-outlier regime with >>
50% outliers) to the setup commonly found in robotics and vision, where (i)
variables (e.g., rotations, poses) belong to a non-convex domain, (ii)
measurements are vector-valued, and (iii) the number of outliers is not known a
priori. The emphasis here is on performance guarantees: rather than proposing
radically new algorithms, we provide conditions on the input measurements under
which modern estimation algorithms (possibly after small modifications) are
guaranteed to recover an estimate close to the ground truth in the presence of
outliers. These conditions are what we call an "estimation contract". Besides
the proposed extensions of existing results, we believe the main contributions
of this monograph are (i) to unify parallel research lines by pointing out
commonalities and differences, (ii) to introduce advanced material (e.g.,
sum-of-squares proofs) in an accessible and self-contained presentation for the
practitioner, and (iii) to point out a few immediate opportunities and open
questions in outlier-robust geometric perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>95 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Philine Bouscasse, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partially Observable RL with B-Stability: Unified Structural Condition
  and Sharp Sample-Efficient Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Chen, Yu Bai, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Observability -- where agents can only observe partial information
about the true underlying state of the system -- is ubiquitous in real-world
applications of Reinforcement Learning (RL). Theoretically, learning a
near-optimal policy under partial observability is known to be hard in the
worst case due to an exponential sample complexity lower bound. Recent work has
identified several tractable subclasses that are learnable with polynomial
samples, such as Partially Observable Markov Decision Processes (POMDPs) with
certain revealing or decodability conditions. However, this line of research is
still in its infancy, where (1) unified structural conditions enabling
sample-efficient learning are lacking; (2) existing sample complexities for
known tractable subclasses are far from sharp; and (3) fewer sample-efficient
algorithms are available than in fully observable RL.
  This paper advances all three aspects above for Partially Observable RL in
the general setting of Predictive State Representations (PSRs). First, we
propose a natural and unified structural condition for PSRs called
\emph{B-stability}. B-stable PSRs encompasses the vast majority of known
tractable subclasses such as weakly revealing POMDPs, low-rank
future-sufficient POMDPs, decodable POMDPs, and regular PSRs. Next, we show
that any B-stable PSR can be learned with polynomial samples in relevant
problem parameters. When instantiated in the aforementioned subclasses, our
sample complexities improve substantially over the current best ones. Finally,
our results are achieved by three algorithms simultaneously: Optimistic Maximum
Likelihood Estimation, Estimation-to-Decisions, and Model-Based Optimistic
Posterior Sampling. The latter two algorithms are new for sample-efficient
learning of POMDPs/PSRs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning and Its Applications to WiFi Human Sensing: A Benchmark
  and A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Yang, Xinyan Chen, Dazhuo Wang, Han Zou, Chris Xiaoxuan Lu, Sumei Sun, Lihua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  WiFi sensing has been evolving rapidly in recent years. Empowered by
propagation models and deep learning methods, many challenging applications are
realized such as WiFi-based human activity recognition and gesture recognition.
However, in contrast to deep learning for visual recognition and natural
language processing, no sufficiently comprehensive public benchmark exists. In
this paper, we highlight the recent progress on deep learning enabled WiFi
sensing, and then propose a benchmark, SenseFi, to study the effectiveness of
various deep learning models for WiFi sensing. These advanced models are
compared in terms of distinct sensing tasks, WiFi platforms, recognition
accuracy, model size, computational complexity, feature transferability, and
adaptability of unsupervised learning. It is also regarded as a tutorial for
deep learning based WiFi sensing, starting from CSI hardware platform to
sensing algorithms. The extensive experiments provide us with experiences in
deep model design, learning strategy skills and training techniques for
real-world applications. To the best of our knowledge, this is the first
benchmark with an open-source library for deep learning in WiFi sensing
research. The benchmark codes are available at
https://github.com/CHENXINYAN-sg/WiFi-CSI-Sensing-Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A benchmark and tutorial for WiFi CSI Human sensing based on deep
  learning methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Backdoor Attacks on Time Series: A Generative Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07915v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07915v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks have emerged as one of the major security threats to deep
learning models as they can easily control the model's test-time predictions by
pre-injecting a backdoor trigger into the model at training time. While
backdoor attacks have been extensively studied on images, few works have
investigated the threat of backdoor attacks on time series data. To fill this
gap, in this paper we present a novel generative approach for time series
backdoor attacks against deep learning based time series classifiers. Backdoor
attacks have two main goals: high stealthiness and high attack success rate. We
find that, compared to images, it can be more challenging to achieve the two
goals on time series. This is because time series have fewer input dimensions
and lower degrees of freedom, making it hard to achieve a high attack success
rate without compromising stealthiness. Our generative approach addresses this
challenge by generating trigger patterns that are as realistic as real-time
series patterns while achieving a high attack success rate without causing a
significant drop in clean accuracy. We also show that our proposed attack is
resistant to potential backdoor defenses. Furthermore, we propose a novel
universal generator that can poison any type of time series with a single
generator that allows universal attacks without the need to fine-tune the
generative model for new time series datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Recurrent Neural Network to Identify Ship Motion in Open Water
  with Performance Guarantees -- Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Frank, Decky Aspandi Latif, Michael Muehlebach, Benjamin Unger, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks are capable of learning the dynamics of an unknown
nonlinear system purely from input-output measurements. However, the resulting
models do not provide any stability guarantees on the input-output mapping. In
this work, we represent a recurrent neural network as a linear time-invariant
system with nonlinear disturbances. By introducing constraints on the
parameters, we can guarantee finite gain stability and incremental finite gain
stability. We apply this identification method to learn the motion of a
four-degrees-of-freedom ship that is moving in open water and compare it
against other purely learning-based approaches with unconstrained parameters.
Our analysis shows that the constrained recurrent neural network has a lower
prediction accuracy on the test set, but it achieves comparable results on an
out-of-distribution set and respects stability conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and
  Distribution-Aware Criterion <span class="chip">ICSE
  '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.01955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.01955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyuan Yuan, Qi Pang, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various deep neural network (DNN) coverage criteria have been proposed to
assess DNN test inputs and steer input mutations. The coverage is characterized
via neurons having certain outputs, or the discrepancy between neuron outputs.
Nevertheless, recent research indicates that neuron coverage criteria show
little correlation with test suite quality.
  In general, DNNs approximate distributions, by incorporating hierarchical
layers, to make predictions for inputs. Thus, we champion to deduce DNN
behaviors based on its approximated distributions from a layer perspective. A
test suite should be assessed using its induced layer output distributions.
Accordingly, to fully examine DNN behaviors, input mutation should be directed
toward diversifying the approximated distributions.
  This paper summarizes eight design requirements for DNN coverage criteria,
taking into account distribution properties and practical concerns. We then
propose a new criterion, NeuraL Coverage (NLC), that satisfies all design
requirements. NLC treats a single DNN layer as the basic computational unit
(rather than a single neuron) and captures four critical properties of neuron
output distributions. Thus, NLC accurately describes how DNNs comprehend inputs
via approximated distributions. We demonstrate that NLC is significantly
correlated with the diversity of a test suite across a number of tasks
(classification and generation) and data formats (image and text). Its capacity
to discover DNN prediction errors is promising. Test input mutation guided by
NLC results in a greater quality and diversity of exposed erroneous behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extended version of a paper to appear in the Proceedings of the
  45th IEEE/ACM International Conference on Software Engineering, 2023, (ICSE
  '23), 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental limits to learning closed-form mathematical models from data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Fajardo-Fontiveros, Ignasi Reichardt, Harry R. De Los Rios, Jordi Duch, Marta Sales-Pardo, Roger Guimera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a finite and noisy dataset generated with a closed-form mathematical
model, when is it possible to learn the true generating model from the data
alone? This is the question we investigate here. We show that this
model-learning problem displays a transition from a low-noise phase in which
the true model can be learned, to a phase in which the observation noise is too
high for the true model to be learned by any method. Both in the low-noise
phase and in the high-noise phase, probabilistic model selection leads to
optimal generalization to unseen data. This is in contrast to standard machine
learning approaches, including artificial neural networks, which in this
particular problem are limited, in the low-noise phase, by their ability to
interpolate. In the transition region between the learnable and unlearnable
phases, generalization is hard for all approaches including probabilistic model
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Volatility and Dependence of European Carbon and Energy Prices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.14311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.14311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Berrisch, Sven Pappert, Florian Ziel, Antonia Arsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the prices of European Emission Allowances (EUA), whereby we analyze
their uncertainty and dependencies on related energy prices (natural gas, coal,
and oil). We propose a probabilistic multivariate conditional time series model
with a VECM-Copula-GARCH structure which exploits key characteristics of the
data. Data are normalized with respect to inflation and carbon emissions to
allow for proper cross-series evaluation. The forecasting performance is
evaluated in an extensive rolling-window forecasting study, covering eight
years out-of-sample. We discuss our findings for both levels- and
log-transformed data, focusing on time-varying correlations, and in view of the
Russian invasion of Ukraine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Finance Research Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Successor Feature Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Reinke, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer in Reinforcement Learning aims to improve learning performance on
target tasks using knowledge from experienced source tasks. Successor
Representations (SR) and their extension Successor Features (SF) are prominent
transfer mechanisms in domains where reward functions change between tasks.
They reevaluate the expected return of previously learned policies in a new
target task to transfer their knowledge. The SF framework extended SR by
linearly decomposing rewards into successor features and a reward weight vector
allowing their application in high-dimensional tasks. But this came with the
cost of having a linear relationship between reward functions and successor
features, limiting its application to such tasks. We propose a novel
formulation of SR based on learning the cumulative discounted probability of
successor features, called Successor Feature Representations (SFR). Crucially,
SFR allows to reevaluate the expected return of policies for general reward
functions. We introduce different SFR variations, prove its convergence, and
provide a guarantee on its transfer performance. Experimental evaluations based
on SFR with function approximation demonstrate its advantage over SF not only
for general reward functions but also in the case of linearly decomposable
reward functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>source code available at
  https://gitlab.inria.fr/robotlearn/sfr_learning [v2] added experiments with
  learned features [v3] renamed paper and changed scope</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing Label Characteristics in VAEs <span class="chip">ICLR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.10102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.10102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Joy, Sebastian M. Schmon, Philip H. S. Torr, N. Siddharth, Tom Rainforth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a principled approach to incorporating labels in VAEs that
captures the rich characteristic information associated with those labels.
While prior work has typically conflated these by learning latent variables
that directly correspond to label values, we argue this is contrary to the
intended effect of supervision in VAEs-capturing rich label characteristics
with the latents. For example, we may want to capture the characteristics of a
face that make it look young, rather than just the age of the person. To this
end, we develop the CCVAE, a novel VAE model and concomitant variational
objective which captures label characteristics explicitly in the latent space,
eschewing direct correspondences between label values and latents. Through
judicious structuring of mappings between such characteristic latents and
labels, we show that the CCVAE can effectively learn meaningful representations
of the characteristics of interest across a variety of supervision schemes. In
particular, we show that the CCVAE allows for more effective and more general
interventions to be performed, such as smooth traversals within the
characteristics for a given label, diverse conditional generation, and
transferring characteristics across datapoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AUC Maximization for Low-Resource Named Entity Recognition <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ngoc Dang Nguyen, Wei Tan, Wray Buntine, Richard Beare, Changyou Chen, Lan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, AAAI 2023 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object
  Detection System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Yang, Jianan Liu, Tao Huang, Qing-Long Han, Gang Ma, Bing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving systems, LiDAR and radar play important roles in the
perception of the surrounding environment.LiDAR provides accurate 3D spatial
sensing information but cannot work in adverse weather like fog. On the other
hand, the radar signal can be diffracted when encountering raindrops or mist
particles thanks to its wavelength, but it suffers from large noise. Recent
state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust
detection in adverse weather. The existing works adopt convolutional neural
network architecture to extract features from each sensor data stream, then
align and aggregate the two branch features to predict object detection
results. However, these methods have low accuracy of bounding box estimations
due to a simple design of label assignment and fusion strategies. In this
paper, we propose a bird's-eye view fusion learning-based anchor box-free
object detection system, which fuses the feature derived from the radar
range-azimuth heatmap and the LiDAR point cloud to estimate the possible
objects. Different label assignment strategies have been designed to facilitate
the consistency between the classification of foreground or background anchor
points and the corresponding bounding box regressions. In addition, the
performance of the proposed object detector is further enhanced by employing a
novel interactive transformer module. The superior performance of the proposed
methods in this paper has been demonstrated using the recently published Oxford
radar robotCar dataset, showing that the average precision of our system
significantly outperforms the best state-of-the-art method by 14.4% and 20.5%
at IoU equals 0.8 in clear and foggy weather testing, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multimodal VAEs through Mutual Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.12570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.12570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Joy, Yuge Shi, Philip H. S. Torr, Tom Rainforth, Sebastian M. Schmon, N. Siddharth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal VAEs seek to model the joint distribution over heterogeneous data
(e.g.\ vision, language), whilst also capturing a shared representation across
such modalities. Prior work has typically combined information from the
modalities by reconciling idiosyncratic representations directly in the
recognition model through explicit products, mixtures, or other such
factorisations. Here we introduce a novel alternative, the MEME, that avoids
such explicit combinations by repurposing semi-supervised VAEs to combine
information between modalities implicitly through mutual supervision. This
formulation naturally allows learning from partially-observed data where some
modalities can be entirely missing -- something that most existing approaches
either cannot handle, or do so to a limited extent. We demonstrate that MEME
outperforms baselines on standard metrics across both partial and complete
observation schemes on the MNIST-SVHN (image-image) and CUB (image-text)
datasets. We also contrast the quality of the representations learnt by mutual
supervision against standard approaches and observe interesting trends in its
ability to capture relatedness between data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CgAT: Center-Guided Adversarial Training for Deep Hashing-Based
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10779v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10779v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Yinqun Lin, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing has been extensively utilized in massive image retrieval because
of its efficiency and effectiveness. However, deep hashing models are
vulnerable to adversarial examples, making it essential to develop adversarial
defense methods for image retrieval. Existing solutions achieved limited
defense performance because of using weak adversarial samples for training and
lacking discriminative optimization objectives to learn robust features. In
this paper, we present a min-max based Center-guided Adversarial Training,
namely CgAT, to improve the robustness of deep hashing networks through worst
adversarial examples. Specifically, we first formulate the center code as a
semantically-discriminative representative of the input image content, which
preserves the semantic similarity with positive samples and dissimilarity with
negative examples. We prove that a mathematical formula can calculate the
center code immediately. After obtaining the center codes in each optimization
iteration of the deep hashing network, they are adopted to guide the
adversarial training process. On the one hand, CgAT generates the worst
adversarial examples as augmented data by maximizing the Hamming distance
between the hash codes of the adversarial examples and the center codes. On the
other hand, CgAT learns to mitigate the effects of adversarial samples by
minimizing the Hamming distance to the center codes. Extensive experiments on
the benchmark datasets demonstrate the effectiveness of our adversarial
training algorithm in defending against adversarial attacks for deep
hashing-based retrieval. Compared with the current state-of-the-art defense
method, we significantly improve the defense performance by an average of
18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jujutsu: A Two-stage Defense against Adversarial Patch Attacks on Deep
  Neural Networks <span class="chip">AsiaCCS'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.05075v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.05075v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitao Chen, Pritam Dash, Karthik Pattabiraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial patch attacks create adversarial examples by injecting arbitrary
distortions within a bounded region of the input to fool deep neural networks
(DNNs). These attacks are robust (i.e., physically-realizable) and universally
malicious, and hence represent a severe security threat to real-world DNN-based
systems.
  We propose Jujutsu, a two-stage technique to detect and mitigate robust and
universal adversarial patch attacks. We first observe that adversarial patches
are crafted as localized features that yield large influence on the prediction
output, and continue to dominate the prediction on any input. Jujutsu leverages
this observation for accurate attack detection with low false positives. Patch
attacks corrupt only a localized region of the input, while the majority of the
input remains unperturbed. Therefore, Jujutsu leverages generative adversarial
networks (GAN) to perform localized attack recovery by synthesizing the
semantic contents of the input that are corrupted by the attacks, and
reconstructs a ``clean'' input for correct prediction.
  We evaluate Jujutsu on four diverse datasets spanning 8 different DNN models,
and find that it achieves superior performance and significantly outperforms
four existing defenses. We further evaluate Jujutsu against physical-world
attacks, as well as adaptive attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AsiaCCS'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When to Update Your Model: Constrained Model-based Reinforcement
  Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianying Ji, Yu Luo, Fuchun Sun, Mingxuan Jing, Fengxiang He, Wenbing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing and analyzing model-based RL (MBRL) algorithms with guaranteed
monotonic improvement has been challenging, mainly due to the interdependence
between policy optimization and model learning. Existing discrepancy bounds
generally ignore the impacts of model shifts, and their corresponding
algorithms are prone to degrade performance by drastic model updating. In this
work, we first propose a novel and general theoretical scheme for a
non-decreasing performance guarantee of MBRL. Our follow-up derived bounds
reveal the relationship between model shifts and performance improvement. These
discoveries encourage us to formulate a constrained lower-bound optimization
problem to permit the monotonicity of MBRL. A further example demonstrates that
learning models from a dynamically-varying number of explorations benefit the
eventual returns. Motivated by these analyses, we design a simple but effective
algorithm CMLO (Constrained Model-shift Lower-bound Optimization), by
introducing an event-triggered mechanism that flexibly determines when to
update the model. Experiments show that CMLO surpasses other state-of-the-art
methods and produces a boost when various policy optimization methods are
employed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Rigorous Information-Theoretic Definition of Redundancy and Relevancy
  in Feature Selection Based on (Partial) Information Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.04187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.04187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patricia Wollstadt, Sebastian Schmitt, Michael Wibral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting a minimal feature set that is maximally informative about a target
variable is a central task in machine learning and statistics. Information
theory provides a powerful framework for formulating feature selection
algorithms -- yet, a rigorous, information-theoretic definition of feature
relevancy, which accounts for feature interactions such as redundant and
synergistic contributions, is still missing. We argue that this lack is
inherent to classical information theory which does not provide measures to
decompose the information a set of variables provides about a target into
unique, redundant, and synergistic contributions. Such a decomposition has been
introduced only recently by the partial information decomposition (PID)
framework. Using PID, we clarify why feature selection is a conceptually
difficult problem when approached using information theory and provide a novel
definition of feature relevancy and redundancy in PID terms. From this
definition, we show that the conditional mutual information (CMI) maximizes
relevancy while minimizing redundancy and propose an iterative, CMI-based
algorithm for practical feature selection. We demonstrate the power of our
CMI-based algorithm in comparison to the unconditional mutual information on
benchmark examples and provide corresponding PID estimates to highlight how PID
allows to quantify information contribution of features and their interactions
in feature-selection problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 12 figures. Reorganization and shortening of manuscript,
  added Appendix with theoretical guarantees, background information on the
  algorithm used, and an additional example application on a larger problem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Đorđe Miladinović, Kumar Shridhar, Kushal Jain, Max B. Paulus, Joachim M. Buhmann, Mrinmaya Sachan, Carl Allen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In principle, applying variational autoencoders (VAEs) to sequential data
offers a method for controlled sequence generation, manipulation, and
structured representation learning. However, training sequence VAEs is
challenging: autoregressive decoders can often explain the data without
utilizing the latent space, known as posterior collapse. To mitigate this,
state-of-the-art models weaken the powerful decoder by applying uniformly
random dropout to the decoder input. We show theoretically that this removes
pointwise mutual information provided by the decoder input, which is
compensated for by utilizing the latent space. We then propose an adversarial
training strategy to achieve information-based stochastic dropout. Compared to
uniform dropout on standard text benchmark datasets, our targeted approach
increases both sequence modeling performance and the information captured in
the latent space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity
  Characterization <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08037v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08037v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Bello, Bryon Aragam, Pradeep Ravikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combinatorial problem of learning directed acyclic graphs (DAGs) from
data was recently framed as a purely continuous optimization problem by
leveraging a differentiable acyclicity characterization of DAGs based on the
trace of a matrix exponential function. Existing acyclicity characterizations
are based on the idea that powers of an adjacency matrix contain information
about walks and cycles. In this work, we propose a new acyclicity
characterization based on the log-determinant (log-det) function, which
leverages the nilpotency property of DAGs. To deal with the inherent
asymmetries of a DAG, we relate the domain of our log-det characterization to
the set of $\textit{M-matrices}$, which is a key difference to the classical
log-det function defined over the cone of positive definite matrices. Similar
to acyclicity functions previously proposed, our characterization is also exact
and differentiable. However, when compared to existing characterizations, our
log-det function: (1) Is better at detecting large cycles; (2) Has
better-behaved gradients; and (3) Its runtime is in practice about an order of
magnitude faster. From the optimization side, we drop the typically used
augmented Lagrangian scheme and propose DAGMA ($\textit{DAGs via M-matrices for
Acyclicity}$), a method that resembles the central path for barrier methods.
Each point in the central path of DAGMA is a solution to an unconstrained
problem regularized by our log-det function, then we show that at the limit of
the central path the solution is guaranteed to be a DAG. Finally, we provide
extensive experiments for $\textit{linear}$ and $\textit{nonlinear}$ SEMs and
show that our approach can reach large speed-ups and smaller structural Hamming
distances against state-of-the-art methods. Code implementing the proposed
method is open-source and publicly available at
https://github.com/kevinsbello/dagma.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 13 figures, published at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter estimation of the homodyned K distribution based on neural
  networks and trainable fractional-order moments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Byra, Ziemowit Klimonda, Piotr Jarosik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homodyned K (HK) distribution has been widely used to describe the scattering
phenomena arising in various research fields, such as ultrasound imaging or
optics. In this work, we propose a machine learning based approach to the
estimation of the HK distribution parameters. We develop neural networks that
can estimate the HK distribution parameters based on the signal-to-noise ratio,
skewness and kurtosis calculated using fractional-order moments. Compared to
the previous approaches, we consider the orders of the moments as trainable
variables that can be optimized along with the network weights using the
back-propagation algorithm. Networks are trained based on samples generated
from the HK distribution. Obtained results demonstrate that the proposed method
can be used to accurately estimate the HK distribution parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Amodal Completion: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Ao, Qiuhong Ke, Krista A. Ehinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing computer vision systems can compete with humans in understanding the
visible parts of objects, but still fall far short of humans when it comes to
depicting the invisible parts of partially occluded objects. Image amodal
completion aims to equip computers with human-like amodal completion functions
to understand an intact object despite it being partially occluded. The main
purpose of this survey is to provide an intuitive understanding of the research
hotspots, key technologies and future trends in the field of image amodal
completion. Firstly, we present a comprehensive review of the latest literature
in this emerging field, exploring three key tasks in image amodal completion,
including amodal shape completion, amodal appearance completion, and order
perception. Then we examine popular datasets related to image amodal completion
along with their common data collection methods and evaluation metrics.
Finally, we discuss real-world applications and future research directions for
image amodal completion, facilitating the reader's understanding of the
challenges of existing technologies and upcoming research trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript is under consideration at Computer Vision and Image
  Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box
  Score-Based Query Attacks <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12134v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12134v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe Chen, Zhehao Huang, Qinghua Tao, Yingwen Wu, Cihang Xie, Xiaolin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The score-based query attacks (SQAs) pose practical threats to deep neural
networks by crafting adversarial perturbations within dozens of queries, only
using the model's output scores. Nonetheless, we note that if the loss trend of
the outputs is slightly perturbed, SQAs could be easily misled and thereby
become much less effective. Following this idea, we propose a novel defense,
namely Adversarial Attack on Attackers (AAA), to confound SQAs towards
incorrect attack directions by slightly modifying the output logits. In this
way, (1) SQAs are prevented regardless of the model's worst-case robustness;
(2) the original model predictions are hardly changed, i.e., no degradation on
clean accuracy; (3) the calibration of confidence scores can be improved
simultaneously. Extensive experiments are provided to verify the above
advantages. For example, by setting $\ell_\infty=8/255$ on CIFAR-10, our
proposed AAA helps WideResNet-28 secure 80.59% accuracy under Square attack
(2500 queries), while the best prior defense (i.e., adversarial training) only
attains 67.44%. Since AAA attacks SQA's general greedy strategy, such
advantages of AAA over 8 defenses can be consistently observed on 8
CIFAR-10/ImageNet models under 6 SQAs, using different attack targets, bounds,
norms, losses, and strategies. Moreover, AAA calibrates better without hurting
the accuracy. Our code is available at https://github.com/Sizhe-Chen/AAA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning particle swarming models from data with Gaussian processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Feng, Charles Kulick, Yunxiang Ren, Sui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interacting particle or agent systems that display a rich variety of swarming
behaviours are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and swarming. In this paper, we study the data-driven discovery of a
second-order particle swarming model that describes the evolution of $N$
particles in $\mathbb{R}^d$ under radial interactions. We propose a learning
approach that models the latent radial interaction function as Gaussian
processes, which can simultaneously fulfill two inference goals: one is the
nonparametric inference of {the} interaction function with pointwise
uncertainty quantification, and the other one is the inference of unknown
scalar parameters in the non-collective friction forces of the system. We
formulate the learning problem as a statistical inverse problem and provide a
detailed analysis of recoverability conditions, establishing that a coercivity
condition is sufficient for recoverability. Given data collected from $M$ i.i.d
trajectories with independent Gaussian observational noise, we provide a
finite-sample analysis, showing that our posterior mean estimator converges in
a Reproducing kernel Hilbert space norm, at an optimal rate in $M$ equal to the
one in the classical 1-dimensional Kernel Ridge regression. As a byproduct, we
show we can obtain a parametric learning rate in $M$ for the posterior marginal
variance using $L^{\infty}$ norm, and the rate could also involve $N$ and $L$
(the number of observation time instances for each trajectory), depending on
the condition number of the inverse problem. Numerical results on systems that
exhibit different swarming behaviors demonstrate efficient learning of our
approach from scarce noisy trajectory data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages; Appendix 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointConvFormer: Revenge of the Point-based Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wu, Qi Shan, Li Fuxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PointConvFormer, a novel building block for point cloud based
deep network architectures. Inspired by generalization theory, PointConvFormer
combines ideas from point convolution, where filter weights are only based on
relative position, and Transformers which utilize feature-based attention. In
PointConvFormer, attention computed from feature difference between neighboring
points is used to modify the convolutional weights at each point. Hence,
invariances from point convolution are preserved, whereas attention helps to
select relevant points in the neighborhood. PointConvFormer is suitable for
multiple tasks that require details at the point level, such as segmentation
and scene flow estimation tasks. We experiment on both tasks with multiple
datasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our
results show that PointConvFormer substantially outperforms classic
convolutions, regular transformers, and voxelized sparse convolution approaches
with much smaller and faster networks. Visualizations show that PointConvFormer
performs similarly to convolution on flat areas, whereas the neighborhood
selection effect is stronger on object boundaries, showing that it has got the
best of both worlds.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGAN: Hierarchical Graph Alignment Network for Image-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Guo, Meiting Wang, Yan Zhou, Bin Song, Yuhao Chi, Wei Fan, Jianglong Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text retrieval (ITR) is a challenging task in the field of multimodal
information processing due to the semantic gap between different modalities. In
recent years, researchers have made great progress in exploring the accurate
alignment between image and text. However, existing works mainly focus on the
fine-grained alignment between image regions and sentence fragments, which
ignores the guiding significance of context background information. Actually,
integrating the local fine-grained information and global context background
information can provide more semantic clues for retrieval. In this paper, we
propose a novel Hierarchical Graph Alignment Network (HGAN) for image-text
retrieval. First, to capture the comprehensive multimodal features, we
construct the feature graphs for the image and text modality respectively.
Then, a multi-granularity shared space is established with a designed
Multi-granularity Feature Aggregation and Rearrangement (MFAR) module, which
enhances the semantic corresponding relations between the local and global
information, and obtains more accurate feature representations for the image
and text modalities. Finally, the ultimate image and text features are further
refined through three-level similarity functions to achieve the hierarchical
alignment. To justify the proposed model, we perform extensive experiments on
MS-COCO and Flickr30K datasets. Experimental results show that the proposed
HGAN outperforms the state-of-the-art methods on both datasets, which
demonstrates the effectiveness and superiority of our model.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-15T00:00:00Z">2022-12-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in
  Zero-Shot Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a chain of thought (CoT) can increase large language model (LLM)
performance on a wide range of tasks. Zero-shot CoT evaluations, however, have
been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In
this paper, we perform a controlled evaluation of zero-shot CoT across two
sensitive domains: harmful questions and stereotype benchmarks. We find that
using zero-shot CoT reasoning in a prompt can significantly increase a model's
likelihood to produce undesirable output. Without future advances in alignment
or explicit mitigation instructions, zero-shot CoT should be avoided on tasks
where models can make inferences about marginalized groups or harmful topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, Peng-Jen Chen, Changhan Wang, Yu-An Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct speech-to-speech translation (S2ST), in which all components can be
optimized jointly, is advantageous over cascaded approaches to achieve fast
inference with a simplified pipeline. We present a novel two-pass direct S2ST
architecture, {\textit UnitY}, which first generates textual representations
and predicts discrete acoustic units subsequently. We enhance the model
performance by subword prediction in the first-pass decoder, advanced two-pass
decoder architecture design and search strategy, and better training
regularization. To leverage large amounts of unlabeled text data, we pre-train
the first-pass text decoder based on the self-supervised denoising
auto-encoding task. Experimental evaluations on benchmark datasets at various
data scales demonstrate that UnitY outperforms a single-pass speech-to-unit
translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show
that the proposed methods boost the performance even when predicting
spectrogram in the second pass. However, predicting discrete units achieves
2.51x decoding speed-up compared to that case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Early draft. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Held, Christopher Hidey, Fei Liu, Eric Zhu, Rahul Goel, Diyi Yang, Rushin Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern virtual assistants use internal semantic parsing engines to convert
user utterances to actionable commands. However, prior work has demonstrated
that semantic parsing is a difficult multilingual transfer task with low
transfer efficiency compared to other tasks. In global markets such as India
and Latin America, this is a critical issue as switching between languages is
prevalent for bilingual users. In this work we dramatically improve the
zero-shot performance of a multilingual and codeswitched semantic parsing
system using two stages of multilingual alignment. First, we show that
constrastive alignment pretraining improves both English performance and
transfer efficiency. We then introduce a constrained optimization approach for
hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned
Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and
81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing
benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer
parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attributed Question Answering: Evaluation and Modeling for Attributed
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, Kellie Webster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive results across a variety
of tasks while requiring little or no direct supervision. Further, there is
mounting evidence that LLMs may have potential in information-seeking
scenarios. We believe the ability of an LLM to attribute the text that it
generates is likely to be crucial for both system developers and users in this
setting. We propose and study Attributed QA as a key first step in the
development of attributed LLMs. We develop a reproducable evaluation framework
for the task, using human annotations as a gold standard and a correlated
automatic metric that we show is suitable for development settings. We describe
and benchmark a broad set of architectures for the task. Our contributions give
some concrete answers to two key questions (How to measure attribution?, and
How well do current state-of-the-art methods perform on attribution?), and give
some hints as to how to address a third key question (How to build LLMs with
attribution?).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-VALUE: A Framework for Cross-Dialectal English NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Ziems, William Held, Jingfeng Yang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialect differences caused by regional, social, and economic barriers cause
performance discrepancies for many groups of users of language technology.
Fair, inclusive, and equitable language technology must critically be dialect
invariant, meaning that performance remains constant over dialectal shifts.
Current English systems often fall significantly short of this ideal since they
are designed and tested on a single dialect: Standard American English. We
introduce Multi-VALUE -- a suite of resources for evaluating and achieving
English dialect invariance. We build a controllable rule-based translation
system spanning 50 English dialects and a total of 189 unique linguistic
features. Our translation maps Standard American English text to synthetic form
of each dialect, which uses an upper-bound on the natural density of features
in that dialect. First, we use this system to build stress tests for question
answering, machine translation, and semantic parsing tasks. Stress tests reveal
significant performance disparities for leading models on non-standard
dialects. Second, we use this system as a data augmentation technique to
improve the dialect robustness of existing systems. Finally, we partner with
native speakers of Chicano and Indian English to release new gold-standard
variants of the popular CoQA task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages (9 pages + appendix); 21 tables; 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span>s are Parameter-Efficient Audio-Visual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved impressive results on various
computer vision tasks in the last several years. In this work, we study the
capability of frozen ViTs, pretrained only on visual data, to generalize to
audio-visual data without finetuning any of its original parameters. To do so,
we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained
ViTs to audio-visual tasks by injecting a small number of trainable parameters
into every layer of a frozen ViT. To efficiently fuse visual and audio cues,
our LAVISH adapter uses a small set of latent tokens, which form an attention
bottleneck, thus, eliminating the quadratic cost of standard cross-attention.
Compared to the existing modality-specific audio-visual methods, our approach
achieves competitive or even better performance on various audio-visual tasks
while using fewer tunable parameters and without relying on costly audio
pretraining or external audio encoders. Our code is available at
https://genjib.github.io/project_page/LAVISH/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://genjib.github.io/project_page/LAVISH/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Gold Standard: Grounding Summarization Evaluation with
  Robust Human Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation protocols and benchmarks for summarization either exhibit low
inter-annotator agreement or lack the scale needed to draw statistically
significant conclusions, and an in-depth analysis of human evaluation is
lacking. In this work, we address the shortcomings of existing summarization
evaluation along the following axes: 1) We propose a modified summarization
salience protocol, Atomic Content Units (ACUs), which relies on fine-grained
semantic units and allows for high inter-annotator agreement. 2) We curate the
Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation
dataset consisting of over 22k summary-level annotations over state-of-the-art
systems on three datasets. 3) We compare our ACU protocol with three other
human evaluation protocols, underscoring potential confounding factors in
evaluation setups. 4) We evaluate existing automatic metrics using the
collected human annotations across evaluation protocols and demonstrate how our
benchmark leads to more statistically stable and significant results.
Furthermore, our findings have important implications for evaluating large
language models (LLMs), as we show that LLMs adjusted by human feedback (e.g.,
GPT-3.5) may overfit unconstrained human evaluation, which is affected by the
annotators' prior, input-agnostic preferences, calling for more robust,
targeted evaluation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RWEN-TTS: Relation-aware Word Encoding Network for Natural
  Text-to-Speech Synthesis <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinhyeok Oh, HyeongRae Noh, Yoonseok Hong, Insoo Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of deep learning, a huge number of text-to-speech (TTS)
models which produce human-like speech have emerged. Recently, by introducing
syntactic and semantic information w.r.t the input text, various approaches
have been proposed to enrich the naturalness and expressiveness of TTS models.
Although these strategies showed impressive results, they still have some
limitations in utilizing language information. First, most approaches only use
graph networks to utilize syntactic and semantic information without
considering linguistic features. Second, most previous works do not explicitly
consider adjacent words when encoding syntactic and semantic information, even
though it is obvious that adjacent words are usually meaningful when encoding
the current word. To address these issues, we propose Relation-aware Word
Encoding Network (RWEN), which effectively allows syntactic and semantic
information based on two modules (i.e., Semantic-level Relation Encoding and
Adjacent Word Relation Encoding). Experimental results show substantial
improvements compared to previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually-augmented <span class="highlight-title">pretrain</span>ed language models for NLP tasks without
  images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Qinyu Zhang, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-trained language models (PLMs) have shown impressive performance
by text-only self-supervised training, they are found lack of visual semantics
or commonsense, e.g., sizes, shapes, and colors of commonplace objects.
Existing solutions often rely on explicit images for visual knowledge
augmentation (requiring time-consuming retrieval or generation), and they also
conduct the augmentation for the whole input text, without considering whether
it is actually needed in specific inputs or tasks. To address these issues, we
propose a novel visually-augmented fine-tuning approach that can be generally
applied to various PLMs or NLP tasks, without using any retrieved or generated
images, namely VAWI. Specifically, we first identify the visually-hungry words
(VH-words) from input text via a token selector, where three different methods
have been proposed, including syntax-, attention- and learning-based
strategies. Then, we adopt a fixed CLIP text encoder to generate the
visually-augmented representations of these VH-words. As it has been
pre-trained by vision-language alignment task on the large-scale corpus, it is
capable of injecting visual semantics into the aligned text representations.
Finally, the visually-augmented features will be fused and transformed into the
pre-designed visual prompts based on VH-words, which can be inserted into PLMs
to enrich the visual semantics in word representations. We conduct extensive
experiments on ten NLP tasks, i.e., GLUE benchmark, CommonsenseQA, CommonGen,
and SNLI-VE. Experimental results show that our approach can consistently
improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and
outperform several competitive baselines significantly. Our codes and data are
publicly available at~\url{https://github.com/RUCAIBox/VAWI}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models show improved downstream task performance when prompted
to generate step-by-step reasoning to justify their final answers. These
reasoning steps greatly improve model interpretability and verification, but
objectively studying their correctness (independent of the final answer) is
difficult without reliable methods for automatic evaluation. We simply do not
know how often the stated reasoning steps actually support the final end task
predictions. In this work, we present ROSCOE, a suite of interpretable,
unsupervised automatic scores that improve and extend previous text generation
evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a
typology of reasoning errors and collect synthetic and human evaluation scores
on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE
can measure semantic consistency, logicality, informativeness, fluency, and
factuality - among other traits - by leveraging properties of step-by-step
rationales. We empirically verify the strength of our metrics on five human
annotated and six programmatically perturbed diagnostics datasets - covering a
diverse set of tasks that require reasoning skills and show that ROSCOE can
consistently outperform baseline metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effects of In-domain Corpus Size on <span class="highlight-title">pre-train</span>ing <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Sanchez, Zheyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many prior language modeling efforts have shown that pre-training on an
in-domain corpus can significantly improve performance on downstream
domain-specific NLP tasks. However, the difficulties associated with collecting
enough in-domain data might discourage researchers from approaching this
pre-training task. In this paper, we conducted a series of experiments by
pre-training Bidirectional Encoder Representations from Transformers (BERT)
with different sizes of biomedical corpora. The results demonstrate that
pre-training on a relatively small amount of in-domain data (4GB) with limited
training steps, can lead to better performance on downstream domain-specific
NLP tasks compared with fine-tuning models pre-trained on general corpora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effects of gender bias in word embeddings on depression prediction <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gizem Sogancioglu, Heysem Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embeddings are extensively used in various NLP problems as a
state-of-the-art semantic feature vector representation. Despite their success
on various tasks and domains, they might exhibit an undesired bias for
stereotypical categories due to statistical and societal biases that exist in
the dataset they are trained on. In this study, we analyze the gender bias in
four different pre-trained word embeddings specifically for the depression
category in the mental disorder domain. We use contextual and non-contextual
embeddings that are trained on domain-independent as well as clinical
domain-specific data. We observe that embeddings carry bias for depression
towards different gender groups depending on the type of embeddings. Moreover,
we demonstrate that these undesired correlations are transferred to the
downstream task for depression phenotype recognition. We find that data
augmentation by simply swapping gender words mitigates the bias significantly
in the downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to and published at "A Participatory Approach to AI for
  Mental Health (PAI4MH)" workshop, co-located with NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention as a guide for Simultaneous Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Papi, Matteo Negri, Marco Turchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of the attention mechanism has sparked interest in many fields,
such as language modeling and machine translation. Although its patterns have
been exploited to perform different tasks, from neural network understanding to
textual alignment, no previous work has analysed the encoder-decoder attention
behavior in speech translation (ST) nor used it to improve ST on a specific
task. In this paper, we fill this gap by proposing an attention-based policy
(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the
existing attention relations between audio input and textual output. Its goal
is to leverage the encoder-decoder attention scores to guide inference in real
time. Results on en->{de, es} show that the EDAtt policy achieves overall
better results compared to the SimulST state of the art, especially in terms of
computational-aware latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASTER: Multi-task <span class="highlight-title">Pre-train</span>ed Bottlenecked Masked Autoencoders are
  Better Dense Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrieval aims to map queries and passages into low-dimensional vector
space for efficient similarity measuring, showing promising effectiveness in
various large-scale retrieval tasks. Since most existing methods commonly adopt
pre-trained Transformers (e.g. BERT) for parameter initialization, some work
focuses on proposing new pre-training tasks for compressing the useful semantic
information from passages into dense vectors, achieving remarkable
performances. However, it is still challenging to effectively capture the rich
semantic information and relations about passages into the dense vectors via
one single particular pre-training task. In this work, we propose a multi-task
pre-trained model, MASTER, that unifies and integrates multiple pre-training
tasks with different learning objectives under the bottlenecked masked
autoencoder architecture. Concretely, MASTER utilizes a multi-decoder
architecture to integrate three types of pre-training tasks: corrupted passages
recovering, related passage recovering and PLMs outputs recovering. By
incorporating a shared deep encoder, we construct a representation bottleneck
in our architecture, compressing the abundant semantic information across tasks
into dense vectors. The first two types of tasks concentrate on capturing the
semantic information of passages and relationships among them within the
pre-training corpus. The third one can capture the knowledge beyond the corpus
from external PLMs (e.g. GPT-2). Extensive experiments on several large-scale
passage retrieval datasets have shown that our approach outperforms the
previous state-of-the-art dense retrieval methods. Our code and data are
publicly released in https://github.com/microsoft/SimXNS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeTIm-Eval: a novel curated evaluation data set for comparing
  text-to-image models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating and comparing text-to-image models is a challenging problem.
Significant advances in the field have recently been made, piquing interest of
various industrial sectors. As a consequence, a gold standard in the field
should cover a variety of tasks and application contexts. In this paper a novel
evaluation approach is experimented, on the basis of: (i) a curated data set,
made by high-quality royalty-free image-text pairs, divided into ten
categories; (ii) a quantitative metric, the CLIP-score, (iii) a human
evaluation task to distinguish, for a given text, the real and the generated
images. The proposed method has been applied to the most recent models, i.e.,
DALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early
experimental results show that the accuracy of the human judgement is fully
coherent with the CLIP-score. The dataset has been made available to the
public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAM: Selective Clarification for Ambiguous Questions with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Kuhn, Yarin Gal, Sebastian Farquhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models are often accurate on many
question-answering benchmarks with well-defined questions. Yet, in real
settings questions are often unanswerable without asking the user for
clarifying information. We show that current SotA models often do not ask the
user for clarification when presented with imprecise questions and instead
provide incorrect answers or "hallucinate". To address this, we introduce CLAM,
a framework that first uses the model to detect ambiguous questions, and if an
ambiguous question is detected, prompts the model to ask the user for
clarification. Furthermore, we show how to construct a scalable and
cost-effective automatic evaluation protocol using an oracle language model
with privileged information to provide clarifying information. We show that our
method achieves a 20.15 percentage point accuracy improvement over SotA on a
novel ambiguous question-answering answering data set derived from TriviaQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COLA: Improving Conversational Recommender Systems by Collaborative
  Augmentation <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongding Lin, Jian Wang, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) aim to employ natural language
conversations to suggest suitable products to users. Understanding user
preferences for prospective items and learning efficient item representations
are crucial for CRS. Despite various attempts, earlier studies mostly learned
item representations based on individual conversations, ignoring item
popularity embodied among all others. Besides, they still need support in
efficiently capturing user preferences since the information reflected in a
single conversation is limited. Inspired by collaborative filtering, we propose
a collaborative augmentation (COLA) method to simultaneously improve both item
representation learning and user preference modeling to address these issues.
We construct an interactive user-item graph from all conversations, which
augments item representations with user-aware information, i.e., item
popularity. To improve user preference modeling, we retrieve similar
conversations from the training corpus, where the involved items and attributes
that reflect the user's potential interests are used to augment the user
representation through gate control. Extensive experiments on two benchmark
datasets demonstrate the effectiveness of our method. Our code and data are
available at https://github.com/DongdingLin/COLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: Triangular Document-level <span class="highlight-title">Pre-train</span>ing for Multilingual Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyuan Lu, Haoyang Huang, Shuming Ma, Dongdong Zhang, Wai Lam, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the current success of multilingual pre-training, most prior works
focus on leveraging monolingual data or bilingual parallel data and overlooked
the value of trilingual parallel data. This paper presents \textbf{Tri}angular
Document-level \textbf{P}re-training (\textbf{TRIP}), which is the first in the
field to extend the conventional monolingual and bilingual pre-training to a
trilingual setting by (i) \textbf{Grafting} the same documents in two languages
into one mixed document, and (ii) predicting the remaining one language as the
reference translation. Our experiments on document-level MT and cross-lingual
abstractive summarization show that TRIP brings by up to 3.65 d-BLEU points and
6.2 ROUGE-L points on three multilingual document-level machine translation
benchmarks and one cross-lingual abstractive summarization benchmark, including
multiple strong state-of-the-art (SOTA) scores. In-depth analysis indicates
that TRIP improves document-level machine translation and captures better
document contexts in at least three characteristics: (i) tense consistency,
(ii) noun consistency and (iii) conjunction presence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreCDo: A Large Corpus for French Cross-Domain Dialect Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihaela Gaman, Adrian-Gabriel Chifu, William Domingues, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel corpus for French dialect identification comprising
413,522 French text samples collected from public news websites in Belgium,
Canada, France and Switzerland. To ensure an accurate estimation of the dialect
identification performance of models, we designed the corpus to eliminate
potential biases related to topic, writing style, and publication source. More
precisely, the training, validation and test splits are collected from
different news websites, while searching for different keywords (topics). This
leads to a French cross-domain (FreCDo) dialect identification task. We conduct
experiments with four competitive baselines, a fine-tuned CamemBERT model, an
XGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)
classifier based on fine-tuned CamemBERT features, and an SVM based on word
n-grams. Aside from presenting quantitative results, we also make an analysis
of the most discriminative features learned by CamemBERT. Our corpus is
available at https://github.com/MihaelaGaman/FreCDo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-based Disentanglement with Distant Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disentangled representation learning remains challenging as ground truth
factors of variation do not naturally exist. To address this, we present
Vocabulary Disentanglement Retrieval~(VDR), a simple yet effective
retrieval-based disentanglement framework that leverages nature language as
distant supervision. Our approach is built upon the widely-used bi-encoder
architecture with disentanglement heads and is trained on data-text pairs that
are readily available on the web or in existing datasets. This makes our
approach task- and modality-agnostic with potential for a wide range of
downstream applications. We conduct experiments on 16 datasets in both
text-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.
With the incorporation of disentanglement heads and a minor increase in
parameters, VDR achieves significant improvements over the base retriever it is
built upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text
retrieval and an average of 13% higher recall in cross-modal retrieval. In
comparison to other baselines, VDR outperforms them in most tasks, while also
improving explainability and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s learn in-context by gradient descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the state-of-the-art neural network architecture
across numerous domains of machine learning. This is partly due to their
celebrated ability to transfer and to learn in-context based on few examples.
Nevertheless, the mechanisms by which Transformers become in-context learners
are not well understood and remain mostly an intuition. Here, we argue that
training Transformers on auto-regressive tasks can be closely related to
well-known gradient-based meta-learning formulations. We start by providing a
simple weight construction that shows the equivalence of data transformations
induced by 1) a single linear self-attention layer and by 2) gradient-descent
(GD) on a regression loss. Motivated by that construction, we show empirically
that when training self-attention-only Transformers on simple regression tasks
either the models learned by GD and Transformers show great similarity or,
remarkably, the weights found by optimization match the construction. Thus we
show how trained Transformers implement gradient descent in their forward pass.
This allows us, at least in the domain of regression problems, to
mechanistically understand the inner workings of optimized Transformers that
learn in-context. Furthermore, we identify how Transformers surpass plain
gradient descent by an iterative curvature correction and learn linear models
on deep data representations to solve non-linear regression tasks. Finally, we
discuss intriguing parallels to a mechanism identified to be crucial for
in-context learning termed induction-head (Olsson et al., 2022) and show how it
could be understood as a specific case of in-context learning by gradient
descent learning within Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summary-Oriented Vision Modeling for Multimodal Abstractive
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal abstractive summarization (MAS) is to produce a
concise summary given the multimodal data (text and vision). Existing studies
on MAS mainly focus on how to effectively use the extracted visual features,
having achieved impressive success on the high-resource English dataset.
However, less attention has been paid to the quality of the visual features to
the summary, which may limit the model performance especially in the low- and
zero-resource scenarios. In this paper, we propose to improve the summary
quality through summary-oriented visual features. To this end, we devise two
auxiliary tasks including \emph{vision to summary task} and \emph{masked image
modeling task}. Together with the main summarization task, we optimize the MAS
model via the training objectives of all these tasks. By these means, the MAS
model can be enhanced by capturing the summary-oriented visual features,
thereby yielding more accurate summaries. Experiments on 44 languages, covering
mid-high-, low-, and zero-resource scenarios, verify the effectiveness and
superiority of the proposed approach, which achieves state-of-the-art
performance under all scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Two Losses and Two <span class="highlight-title">Dataset</span>s Simultaneously to Improve TempoWiC
  Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Pirhadi, Motahhare Mirzaei, Sauleh Eetemadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  WSD (Word Sense Disambiguation) is the task of identifying which sense of a
word is meant in a sentence or other segment of text. Researchers have worked
on this task (e.g. Pustejovsky, 2002) for years but it's still a challenging
one even for SOTA (state-of-the-art) LMs (language models). The new dataset,
TempoWiC introduced by Loureiro et al. (2022b) focuses on the fact that words
change over time. Their best baseline achieves 70.33% macro-F1. In this work,
we use two different losses simultaneously to train RoBERTa-based
classification models. We also improve our model by using another similar
dataset to generalize better. Our best configuration beats their best baseline
by 4.23% and reaches 74.56% macroF1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Text Classification Accuracy with Intent Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification, a core component of task-oriented dialogue systems,
attracts continuous research from both the research and industry community, and
has resulted in tremendous progress. However, existing method does not consider
the use of label information, which may weaken the performance of text
classification systems in some token-aware scenarios. To address the problem,
in this paper, we introduce the use of label information as label embedding for
the task of text classification and achieve remarkable performance on benchmark
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient-based Intra-attention Pruning on <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqing Yang, Yiming Cui, Xin Yao, Shijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models achieve superior performance, but they are
computationally expensive due to their large size. Techniques such as pruning
and knowledge distillation (KD) have been developed to reduce their size and
latency. In most structural pruning methods, the pruning units, such as
attention heads and feed-forward hidden dimensions, only span a small model
structure space and limit the structures that the pruning algorithm can
explore. In this work, we propose Gradient-based Intra-attention pruning
(GRAIN), which inspects fine intra-attention structures, and allows different
heads to have different sizes. Intra-attention pruning greatly expands the
searching space of model structures and yields highly heterogeneous structures.
We further propose structure regularization to encourage generating more
regular structures, which achieves higher speedups than heterogeneous ones. We
also integrate KD into the pruning process with a gradient separation strategy
to reduce the interference of KD with the pruning process. GRAIN is evaluated
on a variety of tasks. Results show that it notably outperforms other methods
at the same or similar model size. Even under extreme compression where only
$3\%$ weights in transformers remain, the pruned model is still competitive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Pre-train</span>ing of Masked Language Model via Concept-based
  Curriculum Masking <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Lee, Jun-Hyung Park, Junho Kim, Kang-Min Kim, SangKeun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked language modeling (MLM) has been widely used for pre-training
effective bidirectional representations, but incurs substantial training costs.
In this paper, we propose a novel concept-based curriculum masking (CCM) method
to efficiently pre-train a language model. CCM has two key differences from
existing curriculum learning approaches to effectively reflect the nature of
MLM. First, we introduce a carefully-designed linguistic difficulty criterion
that evaluates the MLM difficulty of each token. Second, we construct a
curriculum that gradually masks words related to the previously masked words by
retrieving a knowledge graph. Experimental results show that CCM significantly
improves pre-training efficiency. Specifically, the model trained with CCM
shows comparative performance with the original BERT on the General Language
Understanding Evaluation benchmark at half of the training cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual
  Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maha Elbayad, Anna Sun, Shruti Bhosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely gated Mixture of Experts (MoE) models have been shown to be a
compute-efficient method to scale model capacity for multilingual machine
translation. However, for low-resource tasks, MoE models severely over-fit. We
show effective regularization strategies, namely dropout techniques for MoE
layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods
that prevent over-fitting and improve the performance of MoE models on
low-resource tasks without adversely affecting high-resource tasks. On a
massively multilingual machine translation benchmark, our strategies result in
about +1 chrF++ improvement in very low resource language pairs. We perform an
extensive analysis of the learned MoE routing to better understand the impact
of our regularization methods and how we can improve them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Saved You A Click: Automatically Answering Clickbait Titles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Johnson, Beicheng Lou, Janet Zhong, Andrey Kurenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Often clickbait articles have a title that is phrased as a question or vague
teaser that entices the user to click on the link and read the article to find
the explanation. We developed a system that will automatically find the answer
or explanation of the clickbait hook from the website text so that the user
does not need to read through the text themselves. We fine-tune an extractive
question and answering model (RoBERTa) and an abstractive one (T5), using data
scraped from the 'StopClickbait' Facebook pages and Reddit's 'SavedYouAClick'
subforum. We find that both extractive and abstractive models improve
significantly after finetuning. We find that the extractive model performs
slightly better according to ROUGE scores, while the abstractive one has a
slight edge in terms of BERTscores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Chess Commentaries by Combining Language Models with Symbolic
  Reasoning Engines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lee, David Wu, Emily Dinan, Mike Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite many recent advancements in language modeling, state-of-the-art
language models lack grounding in the real world and struggle with tasks
involving complex reasoning. Meanwhile, advances in the symbolic reasoning
capabilities of AI have led to systems that outperform humans in games like
chess and Go (Silver et al., 2018). Chess commentary provides an interesting
domain for bridging these two fields of research, as it requires reasoning over
a complex board state and providing analyses in natural language. In this work
we demonstrate how to combine symbolic reasoning engines with controllable
language models to generate chess commentaries. We conduct experiments to
demonstrate that our approach generates commentaries that are preferred by
human judges over previous baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources
  in Natural Language Understanding Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, Jackie Chi Kit Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many state-of-the-art natural language understanding (NLU) models are based
on pretrained neural language models. These models often make inferences using
information from multiple sources. An important class of such inferences are
those that require both background knowledge, presumably contained in a model's
pretrained parameters, and instance-specific information that is supplied at
inference time. However, the integration and reasoning abilities of NLU models
in the presence of multiple knowledge sources have been largely understudied.
In this work, we propose a test suite of coreference resolution tasks that
require reasoning over multiple facts. Our dataset is organized into subtasks
that differ in terms of which knowledge sources contain relevant facts. We
evaluate state-of-the-art coreference resolution models on our dataset. Our
results indicate that several models struggle to reason on-the-fly over
knowledge observed both at pretrain time and at inference time. However, with
task-specific training, a subset of models demonstrates the ability to
integrate certain knowledge types from multiple sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NBC-Softmax : Darkweb Author fingerprinting and migration tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayan K. Kulatilleke, Shekhar S. Chandra, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn distances from the data, which enhances the
performance of similarity-based algorithms. An author style detection task is a
metric learning problem, where learning style features with small intra-class
variations and larger inter-class differences is of great importance to achieve
better performance. Recently, metric learning based on softmax loss has been
used successfully for style detection. While softmax loss can produce separable
representations, its discriminative power is relatively poor. In this work, we
propose NBC-Softmax, a contrastive loss based clustering technique for softmax
loss, which is more intuitive and able to achieve superior performance. Our
technique meets the criterion for larger number of samples, thus achieving
block contrastiveness, which is proven to outperform pair-wise losses. It uses
mini-batch sampling effectively and is scalable. Experiments on 4 darkweb
social forums, with NBCSAuthor that uses the proposed NBC-Softmax for author
and sybil detection, shows that our negative block contrastive approach
constantly outperforms state-of-the-art methods using the same network
architecture.
  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Measures of Spread in High Dimensional Latent Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna C. Marbut, Katy McKinney-Bock, Travis J. Wheeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding geometric properties of natural language processing models'
latent spaces allows the manipulation of these properties for improved
performance on downstream tasks. One such property is the amount of data spread
in a model's latent space, or how fully the available latent space is being
used. In this work, we define data spread and demonstrate that the commonly
used measures of data spread, Average Cosine Similarity and a partition
function min/max ratio I(V), do not provide reliable metrics to compare the use
of latent space across models. We propose and examine eight alternative
measures of data spread, all but one of which improve over these current
metrics when applied to seven synthetic data distributions. Of our proposed
measures, we recommend one principal component-based measure and one
entropy-based measure that provide reliable, relative measures of spread and
can be used to compare models of different sizes and dimensionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal
  Contributions in Vision and Language Models & Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letitia Parcalabescu, Anette Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision and language models (VL) are known to exploit unrobust indicators in
individual modalities (e.g., introduced by distributional biases), instead of
focusing on relevant information in each modality. A small drop in accuracy
obtained on a VL task with a unimodal model suggests that so-called unimodal
collapse occurred. But how to quantify the amount of unimodal collapse
reliably, at dataset and instance-level, to diagnose and combat unimodal
collapse in a targeted way? We present MM-SHAP, a performance-agnostic
multimodality score that quantifies the proportion by which a model uses
individual modalities in multimodal tasks. MM-SHAP is based on Shapley values
and will be applied in two ways: (1) to compare models for their degree of
multimodality, and (2) to measure the contribution of individual modalities for
a given task and dataset. Experiments with 6 VL models -- LXMERT, CLIP and four
ALBEF variants -- on four VL tasks highlight that unimodal collapse can occur
to different degrees and in different directions, contradicting the wide-spread
assumption that unimodal collapse is one-sided. We recommend MM-SHAP for
analysing multimodal tasks, to diagnose and guide progress towards multimodal
integration. Code available at: https://github.com/Heidelberg-NLP/MM-SHAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 13 appendix pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiDO: Fusion-in-Decoder optimized for stronger performance and faster
  inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD
suffers from very expensive inference. We show that the majority of inference
time results from memory bandwidth constraints in the decoder, and propose two
simple changes to the FiD architecture to speed up inference by 7x. The faster
decoder inference then allows for a much larger decoder. We denote FiD with the
above modifications as FiDO, and show that it strongly improves performance
over existing FiD models for a wide range of inference budgets. For example,
FiDO-Large-XXL performs faster inference than FiD-Base and achieves better
performance than FiD-Large.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Long Sequence Modeling via State Space Augmented <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue
  Systems <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Emelin, Daniele Bonadiman, Sawsan Alqahtani, Yi Zhang, Saab Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLM) have advanced the state-of-the-art across
NLP applications, but lack domain-specific knowledge that does not naturally
occur in pre-training data. Previous studies augmented PLMs with symbolic
knowledge for different downstream NLP tasks. However, knowledge bases (KBs)
utilized in these studies are usually large-scale and static, in contrast to
small, domain-specific, and modifiable knowledge bases that are prominent in
real-world task-oriented dialogue (TOD) systems. In this paper, we showcase the
advantages of injecting domain-specific knowledge prior to fine-tuning on TOD
tasks. To this end, we utilize light-weight adapters that can be easily
integrated with PLMs and serve as a repository for facts learned from different
KBs. To measure the efficacy of proposed knowledge injection methods, we
introduce Knowledge Probing using Response Selection (KPRS) -- a probe designed
specifically for TOD models. Experiments on KPRS and the response generation
task show improvements of knowledge injection with adapters over strong
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint processing of linguistic properties in brains and language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subba Reddy Oota, Manish Gupta, Mariya Toneva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have been shown to be very effective in predicting brain
recordings of subjects experiencing complex language stimuli. For a deeper
understanding of this alignment, it is important to understand the alignment
between the detailed processing of linguistic information by the human brain
versus language models. In NLP, linguistic probing tasks have revealed a
hierarchy of information processing in neural language models that progresses
from simple to complex with an increase in depth. On the other hand, in
neuroscience, the strongest alignment with high-level language brain regions
has consistently been observed in the middle layers. These findings leave an
open question as to what linguistic information actually underlies the observed
alignment between brains and language models. We investigate this question via
a direct approach, in which we eliminate information related to specific
linguistic properties in the language model representations and observe how
this intervention affects the alignment with fMRI brain recordings obtained
while participants listened to a story. We investigate a range of linguistic
properties (surface, syntactic and semantic) and find that the elimination of
each one results in a significant decrease in brain alignment across all layers
of a language model. These findings provide direct evidence for the role of
specific linguistic information in the alignment between brain and language
models, and opens new avenues for mapping the joint information processing in
both systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constitutional AI: Harmlessness from AI Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best-Answer Prediction in Q&A Sites Using User Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafik Hadfi, Ahmed Moustafa, Kai Yoshino, Takayuki Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community Question Answering (CQA) sites have spread and multiplied
significantly in recent years. Sites like Reddit, Quora, and Stack Exchange are
becoming popular amongst people interested in finding answers to diverse
questions. One practical way of finding such answers is automatically
predicting the best candidate given existing answers and comments. Many studies
were conducted on answer prediction in CQA but with limited focus on using the
background information of the questionnaires. We address this limitation using
a novel method for predicting the best answers using the questioner's
background information and other features, such as the textual content or the
relationships with other participants. Our answer classification model was
trained using the Stack Exchange dataset and validated using the Area Under the
Curve (AUC) metric. The experimental results show that the proposed method
complements previous methods by pointing out the importance of the
relationships between users, particularly throughout the level of involvement
in different communities on Stack Exchange. Furthermore, we point out that
there is little overlap between user-relation information and the information
represented by the shallow text features and the meta-features, such as time
differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual
  Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maha Elbayad, Anna Sun, Shruti Bhosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely gated Mixture of Experts (MoE) models have been shown to be a
compute-efficient method to scale model capacity for multilingual machine
translation. However, for low-resource tasks, MoE models severely over-fit. We
show effective regularization strategies, namely dropout techniques for MoE
layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods
that prevent over-fitting and improve the performance of MoE models on
low-resource tasks without adversely affecting high-resource tasks. On a
massively multilingual machine translation benchmark, our strategies result in
about +1 chrF++ improvement in very low resource language pairs. We perform an
extensive analysis of the learned MoE routing to better understand the impact
of our regularization methods and how we can improve them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.04672</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Preference Learning for Storytelling via Contrastive
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Castricato, Alexander Havrilla, Shahbuland Matiana, Michael Pieler, Anbang Ye, Ian Yang, Spencer Frazier, Mark Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlled automated story generation seeks to generate natural language
stories satisfying constraints from natural language critiques or preferences.
Existing methods to control for story preference utilize prompt engineering
which is labor intensive and often inconsistent. They may also use
logit-manipulation methods which require annotated datasets to exist for the
desired attributes. To address these issues, we first train a contrastive
bi-encoder model to align stories with corresponding human critiques, named
CARP, building a general purpose preference model. This is subsequently used as
a reward function to fine-tune a generative language model via reinforcement
learning. However, simply fine-tuning a generative language model with a
contrastive reward model does not always reliably result in a story generation
system capable of generating stories that meet user preferences. To increase
story generation robustness we further fine-tune the contrastive reward model
using a prompt-learning technique. A human participant study is then conducted
comparing generations from our full system, ablations, and two baselines. We
show that the full fine-tuning pipeline results in a story generator preferred
over a LLM 20x as large as well as logit-based methods. This motivates the use
of contrastive learning for general purpose human preference modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Tokenization Learning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11443v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11443v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kolonin, Vignav Ramesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the presented study, we discover that the so-called "transition freedom"
metric appears superior for unsupervised tokenization purposes in comparison to
statistical metrics such as mutual information and conditional probability,
providing F-measure scores in range from 0.71 to 1.0 across explored
multilingual corpora. We find that different languages require different
offshoots of that metric (such as derivative, variance, and "peak values") for
successful tokenization. Larger training corpora do not necessarily result in
better tokenization quality, while compressing the models by eliminating
statistically weak evidence tends to improve performance. The proposed
unsupervised tokenization technique provides quality better than or comparable
to lexicon-based ones, depending on the language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures; Paper accepted to the EMNLP 2022 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Label Errors by using <span class="highlight-title">Pre-Train</span>ed Language Models <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12702v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12702v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derek Chong, Jenny Hong, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that large pre-trained language models are inherently highly capable
of identifying label errors in natural language datasets: simply examining
out-of-sample data points in descending order of fine-tuned task loss
significantly outperforms more complex error-detection mechanisms proposed in
previous work.
  To this end, we contribute a novel method for introducing realistic,
human-originated label noise into existing crowdsourced datasets such as SNLI
and TweetNLP. We show that this noise has similar properties to real,
hand-verified label errors, and is harder to detect than existing synthetic
noise, creating challenges for model robustness. We argue that human-originated
noise is a better standard for evaluation than synthetic noise.
  Finally, we use crowdsourced verification to evaluate the detection of real
errors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models
perform at a 9-36% higher absolute Area Under the Precision-Recall Curve than
existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures. Accepted to EMNLP 2022; typesetting of this
  version slightly differs from conference version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous diffusion for categorical data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, Jonas Adler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures; corrections and additional information about
  hyperparameters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalized and Explainable Long-Range Context Representation
  for Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvodip Dey, Maunendra Sankar Desarkar, P. K. Srijith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-range context modeling is crucial to both dialogue understanding and
generation. The most popular method for dialogue context representation is to
concatenate the last-$k$ previous utterances. However, this method may not be
ideal for conversations containing long-range dependencies. In this work, we
propose DialoGX, a novel encoder-decoder based framework for conversational
response generation with a generalized and explainable context representation
that can look beyond the last-$k$ utterances. Hence the method is adaptive to
conversations with long-range dependencies. The main idea of our approach is to
identify and utilize the most relevant historical utterances instead of the
last-$k$ utterances in chronological order. We study the effectiveness of our
proposed method on both dialogue generation (open-domain) and understanding
(DST) tasks. DialoGX achieves comparable performance with the state-of-the-art
models on DailyDialog dataset. We also observe performance gain in existing DST
models with our proposed context representation strategy on MultiWOZ dataset.
We justify our context representation through the lens of psycholinguistics and
show that the relevance score of previous utterances agrees well with human
cognition which makes DialoGX explainable as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAL<span class="highlight-title">BERT</span>: Teaching AL<span class="highlight-title">BERT</span> to Ponder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Balagansky, Daniil Gavrilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
in wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layer's
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from the $i$-th layer, introduces major variance in exit layer indices,
significantly reducing the resulting model's performance. In this paper, we
propose improving PonderNet with a novel deterministic Q-exit criterion and a
revisited model architecture. We adapted the proposed mechanism to ALBERT and
RoBERTa and compared it with recent methods for performing an early exit. We
observed that the proposed changes can be considered significant improvements
on the original PonderNet architecture and outperform PABEE on a wide range of
GLUE tasks. In addition, we also performed an in-depth ablation study of the
proposed architecture to further understand Lambda layers and their
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndicSUPERB: A Speech Processing Universal Performance Benchmark for
  Indian languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahir Javed, Kaushal Santosh Bhogale, Abhigyan Raman, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A cornerstone in AI research has been the creation and adoption of
standardized training and test datasets to earmark the progress of
state-of-the-art models. A particularly successful example is the GLUE dataset
for training and evaluating Natural Language Understanding (NLU) models for
English. The large body of research around self-supervised BERT-based language
models revolved around performance improvements on NLU tasks in GLUE. To
evaluate language models in other languages, several language-specific GLUE
datasets were created. The area of speech language understanding (SLU) has
followed a similar trajectory. The success of large self-supervised models such
as wav2vec2 enable creation of speech models with relatively easy to access
unlabelled data. These models can then be evaluated on SLU tasks, such as the
SUPERB benchmark. In this work, we extend this to Indic languages by releasing
the IndicSUPERB benchmark. Specifically, we make the following three
contributions. (i) We collect Kathbath containing 1,684 hours of labelled
speech data across 12 Indian languages from 1,218 contributors located in 203
districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech
tasks: Automatic Speech Recognition, Speaker Verification, Speaker
Identification (mono/multi), Language Identification, Query By Example, and
Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train
and evaluate different self-supervised models alongside a commonly used
baseline FBANK. We show that language-specific fine-tuned models are more
accurate than baseline on most of the tasks, including a large gap of 76\% for
the Language Identification task. However, for speaker identification,
self-supervised models trained on large datasets demonstrate an advantage. We
hope IndicSUPERB contributes to the progress of developing speech language
understanding models for Indian languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposing a Recurrent Neural Network into Modules for Enabling
  Reusability and Replacement <span class="chip">ICSE'2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayem Mohammad Imtiaz, Fraol Batole, Astha Singh, Rangeet Pan, Breno Dantas Cruz, Hridesh Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we take a recurrent neural network (RNN) trained to translate between
languages and augment it to support a new natural language without retraining
the model from scratch? Can we fix the faulty behavior of the RNN by replacing
portions associated with the faulty behavior? Recent works on decomposing a
fully connected neural network (FCNN) and convolutional neural network (CNN)
into modules have shown the value of engineering deep models in this manner,
which is standard in traditional SE but foreign for deep learning models.
However, prior works focus on the image-based multiclass classification
problems and cannot be applied to RNN due to (a) different layer structures,
(b) loop structures, (c) different types of input-output architectures, and (d)
usage of both nonlinear and logistic activation functions. In this work, we
propose the first approach to decompose an RNN into modules. We study different
types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN
modules can be reused and replaced in various scenarios. We evaluate our
approach against 5 canonical datasets (i.e., Math QA, Brown Corpus,
Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.
We found that decomposing a trained model has a small cost (Accuracy: -0.6%,
BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced
without needing to retrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 45th international conference on software engineering
  (ICSE'2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FastClass: A Time-Efficient Approach to Weakly-Supervised Text
  Classification <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised text classification aims to train a classifier using only
class descriptions and unlabeled data. Recent research shows that
keyword-driven methods can achieve state-of-the-art performance on various
tasks. However, these methods not only rely on carefully-crafted class
descriptions to obtain class-specific keywords but also require substantial
amount of unlabeled data and takes a long time to train. This paper proposes
FastClass, an efficient weakly-supervised classification approach. It uses
dense text representation to retrieve class-relevant documents from external
unlabeled corpus and selects an optimal subset to train a classifier. Compared
to keyword-driven methods, our approach is less reliant on initial class
descriptions as it no longer needs to expand each class description into a set
of class-specific keywords. Experiments on a wide range of classification tasks
show that the proposed approach frequently outperforms keyword-driven models in
terms of classification accuracy and often enjoys orders-of-magnitude faster
training speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TwHIN-<span class="highlight-title">BERT</span>: A Socially-Enriched <span class="highlight-title">Pre-train</span>ed Language Model for
  Multilingual Tweet Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed El-Kishky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TwHIN-BERT, a multilingual language model trained on in-domain
data from the popular social network Twitter. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on a variety of multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We will freely open-source
TwHIN-BERT and our curated hashtag prediction and social engagement benchmark
datasets to the research community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot
  Document-Level Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tavish McDonald, Brian Tsan, Amar Saini, Juanita Ordonez, Luis Gutierrez, Phan Nguyen, Blake Mason, Brenda Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Numerical Optimizations for Weighted Low-rank Estimation on Language
  Model <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>long paper EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Probabilistic-Logic based Commonsense Representation Framework for
  Modelling Inferences with Multiple Antecedents and Varying Likelihoods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Jaiswal, Liu Yan, Dongkyu Choi, Kenneth Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense knowledge-graphs (CKGs) are important resources towards building
machines that can 'reason' on text or environmental inputs and make inferences
beyond perception. While current CKGs encode world knowledge for a large number
of concepts and have been effectively utilized for incorporating commonsense in
neural models, they primarily encode declarative or single-condition
inferential knowledge and assume all conceptual beliefs to have the same
likelihood. Further, these CKGs utilize a limited set of relations shared
across concepts and lack a coherent knowledge organization structure resulting
in redundancies as well as sparsity across the larger knowledge graph.
Consequently, today's CKGs, while useful for a first level of reasoning, do not
adequately capture deeper human-level commonsense inferences which can be more
nuanced and influenced by multiple contextual or situational factors.
  Accordingly, in this work, we study how commonsense knowledge can be better
represented by -- (i) utilizing a probabilistic logic representation scheme to
model composite inferential knowledge and represent conceptual beliefs with
varying likelihoods and (ii) incorporating a hierarchical conceptual ontology
to identify salient concept-relevant relations and organize beliefs at
different conceptual levels. Our resulting knowledge representation framework
can encode a wider variety of world knowledge and represent beliefs flexibly
using grounded concepts as well as free-text phrases. As a result, the
framework can be utilized as both a traditional free-text knowledge graph and a
grounded logic-based inference system more suitable for neuro-symbolic
applications. We describe how we extend the PrimeNet knowledge base with our
framework through crowd-sourcing and expert-annotation, and demonstrate its
application for more interpretable passage-based semantic parsing and question
answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting the Tesseract Open-Source OCR Engine for Tamil and Sinhala
  Legacy Fonts and Creating a Parallel Corpus for Tamil-Sinhala-English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.05952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.05952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charangan Vasantharajan, Laksika Tharmalingam, Uthayasanker Thayasivam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most low-resource languages do not have the necessary resources to create
even a substantial monolingual corpus. These languages may often be found in
government proceedings but mainly in Portable Document Format (PDF) that
contains legacy fonts. Extracting text from these documents to create a
monolingual corpus is challenging due to legacy font usage and printer-friendly
encoding, which are not optimized for text extraction. Therefore, we propose a
simple, automatic, and novel idea that can scale for Tamil, Sinhala, English
languages, and many documents along with parallel corpora. Since Tamil and
Sinhala are Low-Resource Languages, we improved the performance of Tesseract by
employing LSTM-based training on more than 20 legacy fonts to recognize printed
characters in these languages. Especially, our model detects code-mixed text,
numbers, and special characters from the printed document. It is shown that
this approach can reduce the character-level error rate of Tesseract from 6.03
to 2.61 for Tamil (-3.42% relative change) and 7.61 to 4.74 for Sinhala (-2.87%
relative change), as well as the word-level error rate from 39.68 to 20.61 for
Tamil (-19.07% relative change) and 35.04 to 26.58 for Sinhala (-8.46% relative
change) on the test set. Also, our newly created parallel corpus consists of
185.4k, 168.9k, and 181.04k sentences and 2.11M, 2.22M, and 2.33M Words in
Tamil, Sinhala, and English respectively. This study shows that fine-tuning
Tesseract models on multiple new fonts help to understand the texts and
enhances the performance of the OCR. We made newly trained models and the
source code for fine-tuning Tesseract, freely available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAViL: Masked Audio-Video Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Jitendra Malik, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks. Code will be available
soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-Art: Text-Driven Neural Radiance Fields Stylization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a powerful representation of 3D scenes, the neural radiance field (NeRF)
enables high-quality novel view synthesis from multi-view images. Stylizing
NeRF, however, remains challenging, especially on simulating a text-guided
style with both the appearance and the geometry altered simultaneously. In this
paper, we present NeRF-Art, a text-guided NeRF stylization approach that
manipulates the style of a pre-trained NeRF model with a simple text prompt.
Unlike previous approaches that either lack sufficient geometry deformations
and texture details or require meshes to guide the stylization, our method can
shift a 3D scene to the target style characterized by desired geometry and
appearance variations without any mesh guidance. This is achieved by
introducing a novel global-local contrastive learning strategy, combined with
the directional constraint to simultaneously control both the trajectory and
the strength of the target style. Moreover, we adopt a weight regularization
method to effectively suppress cloudy artifacts and geometry noises which arise
easily when the density field is transformed during geometry stylization.
Through extensive experiments on various styles, we demonstrate that our method
is effective and robust regarding both single-view stylization quality and
cross-view consistency. The code and more results can be found in our project
page: https://cassiepython.github.io/nerfart/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cassiepython.github.io/nerfart/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VolRecon: Volume Rendering of Signed Ray Distance Functions for
  Generalizable Multi-View Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the success of neural volume rendering in novel view synthesis, neural
implicit reconstruction with volume rendering has become popular. However, most
methods optimize per-scene functions and are unable to generalize to novel
scenes. We introduce VolRecon, a generalizable implicit reconstruction method
with Signed Ray Distance Function (SRDF). To reconstruct with fine details and
little noise, we combine projection features, aggregated from multi-view
features with a view transformer, and volume features interpolated from a
coarse global feature volume. A ray transformer computes SRDF values of all the
samples along a ray to estimate the surface location, which are used for volume
rendering of color and depth. Extensive experiments on DTU and ETH3D
demonstrate the effectiveness and generalization ability of our method. On DTU,
our method outperforms SparseNeuS by about 30% in sparse view reconstruction
and achieves comparable quality as MVSNet in full view reconstruction. Besides,
our method shows good generalization ability on the large-scale ETH3D
benchmark. Project page: https://fangjinhuawang.github.io/VolRecon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik Learned-Miller, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization in multi-task learning (MTL) is more challenging than
single-task learning (STL), as the gradient from different tasks can be
contradictory. When tasks are related, it can be beneficial to share some
parameters among them (cooperation). However, some tasks require additional
parameters with expertise in a specific type of data or discrimination
(specialization). To address the MTL challenge, we propose Mod-Squad, a new
model that is Modularized into groups of experts (a 'Squad'). This structure
allows us to formalize cooperation and specialization as the process of
matching experts and tasks. We optimize this matching process during the
training of a single model. Specifically, we incorporate mixture of experts
(MoE) layers into a transformer model, with a new loss that incorporates the
mutual dependence between tasks and experts. As a result, only a small set of
experts are activated for each task. This prevents the sharing of the entire
backbone model between all tasks, which strengthens the model, especially when
the training set size and the number of tasks scale up. More interestingly, for
each task, we can extract the small set of experts as a standalone model that
maintains the same performance as the large model. Extensive experiments on the
Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5
vision tasks show the superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaPortrait: Identity-Preserving Talking Head Generation with Fast
  Personalized Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, Fang Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose an ID-preserving talking head generation framework,
which advances previous methods in two aspects. First, as opposed to
interpolating from sparse flow, we claim that dense landmarks are crucial to
achieving accurate geometry-aware flow fields. Second, inspired by
face-swapping methods, we adaptively fuse the source identity during synthesis,
so that the network better preserves the key characteristics of the image
portrait. Although the proposed model surpasses prior generation fidelity on
established benchmarks, to further make the talking head generation qualified
for real usage, personalized fine-tuning is usually needed. However, this
process is rather computationally demanding that is unaffordable to standard
users. To solve this, we propose a fast adaptation model using a meta-learning
approach. The learned model can be adapted to a high-quality personalized model
as fast as 30 seconds. Last but not the least, a spatial-temporal enhancement
module is proposed to improve the fine details while ensuring temporal
coherency. Extensive experiments prove the significant superiority of our
approach over the state of the arts in both one-shot and personalized settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://meta-portrait.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Vision <span class="highlight-title">Transformer</span>s for MobileNet Size and Speed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the success of Vision Transformers (ViTs) in computer vision tasks,
recent arts try to optimize the performance and complexity of ViTs to enable
efficient deployment on mobile devices. Multiple approaches are proposed to
accelerate attention mechanism, improve inefficient designs, or incorporate
mobile-friendly lightweight convolutions to form hybrid architectures. However,
ViT and its variants still have higher latency or considerably more parameters
than lightweight CNNs, even true for the years-old MobileNet. In practice,
latency and size are both crucial for efficient deployment on
resource-constraint hardware. In this work, we investigate a central question,
can transformer models run as fast as MobileNet and maintain a similar size? We
revisit the design choices of ViTs and propose an improved supernet with low
latency and high parameter efficiency. We further introduce a fine-grained
joint search strategy that can find efficient architectures by optimizing
latency and number of parameters simultaneously. The proposed models,
EfficientFormerV2, achieve about $4\%$ higher top-1 accuracy than MobileNetV2
and MobileNetV2$\times1.4$ on ImageNet-1K with similar latency and parameters.
We demonstrate that properly designed and optimized vision transformers can
achieve high performance with MobileNet-level size and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at:
  https://github.com/snap-research/EfficientFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning a Fast 3D Spectral Approach to Object Segmentation and Tracking
  over Space and Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Burceanu, Marius Leordeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We pose video object segmentation as spectral graph clustering in space and
time, with one graph node for each pixel and edges forming local space-time
neighborhoods. We claim that the strongest cluster in this video graph
represents the salient object. We start by introducing a novel and efficient
method based on 3D filtering for approximating the spectral solution, as the
principal eigenvector of the graph's adjacency matrix, without explicitly
building the matrix. This key property allows us to have a fast parallel
implementation on GPU, orders of magnitude faster than classical approaches for
computing the eigenvector. Our motivation for a spectral space-time clustering
approach, unique in video semantic segmentation literature, is that such
clustering is dedicated to preserving object consistency over time, which we
evaluate using our novel segmentation consistency measure. Further on, we show
how to efficiently learn the solution over multiple input feature channels.
Finally, we extend the formulation of our approach beyond the segmentation
task, into the realm of object tracking. In extensive experiments we show
significant improvements over top methods, as well as over powerful ensembles
that combine them, achieving state-of-the-art on multiple benchmarks, both for
tracking and segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Neural Light Field on Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts in Neural Rendering Fields (NeRF) have shown impressive
results on novel view synthesis by utilizing implicit neural representation to
represent 3D scenes. Due to the process of volumetric rendering, the inference
speed for NeRF is extremely slow, limiting the application scenarios of
utilizing NeRF on resource-constrained hardware, such as mobile devices. Many
works have been conducted to reduce the latency of running NeRF models.
However, most of them still require high-end GPU for acceleration or extra
storage memory, which is all unavailable on mobile devices. Another emerging
direction utilizes the neural light field (NeLF) for speedup, as only one
forward pass is performed on a ray to predict the pixel color. Nevertheless, to
reach a similar rendering quality as NeRF, the network in NeLF is designed with
intensive computation, which is not mobile-friendly. In this work, we propose
an efficient network that runs in real-time on mobile devices for neural
rendering. We follow the setting of NeLF to train our network. Unlike existing
works, we introduce a novel network architecture that runs efficiently on
mobile devices with low latency and small size, i.e., saving $15\times \sim
24\times$ storage compared with MobileNeRF. Our model achieves high-resolution
generation while maintaining real-time inference for both synthetic and
real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering
one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar
image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.
$25.91$ on the real-world forward-facing dataset).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://snap-research.github.io/MobileR2L/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objaverse: A Universe of Annotated 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive data corpora like WebText, Wikipedia, Conceptual Captions,
WebImageText, and LAION have propelled recent dramatic progress in AI. Large
neural models trained on such datasets produce impressive results and top many
of today's benchmarks. A notable omission within this family of large-scale
datasets is 3D data. Despite considerable interest and potential applications
in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with
limited diversity of object categories. Addressing this gap, we present
Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models
with descriptive captions, tags, and animations. Objaverse improves upon
present day 3D repositories in terms of scale, number of categories, and in the
visual diversity of instances within a category. We demonstrate the large
potential of Objaverse via four diverse applications: training generative 3D
models, improving tail category segmentation on the LVIS benchmark, training
open-vocabulary object-navigation models for Embodied AI, and creating a new
benchmark for robustness analysis of vision models. Objaverse can open new
directions for research and enable new applications across the field of AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: objaverse.allenai.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-and-Language Understanding from Pixels Only 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Tschannen, Basil Mustafa, Neil Houlsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models are becoming increasingly effective, in part due to unified
components, such as the Transformer architecture. However, multimodal models
still often consist of many task- and modality-specific pieces and training
procedures. For example, CLIP (Radford et al., 2021) trains independent text
and image towers via a contrastive loss. We explore an additional unification:
the use of a pure pixel-based model to perform image, text, and multimodal
tasks. Our model is trained with contrastive loss alone, so we call it
CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both
regular images and text rendered as images. CLIPPO performs image-based tasks
such as retrieval and zero-shot image classification almost as well as CLIP,
with half the number of parameters and no text-specific tower or embedding.
When trained jointly via image-text contrastive learning and next-sentence
contrastive learning, CLIPPO can perform well on natural language understanding
tasks, without any word-level loss (language modelling or masked language
modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can
obtain good accuracy in visual question answering, simply by rendering the
question and image together. Finally, we exploit the fact that CLIPPO does not
require a tokenizer to show that it can achieve strong performance on
multilingual multimodal retrieval without
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Multimodal Models Robust to Image and Text Perturbations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, Mu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal image-text models have shown remarkable performance in the past
few years. However, evaluating their robustness against distribution shifts is
crucial before adopting them in real-world applications. In this paper, we
investigate the robustness of 9 popular open-sourced image-text models under
common perturbations on five tasks (image-text retrieval, visual reasoning,
visual entailment, image captioning, and text-to-image generation). In
particular, we propose several new multimodal robustness benchmarks by applying
17 image perturbation and 16 text perturbation techniques on top of existing
datasets. We observe that multimodal models are not robust to image and text
perturbations, especially to image perturbations. Among the tested perturbation
methods, character-level perturbations constitute the most severe distribution
shift for text, and zoom blur is the most severe shift for image data. We also
introduce two new robustness metrics (MMI and MOR) for proper evaluations of
multimodal models. We hope our extensive study sheds light on new directions
for the development of robust multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project webpage is at: https://mmrobustness.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexiViT: One Model for All Patch Sizes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers convert images to sequences by slicing them into patches.
The size of these patches controls a speed/accuracy tradeoff, with smaller
patches leading to higher accuracy at greater computational cost, but changing
the patch size typically requires retraining the model. In this paper, we
demonstrate that simply randomizing the patch size at training time leads to a
single set of weights that performs well across a wide range of patch sizes,
making it possible to tailor the model to different compute budgets at
deployment time. We extensively evaluate the resulting model, which we call
FlexiViT, on a wide range of tasks, including classification, image-text
retrieval, open-world detection, panoptic segmentation, and semantic
segmentation, concluding that it usually matches, and sometimes outperforms,
standard ViT models trained at a single patch size in an otherwise identical
setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that
makes it easy to add compute-adaptive capabilities to most models relying on a
ViT backbone architecture. Code and pre-trained models are available at
https://github.com/google-research/big_vision
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and pre-trained models available at
  https://github.com/google-research/big_vision. All authors made significant
  technical contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Deep Boosted CNN and Ensemble Learning based IoT Malware Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saddam Hussain Khan, Wasi Ullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security issues are threatened in various types of networks, especially in
the Internet of Things (IoT) environment that requires early detection. IoT is
the network of real-time devices like home automation systems and can be
controlled by open-source android devices, which can be an open ground for
attackers. Attackers can access the network, initiate a different kind of
security breach, and compromises network control. Therefore, timely detecting
the increasing number of sophisticated malware attacks is the challenge to
ensure the credibility of network protection. In this regard, we have developed
a new malware detection framework, Deep Squeezed-Boosted and Ensemble Learning
(DSBEL), comprised of novel Squeezed-Boosted Boundary-Region
Split-Transform-Merge (SB-BR-STM) CNN and ensemble learning. The proposed
S.T.M. block employs multi-path dilated convolutional, Boundary, and regional
operations to capture the homogenous and heterogeneous global malicious
patterns. Moreover, diverse feature maps are achieved using transfer learning
and multi-path-based squeezing and boosting at initial and final levels to
learn minute pattern variations. Finally, the boosted discriminative features
are extracted from the developed deep SB-BR-STM CNN and provided to the
ensemble classifiers (SVM, M.L.P., and AdaboostM1) to improve the hybrid
learning generalization. The performance analysis of the proposed DSBEL
framework and SB-BR-STM CNN against the existing techniques have been evaluated
by the IOT_Malware dataset on standard performance measures. Evaluation results
show progressive performance as 98.50% accuracy, 97.12% F1-Score, 91.91% MCC,
95.97 % Recall, and 98.42 % Precision. The proposed malware analysis framework
is helpful for the timely detection of malicious activity and suggests future
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Antoniou, Efthymios Georgiou, Alexandros Potamianos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing powerful adversarial attacks is of paramount importance for the
evaluation of $\ell_p$-bounded adversarial defenses. Projected Gradient Descent
(PGD) is one of the most effective and conceptually simple algorithms to
generate such adversaries. The search space of PGD is dictated by the steepest
ascent directions of an objective. Despite the plethora of objective function
choices, there is no universally superior option and robustness overestimation
may arise from ill-suited objective selection. Driven by this observation, we
postulate that the combination of different objectives through a simple loss
alternating scheme renders PGD more robust towards design choices. We
experimentally verify this assertion on a synthetic-data example and by
evaluating our proposed method across 25 different $\ell_{\infty}$-robust
models and 3 datasets. The performance improvement is consistent, when compared
to the single loss counterparts. In the CIFAR-10 dataset, our strongest
adversarial attack outperforms all of the white-box components of AutoAttack
(AA) ensemble, as well as the most powerful attacks existing on the literature,
achieving state-of-the-art results in the computational budget of our study
($T=100$, no restarts).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span>s are Parameter-Efficient Audio-Visual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved impressive results on various
computer vision tasks in the last several years. In this work, we study the
capability of frozen ViTs, pretrained only on visual data, to generalize to
audio-visual data without finetuning any of its original parameters. To do so,
we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained
ViTs to audio-visual tasks by injecting a small number of trainable parameters
into every layer of a frozen ViT. To efficiently fuse visual and audio cues,
our LAVISH adapter uses a small set of latent tokens, which form an attention
bottleneck, thus, eliminating the quadratic cost of standard cross-attention.
Compared to the existing modality-specific audio-visual methods, our approach
achieves competitive or even better performance on various audio-visual tasks
while using fewer tunable parameters and without relying on costly audio
pretraining or external audio encoders. Our code is available at
https://genjib.github.io/project_page/LAVISH/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://genjib.github.io/project_page/LAVISH/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effects of Character-Level Data Augmentation on Style-Based Dating
  of Historical Manuscripts <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Koopmans, Maruf A. Dhali, Lambert Schomaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the production dates of historical manuscripts is one of the main
goals for paleographers when studying ancient documents. Automatized methods
can provide paleographers with objective tools to estimate dates more
accurately. Previously, statistical features have been used to date digitized
historical manuscripts based on the hypothesis that handwriting styles change
over periods. However, the sparse availability of such documents poses a
challenge in obtaining robust systems. Hence, the research of this article
explores the influence of data augmentation on the dating of historical
manuscripts. Linear Support Vector Machines were trained with k-fold
cross-validation on textural and grapheme-based features extracted from
historical manuscripts of different collections, including the Medieval
Paleographical Scale, early Aramaic manuscripts, and the Dead Sea Scrolls.
Results show that training models with augmented data improve the performance
of historical manuscripts dating by 1% - 3% in cumulative scores. Additionally,
this indicates further enhancement possibilities by considering models specific
to the features and the documents' scripts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted after the peer-review process for ICPRAM 2023; scheduled to
  be presented on 22 February 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Urban Scene Semantic Segmentation with Low-Cost Coarse Annotation <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Das, Yongqin Xian, Yang He, Zeynep Akata, Bernt Schiele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For best performance, today's semantic segmentation methods use large and
carefully labeled datasets, requiring expensive annotation budgets. In this
work, we show that coarse annotation is a low-cost but highly effective
alternative for training semantic segmentation models. Considering the urban
scene segmentation scenario, we leverage cheap coarse annotations for
real-world captured data, as well as synthetic data to train our model and show
competitive performance compared with finely annotated real-world data.
Specifically, we propose a coarse-to-fine self-training framework that
generates pseudo labels for unlabeled regions of the coarsely annotated data,
using synthetic data to improve predictions around the boundaries between
semantic classes, and using cross-domain data augmentation to increase
diversity. Our extensive experimental results on Cityscapes and BDD100k
datasets demonstrate that our method achieves a significantly better
performance vs annotation cost tradeoff, yielding a comparable performance to
fully annotated data with only a small fraction of the annotation budget. Also,
when used as pretraining, our framework performs better compared to the
standard fully supervised setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic vehicle trajectory data reconstruction at scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbing Wang, Derek Gloudemans, Zi Nean Teoh, Lisa Liu, Gergely Zachár, William Barbour, Daniel Work
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectory data has received increasing research attention over the
past decades. With the technological sensing improvements such as
high-resolution video cameras, in-vehicle radars and lidars, abundant
individual and contextual traffic data is now available. However, though the
data quantity is massive, it is by itself of limited utility for traffic
research because of noise and systematic sensing errors, thus necessitates
proper processing to ensure data quality. We draw particular attention to
extracting high-resolution vehicle trajectory data from video cameras as
traffic monitoring cameras are becoming increasingly ubiquitous. We explore
methods for automatic trajectory data reconciliation, given "raw" vehicle
detection and tracking information from automatic video processing algorithms.
We propose a pipeline including a) an online data association algorithm to
match fragments that are associated to the same object (vehicle), which is
formulated as a min-cost network flow problem of a graph, and b) a trajectory
reconciliation method formulated as a quadratic program to enhance raw
detection data. The pipeline leverages vehicle dynamics and physical
constraints to associate tracked objects when they become fragmented, remove
measurement noise on trajectories and impute missing data due to
fragmentations. The accuracy is benchmarked on a sample of manually-labeled
data, which shows that the reconciled trajectories improve the accuracy on all
the tested input data for a wide range of measures. An online version of the
reconciliation pipeline is implemented and will be applied in a continuous
video processing system running on a camera network covering a 4-mile stretch
of Interstate-24 near Nashville, Tennessee.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVAL: Explainable Video Anomaly Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Singh, Michael J. Jones, Erik Learned-Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a novel framework for single-scene video anomaly localization that
allows for human-understandable reasons for the decisions the system makes. We
first learn general representations of objects and their motions (using deep
networks) and then use these representations to build a high-level,
location-dependent model of any particular scene. This model can be used to
detect anomalies in new videos of the same scene. Importantly, our approach is
explainable - our high-level appearance and motion features can provide
human-understandable reasons for why any part of a video is classified as
normal or anomalous. We conduct experiments on standard video anomaly detection
datasets (Street Scene, CUHK Avenue, ShanghaiTech and UCSD Ped1, Ped2) and show
significant improvements over the previous state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent Behaviors in Multi-Agent Target Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush K. Sharma, Erin Zaroukian, Derrik E. Asher, Bryson Howell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Only limited studies and superficial evaluations are available on agents'
behaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using
Reinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)
game, which shares task goals with target acquisition, and we create different
adversarial scenarios by replacing RL-trained pursuers' policies with two
distinct (non-RL) analytical strategies. Using heatmaps of agents' positions
(state-space variable) over time, we are able to categorize an RL-trained
evader's behaviors. The novelty of our approach entails the creation of an
influential feature set that reveals underlying data regularities, which allow
us to classify an agent's behavior. This classification may aid in catching the
(enemy) targets by enabling us to identify and predict their behaviors, and
when extended to pursuers, this approach towards identifying teammates'
behavior may allow agents to coordinate more effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article appeared in the news at:
  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Full Contextual Attention for Multi-resolution <span class="highlight-title">Transformer</span>s in Semantic
  Segmentation <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loic Themyr, Clement Rambour, Nicolas Thome, Toby Collins, Alexandre Hostettler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have proved to be very effective for visual recognition tasks.
In particular, vision transformers construct compressed global representations
through self-attention and learnable class tokens. Multi-resolution
transformers have shown recent successes in semantic segmentation but can only
capture local interactions in high-resolution feature maps. This paper extends
the notion of global tokens to build GLobal Attention Multi-resolution (GLAM)
transformers. GLAM is a generic module that can be integrated into most
existing transformer backbones. GLAM includes learnable global tokens, which
unlike previous methods can model interactions between all image regions, and
extracts powerful representations during training. Extensive experiments show
that GLAM-Swin or GLAM-Swin-UNet exhibit substantially better performances than
their vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used
to segment large 3D medical images, and GLAM-nnFormer achieves new
state-of-the-art performance on the BCV dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Winter Conference on Applications of Computer Vision (WACV 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Learned Kernel For Blind Super-Resolution Kernel Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royson Lee, Rui Li, Stylianos I. Venieris, Timothy Hospedales, Ferenc Huszár, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image degradation estimation methods have enabled single-image
super-resolution (SR) approaches to better upsample real-world images. Among
these methods, explicit kernel estimation approaches have demonstrated
unprecedented performance at handling unknown degradations. Nonetheless, a
number of limitations constrain their efficacy when used by downstream SR
models. Specifically, this family of methods yields i) excessive inference time
due to long per-image adaptation times and ii) inferior image fidelity due to
kernel mismatch. In this work, we introduce a learning-to-learn approach that
meta-learns from the information contained in a distribution of images, thereby
enabling significantly faster adaptation to new images with substantially
improved performance in both kernel estimation and image fidelity.
Specifically, we meta-train a kernel-generating GAN, named MetaKernelGAN, on a
range of tasks, such that when a new image is presented, the generator starts
from an informed kernel estimate and the discriminator starts with a strong
capability to distinguish between patch distributions. Compared with
state-of-the-art methods, our experiments show that MetaKernelGAN better
estimates the magnitude and covariance of the kernel, leading to
state-of-the-art blind SR results within a similar computational regime when
combined with a non-blind SR model. Through supervised learning of an
unsupervised learner, our method maintains the generalizability of the
unsupervised learner, improves the optimization stability of kernel estimation,
and hence image adaptation, and leads to a faster inference with a speedup
between 14.24 to 102.1x over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint: under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localizing Scan Targets from Human Pose for Autonomous Lung Ultrasound
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhi Long, Jicang Cai, Abdullah Al-Battal, Shiwei Jin, Jing Zhang, Dacheng Tao, Truong Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound is progressing toward becoming an affordable and versatile
solution to medical imaging. With the advent of COVID-19 global pandemic, there
is a need to fully automate ultrasound imaging as it requires trained operators
in close proximity to patients for long period of time. In this work, we
investigate the important yet seldom-studied problem of scan target
localization, under the setting of lung ultrasound imaging. We propose a purely
vision-based, data driven method that incorporates learning-based computer
vision techniques. We combine a human pose estimation model with a specially
designed regression model to predict the lung ultrasound scan targets, and
deploy multiview stereo vision to enhance the consistency of 3D target
localization. While related works mostly focus on phantom experiments, we
collect data from 30 human subjects for testing. Our method attains an accuracy
level of 15.52 (9.47) mm for probe positioning and 4.32 (3.69){\deg} for probe
orientation, with a success rate above 80% under an error threshold of 25mm for
all scan targets. Moreover, our approach can serve as a general solution to
other types of ultrasound modalities. The code for implementation has been
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First arxiv submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware
  Part-Level Query <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Xiao, Kai Su, Xiaojuan Wang, Dongdong Yu, Lei Jin, Mingshu He, Zehuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a sparse end-to-end multi-person pose regression framework, termed
QueryPose, which can directly predict multi-person keypoint sequences from the
input image. The existing end-to-end methods rely on dense representations to
preserve the spatial detail and structure for precise keypoint localization.
However, the dense paradigm introduces complex and redundant post-processes
during inference. In our framework, each human instance is encoded by several
learnable spatial-aware part-level queries associated with an instance-level
query. First, we propose the Spatial Part Embedding Generation Module (SPEGM)
that considers the local spatial attention mechanism to generate several
spatial-sensitive part embeddings, which contain spatial details and structural
information for enhancing the part-level queries. Second, we introduce the
Selective Iteration Module (SIM) to adaptively update the sparse part-level
queries via the generated spatial-sensitive part embeddings stage-by-stage.
Based on the two proposed modules, the part-level queries are able to fully
encode the spatial details and structural information for precise keypoint
regression. With the bipartite matching, QueryPose avoids the hand-designed
post-processes and surpasses the existing dense end-to-end methods with 73.6 AP
on MS COCO mini-val set and 72.7 AP on CrowdPose test set. Code is available at
https://github.com/buptxyb666/QueryPose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DETR4D: Direct Multi-View 3D Object Detection with Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Luo, Changqing Zhou, Gongjie Zhang, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection with surround-view images is an essential task for
autonomous driving. In this work, we propose DETR4D, a Transformer-based
framework that explores sparse attention and direct feature query for 3D object
detection in multi-view images. We design a novel projective cross-attention
mechanism for query-image interaction to address the limitations of existing
methods in terms of geometric cue exploitation and information loss for
cross-view objects. In addition, we introduce a heatmap generation technique
that bridges 3D and 2D spaces efficiently via query initialization.
Furthermore, unlike the common practice of fusing intermediate spatial features
for temporal aggregation, we provide a new perspective by introducing a novel
hybrid approach that performs cross-frame fusion over past object queries and
image features, enabling efficient and robust modeling of temporal information.
Extensive experiments on the nuScenes dataset demonstrate the effectiveness and
efficiency of the proposed DETR4D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeTIm-Eval: a novel curated evaluation data set for comparing
  text-to-image models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating and comparing text-to-image models is a challenging problem.
Significant advances in the field have recently been made, piquing interest of
various industrial sectors. As a consequence, a gold standard in the field
should cover a variety of tasks and application contexts. In this paper a novel
evaluation approach is experimented, on the basis of: (i) a curated data set,
made by high-quality royalty-free image-text pairs, divided into ten
categories; (ii) a quantitative metric, the CLIP-score, (iii) a human
evaluation task to distinguish, for a given text, the real and the generated
images. The proposed method has been applied to the most recent models, i.e.,
DALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early
experimental results show that the accuracy of the human judgement is fully
coherent with the CLIP-score. The dataset has been made available to the
public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Object Localization: Observing the Background to Discover
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriane Siméoni, Chloé Sekkat, Gilles Puy, Antonin Vobecky, Éloi Zablocki, Patrick Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in self-supervised visual representation learning have paved
the way for unsupervised methods tackling tasks such as object discovery and
instance segmentation. However, discovering objects in an image with no
supervision is a very hard task; what are the desired objects, when to separate
them into parts, how many are there, and of what classes? The answers to these
questions depend on the tasks and datasets of evaluation. In this work, we take
a different approach and propose to look for the background instead. This way,
the salient objects emerge as a by-product without any strong assumption on
what an object should be. We propose FOUND, a simple model made of a single
$conv1\times1$ initialized with coarse background masks extracted from
self-supervised patch-based representations. After fast training and refining
these seed masks, the model reaches state-of-the-art results on unsupervised
saliency detection and object discovery benchmarks. Moreover, we show that our
approach yields good results in the unsupervised semantic segmentation
retrieval task. The code to reproduce our results is available at
https://github.com/valeoai/FOUND.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Visual Computing with Camera RAW Snapshots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Li, Ming Lu, Xu Zhang, Xin Feng, M. Salman Asif, Zhan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional cameras capture image irradiance on a sensor and convert it to
RGB images using an image signal processor (ISP). The images can then be used
for photography or visual computing tasks in a variety of applications, such as
public safety surveillance and autonomous driving. One can argue that since RAW
images contain all the captured information, the conversion of RAW to RGB using
an ISP is not necessary for visual computing. In this paper, we propose a novel
$\rho$-Vision framework to perform high-level semantic understanding and
low-level compression using RAW images without the ISP subsystem used for
decades. Considering the scarcity of available RAW image datasets, we first
develop an unpaired CycleR2R network based on unsupervised CycleGAN to train
modular unrolled ISP and inverse ISP (invISP) models using unpaired RAW and RGB
images. We can then flexibly generate simulated RAW images (simRAW) using any
existing RGB image dataset and finetune different models originally trained for
the RGB domain to process real-world camera RAW images. We demonstrate object
detection and image compression capabilities in RAW-domain using RAW-domain
YOLOv3 and RAW image compressor (RIC) on snapshots from various cameras.
Quantitative results reveal that RAW-domain task inference provides better
detection accuracy and compression compared to RGB-domain processing.
Furthermore, the proposed \r{ho}-Vision generalizes across various camera
sensors and different task-specific models. Additional advantages of the
proposed $\rho$-Vision that eliminates the ISP are the potential reductions in
computations and processing times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>home page: https://njuvision.github.io/rho-vision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Indic Handwritten Text Recognition Using Global Semantic
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajoy Mondal, C. V. Jawahar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwritten Text Recognition (HTR) is more interesting and challenging than
printed text due to uneven variations in the handwriting style of the writers,
content, and time. HTR becomes more challenging for the Indic languages because
of (i) multiple characters combined to form conjuncts which increase the number
of characters of respective languages, and (ii) near to 100 unique basic
Unicode characters in each Indic script. Recently, many recognition methods
based on the encoder-decoder framework have been proposed to handle such
problems. They still face many challenges, such as image blur and incomplete
characters due to varying writing styles and ink density. We argue that most
encoder-decoder methods are based on local visual features without explicit
global semantic information.
  In this work, we enhance the performance of Indic handwritten text
recognizers using global semantic information. We use a semantic module in an
encoder-decoder framework for extracting global semantic information to
recognize the Indic handwritten texts. The semantic information is used in both
the encoder for supervision and the decoder for initialization. The semantic
information is predicted from the word embedding of a pre-trained language
model. Extensive experiments demonstrate that the proposed framework achieves
state-of-the-art results on handwritten texts of ten Indic languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A scalable framework for annotating photovoltaic cell defects in
  electroluminescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Urtzi Otamendi, Inigo Martinez, Igor G. Olaizola, Marco Quartulli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The correct functioning of photovoltaic (PV) cells is critical to ensuring
the optimal performance of a solar plant. Anomaly detection techniques for PV
cells can result in significant cost savings in operation and maintenance
(O&M). Recent research has focused on deep learning techniques for
automatically detecting anomalies in Electroluminescence (EL) images. Automated
anomaly annotations can improve current O&M methodologies and help develop
decision-making systems to extend the life-cycle of the PV cells and predict
failures. This paper addresses the lack of anomaly segmentation annotations in
the literature by proposing a combination of state-of-the-art data-driven
techniques to create a Golden Standard benchmark. The proposed method stands
out for (1) its adaptability to new PV cell types, (2) cost-efficient
fine-tuning, and (3) leverage public datasets to generate advanced annotations.
The methodology has been validated in the annotation of a widely used dataset,
obtaining a reduction of the annotation cost by 60%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, 1 table, accepted at IEEE Transactions on
  Industrial Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Pautrat, Daniel Barath, Viktor Larsson, Martin R. Oswald, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Line segments are ubiquitous in our human-made world and are increasingly
used in vision tasks. They are complementary to feature points thanks to their
spatial extent and the structural information they provide. Traditional line
detectors based on the image gradient are extremely fast and accurate, but lack
robustness in noisy images and challenging conditions. Their learned
counterparts are more repeatable and can handle challenging images, but at the
cost of a lower accuracy and a bias towards wireframe lines. We propose to
combine traditional and learned approaches to get the best of both worlds: an
accurate and robust line detector that can be trained in the wild without
ground truth lines. Our new line segment detector, DeepLSD, processes images
with a deep network to generate a line attraction field, before converting it
to a surrogate image gradient magnitude and angle, which is then fed to any
existing handcrafted line detector. Additionally, we propose a new optimization
tool to refine line segments based on the attraction field and vanishing
points. This refinement improves the accuracy of current deep detectors by a
large margin. We demonstrate the performance of our method on low-level line
detection metrics, as well as on several downstream tasks using multiple
challenging datasets. The source code and models are available at
https://github.com/cvg/DeepLSD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-based Visual Tracking in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Perez-Salesa, Rodrigo Aldana-Lopez, Carlos Sagues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual object tracking under challenging conditions of motion and light can
be hindered by the capabilities of conventional cameras, prone to producing
images with motion blur. Event cameras are novel sensors suited to robustly
perform vision tasks under these conditions. However, due to the nature of
their output, applying them to object detection and tracking is non-trivial. In
this work, we propose a framework to take advantage of both event cameras and
off-the-shelf deep learning for object tracking. We show that reconstructing
event data into intensity frames improves the tracking performance in
conditions under which conventional cameras fail to provide acceptable results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in ROBOT2022: Fifth Iberian Robotics Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combating Uncertainty and Class Imbalance in Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Fan, Jian Zhou, Xiaoyu Deng, Huabin Wang, Liang Tao, Hon Keung Kwan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognition of facial expression is a challenge when it comes to computer
vision. The primary reasons are class imbalance due to data collection and
uncertainty due to inherent noise such as fuzzy facial expressions and
inconsistent labels. However, current research has focused either on the
problem of class imbalance or on the problem of uncertainty, ignoring the
intersection of how to address these two problems. Therefore, in this paper, we
propose a framework based on Resnet and Attention to solve the above problems.
We design weight for each class. Through the penalty mechanism, our model will
pay more attention to the learning of small samples during training, and the
resulting decrease in model accuracy can be improved by a Convolutional Block
Attention Module (CBAM). Meanwhile, our backbone network will also learn an
uncertain feature for each sample. By mixing uncertain features between
samples, the model can better learn those features that can be used for
classification, thus suppressing uncertainty. Experiments show that our method
surpasses most basic methods in terms of accuracy on facial expression data
sets (e.g., AffectNet, RAF-DB), and it also solves the problem of class
imbalance well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HUM3DIL: Semi-supervised Multi-modal 3D Human Pose Estimation for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Zanfir, Mihai Zanfir, Alexander Gorban, Jingwei Ji, Yin Zhou, Dragomir Anguelov, Cristian Sminchisescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving is an exciting new industry, posing important research
questions. Within the perception module, 3D human pose estimation is an
emerging technology, which can enable the autonomous vehicle to perceive and
understand the subtle and complex behaviors of pedestrians. While hardware
systems and sensors have dramatically improved over the decades -- with cars
potentially boasting complex LiDAR and vision systems and with a growing
expansion of the available body of dedicated datasets for this newly available
information -- not much work has been done to harness these novel signals for
the core problem of 3D human pose estimation. Our method, which we coin HUM3DIL
(HUMan 3D from Images and LiDAR), efficiently makes use of these complementary
signals, in a semi-supervised fashion and outperforms existing methods with a
large margin. It is a fast and compact model for onboard deployment.
Specifically, we embed LiDAR points into pixel-aligned multi-modal features,
which we pass through a sequence of Transformer refinement stages. Quantitative
experiments on the Waymo Open Dataset support these claims, where we achieve
state-of-the-art results on the task of 3D pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 6th Conference on Robot Learning (CoRL 2022),
  Auckland, New Zealand</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-based Multiple Instance Learning for Survival Prediction on
  Lung Cancer Tissue Microarrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Ammeling, Lars-Henning Schmidt, Jonathan Ganz, Tanja Niedermair, Christoph Brochhausen-Delius, Christian Schulz, Katharina Breininger, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based multiple instance learning (AMIL) algorithms have proven to
be successful in utilizing gigapixel whole-slide images (WSIs) for a variety of
different computational pathology tasks such as outcome prediction and cancer
subtyping problems. We extended an AMIL approach to the task of survival
prediction by utilizing the classical Cox partial likelihood as a loss
function, converting the AMIL model into a nonlinear proportional hazards
model. We applied the model to tissue microarray (TMA) slides of 330 lung
cancer patients. The results show that AMIL approaches can handle very small
amounts of tissue from a TMA and reach similar C-index performance compared to
established survival prediction methods trained with highly discriminative
clinical factors such as age, cancer grade, and cancer stage
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Automatic Assessment of AgNOR-scores in
  Histopathology Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ganz, Karoline Lipnik, Jonas Ammeling, Barbara Richter, Chloé Puget, Eda Parlak, Laura Diehl, Robert Klopfleisch, Taryn A. Donovan, Matti Kiupel, Christof A. Bertram, Katharina Breininger, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nucleolar organizer regions (NORs) are parts of the DNA that are involved in
RNA transcription. Due to the silver affinity of associated proteins,
argyrophilic NORs (AgNORs) can be visualized using silver-based staining. The
average number of AgNORs per nucleus has been shown to be a prognostic factor
for predicting the outcome of many tumors. Since manual detection of AgNORs is
laborious, automation is of high interest. We present a deep learning-based
pipeline for automatically determining the AgNOR-score from histopathological
sections. An additional annotation experiment was conducted with six
pathologists to provide an independent performance evaluation of our approach.
Across all raters and images, we found a mean squared error of 0.054 between
the AgNOR- scores of the experts and those of the model, indicating that our
approach offers performance comparable to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colab NAS: Obtaining lightweight task-specific convolutional neural
  networks following Occam's razor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Mattia Garavagno, Daniele Leonardis, Antonio Frisoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current trend of applying transfer learning from CNNs trained on large
datasets can be an overkill when the target application is a custom and
delimited problem with enough data to train a network from scratch. On the
other hand, the training of custom and lighter CNNs requires expertise, in the
from-scratch case, and or high-end resources, as in the case of hardware-aware
neural architecture search (HW NAS), limiting access to the technology by
non-habitual NN developers.
  For this reason, we present Colab NAS, an affordable HW NAS technique for
producing lightweight task-specific CNNs. Its novel derivative-free search
strategy, inspired by Occam's razor, allows it to obtain state-of-the-art
results on the Visual Wake Word dataset in just 4.5 GPU hours using free online
GPU services such as Google Colaboratory and Kaggle Kernel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-based Disentanglement with Distant Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disentangled representation learning remains challenging as ground truth
factors of variation do not naturally exist. To address this, we present
Vocabulary Disentanglement Retrieval~(VDR), a simple yet effective
retrieval-based disentanglement framework that leverages nature language as
distant supervision. Our approach is built upon the widely-used bi-encoder
architecture with disentanglement heads and is trained on data-text pairs that
are readily available on the web or in existing datasets. This makes our
approach task- and modality-agnostic with potential for a wide range of
downstream applications. We conduct experiments on 16 datasets in both
text-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.
With the incorporation of disentanglement heads and a minor increase in
parameters, VDR achieves significant improvements over the base retriever it is
built upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text
retrieval and an average of 13% higher recall in cross-modal retrieval. In
comparison to other baselines, VDR outperforms them in most tasks, while also
improving explainability and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNN-based real-time 2D-3D deformable registration from a single X-ray
  projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Lecomte, Jean-Louis Dillenseger, Stéphane Cotin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: The purpose of this paper is to present a method for real-time 2D-3D
non-rigid registration using a single fluoroscopic image. Such a method can
find applications in surgery, interventional radiology and radiotherapy. By
estimating a three-dimensional displacement field from a 2D X-ray image,
anatomical structures segmented in the preoperative scan can be projected onto
the 2D image, thus providing a mixed reality view. Methods: A dataset composed
of displacement fields and 2D projections of the anatomy is generated from the
preoperative scan. From this dataset, a neural network is trained to recover
the unknown 3D displacement field from a single projection image. Results: Our
method is validated on lung 4D CT data at different stages of the lung
deformation. The training is performed on a 3D CT using random (non
domain-specific) diffeomorphic deformations, to which perturbations mimicking
the pose uncertainty are added. The model achieves a mean TRE over a series of
landmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.
Conclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid
registration is presented. This method is able to cope with pose estimation
uncertainties, making it applicable to actual clinical scenarios, such as lung
surgery, where the C-arm pose is planned before the intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summary-Oriented Vision Modeling for Multimodal Abstractive
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal abstractive summarization (MAS) is to produce a
concise summary given the multimodal data (text and vision). Existing studies
on MAS mainly focus on how to effectively use the extracted visual features,
having achieved impressive success on the high-resource English dataset.
However, less attention has been paid to the quality of the visual features to
the summary, which may limit the model performance especially in the low- and
zero-resource scenarios. In this paper, we propose to improve the summary
quality through summary-oriented visual features. To this end, we devise two
auxiliary tasks including \emph{vision to summary task} and \emph{masked image
modeling task}. Together with the main summarization task, we optimize the MAS
model via the training objectives of all these tasks. By these means, the MAS
model can be enhanced by capturing the summary-oriented visual features,
thereby yielding more accurate summaries. Experiments on 44 languages, covering
mid-high-, low-, and zero-resource scenarios, verify the effectiveness and
superiority of the proposed approach, which achieves state-of-the-art
performance under all scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Fusion for Efficient Panoptic-Part Segmentation <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sravan Kumar Jagadeesh, René Schuster, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel network that generates semantic,
instance, and part segmentation using a shared encoder and effectively fuses
them to achieve panoptic-part segmentation. Unifying these three segmentation
problems allows for mutually improved and consistent representation learning.
To fuse the predictions of all three heads efficiently, we introduce a
parameter-free joint fusion module that dynamically balances the logits and
fuses them to create panoptic-part segmentation. Our method is evaluated on the
Cityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets. For
CPP, the PartPQ of our proposed model with joint fusion surpasses the previous
state-of-the-art by 1.6 and 4.7 percentage points for all areas and segments
with parts, respectively. On PPP, our joint fusion outperforms a model using
the previous top-down merging strategy by 3.3 percentage points in PartPQ and
10.5 percentage points in PartPQ for partitionable classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICPRAM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Writer Retrieval and Writer Identification in Greek Papyri 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Christlein, Isabelle Marthot-Santaniello, Martin Mayr, Anguelos Nicolaou, Mathias Seuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of digitized historical manuscripts is typically addressed by
paleographic experts. Writer identification refers to the classification of
known writers while writer retrieval seeks to find the writer by means of image
similarity in a dataset of images. While automatic writer
identification/retrieval methods already provide promising results for many
historical document types, papyri data is very challenging due to the fiber
structures and severe artifacts. Thus, an important step for an improved writer
identification is the preprocessing and feature sampling process. We
investigate several methods and show that a good binarization is key to an
improved writer identification in papyri writings. We focus mainly on writer
retrieval using unsupervised feature methods based on traditional or
self-supervised-based methods. It is, however, also comparable to the state of
the art supervised deep learning-based method in the case of writer
classification/re-identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Body-Part Joint Detection and Association via Extended Object
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Fei Jiang, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of human body and its related parts (e.g., face, head or hands)
have been intensively studied and greatly improved since the breakthrough of
deep CNNs. However, most of these detectors are trained independently, making
it a challenging task to associate detected body parts with people. This paper
focuses on the problem of joint detection of human body and its corresponding
parts. Specifically, we propose a novel extended object representation that
integrates the center location offsets of body or its parts, and construct a
dense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part
associations in BPJDet are embedded into the unified representation which
contains both the semantic and geometric information. Therefore, BPJDet does
not suffer from error-prone association post-matching, and has a better
accuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to
jointly detect any body part. To verify the effectiveness and superiority of
our method, we conduct extensive experiments on the CityPersons, CrowdHuman and
BodyHands datasets. The proposed BPJDet detector achieves state-of-the-art
association performance on these three benchmarks while maintains high accuracy
of detection. Code will be released to facilitate further studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage Contextual <span class="highlight-title">Transformer</span>-based Convolutional Neural Network for
  Airway Extraction from CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Wu, Shuiqing Zhao, Shouliang Qi, Jie Feng, Haowen Pang, Runsheng Chang, Long Bai, Mengqi Li, Shuyue Xia, Wei Qian, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate airway extraction from computed tomography (CT) images is a critical
step for planning navigation bronchoscopy and quantitative assessment of
airway-related chronic obstructive pulmonary disease (COPD). The existing
methods are challenging to sufficiently segment the airway, especially the
high-generation airway, with the constraint of the limited label and cannot
meet the clinical use in COPD. We propose a novel two-stage 3D contextual
transformer-based U-Net for airway segmentation using CT images. The method
consists of two stages, performing initial and refined airway segmentation. The
two-stage model shares the same subnetwork with different airway masks as
input. Contextual transformer block is performed both in the encoder and
decoder path of the subnetwork to finish high-quality airway segmentation
effectively. In the first stage, the total airway mask and CT images are
provided to the subnetwork, and the intrapulmonary airway mask and
corresponding CT scans to the subnetwork in the second stage. Then the
predictions of the two-stage method are merged as the final prediction.
Extensive experiments were performed on in-house and multiple public datasets.
Quantitative and qualitative analysis demonstrate that our proposed method
extracted much more branches and lengths of the tree while accomplishing
state-of-the-art airway segmentation performance. The code is available at
https://github.com/zhaozsq/airway_segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relightable Neural Human Assets from Multi-view Gradient Illuminations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, Jingyi Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human modeling and relighting are two fundamental problems in computer vision
and graphics, where high-quality datasets can largely facilitate related
research. However, most existing human datasets only provide multi-view human
images captured under the same illumination. Although valuable for modeling
tasks, they are not readily used in relighting problems. To promote research in
both fields, in this paper, we present UltraStage, a new 3D human dataset that
contains more than 2K high-quality human assets captured under both multi-view
and multi-illumination settings. Specifically, for each example, we provide 32
surrounding views illuminated with one white light and two gradient
illuminations. In addition to regular multi-view images, gradient illuminations
help recover detailed surface normal and spatially-varying material maps,
enabling various relighting applications. Inspired by recent advances in neural
representation, we further interpret each example into a neural human asset
which allows novel view synthesis under arbitrary lighting conditions. We show
our neural human assets can achieve extremely high capture performance and are
capable of representing fine details such as facial wrinkles and cloth folds.
We also validate UltraStage in single image relighting tasks, training neural
networks with virtual relighted data from neural assets and demonstrating
realistic rendering improvements over prior arts. UltraStage will be publicly
available to the community to stimulate significant future developments in
various human modeling and rendering tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-like Adaptive Modeling Multi-Agent Learning System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Qian, Aximu Yuemaier, Longfei Liang, Wen-Chi Yang, Xiaogang Chen, Shunfen Li, Weibang Dai, Zhitang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a self-supervised multi-agent system, termed a
memory-like adaptive modeling multi-agent learning system (MAMMALS), that
realizes online learning towards behavioral pattern clustering tasks for time
series. Encoding the visual behaviors as discrete time series(DTS), and
training and modeling them in the multi-agent system with a bio-memory-like
form. We finally implemented a fully decentralized multi-agent system design
framework and completed its feasibility verification in a surveillance video
application scenario on vehicle path clustering. In multi-agent learning, using
learning methods designed for individual agents will typically perform poorly
globally because of the behavior of ignoring the synergy between agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level
  Weakly Supervised Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Laurent Itti, Vibhav Vineet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EM-PASTE: an Expectation Maximization(EM) guided Cut-Paste
compositional dataset augmentation approach for weakly-supervised instance
segmentation using only image-level supervision. The proposed method consists
of three main components. The first component generates high-quality foreground
object masks. To this end, an EM-like approach is proposed that iteratively
refines an initial set of object mask proposals generated by a generic region
proposal method. Next, in the second component, high-quality context-aware
background images are generated using a text-to-image compositional synthesis
method like DALL-E. Finally, the third component creates a large-scale
pseudo-labeled instance segmentation training dataset by compositing the
foreground object masks onto the original and generated background images. The
proposed approach achieves state-of-the-art weakly-supervised instance
segmentation results on both the PASCAL VOC 2012 and MS COCO datasets by using
only image-level, weak label information. In particular, it outperforms the
best baseline by +7.4 and +2.8 mAP0.50 on PASCAL and COCO, respectively.
Further, the method provides a new solution to the long-tail weakly-supervised
instance segmentation problem (when many classes may only have few training
samples), by selectively augmenting under-represented classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages (including appendix), 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juze Zhang, Haimin Luo, Hongdi Yang, Xinru Xu, Qianyang Wu, Ye Shi, Jingyi Yu, Lan Xu, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans constantly interact with objects in daily life tasks. Capturing such
processes and subsequently conducting visual inferences from a fixed viewpoint
suffers from occlusions, shape and texture ambiguities, motions, etc. To
mitigate the problem, it is essential to build a training dataset that captures
free-viewpoint interactions. We construct a dense multi-view dome to acquire a
complex human object interaction dataset, named HODome, that consists of
$\sim$75M frames on 10 subjects interacting with 23 objects. To process the
HODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline
tailored for multi-view video inputs to conduct accurate tracking, geometry
reconstruction and free-view rendering, for both human subjects and objects.
Extensive experiments on the HODome dataset demonstrate the effectiveness of
NeuralDome on a variety of inference, modeling, and rendering tasks. Both the
dataset and the NeuralDome tools will be disseminated to the community for
further development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SBSS: Stacking-Based Semantic Segmentation Framework for Very High
  Resolution Remote Sensing Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhi Cai, Lei Fan, Yuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of Very High Resolution (VHR) remote sensing images is
a fundamental task for many applications. However, large variations in the
scales of objects in those VHR images pose a challenge for performing accurate
semantic segmentation. Existing semantic segmentation networks are able to
analyse an input image at up to four resizing scales, but this may be
insufficient given the diversity of object scales. Therefore, Multi Scale (MS)
test-time data augmentation is often used in practice to obtain more accurate
segmentation results, which makes equal use of the segmentation results
obtained at the different resizing scales. However, it was found in this study
that different classes of objects had their preferred resizing scale for more
accurate semantic segmentation. Based on this behaviour, a Stacking-Based
Semantic Segmentation (SBSS) framework is proposed to improve the segmentation
results by learning this behaviour, which contains a learnable Error Correction
Module (ECM) for segmentation result fusion and an Error Correction Scheme
(ECS) for computational complexity control. Two ECS, i.e., ECS-MS and ECS-SS,
are proposed and investigated in this study. The Floating-point operations
(Flops) required for ECS-MS and ECS-SS are similar to the commonly used MS test
and the Single-Scale (SS) test, respectively. Extensive experiments on four
datasets (i.e., Cityscapes, UAVid, LoveDA and Potsdam) show that SBSS is an
effective and flexible framework. It achieved higher accuracy than MS when
using ECS-MS, and similar accuracy as SS with a quarter of the memory footprint
when using ECS-SS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proposal Distribution Calibration for Few-Shot Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Li, Chang Liu, Mengnan Shi, Xiaozhong Chen, Xiangyang Ji, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting object detectors learned with sufficient supervision to novel
classes under low data regimes is charming yet challenging. In few-shot object
detection (FSOD), the two-step training paradigm is widely adopted to mitigate
the severe sample imbalance, i.e., holistic pre-training on base classes, then
partial fine-tuning in a balanced setting with all classes. Since unlabeled
instances are suppressed as backgrounds in the base training phase, the learned
RPN is prone to produce biased proposals for novel instances, resulting in
dramatic performance degradation. Unfortunately, the extreme data scarcity
aggravates the proposal distribution bias, hindering the RoI head from evolving
toward novel classes. In this paper, we introduce a simple yet effective
proposal distribution calibration (PDC) approach to neatly enhance the
localization and classification abilities of the RoI head by recycling its
localization ability endowed in base training and enriching high-quality
positive samples for semantic fine-tuning. Specifically, we sample proposals
based on the base proposal statistics to calibrate the distribution bias and
impose additional localization and classification losses upon the sampled
proposals for fast expanding the base detector to novel classes. Experiments on
the commonly used Pascal VOC and MS COCO datasets with explicit
state-of-the-art performances justify the efficacy of our PDC for FSOD. Code is
available at github.com/Bohao-Lee/PDC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under review in IEEE TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCS-RISR: Dynamic Channel Splitting for Efficient Real-world Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbo Qiao, Shaohui Lin, Yunlun Zhang, Wei Li, Hu Jie, Gaoqi He, Changbo Wang, Zhuangli Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world image super-resolution (RISR) has received increased focus for
improving the quality of SR images under unknown complex degradation. Existing
methods rely on the heavy SR models to enhance low-resolution (LR) images of
different degradation levels, which significantly restricts their practical
deployments on resource-limited devices. In this paper, we propose a novel
Dynamic Channel Splitting scheme for efficient Real-world Image
Super-Resolution, termed DCS-RISR. Specifically, we first introduce the light
degradation prediction network to regress the degradation vector to simulate
the real-world degradations, upon which the channel splitting vector is
generated as the input for an efficient SR model. Then, a learnable octave
convolution block is proposed to adaptively decide the channel splitting scale
for low- and high-frequency features at each block, reducing computation
overhead and memory cost by offering the large scale to low-frequency features
and the small scale to the high ones. To further improve the RISR performance,
Non-local regularization is employed to supplement the knowledge of patches
from LR and HR subspace with free-computation inference. Extensive experiments
demonstrate the effectiveness of DCS-RISR on different benchmark datasets. Our
DCS-RISR not only achieves the best trade-off between computation/parameter and
PSNR/SSIM metric, and also effectively handles real-world images with different
degradation levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-guided mask-free local image retouching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerun Liu, Fan Zhang, Jingxuan He, Jin Wang, Zhangye Wang, Lechao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of multi-modality, text-guided image retouching techniques
emerged with the advent of deep learning. Most currently available text-guided
methods, however, rely on object-level supervision to constrain the region that
may be modified. This not only makes it more challenging to develop these
algorithms, but it also limits how widely deep learning can be used for image
retouching. In this paper, we offer a text-guided mask-free image retouching
approach that yields consistent results to address this concern. In order to
perform image retouching without mask supervision, our technique can construct
plausible and edge-sharp masks based on the text for each object in the image.
Extensive experiments have shown that our method can produce high-quality,
accurate images based on spoken language. The source code will be released
soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Generative Modeling in Dual-domain for Dynamic MR Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic magnetic resonance image reconstruction from incomplete k-space data
has generated great research interest due to its capability to reduce scan
time. Never-theless, the reconstruction problem is still challenging due to its
ill-posed nature. Recently, diffusion models espe-cially score-based generative
models have exhibited great potential in algorithm robustness and usage
flexi-bility. Moreover, the unified framework through the variance exploding
stochastic differential equation (VE-SDE) is proposed to enable new sampling
methods and further extend the capabilities of score-based gener-ative models.
Therefore, by taking advantage of the uni-fied framework, we proposed a k-space
and image Du-al-Domain collaborative Universal Generative Model (DD-UGM) which
combines the score-based prior with low-rank regularization penalty to
reconstruct highly under-sampled measurements. More precisely, we extract prior
components from both image and k-space domains via a universal generative model
and adaptively handle these prior components for faster processing while
maintaining good generation quality. Experimental comparisons demonstrated the
noise reduction and detail preservation abilities of the proposed method. Much
more than that, DD-UGM can reconstruct data of differ-ent frames by only
training a single frame image, which reflects the flexibility of the proposed
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Training of Query-Based Object Detection via Selective Query
  Recollection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyi Chen, Han Zhang, Kai Hu, Yu-kai Huang, Chenchen Zhu, Marios Savvides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solve the Puzzle of Instance Segmentation in Videos: A Weakly Supervised
  Framework with Spatio-Temporal Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liqi Yan, Qifan Wang, Siqi Ma, Jingang Wang, Changbin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation in videos, which aims to segment and track multiple
objects in video frames, has garnered a flurry of research attention in recent
years. In this paper, we present a novel weakly supervised framework with
\textbf{S}patio-\textbf{T}emporal \textbf{C}ollaboration for instance
\textbf{Seg}mentation in videos, namely \textbf{STC-Seg}. Concretely, STC-Seg
demonstrates four contributions. First, we leverage the complementary
representations from unsupervised depth estimation and optical flow to produce
effective pseudo-labels for training deep networks and predicting high-quality
instance masks. Second, to enhance the mask generation, we devise a puzzle
loss, which enables end-to-end training using box-level annotations. Third, our
tracking module jointly utilizes bounding-box diagonal points with
spatio-temporal discrepancy to model movements, which largely improves the
robustness to different object appearances. Finally, our framework is flexible
and enables image-level instance segmentation methods to operate the
video-level task. We conduct an extensive set of experiments on the KITTI MOTS
and YT-VIS datasets. Experimental results demonstrate that our method achieves
strong performance and even outperforms fully supervised TrackR-CNN and
MaskTrack R-CNN. We believe that STC-Seg can be a valuable addition to the
community, as it reflects the tip of an iceberg about the innovative
opportunities in the weakly supervised paradigm for instance segmentation in
videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Learning with <span class="highlight-title">Pre-Train</span>ed Networks Improves Source-Free Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Li Shen, Chuan-Sheng Foo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation aims to adapt a source model trained on
fully-labeled source domain data to a target domain with unlabeled target
domain data. Source data is assumed inaccessible due to proprietary or privacy
reasons. Existing works use the source model to pseudolabel target data, but
the pseudolabels are unreliable due to data distribution shift between source
and target domain. In this work, we propose to leverage an ImageNet pre-trained
feature extractor in a new co-learning framework to improve target pseudolabel
quality for finetuning the source model. Benefits of the ImageNet feature
extractor include that it is not source-biased and it provides an alternate
view of features and classification decisions different from the source model.
Such pre-trained feature extractors are also publicly available, which allows
us to readily leverage modern network architectures that have strong
representation learning ability. After co-learning, we sharpen predictions of
non-pseudolabeled samples by entropy minimization. Evaluation on 3 benchmark
datasets show that our proposed method can outperform existing source-free
domain adaptation methods, as well as unsupervised domain adaptation methods
which assume joint access to source and target data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edema Estimation From Facial Images Taken Before and After Dialysis via
  Contrastive Multi-Patient <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Akamatsu, Yoshifumi Onishi, Hitoshi Imaoka, Junko Kameyama, Hideo Tsurushima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edema is a common symptom of kidney disease, and quantitative measurement of
edema is desired. This paper presents a method to estimate the degree of edema
from facial images taken before and after dialysis of renal failure patients.
As tasks to estimate the degree of edema, we perform pre- and post-dialysis
classification and body weight prediction. We develop a multi-patient
pre-training framework for acquiring knowledge of edema and transfer the
pre-trained model to a model for each patient. For effective pre-training, we
propose a novel contrastive representation learning, called weight-aware
supervised momentum contrast (WeightSupMoCo). WeightSupMoCo aims to make
feature representations of facial images closer in similarity of patient weight
when the pre- and post-dialysis labels are the same. Experimental results show
that our pre-training approach improves the accuracy of pre- and post-dialysis
classification by 15.1% and reduces the mean absolute error of weight
prediction by 0.243 kg compared with training from scratch. The proposed method
accurately estimate the degree of edema from facial images; our edema
estimation system could thus be beneficial to dialysis patients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Journal of Biomedical and Health Informatics
  (J-BHI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Detect Semantic Boundaries with Image-level Class Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namyup Kim, Sehyun Hwang, Suha Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first attempt to learn semantic boundary detection
using image-level class labels as supervision. Our method starts by estimating
coarse areas of object classes through attentions drawn by an image
classification network. Since boundaries will locate somewhere between such
areas of different classes, our task is formulated as a multiple instance
learning (MIL) problem, where pixels on a line segment connecting areas of two
different classes are regarded as a bag of boundary candidates. Moreover, we
design a new neural network architecture that can learn to estimate semantic
boundaries reliably even with uncertain supervision given by the MIL strategy.
Our network is used to generate pseudo semantic boundary labels of training
images, which are in turn used to train fully supervised models. The final
model trained with our pseudo labels achieves an outstanding performance on the
SBD dataset, where it is as competitive as some of previous arts trained with
stronger supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision (IJCV), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of direct attacks to fingerprint verification systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Galbally, J. Fierrez, F. Alonso-Fernandez, M. Martinez-Diaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerabilities of fingerprint-based recognition systems to direct
attacks with and without the cooperation of the user are studied. Two different
systems, one minutiae-based and one ridge feature-based, are evaluated on a
database of real and fake fingerprints. Based on the fingerprint images quality
and on the results achieved on different operational scenarios, we obtain a
number of statistically significant observations regarding the robustness of
the systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Springer Journal of Telecommunication Systems, Special
  Issue of Biometrics Systems & Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Markerless Robot-Depth Camera Calibration and End-Effector Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bugra C. Sefercik, Baris Akgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to extrinsic calibration use fiducial markers and
learning-based approaches rely heavily on simulation data. In this work, we
present a learning-based markerless extrinsic calibration system that uses a
depth camera and does not rely on simulation data. We learn models for
end-effector (EE) segmentation, single-frame rotation prediction and keypoint
detection, from automatically generated real-world data. We use a
transformation trick to get EE pose estimates from rotation predictions and a
matching algorithm to get EE pose estimates from keypoint predictions. We
further utilize the iterative closest point algorithm, multiple-frames,
filtering and outlier detection to increase calibration robustness. Our
evaluations with training data from multiple camera poses and test data from
previously unseen poses give sub-centimeter and sub-deciradian average
calibration and pose estimation errors. We also show that a carefully selected
single training pose gives comparable results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, Conference on Robot Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirfRANS: High Fidelity Computational Fluid Dynamics <span class="highlight-title">Dataset</span> for
  Approximating Reynolds-Averaged Navier-Stokes Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Bonnet, Ahmed Jocelyn Mazari, Paola Cinnella, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are necessary to optimize meaningful quantities in physical
dynamics as their recursive numerical resolutions are often prohibitively
expensive. It is mainly the case for fluid dynamics and the resolution of
Navier-Stokes equations. However, despite the fast-growing field of data-driven
models for physical systems, reference datasets representing real-world
phenomena are lacking. In this work, we develop AirfRANS, a dataset for
studying the two-dimensional incompressible steady-state Reynolds-Averaged
Navier-Stokes equations over airfoils at a subsonic regime and for different
angles of attacks. We also introduce metrics on the stress forces at the
surface of geometries and visualization of boundary layers to assess the
capabilities of models to accurately predict the meaningful information of the
problem. Finally, we propose deep learning baselines on four machine learning
tasks to study AirfRANS under different constraints for generalization
considerations: big and scarce data regime, Reynolds number, and angle of
attack extrapolation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-level and multi-modal feature fusion for accurate 3D object
  detection in Connected and Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Hou, Mahdi Rezaei, Richard Romano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming at highly accurate object detection for connected and automated
vehicles (CAVs), this paper presents a Deep Neural Network based 3D object
detection model that leverages a three-stage feature extractor by developing a
novel LIDAR-Camera fusion scheme. The proposed feature extractor extracts
high-level features from two input sensory modalities and recovers the
important features discarded during the convolutional process. The novel fusion
scheme effectively fuses features across sensory modalities and convolutional
layers to find the best representative global features. The fused features are
shared by a two-stage network: the region proposal network (RPN) and the
detection head (DH). The RPN generates high-recall proposals, and the DH
produces final detection results. The experimental results show the proposed
model outperforms more recent research on the KITTI 2D and 3D detection
benchmark, particularly for distant and highly occluded instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Moving Average Pseudo-Labeling for Source-Free Inductive Domain
  Adaptation <span class="chip">BMVC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yan, Yuhong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation reduces the reliance on data annotation in
deep learning by adapting knowledge from a source to a target domain. For
privacy and efficiency concerns, source-free domain adaptation extends
unsupervised domain adaptation by adapting a pre-trained source model to an
unlabeled target domain without accessing the source data. However, most
existing source-free domain adaptation methods to date focus on the
transductive setting, where the target training set is also the testing set. In
this paper, we address source-free domain adaptation in the more realistic
inductive setting, where the target training and testing sets are mutually
exclusive. We propose a new semi-supervised fine-tuning method named Dual
Moving Average Pseudo-Labeling (DMAPL) for source-free inductive domain
adaptation. We first split the unlabeled training set in the target domain into
a pseudo-labeled confident subset and an unlabeled less-confident subset
according to the prediction confidence scores from the pre-trained source
model. Then we propose a soft-label moving-average updating strategy for the
unlabeled subset based on a moving-average prototypical classifier, which
gradually adapts the source model towards the target domain. Experiments show
that our proposed method achieves state-of-the-art performance and outperforms
previous methods by large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal
  Contributions in Vision and Language Models & Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letitia Parcalabescu, Anette Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision and language models (VL) are known to exploit unrobust indicators in
individual modalities (e.g., introduced by distributional biases), instead of
focusing on relevant information in each modality. A small drop in accuracy
obtained on a VL task with a unimodal model suggests that so-called unimodal
collapse occurred. But how to quantify the amount of unimodal collapse
reliably, at dataset and instance-level, to diagnose and combat unimodal
collapse in a targeted way? We present MM-SHAP, a performance-agnostic
multimodality score that quantifies the proportion by which a model uses
individual modalities in multimodal tasks. MM-SHAP is based on Shapley values
and will be applied in two ways: (1) to compare models for their degree of
multimodality, and (2) to measure the contribution of individual modalities for
a given task and dataset. Experiments with 6 VL models -- LXMERT, CLIP and four
ALBEF variants -- on four VL tasks highlight that unimodal collapse can occur
to different degrees and in different directions, contradicting the wide-spread
assumption that unimodal collapse is one-sided. We recommend MM-SHAP for
analysing multimodal tasks, to diagnose and guide progress towards multimodal
integration. Code available at: https://github.com/Heidelberg-NLP/MM-SHAP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 13 appendix pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Evaluating Adversarial Robustness of Chest X-ray Classification:
  Pitfalls and Best Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah Ghamizi, Maxime Cordy, Michail Papadakis, Yves Le Traon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vulnerability to adversarial attacks is a well-known weakness of Deep Neural
Networks. While most of the studies focus on natural images with standardized
benchmarks like ImageNet and CIFAR, little research has considered real world
applications, in particular in the medical domain. Our research shows that,
contrary to previous claims, robustness of chest x-ray classification is much
harder to evaluate and leads to very different assessments based on the
dataset, the architecture and robustness metric. We argue that previous studies
did not take into account the peculiarity of medical diagnosis, like the
co-occurrence of diseases, the disagreement of labellers (domain experts), the
threat model of the attacks and the risk implications for each successful
attack.
  In this paper, we discuss the methodological foundations, review the pitfalls
and best practices, and suggest new methodological considerations for
evaluating the robustness of chest xray classification models. Our evaluation
on 3 datasets, 7 models, and 18 diseases is the largest evaluation of
robustness of chest x-ray classification models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian posterior approximation with stochastic ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For CIFAR, the stochastic ensembles are
quantitatively compared to published Hamiltonian Monte Carlo results for a
ResNet-20 architecture. We also test the quality of the posteriors directly
against Hamiltonian Monte Carlo simulations in a simplified toy model. Our
results show that in a number of settings, stochastic ensembles provide more
accurate posterior estimates than regular deep ensembles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Attack Detection in Computer Vision by Applying Matrix
  Factorization on the Weights of Deep Networks <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khondoker Murad Hossain, Tim Oates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing importance of both deep neural networks (DNNs) and cloud
services for training them means that bad actors have more incentive and
opportunity to insert backdoors to alter the behavior of trained models. In
this paper, we introduce a novel method for backdoor detection that extracts
features from pre-trained DNN's weights using independent vector analysis (IVA)
followed by a machine learning classifier. In comparison to other detection
techniques, this has a number of benefits, such as not requiring any training
data, being applicable across domains, operating with a wide range of network
architectures, not assuming the nature of the triggers used to change network
behavior, and being highly scalable. We discuss the detection pipeline, and
then demonstrate the results on two computer vision datasets regarding image
classification and object detection. Our method outperforms the competing
algorithms in terms of efficiency and is more accurate, helping to ensure the
safe application of deep learning and AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 5 tables, AAAI Workshop on Safe AI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-Aware Adversarial <span class="highlight-title">Transformer</span>s for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10737v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10737v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: (1) existing
methods fail to capture the important features of the images due to the naive
tokenization scheme; (2) the models suffer from information loss because they
only consider single-scale feature representations; and (3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CASTformer, a novel type of adversarial transformers, for 2D medical image
segmentation. First, we take advantage of the pyramid structure to construct
multi-scale representations and handle multi-scale variations. We then design a
novel class-aware transformer module to better learn the discriminative regions
of objects with semantic structures. Lastly, we utilize an adversarial training
strategy that boosts segmentation accuracy and correspondingly allows a
transformer-based discriminator to capture high-level semantically correlated
contents and low-level anatomical features. Our experiments demonstrate that
CASTformer dramatically outperforms previous state-of-the-art transformer-based
approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in
Dice over previous models. Further qualitative experiments provide a more
detailed picture of the model's inner workings, shed light on the challenges in
improved transparency, and demonstrate that transfer learning can greatly
improve performance and reduce the size of medical image datasets in training,
making CASTformer a strong starting point for downstream medical image analysis
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POLCOVID: a multicenter multiclass chest X-ray database (Poland,
  2020-2021) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandra Suwalska, Joanna Tobiasz, Wojciech Prazuch, Marek Socha, Pawel Foszner, Damian Piotrowski, Katarzyna Gruszczynska, Magdalena Sliwinska, Jerzy Walecki, Tadeusz Popiela, Grzegorz Przybylski, Mateusz Nowak, Piotr Fiedor, Malgorzata Pawlowska, Robert Flisiak, Krzysztof Simon, Gabriela Zapolska, Barbara Gizycka, Edyta Szurowska, POLCOVID Study Group, Michal Marczyk, Andrzej Cieszanowski, Joanna Polanska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The outbreak of the SARS-CoV-2 pandemic has put healthcare systems worldwide
to their limits, resulting in increased waiting time for diagnosis and required
medical assistance. With chest radiographs (CXR) being one of the most common
COVID-19 diagnosis methods, many artificial intelligence tools for image-based
COVID-19 detection have been developed, often trained on a small number of
images from COVID-19-positive patients. Thus, the need for high-quality and
well-annotated CXR image databases increased. This paper introduces POLCOVID
dataset, containing chest X-ray (CXR) images of patients with COVID-19 or
other-type pneumonia, and healthy individuals gathered from 15 Polish
hospitals. The original radiographs are accompanied by the preprocessed images
limited to the lung area and the corresponding lung masks obtained with the
segmentation model. Moreover, the manually created lung masks are provided for
a part of POLCOVID dataset and the other four publicly available CXR image
collections. POLCOVID dataset can help in pneumonia or COVID-19 diagnosis,
while the set of matched images and lung masks may serve for the development of
lung segmentation solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Segmentation with Fully Trainable Gabor Kernels and Pearson's
  Correlation Coefficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken C. L. Wong, Mehdi Moradi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convolutional layer and loss function are two fundamental components in
deep learning. Because of the success of conventional deep learning kernels,
the less versatile Gabor kernels become less popular despite the fact that they
can provide abundant features at different frequencies, orientations, and
scales with much fewer parameters. For existing loss functions for multi-class
image segmentation, there is usually a tradeoff among accuracy, robustness to
hyperparameters, and manual weight selections for combining different losses.
Therefore, to gain the benefits of using Gabor kernels while keeping the
advantage of automatic feature generation in deep learning, we propose a fully
trainable Gabor-based convolutional layer where all Gabor parameters are
trainable through backpropagation. Furthermore, we propose a loss function
based on the Pearson's correlation coefficient, which is accurate, robust to
learning rates, and does not require manual weight selections. Experiments on
43 3D brain magnetic resonance images with 19 anatomical structures show that,
using the proposed loss function with a proper combination of conventional and
Gabor-based kernels, we can train a network with only 1.6 million parameters to
achieve an average Dice coefficient of 83%. This size is 44 times smaller than
the original V-Net which has 71 million parameters. This paper demonstrates the
potentials of using learnable parametric kernels in deep learning for 3D
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the International Workshop on Machine
  Learning in Medical Imaging (MLMI 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EpiGRAF: Rethinking training of 3D GANs <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, Peter Wonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A very recent trend in generative modeling is building 3D-aware generators
from 2D image collections. To induce the 3D bias, such models typically rely on
volumetric rendering, which is expensive to employ at high resolutions. During
the past months, there appeared more than 10 works that address this scaling
issue by training a separate 2D decoder to upsample a low-resolution image (or
a feature tensor) produced from a pure 3D generator. But this solution comes at
a cost: not only does it break multi-view consistency (i.e. shape and texture
change when the camera moves), but it also learns the geometry in a low
fidelity. In this work, we show that it is possible to obtain a high-resolution
3D generator with SotA image quality by following a completely different route
of simply training the model patch-wise. We revisit and improve this
optimization scheme in two ways. First, we design a location- and scale-aware
discriminator to work on patches of different proportions and spatial
positions. Second, we modify the patch sampling strategy based on an annealed
beta distribution to stabilize training and accelerate the convergence. The
resulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D
generator, and we test it on four datasets (two introduced in this work) at
$256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality,
high-fidelity geometry and trains ${\approx} 2.5 \times$ faster than the
upsampler-based counterparts. Project website:
https://universome.github.io/epigraf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, Paul Guerrero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown great promise for image generation, beating GANs
in terms of generation diversity, with comparable image quality. However, their
application to 3D shapes has been limited to point or voxel representations
that can in practice not accurately represent a 3D surface. We propose a
diffusion model for neural implicit representations of 3D shapes that operates
in the latent space of an auto-decoder. This allows us to generate diverse and
high quality 3D surfaces. We additionally show that we can condition our model
on images or text to enable image-to-3D generation and text-to-3D generation
using CLIP embeddings. Furthermore, adding noise to the latent codes of
existing shapes allows us to explore shape variations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UQGAN: A Unified Model for Uncertainty Quantification of Deep
  Classifiers trained via Conditional GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13279v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13279v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Oberdiek, Gernot A. Fink, Matthias Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach to quantifying both aleatoric and epistemic
uncertainty for deep neural networks in image classification, based on
generative adversarial networks (GANs). While most works in the literature that
use GANs to generate out-of-distribution (OoD) examples only focus on the
evaluation of OoD detection, we present a GAN based approach to learn a
classifier that produces proper uncertainties for OoD examples as well as for
false positives (FPs). Instead of shielding the entire in-distribution data
with GAN generated OoD examples which is state-of-the-art, we shield each class
separately with out-of-class examples generated by a conditional GAN and
complement this with a one-vs-all image classifier. In our experiments, in
particular on CIFAR10, CIFAR100 and Tiny ImageNet, we improve over the OoD
detection and FP detection performance of state-of-the-art GAN-training based
classifiers. Furthermore, we also find that the generated GAN examples do not
significantly affect the calibration error of our classifier and result in a
significant gain in model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Neural Textures Make Sim2Real Consistent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Burgert, Jinghuan Shang, Xiang Li, Michael Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unpaired image translation algorithms can be used for sim2real tasks, but
many fail to generate temporally consistent results. We present a new approach
that combines differentiable rendering with image translation to achieve
temporal consistency over indefinite timescales, using surface consistency
losses and \emph{neural neural textures}. We call this algorithm TRITON
(Texture Recovering Image Translation Network): an unsupervised, end-to-end,
stateless sim2real algorithm that leverages the underlying 3D geometry of input
scenes by generating realistic-looking learnable neural textures. By settling
on a particular texture for the objects in a scene, we ensure consistency
between frames statelessly. Unlike previous algorithms, TRITON is not limited
to camera movements -- it can handle the movement of objects as well, making it
useful for downstream tasks such as robotic manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures (without references or appendix); 16 pages, 16
  figures (with appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Lin Zhang, Ran Song, Lin Ma, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal domain adaptation (UDA) aims to transfer the knowledge of common
classes from source domain to target domain without any prior knowledge on the
label set, which requires to distinguish the unknown samples from the known
ones in the target domain. Recent methods preferred to increase the
inter-sample affinity within a known class, while they ignored the inter-sample
affinity between the unknown samples and the known ones. This paper reveals
that exploiting such inter-sample affinity can significantly improve the
performance of UDA and proposes a knowability-aware UDA framework based on it.
First, we estimate the knowability of each target sample by searching its
neighboring samples in the source domain. Then, we propose an auto-thresholding
scheme applied to the estimated knowability to determine whether a target
sample is unknown or known. Next, in addition to increasing the inter-sample
affinity within each known class like previous methods, we design new losses
based on the estimated knowability to reduce the inter-sample affinity between
the unknown target samples and the known ones. Finally, experiments on four
public datasets demonstrate that our method significantly outperforms existing
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Diagonal Earlobe Crease Detection <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara L. Almonacid-Uribe, Oliverio J. Santana, Daniel Hernández-Sosa, David Freire-Obregón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An article published on Medical News Today in June 2022 presented a
fundamental question in its title: Can an earlobe crease predict heart attacks?
The author explained that end arteries supply the heart and ears. In other
words, if they lose blood supply, no other arteries can take over, resulting in
tissue damage. Consequently, some earlobes have a diagonal crease, line, or
deep fold that resembles a wrinkle. In this paper, we take a step toward
detecting this specific marker, commonly known as DELC or Frank's Sign. For
this reason, we have made the first DELC dataset available to the public. In
addition, we have investigated the performance of numerous cutting-edge
backbones on annotated photos. Experimentally, we demonstrate that it is
possible to solve this challenge by combining pre-trained encoders with a
customized classifier to achieve 97.7% accuracy. Moreover, we have analyzed the
backbone trade-off between performance and size, estimating MobileNet as the
most promising encoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 12th International Conference on Pattern Recognition
  Applications (ICPRAM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expression-preserving face frontalization improves visually assisted
  speech processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02810v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02810v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Kang, Mostafa Sadeghi, Radu Horaud, Xavier Alameda-Pineda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face frontalization consists of synthesizing a frontally-viewed face from an
arbitrarily-viewed one. The main contribution of this paper is a frontalization
methodology that preserves non-rigid facial deformations in order to boost the
performance of visually assisted speech communication. The method alternates
between the estimation of (i)~the rigid transformation (scale, rotation, and
translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed
face and a face model. The method has two important merits: it can deal with
non-Gaussian errors in the data and it incorporates a dynamical face
deformation model. For that purpose, we use the generalized Student
t-distribution in combination with a linear dynamic system in order to account
for both rigid head motions and time-varying facial deformations caused by
speech production. We propose to use the zero-mean normalized cross-correlation
(ZNCC) score to evaluate the ability of the method to preserve facial
expressions. The method is thoroughly evaluated and compared with several state
of the art methods, either based on traditional geometric models or on deep
learning. Moreover, we show that the method, when incorporated into deep
learning pipelines, namely lip reading and speech enhancement, improves word
recognition and speech intelligibilty scores by a considerable margin.
Supplemental material is accessible at
https://team.inria.fr/robotlearn/research/facefrontalization/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.00538</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAINNFlow: Convolutional block Attention modules and Invertible Neural
  Networks Flow for anomaly detection and localization tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01992v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01992v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqing Yan, Fan Zhang, Mengyuan Huang, Wu Liu, Dongyu Hu, Jinfeng Li, Qiang Liu, Jinrong Jiang, Qianjin Guo, Linghan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of object anomalies is crucial in industrial processes, but
unsupervised anomaly detection and localization is particularly important due
to the difficulty of obtaining a large number of defective samples and the
unpredictable types of anomalies in real life. Among the existing unsupervised
anomaly detection and localization methods, the NF-based scheme has achieved
better results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and
$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze
the input visual features from 2D flattening to 1D, destroying the spatial
location relationship in the feature map and losing the spatial structure
information. In order to retain and effectively extract spatial structure
information, we design in this study a complex function model with alternating
CBAM embedded in a stacked $3\times3$ full convolution, which is able to retain
and effectively extract spatial structure information in the normalized flow
model. Extensive experimental results on the MVTec AD dataset show that
CAINNFlow achieves advanced levels of accuracy and inference efficiency based
on CNN and Transformer backbone networks as feature extractors, and CAINNFlow
achieves a pixel-level AUC of $98.64\%$ for anomaly detection in MVTec AD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAMO-YOLO : A Report on Real-Time Object Detection Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, Yuan Zhang, Xiuyu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we present a fast and accurate object detection method dubbed
DAMO-YOLO, which achieves higher performance than the state-of-the-art YOLO
series. DAMO-YOLO is extended from YOLO with some new technologies, including
Neural Architecture Search (NAS), efficient Reparameterized Generalized-FPN
(RepGFPN), a lightweight head with AlignedOTA label assignment, and
distillation enhancement. In particular, we use MAE-NAS, a method guided by the
principle of maximum entropy, to search our detection backbone under the
constraints of low latency and high performance, producing ResNet-like /
CSP-like structures with spatial pyramid pooling and focus modules. In the
design of necks and heads, we follow the rule of "large neck, small head". We
import Generalized-FPN with accelerated queen-fusion to build the detector neck
and upgrade its CSPNet with efficient layer aggregation networks (ELAN) and
reparameterization. Then we investigate how detector head size affects
detection performance and find that a heavy neck with only one task projection
layer would yield better results. In addition, AlignedOTA is proposed to solve
the misalignment problem in label assignment. And a distillation schema is
introduced to improve performance to a higher level. Based on these new techs,
we build a suite of models at various scales to meet the needs of different
scenarios, i.e., DAMO-YOLO-Tiny/Small/Medium. They can achieve 43.0/46.8/50.0
mAPs on COCO with the latency of 2.78/3.83/5.62 ms on T4 GPUs respectively. The
code is available at https://github.com/tinyvision/damo-yolo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influence of uncertainty estimation techniques on false-positive
  reduction in liver lesion detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Bhat, Josien P. W. Pluim, Max A. Viergever, Hugo J. Kuijf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques show success in detecting objects in medical images,
but still suffer from false-positive predictions that may hinder accurate
diagnosis. The estimated uncertainty of the neural network output has been used
to flag incorrect predictions. We study the role played by features computed
from neural network uncertainty estimates and shape-based features computed
from binary predictions in reducing false positives in liver lesion detection
by developing a classification-based post-processing step for different
uncertainty estimation methods. We demonstrate an improvement in the lesion
detection performance of the neural network (with respect to F1-score) for all
uncertainty estimation methods on two datasets, comprising abdominal MR and CT
images, respectively. We show that features computed from neural network
uncertainty estimates tend not to contribute much toward reducing false
positives. Our results show that factors like class imbalance (true over false
positive ratio) and shape-based features extracted from uncertainty maps play
an important role in distinguishing false positive from true positive
predictions. Our code can be found at https://github.com/ishaanb92/FPCPipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Journal of Machine Learning for
  Biomedical Imaging (MELBA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Realistic Large-Scale Fine-Depth Dehazing <span class="highlight-title">Dataset</span> from 3D Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.08554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.08554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoteng Li, Xiaoyi Zhang, Shaodi You, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image dehazing is one of the important and popular topics in computer vision
and machine learning. A reliable real-time dehazing method with reliable
performance is highly desired for many applications such as autonomous driving,
security surveillance, etc. While recent learning-based methods require
datasets containing pairs of hazy images and clean ground truth, it is
impossible to capture them in real scenes. Many existing works compromise this
difficulty to generate hazy images by rendering the haze from depth on common
RGBD datasets using the haze imaging model. However, there is still a gap
between the synthetic datasets and real hazy images as large datasets with
high-quality depth are mostly indoor and depth maps for outdoor are imprecise.
In this paper, we complement the existing datasets with a new, large, and
diverse dehazing dataset containing real outdoor scenes from High-Definition
(HD) 3D movies. We select a large number of high-quality frames of real outdoor
scenes and render haze on them using depth from stereo. Our dataset is clearly
more realistic and more diversified with better visual quality than existing
ones. More importantly, we demonstrate that using this dataset greatly improves
the dehazing performance on real scenes. In addition to the dataset, we also
evaluate a series state of the art methods on the proposed benchmarking
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signal Processing for Implicit Neural Representations <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejia Xu, Peihao Wang, Yifan Jiang, Zhiwen Fan, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) encoding continuous multi-media data
via multi-layer perceptrons has shown undebatable promise in various computer
vision tasks. Despite many successful applications, editing and processing an
INR remains intractable as signals are represented by latent parameters of a
neural network. Existing works manipulate such continuous representations via
processing on their discretized instance, which breaks down the compactness and
continuous nature of INR. In this work, we present a pilot study on the
question: how to directly modify an INR without explicit decoding? We answer
this question by proposing an implicit neural signal processing network, dubbed
INSP-Net, via differential operators on INR. Our key insight is that spatial
gradients of neural networks can be computed analytically and are invariant to
translation, while mathematically we show that any continuous convolution
filter can be uniformly approximated by a linear combination of high-order
differential operators. With these two knobs, INSP-Net instantiates the signal
processing operator as a weighted composition of computational graphs
corresponding to the high-order derivatives of INRs, where the weighting
parameters can be data-driven learned. Based on our proposed INSP-Net, we
further build the first Convolutional Neural Network (CNN) that implicitly runs
on INRs, named INSP-ConvNet. Our experiments validate the expressiveness of
INSP-Net and INSP-ConvNet in fitting low-level image and geometry processing
kernels (e.g. blurring, deblurring, denoising, inpainting, and smoothening) as
well as for high-level tasks on implicit fields such as image classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems (NeurIPS), 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distortion-Aware Network Pruning and Feature Reuse for Real-time Video
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsu Rhee, Dongchan Min, Sunil Hwang, Bruno Andreis, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time video segmentation is a crucial task for many real-world
applications such as autonomous driving and robot control. Since
state-of-the-art semantic segmentation models are often too heavy for real-time
applications despite their impressive performance, researchers have proposed
lightweight architectures with speed-accuracy trade-offs, achieving real-time
speed at the expense of reduced accuracy. In this paper, we propose a novel
framework to speed up any architecture with skip-connections for real-time
vision tasks by exploiting the temporal locality in videos. Specifically, at
the arrival of each frame, we transform the features from the previous frame to
reuse them at specific spatial bins. We then perform partial computation of the
backbone network on the regions of the current frame that captures temporal
differences between the current and previous frame. This is done by dynamically
dropping out residual blocks using a gating mechanism which decides which
blocks to drop based on inter-frame distortion. We validate our
Spatial-Temporal Mask Generator (STMG) on video semantic segmentation
benchmarks with multiple backbone networks, and show that our method largely
speeds up inference with minimal loss of accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAS-NeXt: Few-Shot Cross-Lingual Font Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang He, Xin Jin, Angela Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating new fonts is a time-consuming and labor-intensive task, especially
in a language with a huge amount of characters like Chinese. Various deep
learning models have demonstrated the ability to efficiently generate new fonts
with a few reference characters of that style, but few models support
cross-lingual font generation. This paper presents GAS-NeXt, a novel few-shot
cross-lingual font generator based on AGIS-Net and Font Translator GAN, and
improve the performance metrics such as Fr\'echet Inception Distance (FID),
Structural Similarity Index Measure(SSIM), and Pixel-level Accuracy (pix-acc).
Our approaches include replacing the original encoder and decoder with the idea
of layer attention and context-aware attention from Font Translator GAN, while
utilizing the shape, texture, and local discriminators of AGIS-Net. In our
experiment on English-to-Chinese font translation, we observed better results
in fonts with distinct local features than conventional Chinese fonts compared
to results obtained from Font Translator GAN. We also validate our method on
multiple languages and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Difference Among <span class="highlight-title">Transformer</span>s and CNNs with Explanation
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Jiang, Saeed Khorram, Li Fuxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a methodology that systematically applies deep explanation
algorithms on a dataset-wide basis, to compare different types of visual
recognition backbones, such as convolutional networks (CNNs), global attention
networks, and local attention networks. Examination of both qualitative
visualizations and quantitative statistics across the dataset helps us to gain
intuitions that are not just anecdotal, but are supported by the statistics
computed on the entire dataset. Specifically, we propose two methods. The first
one, sub-explanation counting, systematically searches for minimally-sufficient
explanations of all images and count the amount of sub-explanations for each
network. The second one, called cross-testing, computes salient regions using
one network and then evaluates the performance by only showing these regions as
an image to other networks. Through a combination of qualitative insights and
quantitative statistics, we illustrate that 1) there are significant
differences between the salient features of CNNs and attention models; 2) the
occlusion-robustness in local attention models and global attention models may
come from different decision-making mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages with 39 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new trigonometric kernel function for support vector machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08585v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08585v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Fathi Hafshejani, Zahra Moberfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, various types of machine learning algorithms, such as
Support Vector Machine (SVM), Support Vector Regression (SVR), and Non-negative
Matrix Factorization (NMF) have been introduced. The kernel approach is an
effective method for increasing the classification accuracy of machine learning
algorithms. This paper introduces a family of one-parameter kernel functions
for improving the accuracy of SVM classification. The proposed kernel function
consists of a trigonometric term and differs from all existing kernel
functions. We show this function is a positive definite kernel function.
Finally, we evaluate the SVM method based on the new trigonometric kernel, the
Gaussian kernel, the polynomial kernel, and a convex combination of the new
kernel function and the Gaussian kernel function on various types of datasets.
Empirical results show that the SVM based on the new trigonometric kernel
function and the mixed kernel function achieve the best classification
accuracy. Moreover, some numerical results of performing the SVR based on the
new trigonometric kernel function and the mixed kernel function are presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Spatio-Temporal Modeling for the Semantic Change Detection in
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Ding, Jing Zhang, Kai Zhang, Haitao Guo, Bing Liu, Lorenzo Bruzzone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic Change Detection (SCD) refers to the task of simultaneously
extracting the changed areas and the semantic categories (before and after the
changes) in Remote Sensing Images (RSIs). This is more meaningful than Binary
Change Detection (BCD) since it enables detailed change analysis in the
observed areas. Previous works established triple-branch Convolutional Neural
Network (CNN) architectures as the paradigm for SCD. However, it remains
challenging to exploit semantic information with a limited amount of change
samples. In this work, we investigate to jointly consider the spatio-temporal
dependencies to improve the accuracy of SCD. First, we propose a Semantic
Change Transformer (SCanFormer) to explicitly model the 'from-to' semantic
transitions between the bi-temporal RSIs. Then, we introduce a semantic
learning scheme to leverage the spatio-temporal constraints, which are coherent
to the SCD task, to guide the learning of semantic changes. The resulting
network (SCanNet) significantly outperforms the baseline method in terms of
both detection of critical semantic changes and semantic consistency in the
obtained bi-temporal results. It achieves the SOTA accuracy on two benchmark
datasets for the SCD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning with Evolving Class Ontologies <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04993v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04993v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Lin, Deepak Pathak, Yu-Xiong Wang, Deva Ramanan, Shu Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong learners must recognize concept vocabularies that evolve over time.
A common yet underexplored scenario is learning with class labels that
continually refine/expand old classes. For example, humans learn to recognize
${\tt dog}$ before dog breeds. In practical settings, dataset
$\textit{versioning}$ often introduces refinement to ontologies, such as
autonomous vehicle benchmarks that refine a previous ${\tt vehicle}$ class into
${\tt school-bus}$ as autonomous operations expand to new cities. This paper
formalizes a protocol for studying the problem of $\textit{Learning with
Evolving Class Ontology}$ (LECO). LECO requires learning classifiers in
distinct time periods (TPs); each TP introduces a new ontology of "fine" labels
that refines old ontologies of "coarse" labels (e.g., dog breeds that refine
the previous ${\tt dog}$). LECO explores such questions as whether to annotate
new data or relabel the old, how to leverage coarse labels, and whether to
finetune the previous TP's model or train from scratch. To answer these
questions, we leverage insights from related problems such as class-incremental
learning. We validate them under the LECO protocol through the lens of image
classification (CIFAR and iNaturalist) and semantic segmentation (Mapillary).
Our experiments lead to surprising conclusions; while the current status quo is
to relabel existing datasets with new ontologies (such as COCO-to-LVIS or
Mapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to
annotate $\textit{new}$ data with the new ontology. However, this produces an
aggregate dataset with inconsistent old-vs-new labels, complicating learning.
To address this challenge, we adopt methods from semi-supervised and
partial-label learning. Such strategies can surprisingly be made near-optimal,
approaching an "oracle" that learns on the aggregate dataset exhaustively
labeled with the newest ontology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022; Website: https://linzhiqiu.github.io/papers/leco/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Py-Feat: Python Facial Expression Analysis Toolbox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshin Jolly, Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Matthew Kenny, Luke J. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying facial expressions is a notoriously difficult endeavor. Recent
advances in the field of affective computing have yielded impressive progress
in automatically detecting facial expressions from pictures and videos.
However, much of this work has yet to be widely disseminated in social science
domains such as psychology. Current state of the art models require
considerable domain expertise that is not traditionally incorporated into
social science training programs. Furthermore, there is a notable absence of
user-friendly and open-source software that provides a comprehensive set of
tools and functions that support facial expression research. In this paper, we
introduce Py-Feat, an open-source Python toolbox that provides support for
detecting, preprocessing, analyzing, and visualizing facial expression data.
Py-Feat makes it easy for domain experts to disseminate and benchmark computer
vision models and also for end users to quickly process, analyze, and visualize
face expression data. We hope this platform will facilitate increased use of
facial expression data in human behavior research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Real-Time Monocular SLAM Using Semantic Segmentation on
  Selective Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.00114v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.00114v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkyu Lee, Muhyun Back, Sung Soo Hwang, Il Yong Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular simultaneous localization and mapping (SLAM) is emerging in
advanced driver assistance systems and autonomous driving, because a single
camera is cheap and easy to install. Conventional monocular SLAM has two major
challenges leading inaccurate localization and mapping. First, it is
challenging to estimate scales in localization and mapping. Second,
conventional monocular SLAM uses inappropriate mapping factors such as dynamic
objects and low-parallax areas in mapping. This paper proposes an improved
real-time monocular SLAM that resolves the aforementioned challenges by
efficiently using deep learning-based semantic segmentation. To achieve the
real-time execution of the proposed method, we apply semantic segmentation only
to downsampled keyframes in parallel with mapping processes. In addition, the
proposed method corrects scales of camera poses and three-dimensional (3D)
points, using estimated ground plane from road-labeled 3D points and the real
camera height. The proposed method also removes inappropriate corner features
labeled as moving objects and low parallax areas. Experiments with eight video
sequences demonstrate that the proposed monocular SLAM system achieves
significantly improved and comparable trajectory tracking accuracy, compared to
existing state-of-the-art monocular and stereo SLAM systems, respectively. The
proposed system can achieve real-time tracking on a standard CPU potentially
with a standard GPU support, whereas existing segmentation-aided monocular SLAM
does not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Play to Policy: Conditional Behavior Generation from Uncurated
  Robot Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale sequence modeling from offline data has led to impressive
performance gains in natural language and image generation, directly
translating such ideas to robotics has been challenging. One critical reason
for this is that uncurated robot demonstration data, i.e. play data, collected
from non-expert human demonstrators are often noisy, diverse, and
distributionally multi-modal. This makes extracting useful, task-centric
behaviors from such data a difficult generative modeling problem. In this work,
we present Conditional Behavior Transformers (C-BeT), a method that combines
the multi-modal generation ability of Behavior Transformer with
future-conditioned goal specification. On a suite of simulated benchmark tasks,
we find that C-BeT improves upon prior state-of-the-art work in learning from
play data by an average of 45.7%. Further, we demonstrate for the first time
that useful task-centric behaviors can be learned on a real-world robot purely
from play data without any task labels or reward information. Robot videos are
best viewed on our project website: https://play-to-policy.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at: https://play-to-policy.github.io; (fixed
  metadata author name format)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Preferential Direction of the Model Gradient in
  Adversarial Training With Projected Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.04709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.04709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training, especially projected gradient descent (PGD), has proven
to be a successful approach for improving robustness against adversarial
attacks. After adversarial training, gradients of models with respect to their
inputs have a preferential direction. However, the direction of alignment is
not mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Diffusion Modeling of Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11495v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11495v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIT: A Generative Image-to-text <span class="highlight-title">Transformer</span> for Vision and Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.14100v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.14100v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design and train a Generative Image-to-text Transformer,
GIT, to unify vision-language tasks such as image/video captioning and question
answering. While generative models provide a consistent network architecture
between pre-training and fine-tuning, existing work typically contains complex
structures (uni/multi-modal encoder/decoder) and depends on external modules
such as object detectors/taggers and optical character recognition (OCR). In
GIT, we simplify the architecture as one image encoder and one text decoder
under a single language modeling task. We also scale up the pre-training data
and the model size to boost the model performance. Without bells and whistles,
our GIT establishes new state of the arts on 12 challenging benchmarks with a
large margin. For instance, our model surpasses the human performance for the
first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a
new scheme of generation-based image classification and scene text recognition,
achieving decent performance on standard benchmarks. Codes are released at
\url{https://github.com/microsoft/GenerativeImage2Text}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An
  Accessible <span class="highlight-title">Dataset</span> and Benchmark <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Tyree, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Jeffrey Smith, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new dataset for 6-DoF pose estimation of known objects, with a
focus on robotic manipulation research. We propose a set of toy grocery
objects, whose physical instantiations are readily available for purchase and
are appropriately sized for robotic grasping and manipulation. We provide 3D
scanned textured models of these objects, suitable for generating synthetic
training data, as well as RGBD images of the objects in challenging, cluttered
scenes exhibiting partial occlusion, extreme lighting variations, multiple
instances per image, and a large variety of poses. Using semi-automated
RGBD-to-model texture correspondences, the images are annotated with ground
truth poses accurate within a few millimeters. We also propose a new pose
evaluation metric called ADD-H based on the Hungarian assignment algorithm that
is robust to symmetries in object geometry without requiring their explicit
enumeration. We share pre-trained pose estimators for all the toy grocery
objects, along with their baseline performance on both validation and test
sets. We offer this dataset to the community to help connect the efforts of
computer vision researchers with the needs of roboticists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS 2022. Project page is at https://github.com/swtyree/hope-dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Light Field Reconstruction via Deep Adaptive Fusion of Hybrid Lenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.07085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.07085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Jin, Mantang Guo, Hui Liu, Junhui Hou, Hongkai Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the problem of reconstructing high-resolution light field
(LF) images from hybrid lenses, including a high-resolution camera surrounded
by multiple low-resolution cameras. The performance of existing methods is
still limited, as they produce either blurry results on plain textured areas or
distortions around depth discontinuous boundaries. To tackle this challenge, we
propose a novel end-to-end learning-based approach, which can comprehensively
utilize the specific characteristics of the input from two complementary and
parallel perspectives. Specifically, one module regresses a spatially
consistent intermediate estimation by learning a deep multidimensional and
cross-domain feature representation, while the other module warps another
intermediate estimation, which maintains the high-frequency textures, by
propagating the information of the high-resolution view. We finally leverage
the advantages of the two intermediate estimations adaptively via the learned
attention maps, leading to the final high-resolution LF image with satisfactory
results on both plain textured areas and depth discontinuous boundaries.
Besides, to promote the effectiveness of our method trained with simulated
hybrid data on real hybrid data captured by a hybrid LF imaging system, we
carefully design the network architecture and the training strategy. Extensive
experiments on both real and simulated hybrid data demonstrate the significant
superiority of our approach over state-of-the-art ones. To the best of our
knowledge, this is the first end-to-end deep learning method for LF
reconstruction from a real hybrid input. We believe our framework could
potentially decrease the cost of high-resolution LF data acquisition and
benefit LF data storage and transmission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:1907.09640</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Geometry-Aware Encoder for Style-Based 3D GAN Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StyleGAN has achieved great progress in 2D face reconstruction and semantic
editing via image inversion and latent editing. While studies over extending 2D
StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion
framework is still missing, limiting the applications of 3D face reconstruction
and semantic editing. In this paper, we study the challenging problem of 3D GAN
inversion where a latent code is predicted given a single face image to
faithfully recover its 3D shapes and detailed textures. The problem is
ill-posed: innumerable compositions of shape and texture could be rendered to
the current image. Furthermore, with the limited capacity of a global latent
code, 2D inversion methods cannot preserve faithful shape and texture at the
same time when applied to 3D models. To solve this problem, we devise an
effective self-training scheme to constrain the learning of inversion. The
learning is done efficiently without any real-world 2D-3D training pairs but
proxy samples generated from a 3D GAN. In addition, apart from a global latent
code that captures the coarse shape and texture information, we augment the
generation network with a local branch, where pixel-aligned features are added
to faithfully reconstruct face details. We further consider a new pipeline to
perform 3D view-consistent editing. Extensive experiments show that our method
outperforms state-of-the-art inversion methods in both shape and texture
reconstruction quality. Code and data will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An encoder-based 3D GAN inversion method. Project page:
  https://nirvanalan.github.io/projects/E3DGE/index.html</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASTER: Multi-task <span class="highlight-title">Pre-train</span>ed Bottlenecked Masked Autoencoders are
  Better Dense Retrievers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhou, Xiao Liu, Yeyun Gong, Wayne Xin Zhao, Daxin Jiang, Nan Duan, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrieval aims to map queries and passages into low-dimensional vector
space for efficient similarity measuring, showing promising effectiveness in
various large-scale retrieval tasks. Since most existing methods commonly adopt
pre-trained Transformers (e.g. BERT) for parameter initialization, some work
focuses on proposing new pre-training tasks for compressing the useful semantic
information from passages into dense vectors, achieving remarkable
performances. However, it is still challenging to effectively capture the rich
semantic information and relations about passages into the dense vectors via
one single particular pre-training task. In this work, we propose a multi-task
pre-trained model, MASTER, that unifies and integrates multiple pre-training
tasks with different learning objectives under the bottlenecked masked
autoencoder architecture. Concretely, MASTER utilizes a multi-decoder
architecture to integrate three types of pre-training tasks: corrupted passages
recovering, related passage recovering and PLMs outputs recovering. By
incorporating a shared deep encoder, we construct a representation bottleneck
in our architecture, compressing the abundant semantic information across tasks
into dense vectors. The first two types of tasks concentrate on capturing the
semantic information of passages and relationships among them within the
pre-training corpus. The third one can capture the knowledge beyond the corpus
from external PLMs (e.g. GPT-2). Extensive experiments on several large-scale
passage retrieval datasets have shown that our approach outperforms the
previous state-of-the-art dense retrieval methods. Our code and data are
publicly released in https://github.com/microsoft/SimXNS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You were saying? -- Spoken Language in the V3C <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rossetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of the distribution of spoken language in the
V3C video retrieval benchmark dataset based on automatically generated
transcripts. It finds that a large portion of the dataset is covered by spoken
language. Since language transcripts can be quickly and accurately described,
this has implications for retrieval tasks such as known-item search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COLA: Improving Conversational Recommender Systems by Collaborative
  Augmentation <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongding Lin, Jian Wang, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRS) aim to employ natural language
conversations to suggest suitable products to users. Understanding user
preferences for prospective items and learning efficient item representations
are crucial for CRS. Despite various attempts, earlier studies mostly learned
item representations based on individual conversations, ignoring item
popularity embodied among all others. Besides, they still need support in
efficiently capturing user preferences since the information reflected in a
single conversation is limited. Inspired by collaborative filtering, we propose
a collaborative augmentation (COLA) method to simultaneously improve both item
representation learning and user preference modeling to address these issues.
We construct an interactive user-item graph from all conversations, which
augments item representations with user-aware information, i.e., item
popularity. To improve user preference modeling, we retrieve similar
conversations from the training corpus, where the involved items and attributes
that reflect the user's potential interests are used to augment the user
representation through gate control. Extensive experiments on two benchmark
datasets demonstrate the effectiveness of our method. Our code and data are
available at https://github.com/DongdingLin/COLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of information cascading and propagation barriers across
  distinctive news events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Sittar, Dunja Mladenic, Marko Grobelnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News reporting on events that occur in our society can have different styles
and structures as well as different dynamics of news spreading over time. News
publishers have the potential to spread their news and reach out to a large
number of readers worldwide. In this paper we would like to understand how well
they are doing it and which kind of obstacles the news may encounter when
spreading. The news to be spread wider cross multiple barriers such as
linguistic (the most evident one as they get published in other natural
languages), economic, geographical, political, time zone, and cultural
barriers. Observing potential differences between spreading of news on
different events published by multiple publishers can bring insights into what
may influence the differences in the spreading patterns. There are multiple
reasons, possibly many hidden, influencing the speed and geographical spread of
news. This paper studies information cascading and propagation barriers,
applying the proposed methodology on three distinctive kinds of events: Global
Warming, earthquakes, and FIFA World Cup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact fixed-radius nearest neighbor search with an application to
  clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Chen, Stefan Güttel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fixed-radius nearest-neighbor search is a common database operation that
retrieves all data points within a user-specified distance to a query point.
There are efficient approximate nearest neighbor search algorithms that provide
fast query responses but they often have a very compute-intensive indexing
phase and require parameter tuning. Therefore, exact brute force and tree-based
search methods are still widely used. Here we propose a new fixed-radius
nearest neighbor search method that significantly improves over brute force and
tree-based methods in terms of index and query time, returns exact results, and
requires no parameter tuning. The method exploits a sorting of the data points
by their first principal component, thereby facilitating a reduction in query
search space. Further speedup is gained from an efficient implementation using
high-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical
analysis of our method and demonstrate its practical performance when used
stand-alone and when applied within a clustering algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2202.01456</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NBC-Softmax : Darkweb Author fingerprinting and migration tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayan K. Kulatilleke, Shekhar S. Chandra, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn distances from the data, which enhances the
performance of similarity-based algorithms. An author style detection task is a
metric learning problem, where learning style features with small intra-class
variations and larger inter-class differences is of great importance to achieve
better performance. Recently, metric learning based on softmax loss has been
used successfully for style detection. While softmax loss can produce separable
representations, its discriminative power is relatively poor. In this work, we
propose NBC-Softmax, a contrastive loss based clustering technique for softmax
loss, which is more intuitive and able to achieve superior performance. Our
technique meets the criterion for larger number of samples, thus achieving
block contrastiveness, which is proven to outperform pair-wise losses. It uses
mini-batch sampling effectively and is scalable. Experiments on 4 darkweb
social forums, with NBCSAuthor that uses the proposed NBC-Softmax for author
and sybil detection, shows that our negative block contrastive approach
constantly outperforms state-of-the-art methods using the same network
architecture.
  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best-Answer Prediction in Q&A Sites Using User Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafik Hadfi, Ahmed Moustafa, Kai Yoshino, Takayuki Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community Question Answering (CQA) sites have spread and multiplied
significantly in recent years. Sites like Reddit, Quora, and Stack Exchange are
becoming popular amongst people interested in finding answers to diverse
questions. One practical way of finding such answers is automatically
predicting the best candidate given existing answers and comments. Many studies
were conducted on answer prediction in CQA but with limited focus on using the
background information of the questionnaires. We address this limitation using
a novel method for predicting the best answers using the questioner's
background information and other features, such as the textual content or the
relationships with other participants. Our answer classification model was
trained using the Stack Exchange dataset and validated using the Area Under the
Curve (AUC) metric. The experimental results show that the proposed method
complements previous methods by pointing out the importance of the
relationships between users, particularly throughout the level of involvement
in different communities on Stack Exchange. Furthermore, we point out that
there is little overlap between user-relation information and the information
represented by the shallow text features and the meta-features, such as time
differences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Hengshuang Zhao, Erik Learned-Miller, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization in multi-task learning (MTL) is more challenging than
single-task learning (STL), as the gradient from different tasks can be
contradictory. When tasks are related, it can be beneficial to share some
parameters among them (cooperation). However, some tasks require additional
parameters with expertise in a specific type of data or discrimination
(specialization). To address the MTL challenge, we propose Mod-Squad, a new
model that is Modularized into groups of experts (a 'Squad'). This structure
allows us to formalize cooperation and specialization as the process of
matching experts and tasks. We optimize this matching process during the
training of a single model. Specifically, we incorporate mixture of experts
(MoE) layers into a transformer model, with a new loss that incorporates the
mutual dependence between tasks and experts. As a result, only a small set of
experts are activated for each task. This prevents the sharing of the entire
backbone model between all tasks, which strengthens the model, especially when
the training set size and the number of tasks scale up. More interestingly, for
each task, we can extract the small set of experts as a standalone model that
maintains the same performance as the large model. Extensive experiments on the
Taskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5
vision tasks show the superiority of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Vision <span class="highlight-title">Transformer</span>s for MobileNet Size and Speed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the success of Vision Transformers (ViTs) in computer vision tasks,
recent arts try to optimize the performance and complexity of ViTs to enable
efficient deployment on mobile devices. Multiple approaches are proposed to
accelerate attention mechanism, improve inefficient designs, or incorporate
mobile-friendly lightweight convolutions to form hybrid architectures. However,
ViT and its variants still have higher latency or considerably more parameters
than lightweight CNNs, even true for the years-old MobileNet. In practice,
latency and size are both crucial for efficient deployment on
resource-constraint hardware. In this work, we investigate a central question,
can transformer models run as fast as MobileNet and maintain a similar size? We
revisit the design choices of ViTs and propose an improved supernet with low
latency and high parameter efficiency. We further introduce a fine-grained
joint search strategy that can find efficient architectures by optimizing
latency and number of parameters simultaneously. The proposed models,
EfficientFormerV2, achieve about $4\%$ higher top-1 accuracy than MobileNetV2
and MobileNetV2$\times1.4$ on ImageNet-1K with similar latency and parameters.
We demonstrate that properly designed and optimized vision transformers can
achieve high performance with MobileNet-level size and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at:
  https://github.com/snap-research/EfficientFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Neural Light Field on Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts in Neural Rendering Fields (NeRF) have shown impressive
results on novel view synthesis by utilizing implicit neural representation to
represent 3D scenes. Due to the process of volumetric rendering, the inference
speed for NeRF is extremely slow, limiting the application scenarios of
utilizing NeRF on resource-constrained hardware, such as mobile devices. Many
works have been conducted to reduce the latency of running NeRF models.
However, most of them still require high-end GPU for acceleration or extra
storage memory, which is all unavailable on mobile devices. Another emerging
direction utilizes the neural light field (NeLF) for speedup, as only one
forward pass is performed on a ray to predict the pixel color. Nevertheless, to
reach a similar rendering quality as NeRF, the network in NeLF is designed with
intensive computation, which is not mobile-friendly. In this work, we propose
an efficient network that runs in real-time on mobile devices for neural
rendering. We follow the setting of NeLF to train our network. Unlike existing
works, we introduce a novel network architecture that runs efficiently on
mobile devices with low latency and small size, i.e., saving $15\times \sim
24\times$ storage compared with MobileNeRF. Our model achieves high-resolution
generation while maintaining real-time inference for both synthetic and
real-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering
one $1008\times756$ image of real 3D scenes. Additionally, we achieve similar
image quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.
$25.91$ on the real-world forward-facing dataset).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://snap-research.github.io/MobileR2L/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Held, Christopher Hidey, Fei Liu, Eric Zhu, Rahul Goel, Diyi Yang, Rushin Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern virtual assistants use internal semantic parsing engines to convert
user utterances to actionable commands. However, prior work has demonstrated
that semantic parsing is a difficult multilingual transfer task with low
transfer efficiency compared to other tasks. In global markets such as India
and Latin America, this is a critical issue as switching between languages is
prevalent for bilingual users. In this work we dramatically improve the
zero-shot performance of a multilingual and codeswitched semantic parsing
system using two stages of multilingual alignment. First, we show that
constrastive alignment pretraining improves both English performance and
transfer efficiency. We then introduce a constrained optimization approach for
hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned
Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and
81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing
benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer
parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliced Optimal Partial Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Bai, Bernard Schmitzer, Mathew Thorpe, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) has become exceedingly popular in machine learning,
data science, and computer vision. The core assumption in the OT problem is the
equal total amount of mass in source and target measures, which limits its
application. Optimal Partial Transport (OPT) is a recently proposed solution to
this limitation. Similar to the OT problem, the computation of OPT relies on
solving a linear programming problem (often in high dimensions), which can
become computationally prohibitive. In this paper, we propose an efficient
algorithm for calculating the OPT problem between two non-negative measures in
one dimension. Next, following the idea of sliced OT distances, we utilize
slicing to define the sliced OPT distance. Finally, we demonstrate the
computational and accuracy benefits of the sliced OPT-based method in various
numerical experiments. In particular, we show an application of our proposed
Sliced-OPT in noisy point cloud registration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Silhouette: Toward Performance-Conscious and Transferable CPU Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarikul Islam Papon, Abdul Wasay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned embeddings are widely used to obtain concise data representation and
enable transfer learning between different data sets and tasks. In this paper,
we present Silhouette, our approach that leverages publicly-available
performance data sets to learn CPU embeddings. We show how these embeddings
enable transfer learning between data sets of different types and sizes. Each
of these scenarios leads to an improvement in accuracy for the target data set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstration of machine-learning-enhanced Bayesian quantum state
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjaya Lohani, Joseph M. Lukens, Atiyya A. Davis, Amirali Khannejad, Sangita Regmi, Daniel E. Jones, Ryan T. Glasser, Thomas A. Searles, Brian T. Kirby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has found broad applicability in quantum information
science in topics as diverse as experimental design, state classification, and
even studies on quantum foundations. Here, we experimentally realize an
approach for defining custom prior distributions that are automatically tuned
using ML for use with Bayesian quantum state estimation methods. Previously,
researchers have looked to Bayesian quantum state tomography due to its unique
advantages like natural uncertainty quantification, the return of reliable
estimates under any measurement condition, and minimal mean-squared error.
However, practical challenges related to long computation times and conceptual
issues concerning how to incorporate prior knowledge most suitably can
overshadow these benefits. Using both simulated and experimental measurement
results, we demonstrate that ML-defined prior distributions reduce net
convergence times and provide a natural way to incorporate both implicit and
explicit information directly into the prior distribution. These results
constitute a promising path toward practical implementations of Bayesian
quantum state tomography.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexiViT: One Model for All Patch Sizes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers convert images to sequences by slicing them into patches.
The size of these patches controls a speed/accuracy tradeoff, with smaller
patches leading to higher accuracy at greater computational cost, but changing
the patch size typically requires retraining the model. In this paper, we
demonstrate that simply randomizing the patch size at training time leads to a
single set of weights that performs well across a wide range of patch sizes,
making it possible to tailor the model to different compute budgets at
deployment time. We extensively evaluate the resulting model, which we call
FlexiViT, on a wide range of tasks, including classification, image-text
retrieval, open-world detection, panoptic segmentation, and semantic
segmentation, concluding that it usually matches, and sometimes outperforms,
standard ViT models trained at a single patch size in an otherwise identical
setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that
makes it easy to add compute-adaptive capabilities to most models relying on a
ViT backbone architecture. Code and pre-trained models are available at
https://github.com/google-research/big_vision
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and pre-trained models available at
  https://github.com/google-research/big_vision. All authors made significant
  technical contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Antoniou, Efthymios Georgiou, Alexandros Potamianos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing powerful adversarial attacks is of paramount importance for the
evaluation of $\ell_p$-bounded adversarial defenses. Projected Gradient Descent
(PGD) is one of the most effective and conceptually simple algorithms to
generate such adversaries. The search space of PGD is dictated by the steepest
ascent directions of an objective. Despite the plethora of objective function
choices, there is no universally superior option and robustness overestimation
may arise from ill-suited objective selection. Driven by this observation, we
postulate that the combination of different objectives through a simple loss
alternating scheme renders PGD more robust towards design choices. We
experimentally verify this assertion on a synthetic-data example and by
evaluating our proposed method across 25 different $\ell_{\infty}$-robust
models and 3 datasets. The performance improvement is consistent, when compared
to the single loss counterparts. In the CIFAR-10 dataset, our strongest
adversarial attack outperforms all of the white-box components of AutoAttack
(AA) ensemble, as well as the most powerful attacks existing on the literature,
achieving state-of-the-art results in the computational budget of our study
($T=100$, no restarts).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span>s are Parameter-Efficient Audio-Visual Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, Yi-Lin Sung, Jie Lei, Mohit Bansal, Gedas Bertasius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved impressive results on various
computer vision tasks in the last several years. In this work, we study the
capability of frozen ViTs, pretrained only on visual data, to generalize to
audio-visual data without finetuning any of its original parameters. To do so,
we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained
ViTs to audio-visual tasks by injecting a small number of trainable parameters
into every layer of a frozen ViT. To efficiently fuse visual and audio cues,
our LAVISH adapter uses a small set of latent tokens, which form an attention
bottleneck, thus, eliminating the quadratic cost of standard cross-attention.
Compared to the existing modality-specific audio-visual methods, our approach
achieves competitive or even better performance on various audio-visual tasks
while using fewer tunable parameters and without relying on costly audio
pretraining or external audio encoders. Our code is available at
https://genjib.github.io/project_page/LAVISH/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://genjib.github.io/project_page/LAVISH/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed-Training-and-Execution Multi-Agent Reinforcement Learning
  for Power Control in HetNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Xu, Nguyen Van Huynh, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In heterogeneous networks (HetNets), the overlap of small cells and the macro
cell causes severe cross-tier interference. Although there exist some
approaches to address this problem, they usually require global channel state
information, which is hard to obtain in practice, and get the sub-optimal power
allocation policy with high computational complexity. To overcome these
limitations, we propose a multi-agent deep reinforcement learning (MADRL) based
power control scheme for the HetNet, where each access point makes power
control decisions independently based on local information. To promote
cooperation among agents, we develop a penalty-based Q learning (PQL) algorithm
for MADRL systems. By introducing regularization terms in the loss function,
each agent tends to choose an experienced action with high reward when
revisiting a state, and thus the policy updating speed slows down. In this way,
an agent's policy can be learned by other agents more easily, resulting in a
more efficient collaboration process. We then implement the proposed PQL in the
considered HetNet and compare it with other distributed-training-and-execution
(DTE) algorithms. Simulation results show that our proposed PQL can learn the
desired power control policy from a dynamic environment where the locations of
users change episodically and outperform existing DTE MADRL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Bayesian Uncertainty Quantification for Neural Network
  Potentials: Promise and Pitfalls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Thaler, Gregor Doehner, Julija Zavadlav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network (NN) potentials promise highly accurate molecular dynamics
(MD) simulations within the computational complexity of classical MD force
fields. However, when applied outside their training domain, NN potential
predictions can be inaccurate, increasing the need for Uncertainty
Quantification (UQ). Bayesian modeling provides the mathematical framework for
UQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are
computationally intractable for NN potentials. By training graph NN potentials
for coarse-grained systems of liquid water and alanine dipeptide, we
demonstrate here that scalable Bayesian UQ via stochastic gradient MCMC
(SG-MCMC) yields reliable uncertainty estimates for MD observables. We show
that cold posteriors can reduce the required training data size and that for
reliable UQ, multiple Markov chains are needed. Additionally, we find that
SG-MCMC and the Deep Ensemble method achieve comparable results, despite
shorter training and less hyperparameter tuning of the latter. We show that
both methods can capture aleatoric and epistemic uncertainty reliably, but not
systematic uncertainty, which needs to be minimized by adequate modeling to
obtain accurate credible intervals for MD observables. Our results represent a
step towards accurate UQ that is of vital importance for trustworthy NN
potential-based MD simulations required for decision-making in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data Source Dependency Analysis Framework for Large Scale Data Science
  Projects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurent Boué, Pratap Kunireddy, Pavle Subotić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dependency hell is a well-known pain point in the development of large
software projects and machine learning (ML) code bases are not immune from it.
In fact, ML applications suffer from an additional form, namely, "data source
dependency hell". This term refers to the central role played by data and its
unique quirks that often lead to unexpected failures of ML models which cannot
be explained by code changes. In this paper, we present an automated dependency
mapping framework that allows MLOps engineers to monitor the whole dependency
map of their models in a fast paced engineering environment and thus mitigate
ahead of time the consequences of any data source changes (e.g., re-train
model, ignore data, set default data etc.). Our system is based on a unified
and generic approach, employing techniques from static analysis, from which
data sources can be identified reliably for any type of dependency on a wide
range of source languages and artefacts. The dependency mapping framework is
exposed as a REST web API where the only input is the path to the Git
repository hosting the code base. Currently used by MLOps engineers at
Microsoft, we expect such dependency map APIs to be adopted more widely by
MLOps engineers in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining information-seeking exploration and reward maximization:
  Unified inference on continuous state and action spaces under partial
  observability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parvin Malekzadeh, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) gained considerable attention by creating
decision-making agents that maximize rewards received from fully observable
environments. However, many real-world problems are partially or noisily
observable by nature, where agents do not receive the true and complete state
of the environment. Such problems are formulated as partially observable Markov
decision processes (POMDPs). Some studies applied RL to POMDPs by recalling
previous decisions and observations or inferring the true state of the
environment from received observations. Nevertheless, aggregating observations
and decisions over time is impractical for environments with high-dimensional
continuous state and action spaces. Moreover, so-called inference-based RL
approaches require large number of samples to perform well since agents eschew
uncertainty in the inferred state for the decision-making. Active inference is
a framework that is naturally formulated in POMDPs and directs agents to select
decisions by minimising expected free energy (EFE). This supplies
reward-maximising (exploitative) behaviour in RL, with an information-seeking
(exploratory) behaviour. Despite this exploratory behaviour of active
inference, its usage is limited to discrete state and action spaces due to the
computational difficulty of the EFE. We propose a unified principle for joint
information-seeking and reward maximization that clarifies a theoretical
connection between active inference and RL, unifies active inference and RL,
and overcomes their aforementioned limitations. Our findings are supported by
strong theoretical analysis. The proposed framework's superior exploration
property is also validated by experimental results on partial observable tasks
with high-dimensional continuous state and action spaces. Moreover, the results
show that our model solves reward-free problems, making task reward design
optional.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable Clustering via Distributionally Robust Nodewise Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizheng Wang, Xiao Xu, Xun Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a multi-factor block model for variable clustering and connect it to
the regularized subspace clustering by formulating a distributionally robust
version of the nodewise regression. To solve the latter problem, we derive a
convex relaxation, provide guidance on selecting the size of the robust region,
and hence the regularization weighting parameter, based on the data, and
propose an ADMM algorithm for implementation. We validate our method in an
extensive simulation study. Finally, we propose and apply a variant of our
method to stock return data, obtain interpretable clusters that facilitate
portfolio selection and compare its out-of-sample performance with other
clustering methods in an empirical study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RWEN-TTS: Relation-aware Word Encoding Network for Natural
  Text-to-Speech Synthesis <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinhyeok Oh, HyeongRae Noh, Yoonseok Hong, Insoo Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of deep learning, a huge number of text-to-speech (TTS)
models which produce human-like speech have emerged. Recently, by introducing
syntactic and semantic information w.r.t the input text, various approaches
have been proposed to enrich the naturalness and expressiveness of TTS models.
Although these strategies showed impressive results, they still have some
limitations in utilizing language information. First, most approaches only use
graph networks to utilize syntactic and semantic information without
considering linguistic features. Second, most previous works do not explicitly
consider adjacent words when encoding syntactic and semantic information, even
though it is obvious that adjacent words are usually meaningful when encoding
the current word. To address these issues, we propose Relation-aware Word
Encoding Network (RWEN), which effectively allows syntactic and semantic
information based on two modules (i.e., Semantic-level Relation Encoding and
Adjacent Word Relation Encoding). Experimental results show substantial
improvements compared to previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on the Intersection of GPU Utilization and CNN Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Kosaian, Amar Phanishayee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in developing neural network
architectures that both achieve high predictive performance and that also
achieve high application-level inference throughput (e.g., frames per second).
Another metric of increasing importance is GPU utilization during inference:
the measurement of how well a deployed neural network uses the computational
capabilities of the GPU on which it runs. Achieving high GPU utilization is
critical to increasing application-level throughput and ensuring a good return
on investment for deploying GPUs.
  This paper analyzes the GPU utilization of convolutional neural network (CNN)
inference. We first survey the GPU utilization of CNNs to show that there is
room to improve the GPU utilization of many of these CNNs. We then investigate
the GPU utilization of networks within a neural architecture search (NAS)
search space, and explore how using GPU utilization as a metric could
potentially be used to accelerate NAS itself. Our study makes the case that
there is room to improve the inference-time GPU utilization of CNNs and that
knowledge of GPU utilization has the potential to benefit even applications
that do not target utilization itself. We hope that the results of this study
will spur future innovation in designing GPU-efficient neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging POMDPs and Bayesian decision making for robust maintenance
  planning under model uncertainty: An application to railway systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural Health Monitoring (SHM) describes a process for inferring
quantifiable metrics of structural condition, which can serve as input to
support decisions on the operation and maintenance of infrastructure assets.
Given the long lifespan of critical structures, this problem can be cast as a
sequential decision making problem over prescribed horizons. Partially
Observable Markov Decision Processes (POMDPs) offer a formal framework to solve
the underlying optimal planning task. However, two issues can undermine the
POMDP solutions. Firstly, the need for a model that can adequately describe the
evolution of the structural condition under deterioration or corrective actions
and, secondly, the non-trivial task of recovery of the observation process
parameters from available monitoring data. Despite these potential challenges,
the adopted POMDP models do not typically account for uncertainty on model
parameters, leading to solutions which can be unrealistically confident. In
this work, we address both key issues. We present a framework to estimate POMDP
transition and observation model parameters directly from available data, via
Markov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)
conditioned on actions. The MCMC inference estimates distributions of the
involved model parameters. We then form and solve the POMDP problem by
exploiting the inferred distributions, to derive solutions that are robust to
model uncertainty. We successfully apply our approach on maintenance planning
for railway track assets on the basis of a "fractal value" indicator, which is
computed from actual railway monitoring data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effects of Character-Level Data Augmentation on Style-Based Dating
  of Historical Manuscripts <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Koopmans, Maruf A. Dhali, Lambert Schomaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the production dates of historical manuscripts is one of the main
goals for paleographers when studying ancient documents. Automatized methods
can provide paleographers with objective tools to estimate dates more
accurately. Previously, statistical features have been used to date digitized
historical manuscripts based on the hypothesis that handwriting styles change
over periods. However, the sparse availability of such documents poses a
challenge in obtaining robust systems. Hence, the research of this article
explores the influence of data augmentation on the dating of historical
manuscripts. Linear Support Vector Machines were trained with k-fold
cross-validation on textural and grapheme-based features extracted from
historical manuscripts of different collections, including the Medieval
Paleographical Scale, early Aramaic manuscripts, and the Dead Sea Scrolls.
Results show that training models with augmented data improve the performance
of historical manuscripts dating by 1% - 3% in cumulative scores. Additionally,
this indicates further enhancement possibilities by considering models specific
to the features and the documents' scripts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted after the peer-review process for ICPRAM 2023; scheduled to
  be presented on 22 February 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models show improved downstream task performance when prompted
to generate step-by-step reasoning to justify their final answers. These
reasoning steps greatly improve model interpretability and verification, but
objectively studying their correctness (independent of the final answer) is
difficult without reliable methods for automatic evaluation. We simply do not
know how often the stated reasoning steps actually support the final end task
predictions. In this work, we present ROSCOE, a suite of interpretable,
unsupervised automatic scores that improve and extend previous text generation
evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a
typology of reasoning errors and collect synthetic and human evaluation scores
on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE
can measure semantic consistency, logicality, informativeness, fluency, and
factuality - among other traits - by leveraging properties of step-by-step
rationales. We empirically verify the strength of our metrics on five human
annotated and six programmatically perturbed diagnostics datasets - covering a
diverse set of tasks that require reasoning skills and show that ROSCOE can
consistently outperform baseline metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Construction of a Surrogate Model: Multivariate Time Series Prediction
  with a Hybrid Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Carlier, Arnaud Franju, Matthieu Lerasle, Mathias Obrebski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments of advanced driver-assistance systems necessitate an
increasing number of tests to validate new technologies. These tests cannot be
carried out on track in a reasonable amount of time and automotive groups rely
on simulators to perform most tests. The reliability of these simulators for
constantly refined tasks is becoming an issue and, to increase the number of
tests, the industry is now developing surrogate models, that should mimic the
behavior of the simulator while being much faster to run on specific tasks.
  In this paper we aim to construct a surrogate model to mimic and replace the
simulator. We first test several classical methods such as random forests,
ridge regression or convolutional neural networks. Then we build three hybrid
models that use all these methods and combine them to obtain an efficient
hybrid surrogate model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical
  Systems <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Brenner, Georgia Koppe, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many, if not most, systems of interest in science are naturally described as
nonlinear dynamical systems (DS). Empirically, we commonly access these systems
through time series measurements, where often we have time series from
different types of data modalities simultaneously. For instance, we may have
event counts in addition to some continuous signal. While by now there are many
powerful machine learning (ML) tools for integrating different data modalities
into predictive models, this has rarely been approached so far from the
perspective of uncovering the underlying, data-generating DS (aka DS
reconstruction). Recently, sparse teacher forcing (TF) has been suggested as an
efficient control-theoretic method for dealing with exploding loss gradients
when training ML models on chaotic DS. Here we incorporate this idea into a
novel recurrent neural network (RNN) training framework for DS reconstruction
based on multimodal variational autoencoders (MVAE). The forcing signal for the
RNN is generated by the MVAE which integrates different types of simultaneously
given time series data into a joint latent code optimal for DS reconstruction.
We show that this training method achieves significantly better reconstructions
on multimodal datasets generated from chaotic DS benchmarks than various
alternative methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a workshop paper for the AAAI 2023 Workshop MLmDS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergent Behaviors in Multi-Agent Target Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush K. Sharma, Erin Zaroukian, Derrik E. Asher, Bryson Howell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Only limited studies and superficial evaluations are available on agents'
behaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using
Reinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)
game, which shares task goals with target acquisition, and we create different
adversarial scenarios by replacing RL-trained pursuers' policies with two
distinct (non-RL) analytical strategies. Using heatmaps of agents' positions
(state-space variable) over time, we are able to categorize an RL-trained
evader's behaviors. The novelty of our approach entails the creation of an
influential feature set that reveals underlying data regularities, which allow
us to classify an agent's behavior. This classification may aid in catching the
(enemy) targets by enabling us to identify and predict their behaviors, and
when extended to pursuers, this approach towards identifying teammates'
behavior may allow agents to coordinate more effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article appeared in the news at:
  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forgetful Forests: high performance learning data structures for
  streaming data under concept drift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehu Yuan, Yinqi Sun, Dennis Shasha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Database research can help machine learning performance in many ways. One way
is to design better data structures. This paper combines the use of incremental
computation and sequential and probabilistic filtering to enable "forgetful"
tree-based learning algorithms to cope with concept drift data (i.e., data
whose function from input to classification changes over time).
  The forgetful algorithms described in this paper achieve high time
performance while maintaining high quality predictions on streaming data.
Specifically, the algorithms are up to 24 times faster than state-of-the-art
incremental algorithms with at most a 2% loss of accuracy, or at least twice
faster without any loss of accuracy. This makes such structures suitable for
high volume streaming applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 Figures, 7 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Association Rule Mining for Wireless Network Time Series
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhu, Chengbo Qiu, Shaoyu Dou, Minghao Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Key performance indicators(KPIs) are of great significance in the monitoring
of wireless network service quality. The network service quality can be
improved by adjusting relevant configuration parameters(CPs) of the base
station. However, there are numerous CPs and different cells may affect each
other, which bring great challenges to the association analysis of wireless
network data. In this paper, we propose an adjustable multi-level association
rule mining framework, which can quantitatively mine association rules at each
level with environmental information, including engineering parameters and
performance management(PMs), and it has interpretability at each level.
Specifically, We first cluster similar cells, then quantify KPIs and CPs, and
integrate expert knowledge into the association rule mining model, which
improve the robustness of the model. The experimental results in real world
dataset prove the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The effects of gender bias in word embeddings on depression prediction <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gizem Sogancioglu, Heysem Kaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word embeddings are extensively used in various NLP problems as a
state-of-the-art semantic feature vector representation. Despite their success
on various tasks and domains, they might exhibit an undesired bias for
stereotypical categories due to statistical and societal biases that exist in
the dataset they are trained on. In this study, we analyze the gender bias in
four different pre-trained word embeddings specifically for the depression
category in the mental disorder domain. We use contextual and non-contextual
embeddings that are trained on domain-independent as well as clinical
domain-specific data. We observe that embeddings carry bias for depression
towards different gender groups depending on the type of embeddings. Moreover,
we demonstrate that these undesired correlations are transferred to the
downstream task for depression phenotype recognition. We find that data
augmentation by simply swapping gender words mitigates the bias significantly
in the downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to and published at "A Participatory Approach to AI for
  Mental Health (PAI4MH)" workshop, co-located with NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiating Nonsmooth Solutions to Parametric Monotone Inclusion
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérôme Bolte, Edouard Pauwels, Antonio José Silveti-Falls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We leverage path differentiability and a recent result on nonsmooth implicit
differentiation calculus to give sufficient conditions ensuring that the
solution to a monotone inclusion problem will be path differentiable, with
formulas for computing its generalized gradient. A direct consequence of our
result is that these solutions happen to be differentiable almost everywhere.
Our approach is fully compatible with automatic differentiation and comes with
assumptions which are easy to check, roughly speaking: semialgebraicity and
strong monotonicity. We illustrate the scope of our results by considering
three fundamental composite problem settings: strongly convex problems, dual
solutions to convex minimization problems and primal-dual solutions to min-max
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeTIm-Eval: a novel curated evaluation data set for comparing
  text-to-image models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating and comparing text-to-image models is a challenging problem.
Significant advances in the field have recently been made, piquing interest of
various industrial sectors. As a consequence, a gold standard in the field
should cover a variety of tasks and application contexts. In this paper a novel
evaluation approach is experimented, on the basis of: (i) a curated data set,
made by high-quality royalty-free image-text pairs, divided into ten
categories; (ii) a quantitative metric, the CLIP-score, (iii) a human
evaluation task to distinguish, for a given text, the real and the generated
images. The proposed method has been applied to the most recent models, i.e.,
DALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early
experimental results show that the accuracy of the human judgement is fully
coherent with the CLIP-score. The dataset has been made available to the
public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially-resolved Thermometry from Line-of-Sight Emission Spectroscopy
  via Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyuan Kang, Dimitrios C. Kyritsis, Panos Liatsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A methodology is proposed, which addresses the caveat that line-of-sight
emission spectroscopy presents in that it cannot provide spatially resolved
temperature measurements in nonhomogeneous temperature fields. The aim of this
research is to explore the use of data-driven models in measuring temperature
distributions in a spatially resolved manner using emission spectroscopy data.
Two categories of data-driven methods are analyzed: (i) Feature engineering and
classical machine learning algorithms, and (ii) end-to-end convolutional neural
networks (CNN). In total, combinations of fifteen feature groups and fifteen
classical machine learning models, and eleven CNN models are considered and
their performances explored. The results indicate that the combination of
feature engineering and machine learning provides better performance than the
direct use of CNN. Notably, feature engineering which is comprised of
physics-guided transformation, signal representation-based feature extraction
and Principal Component Analysis is found to be the most effective. Moreover,
it is shown that when using the extracted features, the ensemble-based, light
blender learning model offers the best performance with RMSE, RE, RRMSE and R
values of 64.3, 0.017, 0.025 and 0.994, respectively. The proposed method,
based on feature engineering and the light blender model, is capable of
measuring nonuniform temperature distributions from low-resolution spectra,
even when the species concentration distribution in the gas mixtures is
unknown.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures, systematical investigation of feature
  engineering and machine learning for realizing spatially-resolved thermometry
  from line-of-sight spectroscopy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Quantum Generative Adversarial Networks for Molecular Simulation
  and Drug Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Jain, Srinjoy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In molecular research, simulation \& design of molecules are key areas with
significant implications for drug development, material science, and other
fields. Current classical computational power falls inadequate to simulate any
more than small molecules, let alone protein chains on hundreds of peptide.
Therefore these experiment are done physically in wet-lab, but it takes a lot
of time \& not possible to examine every molecule due to the size of the search
area, tens of billions of dollars are spent every year in these research
experiments. Molecule simulation \& design has lately advanced significantly by
machine learning models, A fresh perspective on the issue of chemical synthesis
is provided by deep generative models for graph-structured data. By optimising
differentiable models that produce molecular graphs directly, it is feasible to
avoid costly search techniques in the discrete and huge space of chemical
structures. But these models also suffer from computational limitations when
dimensions become huge and consume huge amount of resources. Quantum Generative
machine learning in recent years have shown some empirical results promising
significant advantages over classical counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Hardware-Specific Automatic Compression of Neural Networks <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Torben Krieger, Bernhard Klein, Holger Fröning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressing neural network architectures is important to allow the deployment
of models to embedded or mobile devices, and pruning and quantization are the
major approaches to compress neural networks nowadays. Both methods benefit
when compression parameters are selected specifically for each layer. Finding
good combinations of compression parameters, so-called compression policies, is
hard as the problem spans an exponentially large search space. Effective
compression policies consider the influence of the specific hardware
architecture on the used compression methods. We propose an algorithmic
framework called Galen to search such policies using reinforcement learning
utilizing pruning and quantization, thus providing automatic compression for
neural networks. Contrary to other approaches we use inference latency measured
on the target hardware device as an optimization goal. With that, the framework
supports the compression of models specific to a given hardware target. We
validate our approach using three different reinforcement learning agents for
pruning, quantization and joint pruning and quantization. Besides proving the
functionality of our approach we were able to compress a ResNet18 for CIFAR-10,
on an embedded ARM processor, to 20% of the original inference latency without
significant loss of accuracy. Moreover, we can demonstrate that a joint search
and compression using pruning and quantization is superior to an individual
search for policies using a single compression method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at the AAAI Conference on Artificial Intelligence
  2023, at the 2nd International Workshop on Practical Deep Learning in the
  Wild</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUIDD: Deep-Unfolded Interleaved Detection and Decoding for MIMO
  Wireless Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reinhard Wiesmayr, Chris Dick, Jakob Hoydis, Christoph Studer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative detection and decoding (IDD) is known to achieve near-capacity
performance in multi-antenna wireless systems. We propose deep-unfolded
interleaved detection and decoding (DUIDD), a new paradigm that reduces the
complexity of IDD while achieving even lower error rates. DUIDD interleaves the
inner stages of the data detector and channel decoder, which expedites
convergence and reduces complexity. Furthermore, DUIDD applies deep unfolding
to automatically optimize algorithmic hyperparameters, soft-information
exchange, message damping, and state forwarding. We demonstrate the efficacy of
DUIDD using NVIDIA's Sionna link-level simulator in a 5G-near multi-user
MIMO-OFDM wireless system with a novel low-complexity soft-input soft-output
data detector, an optimized low-density parity-check decoder, and channel
vectors from a commercial ray-tracer. Our results show that DUIDD outperforms
classical IDD both in terms of block error rate and computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been presented at the Asilomar Conference on Signals,
  Systems, and Computers 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chaotic Variational Auto Encoder based One Class Classifier for
  Insurance Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. S. N. V. K. Gangadhar, B. Akhil Kumar, Yelleti Vivek, Vadlamani Ravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Of late, insurance fraud detection has assumed immense significance owing to
the huge financial & reputational losses fraud entails and the phenomenal
success of the fraud detection techniques. Insurance is majorly divided into
two categories: (i) Life and (ii) Non-life. Non-life insurance in turn includes
health insurance and auto insurance among other things. In either of the
categories, the fraud detection techniques should be designed in such a way
that they capture as many fraudulent transactions as possible. Owing to the
rarity of fraudulent transactions, in this paper, we propose a chaotic
variational autoencoder (C-VAE to perform one-class classification (OCC) on
genuine transactions. Here, we employed the logistic chaotic map to generate
random noise in the latent space. The effectiveness of C-VAE is demonstrated on
the health insurance fraud and auto insurance datasets. We considered vanilla
Variational Auto Encoder (VAE) as the baseline. It is observed that C-VAE
outperformed VAE in both datasets. C-VAE achieved a classification rate of
77.9% and 87.25% in health and automobile insurance datasets respectively.
Further, the t-test conducted at 1% level of significance and 18 degrees of
freedom infers that C-VAE is statistically significant than the VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating AI Models for Wireless Communications via Conformal
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone, Shlomo Shamai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When used in complex engineered systems, such as communication networks,
artificial intelligence (AI) models should be not only as accurate as possible,
but also well calibrated. A well-calibrated AI model is one that can reliably
quantify the uncertainty of its decisions, assigning high confidence levels to
decisions that are likely to be correct and low confidence levels to decisions
that are likely to be erroneous. This paper investigates the application of
conformal prediction as a general framework to obtain AI models that produce
decisions with formal calibration guarantees. Conformal prediction transforms
probabilistic predictors into set predictors that are guaranteed to contain the
correct answer with a probability chosen by the designer. Such formal
calibration guarantees hold irrespective of the true, unknown, distribution
underlying the generation of the variables of interest, and can be defined in
terms of ensemble or time-averaged probabilities. In this paper, conformal
prediction is applied for the first time to the design of AI for communication
systems in conjunction to both frequentist and Bayesian learning, focusing on
demodulation, modulation classification, and channel prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for a journal review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Runtime Monitoring for Out-of-Distribution Detection in Object Detection
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Hashemi, Jan Křetínsky, Sabine Rieder, Jessica Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Runtime monitoring provides a more realistic and applicable alternative to
verification in the setting of real neural networks used in industry. It is
particularly useful for detecting out-of-distribution (OOD) inputs, for which
the network was not trained and can yield erroneous results. We extend a
runtime-monitoring approach previously proposed for classification networks to
perception systems capable of identification and localization of multiple
objects. Furthermore, we analyze its adequacy experimentally on different kinds
of OOD settings, documenting the overall efficacy of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 Pages, 1 Table, 5 Figures. Accepted at the International Symposium
  of Formal Methods 2023 (FM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Put Attention to Temporal Saliency Patterns of Multi-Horizon Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nghia Duong-Trung, Stefan Born, Kiran Madhusudhanan, Randolf Scholz, Johannes Burchert, Danh Le-Phuoc, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series, sets of sequences in chronological order, are essential data in
statistical research with many forecasting applications. Although recent
performance in many Transformer-based models has been noticeable, long
multi-horizon time series forecasting remains a very challenging task. Going
beyond transformers in sequence translation and transduction research, we
observe the effects of down-and-up samplings that can nudge temporal saliency
patterns to emerge in time sequences. Motivated by the mentioned observation,
in this paper, we propose a novel architecture, Temporal Saliency Detection
(TSD), on top of the attention mechanism and apply it to multi-horizon time
series prediction. We renovate the traditional encoder-decoder architecture by
making as a series of deep convolutional blocks to work in tandem with the
multi-head self-attention. The proposed TSD approach facilitates the
multiresolution of saliency patterns upon condensed multi-heads, thus
progressively enhancing complex time series forecasting. Experimental results
illustrate that our proposed approach has significantly outperformed existing
state-of-the-art methods across multiple standard benchmark datasets in many
far-horizon forecasting settings. Overall, TSD achieves 31% and 46% relative
improvement over the current state-of-the-art models in multivariate and
univariate time series forecasting scenarios on standard benchmarks. The Git
repository is available at
https://github.com/duongtrung/time-series-temporal-saliency-patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAM: Selective Clarification for Ambiguous Questions with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Kuhn, Yarin Gal, Sebastian Farquhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models are often accurate on many
question-answering benchmarks with well-defined questions. Yet, in real
settings questions are often unanswerable without asking the user for
clarifying information. We show that current SotA models often do not ask the
user for clarification when presented with imprecise questions and instead
provide incorrect answers or "hallucinate". To address this, we introduce CLAM,
a framework that first uses the model to detect ambiguous questions, and if an
ambiguous question is detected, prompts the model to ask the user for
clarification. Furthermore, we show how to construct a scalable and
cost-effective automatic evaluation protocol using an oracle language model
with privileged information to provide clarifying information. We show that our
method achieves a 20.15 percentage point accuracy improvement over SotA on a
novel ambiguous question-answering answering data set derived from TriviaQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A scalable framework for annotating photovoltaic cell defects in
  electroluminescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Urtzi Otamendi, Inigo Martinez, Igor G. Olaizola, Marco Quartulli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The correct functioning of photovoltaic (PV) cells is critical to ensuring
the optimal performance of a solar plant. Anomaly detection techniques for PV
cells can result in significant cost savings in operation and maintenance
(O&M). Recent research has focused on deep learning techniques for
automatically detecting anomalies in Electroluminescence (EL) images. Automated
anomaly annotations can improve current O&M methodologies and help develop
decision-making systems to extend the life-cycle of the PV cells and predict
failures. This paper addresses the lack of anomaly segmentation annotations in
the literature by proposing a combination of state-of-the-art data-driven
techniques to create a Golden Standard benchmark. The proposed method stands
out for (1) its adaptability to new PV cell types, (2) cost-efficient
fine-tuning, and (3) leverage public datasets to generate advanced annotations.
The methodology has been validated in the annotation of a widely used dataset,
obtaining a reduction of the annotation cost by 60%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, 1 table, accepted at IEEE Transactions on
  Industrial Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-Temporal Anomaly Detection for Sensor Attacks in Autonomous
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Higgins, Devki Jha, David Wallom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-of-flight (ToF) distance measurement devices such as ultrasonics, LiDAR
and radar are widely used in autonomous vehicles for environmental perception,
navigation and assisted braking control. Despite their relative importance in
making safer driving decisions, these devices are vulnerable to multiple attack
types including spoofing, triggering and false data injection. When these
attacks are successful they can compromise the security of autonomous vehicles
leading to severe consequences for the driver, nearby vehicles and pedestrians.
To handle these attacks and protect the measurement devices, we propose a
spatial-temporal anomaly detection model \textit{STAnDS} which incorporates a
residual error spatial detector, with a time-based expected change detection.
This approach is evaluated using a simulated quantitative environment and the
results show that \textit{STAnDS} is effective at detecting multiple attack
types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable ML for Imbalanced Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damien A. Dablain, Colin Bellinger, Bartosz Krawczyk, David W. Aha, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are being increasingly applied to imbalanced data in
high stakes fields such as medicine, autonomous driving, and intelligence
analysis. Imbalanced data compounds the black-box nature of deep networks
because the relationships between classes may be highly skewed and unclear.
This can reduce trust by model users and hamper the progress of developers of
imbalanced learning algorithms. Existing methods that investigate imbalanced
data complexity are geared toward binary classification, shallow learning
models and low dimensional data. In addition, current eXplainable Artificial
Intelligence (XAI) techniques mainly focus on converting opaque deep learning
models into simpler models (e.g., decision trees) or mapping predictions for
specific instances to inputs, instead of examining global data properties and
complexities. Therefore, there is a need for a framework that is tailored to
modern deep networks, that incorporates large, high dimensional, multi-class
datasets, and uncovers data complexities commonly found in imbalanced data
(e.g., class overlap, sub-concepts, and outlier instances). We propose a set of
techniques that can be used by both deep learning model users to identify,
visualize and understand class prototypes, sub-concepts and outlier instances;
and by imbalanced learning algorithm developers to detect features and class
exemplars that are key to model performance. Our framework also identifies
instances that reside on the border of class decision boundaries, which can
carry highly discriminative information. Unlike many existing XAI techniques
which map model decisions to gray-scale pixel locations, we use saliency
through back-propagation to identify and aggregate image color bands across
entire classes. Our framework is publicly available at
\url{https://github.com/dd1github/XAI_for_Imbalanced_Learning}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Lai, Weinan Zhang, Xialin He, Chen Yu, Zheng Tian, Yong Yu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has recently emerged as an appealing alternative
for legged locomotion over multiple terrains by training a policy in physical
simulation and then transferring it to the real world (i.e., sim-to-real
transfer). Despite considerable progress, the capacity and scalability of
traditional neural networks are still limited, which may hinder their
applications in more complex environments. In contrast, the Transformer
architecture has shown its superiority in a wide range of large-scale sequence
modeling tasks, including natural language processing and decision-making
problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity
Transformer model for quadrupedal locomotion control on various terrains.
Furthermore, to better leverage Transformer in sim-to-real scenarios, we
present a novel two-stage training framework consisting of an offline
pretraining stage and an online correction stage, which can naturally integrate
Transformer with privileged training. Extensive experiments in simulation
demonstrate that TERT outperforms state-of-the-art baselines on different
terrains in terms of return, energy consumption and control smoothness. In
further real-world validation, TERT successfully traverses nine challenging
terrains, including sand pit and stair down, which can not be accomplished by
strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A large-scale and PCR-referenced vocal audio <span class="highlight-title">dataset</span> for COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jobie Budd, Kieran Baker, Emma Karoune, Harry Coppock, Selina Patel, Ana Tendero Cañadas, Alexander Titcomb, Richard Payne, David Hurley, Sabrina Egglestone, Lorraine Butler, Jonathon Mellor, George Nicholson, Ivan Kiskin, Vasiliki Koutra, Radka Jersakova, Rachel A. McKendry, Peter Diggle, Sylvia Richardson, Björn W. Schuller, Steven Gilmour, Davide Pigoli, Stephen Roberts, Josef Packham, Tracey Thornley, Chris Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UK COVID-19 Vocal Audio Dataset is designed for the training and
evaluation of machine learning models that classify SARS-CoV-2 infection status
or associated respiratory symptoms using vocal audio. The UK Health Security
Agency recruited voluntary participants through the national Test and Trace
programme and the REACT-1 survey in England from March 2021 to March 2022,
during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and
some Omicron variant sublineages. Audio recordings of volitional coughs,
exhalations, and speech were collected in the 'Speak up to help beat
coronavirus' digital survey alongside demographic, self-reported symptom and
respiratory condition data, and linked to SARS-CoV-2 test results. The UK
COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2
PCR-referenced audio recordings to date. PCR results were linked to 70,794 of
72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms
were reported by 45.62% of participants. This dataset has additional potential
uses for bioacoustics research, with 11.30% participants reporting asthma, and
27.20% with linked influenza PCR test results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Neural Networks for Material Model Calibration from
  Full-Field Displacement Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Anton, Henning Wessels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of material parameters occurring in constitutive models
has a wide range of applications in practice. One of these applications is the
monitoring and assessment of the actual condition of infrastructure buildings,
as the material parameters directly reflect the resistance of the structures to
external impacts. Physics-informed neural networks (PINNs) have recently
emerged as a suitable method for solving inverse problems. The advantages of
this method are a straightforward inclusion of observation data. Unlike
grid-based methods, such as the finite element method updating (FEMU) approach,
no computational grid and no interpolation of the data is required. In the
current work, we aim to further develop PINNs towards the calibration of the
linear-elastic constitutive model from full-field displacement and global force
data in a realistic regime. We show that normalization and conditioning of the
optimization problem play a crucial role in this process. Therefore, among
others, we identify the material parameters for initial estimates and balance
the individual terms in the loss function. In order to reduce the dependence of
the identified material parameters on local errors in the displacement
approximation, we base the identification not on the stress boundary conditions
but instead on the global balance of internal and external work. In addition,
we found that we get a better posed inverse problem if we reformulate it in
terms of bulk and shear modulus instead of Young's modulus and Poisson's ratio.
We demonstrate that the enhanced PINNs are capable of identifying material
parameters from both experimental one-dimensional data and synthetic full-field
displacement data in a realistic regime. Since displacement data measured by,
e.g., a digital image correlation (DIC) system is noisy, we additionally
investigate the robustness of the method to different levels of noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreCDo: A Large Corpus for French Cross-Domain Dialect Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihaela Gaman, Adrian-Gabriel Chifu, William Domingues, Radu Tudor Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel corpus for French dialect identification comprising
413,522 French text samples collected from public news websites in Belgium,
Canada, France and Switzerland. To ensure an accurate estimation of the dialect
identification performance of models, we designed the corpus to eliminate
potential biases related to topic, writing style, and publication source. More
precisely, the training, validation and test splits are collected from
different news websites, while searching for different keywords (topics). This
leads to a French cross-domain (FreCDo) dialect identification task. We conduct
experiments with four competitive baselines, a fine-tuned CamemBERT model, an
XGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)
classifier based on fine-tuned CamemBERT features, and an SVM based on word
n-grams. Aside from presenting quantitative results, we also make an analysis
of the most discriminative features learned by CamemBERT. Our corpus is
available at https://github.com/MihaelaGaman/FreCDo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomaly Detection in Driving by Cluster Analysis Twice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung-Hao Lee, Yen-Fu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Events deviating from normal traffic patterns in driving, anomalies, such as
aggressive driving or bumpy roads, may harm delivery efficiency for
transportation and logistics (T&L) business. Thus, detecting anomalies in
driving is critical for the T&L industry. So far numerous researches have used
vehicle sensor data to identify anomalies. Most previous works captured
anomalies by using deep learning or machine learning algorithms, which require
prior training processes and huge computational costs. This study proposes a
method namely Anomaly Detection in Driving by Cluster Analysis Twice (ADDCAT)
which clusters the processed sensor data in different physical properties. An
event is said to be an anomaly if it never fits with the major cluster, which
is considered as the pattern of normality in driving. This method provides a
way to detect anomalies in driving with no prior training processes and huge
computational costs needed. This paper validated the performance of the method
on an open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Reinforcement Learning with Shared Resources for Inventory
  Management <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Ding, Mingxiao Feng, Guozi Liu, Wei Jiang, Chuheng Zhang, Li Zhao, Lei Song, Houqiang Li, Yan Jin, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the inventory management (IM) problem where we
need to make replenishment decisions for a large number of stock keeping units
(SKUs) to balance their supply and demand. In our setting, the constraint on
the shared resources (such as the inventory capacity) couples the otherwise
independent control for each SKU. We formulate the problem with this structure
as Shared-Resource Stochastic Game (SRSG)and propose an efficient algorithm
called Context-aware Decentralized PPO (CD-PPO). Through extensive experiments,
we demonstrate that CD-PPO can accelerate the learning procedure compared with
standard MARL algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in RL4RealLife@NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s learn in-context by gradient descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the state-of-the-art neural network architecture
across numerous domains of machine learning. This is partly due to their
celebrated ability to transfer and to learn in-context based on few examples.
Nevertheless, the mechanisms by which Transformers become in-context learners
are not well understood and remain mostly an intuition. Here, we argue that
training Transformers on auto-regressive tasks can be closely related to
well-known gradient-based meta-learning formulations. We start by providing a
simple weight construction that shows the equivalence of data transformations
induced by 1) a single linear self-attention layer and by 2) gradient-descent
(GD) on a regression loss. Motivated by that construction, we show empirically
that when training self-attention-only Transformers on simple regression tasks
either the models learned by GD and Transformers show great similarity or,
remarkably, the weights found by optimization match the construction. Thus we
show how trained Transformers implement gradient descent in their forward pass.
This allows us, at least in the domain of regression problems, to
mechanistically understand the inner workings of optimized Transformers that
learn in-context. Furthermore, we identify how Transformers surpass plain
gradient descent by an iterative curvature correction and learn linear models
on deep data representations to solve non-linear regression tasks. Finally, we
discuss intriguing parallels to a mechanism identified to be crucial for
in-context learning termed induction-head (Olsson et al., 2022) and show how it
could be understood as a specific case of in-context learning by gradient
descent learning within Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpolation with the polynomial kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Elefante, Wolfgang Erb, Francesco Marchetti, Emma Perracchione, Davide Poggiali, Gabriele Santin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The polynomial kernels are widely used in machine learning and they are one
of the default choices to develop kernel-based classification and regression
models. However, they are rarely used and considered in numerical analysis due
to their lack of strict positive definiteness. In particular they do not enjoy
the usual property of unisolvency for arbitrary point sets, which is one of the
key properties used to build kernel-based interpolation methods. This paper is
devoted to establish some initial results for the study of these kernels, and
their related interpolation algorithms, in the context of approximation theory.
We will first prove necessary and sufficient conditions on point sets which
guarantee the existence and uniqueness of an interpolant. We will then study
the Reproducing Kernel Hilbert Spaces (or native spaces) of these kernels and
their norms, and provide inclusion relations between spaces corresponding to
different kernel parameters. With these spaces at hand, it will be further
possible to derive generic error estimates which apply to sufficiently smooth
functions, thus escaping the native space. Finally, we will show how to employ
an efficient stable algorithm to these kernels to obtain accurate interpolants,
and we will test them in some numerical experiment. After this analysis several
computational and theoretical aspects remain open, and we will outline possible
further research directions in a concluding section. This work builds some
bridges between kernel and polynomial interpolation, two topics to which the
authors, to different extents, have been introduced under the supervision or
through the work of Stefano De Marchi. For this reason, they wish to dedicate
this work to him in the occasion of his 60th birthday.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage Contextual <span class="highlight-title">Transformer</span>-based Convolutional Neural Network for
  Airway Extraction from CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Wu, Shuiqing Zhao, Shouliang Qi, Jie Feng, Haowen Pang, Runsheng Chang, Long Bai, Mengqi Li, Shuyue Xia, Wei Qian, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate airway extraction from computed tomography (CT) images is a critical
step for planning navigation bronchoscopy and quantitative assessment of
airway-related chronic obstructive pulmonary disease (COPD). The existing
methods are challenging to sufficiently segment the airway, especially the
high-generation airway, with the constraint of the limited label and cannot
meet the clinical use in COPD. We propose a novel two-stage 3D contextual
transformer-based U-Net for airway segmentation using CT images. The method
consists of two stages, performing initial and refined airway segmentation. The
two-stage model shares the same subnetwork with different airway masks as
input. Contextual transformer block is performed both in the encoder and
decoder path of the subnetwork to finish high-quality airway segmentation
effectively. In the first stage, the total airway mask and CT images are
provided to the subnetwork, and the intrapulmonary airway mask and
corresponding CT scans to the subnetwork in the second stage. Then the
predictions of the two-stage method are merged as the final prediction.
Extensive experiments were performed on in-house and multiple public datasets.
Quantitative and qualitative analysis demonstrate that our proposed method
extracted much more branches and lengths of the tree while accomplishing
state-of-the-art airway segmentation performance. The code is available at
https://github.com/zhaozsq/airway_segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's consider more general nonlinear approaches to study
  teleconnections of climate variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        D. Bueso, M. Piles, G. Camps-Valls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent work by (Rieger et al 2021) is concerned with the problem of
extracting features from spatio-temporal geophysical signals. The authors
introduce the complex rotated MCA (xMCA) to deal with lagged effects and
non-orthogonality of the feature representation. This method essentially (1)
transforms the signals to a complex plane with the Hilbert transform; (2)
applies an oblique (Varimax and Promax) rotation to remove the orthogonality
constraint; and (3) performs the eigendecomposition in this complex space
(Horel et al, 1984). We argue that this method is essentially a particular case
of the method called rotated complex kernel principal component analysis
(ROCK-PCA) introduced in (Bueso et al., 2019, 2020), where we proposed the same
approach: first transform the data to the complex plane with the Hilbert
transform and then apply the varimax rotation, with the only difference that
the eigendecomposition is performed in the dual (kernel) Hilbert space. The
latter allows us to generalize the xMCA solution by extracting nonlinear
(curvilinear) features when nonlinear kernel functions are used. Hence, the
solution of xMCA boils down to ROCK-PCA when the inner product is computed in
the input data space instead of in the high-dimensional (possibly infinite)
kernel Hilbert space to which data has been mapped. In this short
correspondence we show theoretical proof that xMCA is a special case of
ROCK-PCA and provide quantitative evidence that more expressive and informative
features can be extracted when working with kernels; results of the
decomposition of global sea surface temperature (SST) fields are shown to
illustrate the capabilities of ROCK-PCA to cope with nonlinear processes,
unlike xMCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ungeneralizable Contextual Logistic Bandit in Credit Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pojtanut Manopanjasiri, Kantapong Visantavarakul, Seksan Kiatsupaibul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of reinforcement learning in credit scoring has created a
unique setting for contextual logistic bandit that does not conform to the
usual exploration-exploitation tradeoff but rather favors exploration-free
algorithms. Through sufficient randomness in a pool of observable contexts, the
reinforcement learning agent can simultaneously exploit an action with the
highest reward while still learning more about the structure governing that
environment. Thus, it is the case that greedy algorithms consistently
outperform algorithms with efficient exploration, such as Thompson sampling.
However, in a more pragmatic scenario in credit scoring, lenders can, to a
degree, classify each borrower as a separate group, and learning about the
characteristics of each group does not infer any information to another group.
Through extensive simulations, we show that Thompson sampling dominates over
greedy algorithms given enough timesteps which increase with the complexity of
underlying features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JAX-Accelerated Neuroevolution of Physics-informed Neural Networks:
  Benchmarks and Experimental Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Sung Wei Yong, Jian Cheng Wong, Pao-Hsiung Chiu, Abhishek Gupta, Chinchun Ooi, Yew-Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the use of evolutionary algorithms for solving
differential equations. The solution is obtained by optimizing a deep neural
network whose loss function is defined by the residual terms from the
differential equations. Recent studies have used stochastic gradient descent
(SGD) variants to train these physics-informed neural networks (PINNs), but
these methods can struggle to find accurate solutions due to optimization
challenges. When solving differential equations, it is important to find the
globally optimum parameters of the network, rather than just finding a solution
that works well during training. SGD only searches along a single gradient
direction, so it may not be the best approach for training PINNs with their
accompanying complex optimization landscapes. In contrast, evolutionary
algorithms perform a parallel exploration of different solutions in order to
avoid getting stuck in local optima and can potentially find more accurate
solutions. However, evolutionary algorithms can be slow, which can make them
difficult to use in practice. To address this, we provide a set of five
benchmark problems with associated performance metrics and baseline results to
support the development of evolutionary algorithms for enhanced PINN training.
As a baseline, we evaluate the performance and speed of using the widely
adopted Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for solving
PINNs. We provide the loss and training time for CMA-ES run on TensorFlow, and
CMA-ES and SGD run on JAX (with GPU acceleration) for the five benchmark
problems. Our results show that JAX-accelerated evolutionary algorithms,
particularly CMA-ES, can be a useful approach for solving differential
equations. We hope that our work will support the exploration and development
of alternative optimization algorithms for the complex task of optimizing
PINNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curriculum Learning Meets Weakly Supervised Modality Correlation
  Learning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Mai, Ya Sun, Haifeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of multimodal sentiment analysis (MSA), a few studies have
leveraged the inherent modality correlation information stored in samples for
self-supervised learning. However, they feed the training pairs in a random
order without consideration of difficulty. Without human annotation, the
generated training pairs of self-supervised learning often contain noise. If
noisy or hard pairs are used for training at the easy stage, the model might be
stuck in bad local optimum. In this paper, we inject curriculum learning into
weakly supervised modality correlation learning. The weakly supervised
correlation learning leverages the label information to generate scores for
negative pairs to learn a more discriminative embedding space, where negative
pairs are defined as two unimodal embeddings from different samples. To assist
the correlation learning, we feed the training pairs to the model according to
difficulty by the proposed curriculum learning, which consists of elaborately
designed scoring and feeding functions. The scoring function computes the
difficulty of pairs using pre-trained and current correlation predictors, where
the pairs with large losses are defined as hard pairs. Notably, the hardest
pairs are discarded in our algorithm, which are assumed as noisy pairs.
Moreover, the feeding function takes the difference of correlation losses as
feedback to determine the feeding actions (`stay', `step back', or `step
forward'). The proposed method reaches state-of-the-art performance on MSA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Policy Learning for Powertrain Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lindsey Kerbel, Beshah Ayalew, Andrej Ivanco, Keith Loiselle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eco-driving strategies have been shown to provide significant reductions in
fuel consumption. This paper outlines an active driver assistance approach that
uses a residual policy learning (RPL) agent trained to provide residual actions
to default power train controllers while balancing fuel consumption against
other driver-accommodation objectives. Using previous experiences, our RPL
agent learns improved traction torque and gear shifting residual policies to
adapt the operation of the powertrain to variations and uncertainties in the
environment. For comparison, we consider a traditional reinforcement learning
(RL) agent trained from scratch. Both agents employ the off-policy Maximum A
Posteriori Policy Optimization algorithm with an actor-critic architecture. By
implementing on a simulated commercial vehicle in various car-following
scenarios, we find that the RPL agent quickly learns significantly improved
policies compared to a baseline source policy but in some measures not as good
as those eventually possible with the RL agent trained from scratch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10th IFAC Symposium on Advances in Automotive Control AAC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Output-Dependent Gaussian Process State-Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhidi Lin, Lei Cheng, Feng Yin, Lexi Xu, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process state-space model (GPSSM) is a fully probabilistic
state-space model that has attracted much attention over the past decade.
However, the outputs of the transition function in the existing GPSSMs are
assumed to be independent, meaning that the GPSSMs cannot exploit the inductive
biases between different outputs and lose certain model capacities. To address
this issue, this paper proposes an output-dependent and more realistic GPSSM by
utilizing the well-known, simple yet practical linear model of
coregionalization (LMC) framework to represent the output dependency. To
jointly learn the output-dependent GPSSM and infer the latent states, we
propose a variational sparse GP-based learning method that only gently
increases the computational complexity. Experiments on both synthetic and real
datasets demonstrate the superiority of the output-dependent GPSSM in terms of
learning and inference performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Driver Assistance Eco-driving and Transmission Control with Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lindsey Kerbel, Beshah Ayalew, Andrej Ivanco, Keith Loiselle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing need to reduce energy consumption and greenhouse gas
emissions, Eco-driving strategies provide a significant opportunity for
additional fuel savings on top of other technological solutions being pursued
in the transportation sector. In this paper, a model-free deep reinforcement
learning (RL) control agent is proposed for active Eco-driving assistance that
trades-off fuel consumption against other driver-accommodation objectives, and
learns optimal traction torque and transmission shifting policies from
experience. The training scheme for the proposed RL agent uses an off-policy
actor-critic architecture that iteratively does policy evaluation with a
multi-step return and policy improvement with the maximum posteriori policy
optimization algorithm for hybrid action spaces. The proposed Eco-driving RL
agent is implemented on a commercial vehicle in car following traffic. It shows
superior performance in minimizing fuel consumption compared to a baseline
controller that has full knowledge of fuel-efficiency tables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Distribution Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Suri, Yifu Lu, Yanjin Chen, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A distribution inference attack aims to infer statistical properties of data
used to train machine learning models. These attacks are sometimes surprisingly
potent, but the factors that impact distribution inference risk are not well
understood and demonstrated attacks often rely on strong and unrealistic
assumptions such as full knowledge of training environments even in supposedly
black-box threat scenarios. To improve understanding of distribution inference
risks, we develop a new black-box attack that even outperforms the best known
white-box attack in most settings. Using this new attack, we evaluate
distribution inference risk while relaxing a variety of assumptions about the
adversary's knowledge under black-box access, like known model architectures
and label-only access. Finally, we evaluate the effectiveness of previously
proposed defenses and introduce new defenses. We find that although noise-based
defenses appear to be ineffective, a simple re-sampling defense can be highly
effective. Code is available at
https://github.com/iamgroot42/dissecting_distribution_inference
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SaTML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepJoin: Joinable Table Discovery with <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Dong, Chuan Xiao, Takuma Nozawa, Masafumi Enomoto, Masafumi Oyamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the usefulness in data enrichment for data analysis tasks, joinable
table discovery has become an important operation in data lake management.
Existing approaches target equi-joins, the most common way of combining tables
for creating a unified view, or semantic joins, which tolerate misspellings and
different formats to deliver more join results. They are either exact solutions
whose running time is linear in the sizes of query column and target table
repository or approximate solutions lacking precision. In this paper, we
propose Deepjoin, a deep learning model for accurate and efficient joinable
table discovery. Our solution is an embedding-based retrieval, which employs a
pre-trained language model (PLM) and is designed as one framework serving both
equi- and semantic joins. We propose a set of contextualization options to
transform column contents to a text sequence. The PLM reads the sequence and is
fine-tuned to embed columns to vectors such that columns are expected to be
joinable if they are close to each other in the vector space. Since the output
of the PLM is fixed in length, the subsequent search procedure becomes
independent of the column size. With a state-of-the-art approximate nearest
neighbor search algorithm, the search time is logarithmic in the repository
size. To train the model, we devise the techniques for preparing training data
as well as data augmentation. The experiments on real datasets demonstrate that
by training on a small subset of a corpus, Deepjoin generalizes to large
datasets and its precision consistently outperforms other approximate
solutions'. Deepjoin is even more accurate than an exact solution to semantic
joins when evaluated with labels from experts. Moreover, when equipped with a
GPU, Deepjoin is up to two orders of magnitude faster than existing solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Learning with <span class="highlight-title">Pre-Train</span>ed Networks Improves Source-Free Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Zhang, Li Shen, Chuan-Sheng Foo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation aims to adapt a source model trained on
fully-labeled source domain data to a target domain with unlabeled target
domain data. Source data is assumed inaccessible due to proprietary or privacy
reasons. Existing works use the source model to pseudolabel target data, but
the pseudolabels are unreliable due to data distribution shift between source
and target domain. In this work, we propose to leverage an ImageNet pre-trained
feature extractor in a new co-learning framework to improve target pseudolabel
quality for finetuning the source model. Benefits of the ImageNet feature
extractor include that it is not source-biased and it provides an alternate
view of features and classification decisions different from the source model.
Such pre-trained feature extractors are also publicly available, which allows
us to readily leverage modern network architectures that have strong
representation learning ability. After co-learning, we sharpen predictions of
non-pseudolabeled samples by entropy minimization. Evaluation on 3 benchmark
datasets show that our proposed method can outperform existing source-free
domain adaptation methods, as well as unsupervised domain adaptation methods
which assume joint access to source and target data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Man-recon: manifold learning for reconstruction with deep autoencoder
  for smart seismic interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Mustafa, Ghassan AlRegib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning can extract rich data representations if provided sufficient
quantities of labeled training data. For many tasks however, annotating data
has significant costs in terms of time and money, owing to the high standards
of subject matter expertise required, for example in medical and geophysical
image interpretation tasks. Active Learning can identify the most informative
training examples for the interpreter to train, leading to higher efficiency.
We propose an Active learning method based on jointly learning representations
for supervised and unsupervised tasks. The learned manifold structure is later
utilized to identify informative training samples most dissimilar from the
learned manifold from the error profiles on the unsupervised task. We verify
the efficiency of the proposed method on a seismic facies segmentation dataset
from the Netherlands F3 block survey, significantly outperforming contemporary
methods to achieve the highest mean Intersection-Over-Union value of 0.773.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirfRANS: High Fidelity Computational Fluid Dynamics <span class="highlight-title">Dataset</span> for
  Approximating Reynolds-Averaged Navier-Stokes Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Bonnet, Ahmed Jocelyn Mazari, Paola Cinnella, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are necessary to optimize meaningful quantities in physical
dynamics as their recursive numerical resolutions are often prohibitively
expensive. It is mainly the case for fluid dynamics and the resolution of
Navier-Stokes equations. However, despite the fast-growing field of data-driven
models for physical systems, reference datasets representing real-world
phenomena are lacking. In this work, we develop AirfRANS, a dataset for
studying the two-dimensional incompressible steady-state Reynolds-Averaged
Navier-Stokes equations over airfoils at a subsonic regime and for different
angles of attacks. We also introduce metrics on the stress forces at the
surface of geometries and visualization of boundary layers to assess the
capabilities of models to accurately predict the meaningful information of the
problem. Finally, we propose deep learning baselines on four machine learning
tasks to study AirfRANS under different constraints for generalization
considerations: big and scarce data regime, Reynolds number, and angle of
attack extrapolation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Machine Learning for Hydrocarbon Prospect Risking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Mustafa, Ghassan AlRegib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hydrocarbon prospect risking is a critical application in geophysics
predicting well outcomes from a variety of data including geological,
geophysical, and other information modalities. Traditional routines require
interpreters to go through a long process to arrive at the probability of
success of specific outcomes. AI has the capability to automate the process but
its adoption has been limited thus far owing to a lack of transparency in the
way complicated, black box models generate decisions. We demonstrate how LIME
-- a model-agnostic explanation technique -- can be used to inject trust in
model decisions by uncovering the model's reasoning process for individual
predictions. It generates these explanations by fitting interpretable models in
the local neighborhood of specific datapoints being queried. On a dataset of
well outcomes and corresponding geophysical attribute data, we show how LIME
can induce trust in model's decisions by revealing the decision-making process
to be aligned to domain knowledge. Further, it has the potential to debug
mispredictions made due to anomalous patterns in the data or faulty training
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness Evaluation of Regression Tasks with Skewed Domain Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuno Costa, Nuno Moniz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural phenomena, data distributions often deviate from normality. One
can think of cataclysms as a self-explanatory example: events that occur almost
never, and at the same time are many standard deviations away from the common
outcome. In many scientific contexts it is exactly these tail events that
researchers are most interested in anticipating, so that adequate measures can
be taken to prevent or attenuate a major impact on society. Despite such
efforts, we have yet to provide definite answers to crucial issues in
evaluating predictive solutions in domains such as weather, pollution, health.
In this paper, we deal with two encapsulated problems simultaneously. First,
assessing the performance of regression models when non-uniform preferences
apply - not all values are equally relevant concerning the accuracy of their
prediction, and there's a particular interest in the most extreme values.
Second, assessing the robustness of models when dealing with uncertainty
regarding the actual underlying distribution of values relevant for such
problems. We show how different levels of relevance associated with target
values may impact experimental conclusions, and demonstrate the practical
utility of the proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOC-NAD: A Hybrid Deep One-class Classifier for Network Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohanad Sarhan, Gayan Kulatilleke, Wai Weng Lo, Siamak Layeghy, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) approaches have been used to enhance the detection
capabilities of Network Intrusion Detection Systems (NIDSs). Recent work has
achieved near-perfect performance by following binary- and multi-class network
anomaly detection tasks. Such systems depend on the availability of both
(benign and malicious) network data classes during the training phase. However,
attack data samples are often challenging to collect in most organisations due
to security controls preventing the penetration of known malicious traffic to
their networks. Therefore, this paper proposes a Deep One-Class (DOC)
classifier for network intrusion detection by only training on benign network
data samples. The novel one-class classification architecture consists of a
histogram-based deep feed-forward classifier to extract useful network data
features and use efficient outlier detection. The DOC classifier has been
extensively evaluated using two benchmark NIDS datasets. The results
demonstrate its superiority over current state-of-the-art one-class classifiers
in terms of detection and false positive rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic Analysis of Deep Residual Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rama Cont, Alain Rossier, Renyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the asymptotic properties of deep Residual networks (ResNets)
as the number of layers increases. We first show the existence of scaling
regimes for trained weights markedly different from those implicitly assumed in
the neural ODE literature. We study the convergence of the hidden state
dynamics in these scaling regimes, showing that one may obtain an ODE, a
stochastic differential equation (SDE) or neither of these. In particular, our
findings point to the existence of a diffusive regime in which the deep network
limit is described by a class of stochastic differential equations (SDEs).
Finally, we derive the corresponding scaling limits for the backpropagation
dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 12 figures. arXiv admin note: substantial text overlap with
  arXiv:2105.12245</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources
  in Natural Language Understanding Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, Jackie Chi Kit Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many state-of-the-art natural language understanding (NLU) models are based
on pretrained neural language models. These models often make inferences using
information from multiple sources. An important class of such inferences are
those that require both background knowledge, presumably contained in a model's
pretrained parameters, and instance-specific information that is supplied at
inference time. However, the integration and reasoning abilities of NLU models
in the presence of multiple knowledge sources have been largely understudied.
In this work, we propose a test suite of coreference resolution tasks that
require reasoning over multiple facts. Our dataset is organized into subtasks
that differ in terms of which knowledge sources contain relevant facts. We
evaluate state-of-the-art coreference resolution models on our dataset. Our
results indicate that several models struggle to reason on-the-fly over
knowledge observed both at pretrain time and at inference time. However, with
task-specific training, a subset of models demonstrates the ability to
integrate certain knowledge types from multiple sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Resolution Online Deterministic Annealing: A Hierarchical and
  Progressive Learning Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Mavridis, John Baras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical learning algorithms that gradually approximate a solution to a
data-driven optimization problem are essential to decision-making systems,
especially under limitations on time and computational resources. In this
study, we introduce a general-purpose hierarchical learning architecture that
is based on the progressive partitioning of a possibly multi-resolution data
space. The optimal partition is gradually approximated by solving a sequence of
optimization sub-problems that yield a sequence of partitions with increasing
number of subsets. We show that the solution of each optimization problem can
be estimated online using gradient-free stochastic approximation updates. As a
consequence, a function approximation problem can be defined within each subset
of the partition and solved using the theory of two-timescale stochastic
approximation algorithms. This simulates an annealing process and defines a
robust and interpretable heuristic method to gradually increase the complexity
of the learning architecture in a task-agnostic manner, giving emphasis to
regions of the data space that are considered more important according to a
predefined criterion. Finally, by imposing a tree structure in the progression
of the partitions, we provide a means to incorporate potential multi-resolution
structure of the data space into this approach, significantly reducing its
complexity, while introducing hierarchical feature extraction properties
similar to certain classes of deep learning architectures. Asymptotic
convergence analysis and experimental results are provided for clustering,
classification, and regression problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Moving Average Pseudo-Labeling for Source-Free Inductive Domain
  Adaptation <span class="chip">BMVC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yan, Yuhong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation reduces the reliance on data annotation in
deep learning by adapting knowledge from a source to a target domain. For
privacy and efficiency concerns, source-free domain adaptation extends
unsupervised domain adaptation by adapting a pre-trained source model to an
unlabeled target domain without accessing the source data. However, most
existing source-free domain adaptation methods to date focus on the
transductive setting, where the target training set is also the testing set. In
this paper, we address source-free domain adaptation in the more realistic
inductive setting, where the target training and testing sets are mutually
exclusive. We propose a new semi-supervised fine-tuning method named Dual
Moving Average Pseudo-Labeling (DMAPL) for source-free inductive domain
adaptation. We first split the unlabeled training set in the target domain into
a pseudo-labeled confident subset and an unlabeled less-confident subset
according to the prediction confidence scores from the pre-trained source
model. Then we propose a soft-label moving-average updating strategy for the
unlabeled subset based on a moving-average prototypical classifier, which
gradually adapts the source model towards the target domain. Experiments show
that our proposed method achieves state-of-the-art performance and outperforms
previous methods by large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Sparsity and Randomness for Data-driven Low Rank Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejin Chen, Yicheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based low rank approximation algorithms can significantly improve
the performance of randomized low rank approximation with sketch matrix. With
the learned value and fixed non-zero positions for sketch matrices from
learning-based algorithms, these matrices can reduce the test error of low rank
approximation significantly. However, there is still no good method to learn
non-zero positions as well as overcome the out-of-distribution performance
loss. In this work, we introduce two new methods Learning Sparsity and Learning
Randomness which try to learn a better sparsity patterns and add randomness to
the value of sketch matrix. These two methods can be applied with any
learning-based algorithms which use sketch matrix directly. Our experiments
show that these two methods can improve the performance of previous
learning-based algorithm for both test error and out-of-distribution test error
without adding too much complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NBC-Softmax : Darkweb Author fingerprinting and migration tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayan K. Kulatilleke, Shekhar S. Chandra, Marius Portmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning aims to learn distances from the data, which enhances the
performance of similarity-based algorithms. An author style detection task is a
metric learning problem, where learning style features with small intra-class
variations and larger inter-class differences is of great importance to achieve
better performance. Recently, metric learning based on softmax loss has been
used successfully for style detection. While softmax loss can produce separable
representations, its discriminative power is relatively poor. In this work, we
propose NBC-Softmax, a contrastive loss based clustering technique for softmax
loss, which is more intuitive and able to achieve superior performance. Our
technique meets the criterion for larger number of samples, thus achieving
block contrastiveness, which is proven to outperform pair-wise losses. It uses
mini-batch sampling effectively and is scalable. Experiments on 4 darkweb
social forums, with NBCSAuthor that uses the proposed NBC-Softmax for author
and sybil detection, shows that our negative block contrastive approach
constantly outperforms state-of-the-art methods using the same network
architecture.
  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-IID Transfer Learning on Graphs <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wu, Jingrui He, Elizabeth Ainsworth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning refers to the transfer of knowledge or information from a
relevant source domain to a target domain. However, most existing transfer
learning theories and algorithms focus on IID tasks, where the source/target
samples are assumed to be independent and identically distributed. Very little
effort is devoted to theoretically studying the knowledge transferability on
non-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we
propose rigorous generalization bounds and algorithms for cross-network
transfer learning from a source graph to a target graph. The crucial idea is to
characterize the cross-network knowledge transferability from the perspective
of the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a
novel Graph Subtree Discrepancy to measure the graph distribution shift between
source and target graphs. Then the generalization error bounds on cross-network
transfer learning, including both cross-network node classification and link
prediction tasks, can be derived in terms of the source knowledge and the Graph
Subtree Discrepancy across domains. This thereby motivates us to propose a
generic graph adaptive network (GRADE) to minimize the distribution shift
between source and target graphs for cross-network transfer learning.
Experimental results verify the effectiveness and efficiency of our GRADE
framework on both cross-network node classification and cross-domain
recommendation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Measures of Spread in High Dimensional Latent Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna C. Marbut, Katy McKinney-Bock, Travis J. Wheeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding geometric properties of natural language processing models'
latent spaces allows the manipulation of these properties for improved
performance on downstream tasks. One such property is the amount of data spread
in a model's latent space, or how fully the available latent space is being
used. In this work, we define data spread and demonstrate that the commonly
used measures of data spread, Average Cosine Similarity and a partition
function min/max ratio I(V), do not provide reliable metrics to compare the use
of latent space across models. We propose and examine eight alternative
measures of data spread, all but one of which improve over these current
metrics when applied to seven synthetic data distributions. Of our proposed
measures, we recommend one principal component-based measure and one
entropy-based measure that provide reliable, relative measures of spread and
can be used to compare models of different sizes and dimensionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 11 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graphon Pooling for Reducing Dimensionality of Signals and Convolutional
  Operators on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Parada-Mayorga, Zhiyang Wang, Alejandro Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a pooling approach for convolutional information
processing on graphs relying on the theory of graphons and limits of dense
graph sequences. We present three methods that exploit the induced graphon
representation of graphs and graph signals on partitions of [0, 1]2 in the
graphon space. As a result we derive low dimensional representations of the
convolutional operators, while a dimensionality reduction of the signals is
achieved by simple local interpolation of functions in L2([0, 1]). We prove
that those low dimensional representations constitute a convergent sequence of
graphs and graph signals, respectively. The methods proposed and the
theoretical guarantees that we provide show that the reduced graphs and signals
inherit spectral-structural properties of the original quantities. We evaluate
our approach with a set of numerical experiments performed on graph neural
networks (GNNs) that rely on graphon pooling. We observe that graphon pooling
performs significantly better than other approaches proposed in the literature
when dimensionality reduction ratios between layers are large. We also observe
that when graphon pooling is used we have, in general, less overfitting and
lower computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BNSynth: Bounded Boolean Functional Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Raja, Stanly Samuel, Chiranjib Bhattacharyya, Deepak D'Souza, Aditya Kanade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated synthesis of correct-by-construction Boolean functions from
logical specifications is known as the Boolean Functional Synthesis (BFS)
problem. BFS has many application areas that range from software engineering to
circuit design. In this paper, we introduce a tool BNSynth, that is the first
to solve the BFS problem under a given bound on the solution space. Bounding
the solution space induces the synthesis of smaller functions that benefit
resource constrained areas such as circuit design. BNSynth uses a
counter-example guided, neural approach to solve the bounded BFS problem.
Initial results show promise in synthesizing smaller solutions; we observe at
least \textbf{3.2X} (and up to \textbf{24X}) improvement in the reduction of
solution size on average, as compared to state of the art tools on our
benchmarks. BNSynth is available on GitHub under an open source license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Huber-energy measure quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Turinici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a measure quantization procedure i.e., an algorithm which finds
the best approximation of a target probability law (and more generally signed
finite variation measure) by a sum of Q Dirac masses (Q being the quantization
parameter). The procedure is implemented by minimizing the statistical distance
between the original measure and its quantized version; the distance is built
from a negative definite kernel and, if necessary, can be computed on the fly
and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We
investigate theoretically the fundamental questions of existence of the optimal
measure quantizer and identify what are the required kernel properties that
guarantee suitable behavior. We test the procedure, called HEMQ, on several
databases: multi-dimensional Gaussian mixtures, Wiener space cubature, Italian
wine cultivars and the MNIST image database. The results indicate that the HEMQ
algorithm is robust and versatile and, for the class of Huber-energy kernels,
it matches the expected intuitive behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiDO: Fusion-in-Decoder optimized for stronger performance and faster
  inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that
sets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD
suffers from very expensive inference. We show that the majority of inference
time results from memory bandwidth constraints in the decoder, and propose two
simple changes to the FiD architecture to speed up inference by 7x. The faster
decoder inference then allows for a much larger decoder. We denote FiD with the
above modifications as FiDO, and show that it strongly improves performance
over existing FiD models for a wide range of inference budgets. For example,
FiDO-Large-XXL performs faster inference than FiD-Base and achieves better
performance than FiD-Large.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First De-Trend then Attend: Rethinking Attention for Time-Series
  Forecasting <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiyuan Zhang, Xiaoyong Jin, Karthick Gopalswamy, Gaurav Gupta, Youngsuk Park, Xingjian Shi, Hao Wang, Danielle C. Maddix, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have gained large popularity and demonstrated
promising results in long-term time-series forecasting in recent years. In
addition to learning attention in time domain, recent works also explore
learning attention in frequency domains (e.g., Fourier domain, wavelet domain),
given that seasonal patterns can be better captured in these domains. In this
work, we seek to understand the relationships between attention models in
different time and frequency domains. Theoretically, we show that attention
models in different domains are equivalent under linear conditions (i.e.,
linear kernel to attention scores). Empirically, we analyze how attention
models of different domains show different behaviors through various synthetic
experiments with seasonality, trend and noise, with emphasis on the role of
softmax operation therein. Both these theoretical and empirical analyses
motivate us to propose a new method: TDformer (Trend Decomposition
Transformer), that first applies seasonal-trend decomposition, and then
additively combines an MLP which predicts the trend component with Fourier
attention which predicts the seasonal component to obtain the final prediction.
Extensive experiments on benchmark time-series forecasting datasets demonstrate
that TDformer achieves state-of-the-art performance against existing
attention-based models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 All Things Attention Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Long Sequence Modeling via State Space Augmented <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved superior performance in various natural
language processing tasks. However, the quadratic computational cost of the
attention mechanism limits its practicality for long sequences. There are
existing attention variants that improve the computational efficiency, but they
have limited ability to effectively compute global information. In parallel to
Transformer models, state space models (SSMs) are tailored for long sequences,
but they are not flexible enough to capture complicated local information. We
propose SPADE, short for $\underline{\textbf{S}}$tate
s$\underline{\textbf{P}}$ace
$\underline{\textbf{A}}$ugmente$\underline{\textbf{D}}$
Transform$\underline{\textbf{E}}$r. Specifically, we augment a SSM into the
bottom layer of SPADE, and we employ efficient local attention methods for the
other layers. The SSM augments global information, which complements the lack
of long-range dependency issue in local attention methods. Experimental results
on the Long Range Arena benchmark and language modeling tasks demonstrate the
effectiveness of the proposed method. To further demonstrate the scalability of
SPADE, we pre-train large encoder-decoder models and present fine-tuning
results on natural language understanding and natural language generation
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap Between Offline and Online Reinforcement Learning
  Evaluation Methodologies <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivakanth Sujit, Pedro H. M. Braga, Jorg Bornschein, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has shown great promise with algorithms learning
in environments with large state and action spaces purely from scalar reward
signals. A crucial challenge for current deep RL algorithms is that they
require a tremendous amount of environment interactions for learning. This can
be infeasible in situations where such interactions are expensive; such as in
robotics. Offline RL algorithms try to address this issue by bootstrapping the
learning process from existing logged data without needing to interact with the
environment from the very beginning. While online RL algorithms are typically
evaluated as a function of the number of environment interactions, there exists
no single established protocol for evaluating offline RL methods.In this paper,
we propose a sequential approach to evaluate offline RL algorithms as a
function of the training set size and thus by their data efficiency. Sequential
evaluation provides valuable insights into the data efficiency of the learning
process and the robustness of algorithms to distribution changes in the dataset
while also harmonizing the visualization of the offline and online learning
phases. Our approach is generally applicable and easy to implement. We compare
several existing offline RL algorithms using this approach and present insights
from a variety of tasks and offline datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Offline RL Workshop, NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Evaluating Adversarial Robustness of Chest X-ray Classification:
  Pitfalls and Best Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salah Ghamizi, Maxime Cordy, Michail Papadakis, Yves Le Traon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vulnerability to adversarial attacks is a well-known weakness of Deep Neural
Networks. While most of the studies focus on natural images with standardized
benchmarks like ImageNet and CIFAR, little research has considered real world
applications, in particular in the medical domain. Our research shows that,
contrary to previous claims, robustness of chest x-ray classification is much
harder to evaluate and leads to very different assessments based on the
dataset, the architecture and robustness metric. We argue that previous studies
did not take into account the peculiarity of medical diagnosis, like the
co-occurrence of diseases, the disagreement of labellers (domain experts), the
threat model of the attacks and the risk implications for each successful
attack.
  In this paper, we discuss the methodological foundations, review the pitfalls
and best practices, and suggest new methodological considerations for
evaluating the robustness of chest xray classification models. Our evaluation
on 3 datasets, 7 models, and 18 diseases is the largest evaluation of
robustness of chest x-ray classification models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian posterior approximation with stochastic ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For CIFAR, the stochastic ensembles are
quantitatively compared to published Hamiltonian Monte Carlo results for a
ResNet-20 architecture. We also test the quality of the posteriors directly
against Hamiltonian Monte Carlo simulations in a simplified toy model. Our
results show that in a number of settings, stochastic ensembles provide more
accurate posterior estimates than regular deep ensembles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Deep Learning Models for Vulnerability Detection <span class="chip">ICSE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Steenhoek, Md Mahbubur Rahman, Richard Jiles, Wei Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models of code have recently reported great progress for
vulnerability detection. In some cases, DL-based models have outperformed
static analysis tools. Although many great models have been proposed, we do not
yet have a good understanding of these models. This limits the further
advancement of model robustness, debugging, and deployment for the
vulnerability detection. In this paper, we surveyed and reproduced 9
state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability
detection datasets: Devign and MSR. We investigated 6 research questions in
three areas, namely model capabilities, training data, and model
interpretation. We experimentally demonstrated the variability between
different runs of a model and the low agreement among different models'
outputs. We investigated models trained for specific types of vulnerabilities
compared to a model that is trained on all the vulnerabilities at once. We
explored the types of programs DL may consider "hard" to handle. We
investigated the relations of training data sizes and training data composition
with model performance. Finally, we studied model interpretations and analyzed
important features that the models used to make predictions. We believe that
our findings can help better understand model results, provide guidance on
preparing training data, and improve the robustness of the models. All of our
datasets, code, and results are available at
https://figshare.com/s/284abfba67dba448fdc2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figures. Accepted at ICSE 2023 (not camera-ready
  version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepDFA: Dataflow Analysis-Guided Efficient Graph Learning for
  Vulnerability Detection <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Steenhoek, Wei Le, Hongyang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based vulnerability detection models have recently been shown
to be effective and, in some cases, outperform static analysis tools. However,
the highest-performing approaches use token-based transformer models, which do
not leverage domain knowledge. Classical program analysis techniques such as
dataflow analysis can detect many types of bugs and are the most commonly used
methods in practice. Motivated by the causal relationship between bugs and
dataflow analysis, we present DeepDFA, a dataflow analysis-guided graph
learning framework and embedding that uses program semantic features for
vulnerability detection. We show that DeepDFA is performant and efficient.
DeepDFA ranked first in recall, first in generalizing over unseen projects, and
second in F1 among all the state-of-the-art models we experimented with. It is
also the smallest model in terms of the number of parameters, and was trained
in 9 minutes, 69x faster than the highest-performing baseline. DeepDFA can be
used with other models. By integrating LineVul and DeepDFA, we achieved the
best vulnerability detection performance of 96.4 F1 score, 98.69 precision, and
94.22 recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures. Under review as a conference paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to repeatedly solve routing problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouad Morabit, Guy Desaulniers, Andrea Lodi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last years, there has been a great interest in machine-learning-based
heuristics for solving NP-hard combinatorial optimization problems. The
developed methods have shown potential on many optimization problems. In this
paper, we present a learned heuristic for the reoptimization of a problem after
a minor change in its data. We focus on the case of the capacited vehicle
routing problem with static clients (i.e., same client locations) and changed
demands. Given the edges of an original solution, the goal is to predict and
fix the ones that have a high chance of remaining in an optimal solution after
a change of client demands. This partial prediction of the solution reduces the
complexity of the problem and speeds up its resolution, while yielding a good
quality solution. The proposed approach resulted in solutions with an
optimality gap ranging from 0\% to 1.7\% on different benchmark instances
within a reasonable computing time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-RAFT: A Differentially Private Recipe for Accelerated Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwinee Panda, Xinyu Tang, Vikash Sehwag, Saeed Mahloujifar, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major direction in differentially private machine learning is
differentially private fine-tuning: pretraining a model on a source of "public
data" and transferring the extracted features to downstream tasks.
  This is an important setting because many industry deployments fine-tune
publicly available feature extractors on proprietary data for downstream tasks.
  In this paper, we carefully integrate techniques, both new and from prior
work, to solve benchmark tasks in computer vision and natural language
processing using differentially private fine-tuning. Our key insight is that by
accelerating training with the choice of key hyperparameters, we can quickly
drive the model parameters to regions in parameter space where the impact of
noise is minimized. We obtain new state-of-the art performance on CIFAR10,
CIFAR100, FashionMNIST, STL10, and PersonaChat, including $99 \%$ on CIFAR10
for $\varepsilon=1, \delta=1e-5$-DP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-Aware Adversarial <span class="highlight-title">Transformer</span>s for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.10737v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.10737v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu You, Ruihan Zhao, Fenglin Liu, Siyuan Dong, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: (1) existing
methods fail to capture the important features of the images due to the naive
tokenization scheme; (2) the models suffer from information loss because they
only consider single-scale feature representations; and (3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CASTformer, a novel type of adversarial transformers, for 2D medical image
segmentation. First, we take advantage of the pyramid structure to construct
multi-scale representations and handle multi-scale variations. We then design a
novel class-aware transformer module to better learn the discriminative regions
of objects with semantic structures. Lastly, we utilize an adversarial training
strategy that boosts segmentation accuracy and correspondingly allows a
transformer-based discriminator to capture high-level semantically correlated
contents and low-level anatomical features. Our experiments demonstrate that
CASTformer dramatically outperforms previous state-of-the-art transformer-based
approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in
Dice over previous models. Further qualitative experiments provide a more
detailed picture of the model's inner workings, shed light on the challenges in
improved transparency, and demonstrate that transfer learning can greatly
improve performance and reduce the size of medical image datasets in training,
making CASTformer a strong starting point for downstream medical image analysis
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving SGD convergence by online linear regression of gradients in
  multiple statistically relevant directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1901.11457v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1901.11457v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jarek Duda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are usually trained with stochastic gradient descent
(SGD), which minimizes objective function using very rough approximations of
gradient, only averaging to the real gradient. Standard approaches like
momentum or ADAM only consider a single direction, and do not try to model
distance from extremum - neglecting valuable information from calculated
sequence of gradients, often stagnating in some suboptimal plateau. Second
order methods could exploit these missed opportunities, however, beside
suffering from very large cost and numerical instabilities, many of them
attract to suboptimal points like saddles due to negligence of signs of
curvatures (as eigenvalues of Hessian).
  Saddle-free Newton method is a rare example of addressing this issue -
changes saddle attraction into repulsion, and was shown to provide essential
improvement for final value this way. However, it neglects noise while
modelling second order behavior, focuses on Krylov subspace for numerical
reasons, and requires costly eigendecomposion.
  Maintaining SFN advantages, there are proposed inexpensive ways for
exploiting these opportunities. Second order behavior is linear dependence of
first derivative - we can optimally estimate it from sequence of noisy
gradients with least square linear regression, in online setting here: with
weakening weights of old gradients. Statistically relevant subspace is
suggested by PCA of recent noisy gradients - in online setting it can be made
by slowly rotating considered directions toward new gradients, gradually
replacing old directions with recent statistically relevant. Eigendecomposition
can be also performed online: with regularly performed step of QR method to
maintain diagonal Hessian. Outside the second order modeled subspace we can
simultaneously perform gradient descent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Shaping for Human Learning via Inverse Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.10904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.10904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark A. Rucker, Layne T. Watson, Matthew S. Gerber, Laura E. Barnes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are spectacular reinforcement learners, constantly learning from and
adjusting to experience and feedback. Unfortunately, this doesn't necessarily
mean humans are fast learners. When tasks are challenging, learning can become
unacceptably slow. Fortunately, humans do not have to learn tabula rasa, and
learning speed can be greatly increased with learning aids. In this work we
validate a new type of learning aid -- reward shaping for humans via inverse
reinforcement learning (IRL). The goal of this aid is to increase the speed
with which humans can learn good policies for specific tasks. Furthermore this
approach compliments alternative machine learning techniques such as safety
features that try to prevent individuals from making poor decisions. To achieve
our results we first extend a well known IRL algorithm via kernel methods.
Afterwards we conduct two human subjects experiments using an online game where
players have limited time to learn a good policy. We show with statistical
significance that players who receive our learning aid are able to approach
desired policies more quickly than the control group.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been modified considerably for resubmission to Journal
  of Machine Learning Research, for source code, see
  https://github.com/mrucker/kpirl-kla</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatiotemporal Residual Regularization with Dynamic Mixtures for Traffic
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjin Choi, Nicolas Saunier, Martin Trepanier, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep learning-based traffic forecasting models are mainly trained
with MSE (or MAE) as the loss function, assuming that residuals/errors follow
independent and isotropic Gaussian (or Laplacian) distribution for simplicity.
However, this assumption rarely holds for real-world traffic forecasting tasks,
where the unexplained residuals are often correlated in both space and time. In
this study, we propose Spatiotemporal Residual Regularization by modeling
residuals with a dynamic (e.g., time-varying) mixture of zero-mean multivariate
Gaussian distribution with learnable spatiotemporal covariance matrices. This
approach allows us to directly capture spatiotemporally correlated residuals.
For scalability, we model the spatiotemporal covariance for each mixture
component using a Kronecker product structure, which significantly reduces the
number of parameters and computation complexity. We evaluate the performance of
the proposed method on a traffic speed forecasting task. Our results show that,
by properly modeling residual distribution, the proposed method not only
improves the model performance but also provides interpretable structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EpiGRAF: Rethinking training of 3D GANs <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, Peter Wonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A very recent trend in generative modeling is building 3D-aware generators
from 2D image collections. To induce the 3D bias, such models typically rely on
volumetric rendering, which is expensive to employ at high resolutions. During
the past months, there appeared more than 10 works that address this scaling
issue by training a separate 2D decoder to upsample a low-resolution image (or
a feature tensor) produced from a pure 3D generator. But this solution comes at
a cost: not only does it break multi-view consistency (i.e. shape and texture
change when the camera moves), but it also learns the geometry in a low
fidelity. In this work, we show that it is possible to obtain a high-resolution
3D generator with SotA image quality by following a completely different route
of simply training the model patch-wise. We revisit and improve this
optimization scheme in two ways. First, we design a location- and scale-aware
discriminator to work on patches of different proportions and spatial
positions. Second, we modify the patch sampling strategy based on an annealed
beta distribution to stabilize training and accelerate the convergence. The
resulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D
generator, and we test it on four datasets (two introduced in this work) at
$256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality,
high-fidelity geometry and trains ${\approx} 2.5 \times$ faster than the
upsampler-based counterparts. Project website:
https://universome.github.io/epigraf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An experimental study on Synthetic Tabular Data Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Marin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the findings of various methodologies for measuring
the similarity of synthetic data generated from tabular data samples. We
particularly apply our research to the case where the synthetic data has many
more samples than the real data. This task has a special complexity: validating
the reliability of this synthetically generated data with a much higher number
of samples than the original. We evaluated the most commonly used global
metrics found in the literature. We introduced a novel approach based on the
data's topological signature analysis. Topological data analysis has several
advantages in addressing this latter challenge. The study of qualitative
geometric information focuses on geometric properties while neglecting
quantitative distance function values. This is especially useful with
high-dimensional synthetic data where the sample size has been significantly
increased. It is comparable to introducing new data points into the data space
within the limits set by the original data. Then, in large synthetic data
spaces, points will be much more concentrated than in the original space, and
their analysis will become much more sensitive to both the metrics used and
noise. Instead, the concept of "closeness" between points is used for
qualitative geometric information. Finally, we suggest an approach based on
data Eigen vectors for evaluating the level of noise in synthetic data. This
approach can also be used to assess the similarity of original and synthetic
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Zeroth order Descent with Structured Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Rando, Cesare Molinari, Silvia Villa, Lorenzo Rosasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce and analyze Structured Stochastic Zeroth order Descent (S-SZD),
a finite difference approach which approximates a stochastic gradient on a set
of $l\leq d$ orthogonal directions, where $d$ is the dimension of the ambient
space. These directions are randomly chosen, and may change at each step. For
smooth convex functions we prove almost sure convergence of the iterates and a
convergence rate on the function values of the form $O(d/l k^{-c})$ for every
$c<1/2$, which is arbitrarily close to the one of Stochastic Gradient Descent
(SGD) in terms of number of iterations. Our bound also shows the benefits of
using $l$ multiple directions instead of one. For non-convex functions
satisfying the Polyak-{\L}ojasiewicz condition, we establish the first
convergence rates for stochastic zeroth order algorithms under such an
assumption. We corroborate our theoretical findings in numerical simulations
where assumptions are satisfied and on the real-world problem of
hyper-parameter optimization, observing that S-SZD has very good practical
performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous diffusion for categorical data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15089v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15089v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, Jonas Adler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have quickly become the go-to paradigm for generative
modelling of perceptual signals (such as images and sound) through iterative
refinement. Their success hinges on the fact that the underlying physical
phenomena are continuous. For inherently discrete and categorical data such as
language, various diffusion-inspired alternatives have been proposed. However,
the continuous nature of diffusion models conveys many benefits, and in this
work we endeavour to preserve it. We propose CDCD, a framework for modelling
categorical data with diffusion models that are continuous both in time and
input space. We demonstrate its efficacy on several language modelling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures; corrections and additional information about
  hyperparameters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monitoring MBE substrate deoxidation via RHEED image-sequence analysis
  by deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdourahman Khaireh-Walieh, Alexandre Arnoult, Sébastien Plissard, Peter R. Wiecha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflection high-energy electron diffraction (RHEED) is a powerful tool in
molecular beam epitaxy (MBE), but RHEED images are often difficult to
interpret, requiring experienced operators. We present an approach for
automated surveillance of GaAs substrate deoxidation in MBE reactors using deep
learning based RHEED image-sequence classification. Our approach consists of an
non-supervised auto-encoder (AE) for feature extraction, combined with a
supervised convolutional classifier network. We demonstrate that our
lightweight network model can accurately identify the exact deoxidation moment.
Furthermore we show that the approach is very robust and allows accurate
deoxidation detection during months without requiring re-training. The main
advantage of the approach is that it can be applied to raw RHEED images without
requiring further information such as the rotation angle, temperature, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amortized Inference for Causal Structure Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12934v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12934v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal structure poses a combinatorial search problem that
typically involves evaluating structures with a score or independence test. The
resulting search is costly, and designing suitable scores or tests that capture
prior knowledge is difficult. In this work, we propose to amortize causal
structure learning. Rather than searching over structures, we train a
variational inference model to directly predict the causal structure from
observational or interventional data. This allows our inference model to
acquire domain-specific inductive biases for causal discovery solely from data
generated by a simulator, bypassing both the hand-engineering of suitable score
functions and the search over graphs. The architecture of our inference model
emulates permutation invariances that are crucial for statistical efficiency in
structure learning, which facilitates generalization to significantly larger
problem instances than seen during training. On synthetic data and
semisynthetic gene expression data, our models exhibit robust generalization
capabilities when subject to substantial distribution shifts and significantly
outperform existing algorithms, especially in the challenging genomics domain.
Our code and models are publicly available at:
https://github.com/larslorch/avici.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022, fixed formatting of Figure 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.15013v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.15013v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhong Wang, Yuan Zhang, Yunjie Gu, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Value factorisation is a useful technique for multi-agent reinforcement
learning (MARL) in global reward game, however its underlying mechanism is not
yet fully understood. This paper studies a theoretical framework for value
factorisation with interpretability via Shapley value theory. We generalise
Shapley value to Markov convex game called Markov Shapley value (MSV) and apply
it as a value factorisation method in global reward game, which is obtained by
the equivalence between the two games. Based on the properties of MSV, we
derive Shapley-Bellman optimality equation (SBOE) to evaluate the optimal MSV,
which corresponds to an optimal joint deterministic policy. Furthermore, we
propose Shapley-Bellman operator (SBO) that is proved to solve SBOE. With a
stochastic approximation and some transformations, a new MARL algorithm called
Shapley Q-learning (SHAQ) is established, the implementation of which is guided
by the theoretical results of SBO and MSV. We also discuss the relationship
between SHAQ and relevant value factorisation methods. In the experiments, SHAQ
exhibits not only superior performances on all tasks but also the
interpretability that agrees with the theoretical analysis. The implementation
of this paper is on https://github.com/hsvgbkhgbv/shapley-q-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper for NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Neural Textures Make Sim2Real Consistent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Burgert, Jinghuan Shang, Xiang Li, Michael Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unpaired image translation algorithms can be used for sim2real tasks, but
many fail to generate temporally consistent results. We present a new approach
that combines differentiable rendering with image translation to achieve
temporal consistency over indefinite timescales, using surface consistency
losses and \emph{neural neural textures}. We call this algorithm TRITON
(Texture Recovering Image Translation Network): an unsupervised, end-to-end,
stateless sim2real algorithm that leverages the underlying 3D geometry of input
scenes by generating realistic-looking learnable neural textures. By settling
on a particular texture for the objects in a scene, we ensure consistency
between frames statelessly. Unlike previous algorithms, TRITON is not limited
to camera movements -- it can handle the movement of objects as well, making it
useful for downstream tasks such as robotic manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures (without references or appendix); 16 pages, 16
  figures (with appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decorrelation with conditional normalizing flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Klein, Tobias Golling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sensitivity of many physics analyses can be enhanced by constructing
discriminants that preferentially select signal events. Such discriminants
become much more useful if they are uncorrelated with a set of protected
attributes. In this paper we show that a normalizing flow conditioned on the
protected attributes can be used to find a decorrelated representation for any
discriminant. As a normalizing flow is invertible the separation power of the
resulting discriminant will be unchanged at any fixed value of the protected
attributes. We demonstrate the efficacy of our approach by building supervised
jet taggers that produce almost no sculpting in the mass distribution of the
background.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Factorized Fourier Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alasdair Tran, Alexander Mathews, Lexing Xie, Cheng Soon Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the Factorized Fourier Neural Operator (F-FNO), a learning-based
approach for simulating partial differential equations (PDEs). Starting from a
recently proposed Fourier representation of flow fields, the F-FNO bridges the
performance gap between pure machine learning approaches to that of the best
numerical or hybrid solvers. This is achieved with new representations -
separable spectral layers and improved residual connections - and a combination
of training strategies such as the Markov assumption, Gaussian noise, and
cosine learning rate decay. On several challenging benchmark PDEs on regular
grids, structured meshes, and point clouds, the F-FNO can scale to deeper
networks and outperform both the FNO and the geo-FNO, reducing the error by 83%
on the Navier-Stokes problem, 31% on the elasticity problem, 57% on the airfoil
flow problem, and 60% on the plastic forging problem. Compared to the
state-of-the-art pseudo-spectral method, the F-FNO can take a step size that is
an order of magnitude larger in time and achieve an order of magnitude speedup
to produce the same solution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. Code is available at
  https://github.com/alasdairtran/fourierflow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ fMRI from EEG is only Deep Learning away: the use of interpretable DL to
  unravel EEG-fMRI relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kovalev, Ilia Mikheev, Alexei Ossadtchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The access to activity of subcortical structures offers unique opportunity
for building intention dependent brain-computer interfaces, renders abundant
options for exploring a broad range of cognitive phenomena in the realm of
affective neuroscience including complex decision making processes and the
eternal free-will dilemma and facilitates diagnostics of a range of
neurological deceases. So far this was possible only using bulky, expensive and
immobile fMRI equipment. Here we present an interpretable domain grounded
solution to recover the activity of several subcortical regions from the
multichannel EEG data and demonstrate up to 60% correlation between the actual
subcortical blood oxygenation level dependent sBOLD signal and its EEG-derived
twin. Then, using the novel and theoretically justified weight interpretation
methodology we recover individual spatial and time-frequency patterns of scalp
EEG predictive of the hemodynamic signal in the subcortical nuclei. The
described results not only pave the road towards wearable subcortical activity
scanners but also showcase an automatic knowledge discovery process facilitated
by deep learning technology in combination with an interpretable domain
constrained architecture and the appropriate downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. Add acknowledgment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparison of LSTM and GRU networks for learning symbolic sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.02248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.02248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Cahuantzi, Xinye Chen, Stefan Güttel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore relations between the hyper-parameters of a recurrent neural
network (RNN) and the complexity of string sequences it is able to memorize. We
compare long short-term memory (LSTM) networks and gated recurrent units
(GRUs). We find that an increase of RNN depth does not necessarily result in
better memorization capability when the training time is constrained. Our
results also indicate that the learning rate and the number of units per layer
are among the most important hyper-parameters to be tuned. Generally, GRUs
outperform LSTM networks on low complexity sequences while on high complexity
sequences LSTMs perform better.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAL<span class="highlight-title">BERT</span>: Teaching AL<span class="highlight-title">BERT</span> to Ponder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Balagansky, Daniil Gavrilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, pre-trained models can be considered the default choice for a wide
range of NLP tasks. Despite their SoTA results, there is practical evidence
that these models may require a different number of computing layers for
different input sequences, since evaluating all layers leads to overconfidence
in wrong predictions (namely overthinking). This problem can potentially be
solved by implementing adaptive computation time approaches, which were first
designed to improve inference speed. Recently proposed PonderNet may be a
promising solution for performing an early exit by treating the exit layer's
index as a latent variable. However, the originally proposed exit criterion,
relying on sampling from trained posterior distribution on the probability of
exiting from the $i$-th layer, introduces major variance in exit layer indices,
significantly reducing the resulting model's performance. In this paper, we
propose improving PonderNet with a novel deterministic Q-exit criterion and a
revisited model architecture. We adapted the proposed mechanism to ALBERT and
RoBERTa and compared it with recent methods for performing an early exit. We
observed that the proposed changes can be considered significant improvements
on the original PonderNet architecture and outperform PABEE on a wide range of
GLUE tasks. In addition, we also performed an in-depth ablation study of the
proposed architecture to further understand Lambda layers and their
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothness and continuity of cost functionals for ECG mismatch
  computation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.04487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.04487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Grandits, Simone Pezzuto, Gernot Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of cardiac electrophysiology tries to abstract, describe and
finally model the electrical characteristics of a heartbeat. With recent
advances in cardiac electrophysiology, models have become more powerful and
descriptive as ever. However, to advance to the field of inverse
electrophysiological modeling, i.e. creating models from electrical
measurements such as the ECG, the less investigated field of smoothness of the
simulated ECGs w.r.t. model parameters need to be further explored. The present
paper discusses smoothness in terms of the whole pipeline which describes how
from physiological parameters, we arrive at the simulated ECG. Employing such a
pipeline, we create a test-bench of a simplified idealized left ventricle model
and demonstrate the most important factors for efficient inverse modeling
through smooth cost functionals. Such knowledge will be important for designing
and creating inverse models in future optimization and machine learning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Influence of uncertainty estimation techniques on false-positive
  reduction in liver lesion detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishaan Bhat, Josien P. W. Pluim, Max A. Viergever, Hugo J. Kuijf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques show success in detecting objects in medical images,
but still suffer from false-positive predictions that may hinder accurate
diagnosis. The estimated uncertainty of the neural network output has been used
to flag incorrect predictions. We study the role played by features computed
from neural network uncertainty estimates and shape-based features computed
from binary predictions in reducing false positives in liver lesion detection
by developing a classification-based post-processing step for different
uncertainty estimation methods. We demonstrate an improvement in the lesion
detection performance of the neural network (with respect to F1-score) for all
uncertainty estimation methods on two datasets, comprising abdominal MR and CT
images, respectively. We show that features computed from neural network
uncertainty estimates tend not to contribute much toward reducing false
positives. Our results show that factors like class imbalance (true over false
positive ratio) and shape-based features extracted from uncertainty maps play
an important role in distinguishing false positive from true positive
predictions. Our code can be found at https://github.com/ishaanb92/FPCPipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the Journal of Machine Learning for
  Biomedical Imaging (MELBA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Clustering with k-Means: a Hybrid Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Poggiali, Alessandro Berti, Anna Bernasconi, Gianna M. Del Corso, Riccardo Guidotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing is a promising paradigm based on quantum theory for
performing fast computations. Quantum algorithms are expected to surpass their
classical counterparts in terms of computational complexity for certain tasks,
including machine learning. In this paper, we design, implement, and evaluate
three hybrid quantum k-Means algorithms, exploiting different degree of
parallelism. Indeed, each algorithm incrementally leverages quantum parallelism
to reduce the complexity of the cluster assignment step up to a constant cost.
In particular, we exploit quantum phenomena to speed up the computation of
distances. The core idea is that the computation of distances between records
and centroids can be executed simultaneously, thus saving time, especially for
big datasets. We show that our hybrid quantum k-Means algorithms can be more
efficient than the classical version, still obtaining comparable clustering
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral
  Mapping for Single-channel Speech Enhancement <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Lin Chen, Daniel D. E. Wong, Ke Tan, Buye Xu, Anurag Kumar, Vamsi Krishna Ithapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most speech enhancement (SE) models learn a point estimate, and do not make
use of uncertainty estimation in the learning process. In this paper, we show
that modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian
negative log-likelihood (NLL) improves SE performance at no extra cost. During
training, our approach augments a model learning complex spectral mapping with
a temporary submodel to predict the covariance of the enhancement error at each
time-frequency bin. Due to unrestricted heteroscedastic uncertainty, the
covariance introduces an undersampling effect, detrimental to SE performance.
To mitigate undersampling, our approach inflates the uncertainty lower bound
and weights each loss component with their uncertainty, effectively
compensating severely undersampled components with more penalties. Our
multivariate setting reveals common covariance assumptions such as scalar and
diagonal matrices. By weakening these assumptions, we show that the NLL
achieves superior performance compared to popular losses including the mean
squared error (MSE), mean absolute error (MAE), and scale-invariant
signal-to-distortion ratio (SI-SDR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transposed Variational Auto-encoder with Intrinsic Feature Learning for
  Traffic Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00641v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00641v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyan Deng, Chenwang Wu, Defu Lian, Min Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present our solutions to the Traffic4cast 2022
core challenge and extended challenge. In this competition, the participants
are required to predict the traffic states for the future 15-minute based on
the vehicle counter data in the previous hour. Compared to other competitions
in the same series, this year focuses on the prediction of different data
sources and sparse vertex-to-edge generalization. To address these issues, we
introduce the Transposed Variational Auto-encoder (TVAE) model to reconstruct
the missing data and Graph Attention Networks (GAT) to strengthen the
correlations between learned representations. We further apply feature
selection to learn traffic patterns from diverse but easily available data.
  Our solutions have ranked first in both challenges on the final leaderboard.
The source code is available at \url{https://github.com/Daftstone/Traffic4cast}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Certified Monotonic Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.10219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.10219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchao Liu, Xing Han, Na Zhang, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning monotonic models with respect to a subset of the inputs is a
desirable feature to effectively address the fairness, interpretability, and
generalization issues in practice. Existing methods for learning monotonic
neural networks either require specifically designed model structures to ensure
monotonicity, which can be too restrictive/complicated, or enforce monotonicity
by adjusting the learning process, which cannot provably guarantee the learned
model is monotonic on selected features. In this work, we propose to certify
the monotonicity of the general piece-wise linear neural networks by solving a
mixed integer linear programming problem.This provides a new general approach
for learning monotonic neural networks with arbitrary model structures. Our
method allows us to train neural networks with heuristic monotonicity
regularizations, and we can gradually increase the regularization magnitude
until the learned network is certified monotonic. Compared to prior works, our
approach does not require human-designed constraints on the weight space and
also yields more accurate approximation. Empirical studies on various datasets
demonstrate the efficiency of our approach over the state-of-the-art methods,
such as Deep Lattice Networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GWRBoost:A geographically weighted gradient boosting method for
  explainable quantification of spatially-varying relationships 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Zhou Huang, Ganmin Yin, Yi Bao, Xiao Zhou, Yong Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The geographically weighted regression (GWR) is an essential tool for
estimating the spatial variation of relationships between dependent and
independent variables in geographical contexts. However, GWR suffers from the
problem that classical linear regressions, which compose the GWR model, are
more prone to be underfitting, especially for significant volume and complex
nonlinear data, causing inferior comparative performance. Nevertheless, some
advanced models, such as the decision tree and the support vector machine, can
learn features from complex data more effectively while they cannot provide
explainable quantification for the spatial variation of localized
relationships. To address the above issues, we propose a geographically
gradient boosting weighted regression model, GWRBoost, that applies the
localized additive model and gradient boosting optimization method to alleviate
underfitting problems and retains explainable quantification capability for
spatially-varying relationships between geographically located variables.
Furthermore, we formulate the computation method of the Akaike information
score for the proposed model to conduct the comparative analysis with the
classic GWR algorithm. Simulation experiments and the empirical case study are
applied to prove the efficient performance and practical value of GWRBoost. The
results show that our proposed model can reduce the RMSE by 18.3% in parameter
estimation accuracy and AICc by 67.3% in the goodness of fit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAS-NeXt: Few-Shot Cross-Lingual Font Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang He, Xin Jin, Angela Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating new fonts is a time-consuming and labor-intensive task, especially
in a language with a huge amount of characters like Chinese. Various deep
learning models have demonstrated the ability to efficiently generate new fonts
with a few reference characters of that style, but few models support
cross-lingual font generation. This paper presents GAS-NeXt, a novel few-shot
cross-lingual font generator based on AGIS-Net and Font Translator GAN, and
improve the performance metrics such as Fr\'echet Inception Distance (FID),
Structural Similarity Index Measure(SSIM), and Pixel-level Accuracy (pix-acc).
Our approaches include replacing the original encoder and decoder with the idea
of layer attention and context-aware attention from Font Translator GAN, while
utilizing the shape, texture, and local discriminators of AGIS-Net. In our
experiment on English-to-Chinese font translation, we observed better results
in fonts with distinct local features than conventional Chinese fonts compared
to results obtained from Font Translator GAN. We also validate our method on
multiple languages and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A new trigonometric kernel function for support vector machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08585v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08585v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Fathi Hafshejani, Zahra Moberfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last few years, various types of machine learning algorithms, such as
Support Vector Machine (SVM), Support Vector Regression (SVR), and Non-negative
Matrix Factorization (NMF) have been introduced. The kernel approach is an
effective method for increasing the classification accuracy of machine learning
algorithms. This paper introduces a family of one-parameter kernel functions
for improving the accuracy of SVM classification. The proposed kernel function
consists of a trigonometric term and differs from all existing kernel
functions. We show this function is a positive definite kernel function.
Finally, we evaluate the SVM method based on the new trigonometric kernel, the
Gaussian kernel, the polynomial kernel, and a convex combination of the new
kernel function and the Gaussian kernel function on various types of datasets.
Empirical results show that the SVM based on the new trigonometric kernel
function and the mixed kernel function achieve the best classification
accuracy. Moreover, some numerical results of performing the SVR based on the
new trigonometric kernel function and the mixed kernel function are presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposing a Recurrent Neural Network into Modules for Enabling
  Reusability and Replacement <span class="chip">ICSE'2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayem Mohammad Imtiaz, Fraol Batole, Astha Singh, Rangeet Pan, Breno Dantas Cruz, Hridesh Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we take a recurrent neural network (RNN) trained to translate between
languages and augment it to support a new natural language without retraining
the model from scratch? Can we fix the faulty behavior of the RNN by replacing
portions associated with the faulty behavior? Recent works on decomposing a
fully connected neural network (FCNN) and convolutional neural network (CNN)
into modules have shown the value of engineering deep models in this manner,
which is standard in traditional SE but foreign for deep learning models.
However, prior works focus on the image-based multiclass classification
problems and cannot be applied to RNN due to (a) different layer structures,
(b) loop structures, (c) different types of input-output architectures, and (d)
usage of both nonlinear and logistic activation functions. In this work, we
propose the first approach to decompose an RNN into modules. We study different
types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN
modules can be reused and replaced in various scenarios. We evaluate our
approach against 5 canonical datasets (i.e., Math QA, Brown Corpus,
Wiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.
We found that decomposing a trained model has a small cost (Accuracy: -0.6%,
BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced
without needing to retrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 45th international conference on software engineering
  (ICSE'2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning with Evolving Class Ontologies <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04993v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04993v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Lin, Deepak Pathak, Yu-Xiong Wang, Deva Ramanan, Shu Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong learners must recognize concept vocabularies that evolve over time.
A common yet underexplored scenario is learning with class labels that
continually refine/expand old classes. For example, humans learn to recognize
${\tt dog}$ before dog breeds. In practical settings, dataset
$\textit{versioning}$ often introduces refinement to ontologies, such as
autonomous vehicle benchmarks that refine a previous ${\tt vehicle}$ class into
${\tt school-bus}$ as autonomous operations expand to new cities. This paper
formalizes a protocol for studying the problem of $\textit{Learning with
Evolving Class Ontology}$ (LECO). LECO requires learning classifiers in
distinct time periods (TPs); each TP introduces a new ontology of "fine" labels
that refines old ontologies of "coarse" labels (e.g., dog breeds that refine
the previous ${\tt dog}$). LECO explores such questions as whether to annotate
new data or relabel the old, how to leverage coarse labels, and whether to
finetune the previous TP's model or train from scratch. To answer these
questions, we leverage insights from related problems such as class-incremental
learning. We validate them under the LECO protocol through the lens of image
classification (CIFAR and iNaturalist) and semantic segmentation (Mapillary).
Our experiments lead to surprising conclusions; while the current status quo is
to relabel existing datasets with new ontologies (such as COCO-to-LVIS or
Mapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to
annotate $\textit{new}$ data with the new ontology. However, this produces an
aggregate dataset with inconsistent old-vs-new labels, complicating learning.
To address this challenge, we adopt methods from semi-supervised and
partial-label learning. Such strategies can surprisingly be made near-optimal,
approaching an "oracle" that learns on the aggregate dataset exhaustively
labeled with the newest ontology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022; Website: https://linzhiqiu.github.io/papers/leco/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Py-Feat: Python Facial Expression Analysis Toolbox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshin Jolly, Jin Hyun Cheong, Tiankang Xie, Sophie Byrne, Matthew Kenny, Luke J. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying facial expressions is a notoriously difficult endeavor. Recent
advances in the field of affective computing have yielded impressive progress
in automatically detecting facial expressions from pictures and videos.
However, much of this work has yet to be widely disseminated in social science
domains such as psychology. Current state of the art models require
considerable domain expertise that is not traditionally incorporated into
social science training programs. Furthermore, there is a notable absence of
user-friendly and open-source software that provides a comprehensive set of
tools and functions that support facial expression research. In this paper, we
introduce Py-Feat, an open-source Python toolbox that provides support for
detecting, preprocessing, analyzing, and visualizing facial expression data.
Py-Feat makes it easy for domain experts to disseminate and benchmark computer
vision models and also for end users to quickly process, analyze, and visualize
face expression data. We hope this platform will facilitate increased use of
facial expression data in human behavior research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decomposable Sparse Tensor on Tensor Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyi Mao, Jason Xiaotian Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most regularized tensor regression research focuses on tensors predictors
with scalars responses or vectors predictors to tensors responses. We consider
the sparse low rank tensor on tensor regression where predictors $\mathcal{X}$
and responses $\mathcal{Y}$ are both high-dimensional tensors. By demonstrating
that the general inner product or the contracted product on a unit rank tensor
can be decomposed into standard inner products and outer products, the problem
can be simply transformed into a tensor to scalar regression followed by a
tensor decomposition. So we propose a fast solution based on stagewise search
composed by contraction part and generation part which are optimized
alternatively. We successfully demonstrate our method can out perform current
methods in terms of accuracy and predictors selection by effectively
incorporating the structural information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating multivariate time series with COmmon Source CoordInated GAN
  (COSCI-GAN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Seyfi, Jean-Francois Rajotte, Raymond T. Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating multivariate time series is a promising approach for sharing
sensitive data in many medical, financial, and IoT applications. A common type
of multivariate time series originates from a single source such as the
biometric measurements from a medical patient. This leads to complex dynamical
patterns between individual time series that are hard to learn by typical
generation models such as GANs. There is valuable information in those patterns
that machine learning models can use to better classify, predict or perform
other downstream tasks. We propose a novel framework that takes time series'
common origin into account and favors channel/feature relationships
preservation. The two key points of our method are: 1) the individual time
series are generated from a common point in latent space and 2) a central
discriminator favors the preservation of inter-channel/feature dynamics. We
demonstrate empirically that our method helps preserve channel/feature
correlations and that our synthetic data performs very well in downstream tasks
with medical and financial data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Play to Policy: Conditional Behavior Generation from Uncurated
  Robot Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale sequence modeling from offline data has led to impressive
performance gains in natural language and image generation, directly
translating such ideas to robotics has been challenging. One critical reason
for this is that uncurated robot demonstration data, i.e. play data, collected
from non-expert human demonstrators are often noisy, diverse, and
distributionally multi-modal. This makes extracting useful, task-centric
behaviors from such data a difficult generative modeling problem. In this work,
we present Conditional Behavior Transformers (C-BeT), a method that combines
the multi-modal generation ability of Behavior Transformer with
future-conditioned goal specification. On a suite of simulated benchmark tasks,
we find that C-BeT improves upon prior state-of-the-art work in learning from
play data by an average of 45.7%. Further, we demonstrate for the first time
that useful task-centric behaviors can be learned on a real-world robot purely
from play data without any task labels or reward information. Robot videos are
best viewed on our project website: https://play-to-policy.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data available at: https://play-to-policy.github.io; (fixed
  metadata author name format)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying the Preferential Direction of the Model Gradient in
  Adversarial Training With Projected Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.04709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.04709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training, especially projected gradient descent (PGD), has proven
to be a successful approach for improving robustness against adversarial
attacks. After adversarial training, gradients of models with respect to their
inputs have a preferential direction. However, the direction of alignment is
not mathematically well established, making it difficult to evaluate
quantitatively. We propose a novel definition of this direction as the
direction of the vector pointing toward the closest point of the support of the
closest inaccurate class in decision space. To evaluate the alignment with this
direction after adversarial training, we apply a metric that uses generative
adversarial networks to produce the smallest residual needed to change the
class present in the image. We show that PGD-trained models have a higher
alignment than the baseline according to our definition, that our metric
presents higher alignment values than a competing metric formulation, and that
enforcing this alignment increases the robustness of models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot
  Document-Level Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tavish McDonald, Brian Tsan, Amar Saini, Juanita Ordonez, Luis Gutierrez, Phan Nguyen, Blake Mason, Brenda Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers produce thousands of scholarly documents containing valuable
technical knowledge. The community faces the laborious task of reading these
documents to identify, extract, and synthesize information. To automate
information gathering, document-level question answering (QA) offers a flexible
framework where human-posed questions can be adapted to extract diverse
knowledge. Finetuning QA systems requires access to labeled data (tuples of
context, question and answer). However, data curation for document QA is
uniquely challenging because the context (i.e. answer evidence passage) needs
to be retrieved from potentially long, ill-formatted documents. Existing QA
datasets sidestep this challenge by providing short, well-defined contexts that
are unrealistic in real-world applications. We present a three-stage document
QA approach: (1) text extraction from PDF; (2) evidence retrieval from
extracted texts to form well-posed contexts; (3) QA to extract knowledge from
contexts to return high-quality answers -- extractive, abstractive, or Boolean.
Using QASPER for evaluation, our detect-retrieve-comprehend (DRC) system
achieves a +7.19 improvement in Answer-F1 over existing baselines while
delivering superior context selection. Our results demonstrate that DRC holds
tremendous promise as a flexible framework for practical scientific document
QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Efficient Model-free RL in Leader-Follower MDP with Linear
  Function Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnob Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a multi-agent episodic MDP setup where an agent (leader) takes
action at each step of the episode followed by another agent (follower). The
state evolution and rewards depend on the joint action pair of the leader and
the follower. Such type of interactions can find applications in many domains
such as smart grids, mechanism design, security, and policymaking. We are
interested in how to learn policies for both the players with provable
performance guarantee under a bandit feedback setting. We focus on a setup
where both the leader and followers are {\em non-myopic}, i.e., they both seek
to maximize their rewards over the entire episode and consider a linear MDP
which can model continuous state-space which is very common in many RL
applications. We propose a {\em model-free} RL algorithm and show that
$\tilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret bounds can be achieved for both
the leader and the follower, where $d$ is the dimension of the feature mapping,
$H$ is the length of the episode, and $T$ is the total number of steps under
the bandit feedback information setup. Thus, our result holds even when the
number of states becomes infinite. The algorithm relies on {\em novel}
adaptation of the LSVI-UCB algorithm. Specifically, we replace the standard
greedy policy (as the best response) with the soft-max policy for both the
leader and the follower. This turns out to be key in establishing uniform
concentration bound for the value functions. To the best of our knowledge, this
is the first sub-linear regret bound guarantee for the Markov games with
non-myopic followers with function approximation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Numerical Optimizations for Weighted Low-rank Estimation on Language
  Model <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singular value decomposition (SVD) is one of the most popular compression
methods that approximate a target matrix with smaller matrices. However,
standard SVD treats the parameters within the matrix with equal importance,
which is a simple but unrealistic assumption. The parameters of a trained
neural network model may affect task performance unevenly, which suggests
non-equal importance among the parameters. Compared to SVD, the decomposition
method aware of parameter importance is the more practical choice in real
cases. Unlike standard SVD, weighted value decomposition is a non-convex
optimization problem that lacks a closed-form solution. We systematically
investigated multiple optimization strategies to tackle the problem and
examined our method by compressing Transformer-based language models. Further,
we designed a metric to predict when the SVD may introduce a significant
performance drop, for which our method can be a rescue strategy. The extensive
evaluations demonstrate that our method can perform better than current SOTA
methods in compressing Transformer-based language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>long paper EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimensional criterion for forecasting nonlinear systems by reservoir
  computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.05159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.05159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pauliina Kärkkäinen, Riku Linna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reservoir computers (RC) have proven useful as surrogate models in
forecasting and replicating systems of chaotic dynamics. The quality of
surrogate models based on RCs is crucially dependent on their optimal
implementation that involves selecting optimal reservoir topology and
hyperparameters. By systematically applying Bayesian hyperparameter
optimization and using ensembles of reservoirs of various topology we show that
connectednes of reservoirs is of significance only in forecasting and
replication of chaotic system of sufficient complexity. By applying RCs of
different topology in forecasting and replicating the Lorenz system, a coupled
Wilson-Cowan system, and the Kuramoto-Sivashinsky system, we show that simple
reservoirs of unconnected nodes (RUN) outperform reservoirs of connected nodes
for target systems whose estimated fractal dimension dimension is $d \lesssim
5.5$ and that linked reservoirs are better for systems with $d > 5.5$. This
finding is highly important for evaluation of reservoir computing methods and
on selecting a method for prediction of signals measured on nonlinear systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Transferability Subject to Bounded Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00129v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00129v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatong Chen, Reilly Raab, Jialu Wang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an algorithmic predictor that is "fair" on some source distribution,
will it still be fair on an unknown target distribution that differs from the
source within some bound? In this paper, we study the transferability of
statistical group fairness for machine learning predictors (i.e., classifiers
or regressors) subject to bounded distribution shifts. Such shifts may be
introduced by initial training data uncertainties, user adaptation to a
deployed predictor, dynamic environments, or the use of pre-trained models in
new settings. Herein, we develop a bound that characterizes such
transferability, flagging potentially inappropriate deployments of machine
learning for socially consequential tasks. We first develop a framework for
bounding violations of statistical fairness subject to distribution shift,
formulating a generic upper bound for transferred fairness violations as our
primary result. We then develop bounds for specific worked examples, focusing
on two commonly used fairness definitions (i.e., demographic parity and
equalized odds) and two classes of distribution shift (i.e., covariate shift
and label shift). Finally, we compare our theoretical bounds to deterministic
models of distribution shift and against real-world data, finding that we are
able to estimate fairness violation bounds in practice, even when simplifying
assumptions are only approximately satisfied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flatten the Curve: Efficiently Training Low-Curvature Neural Networks <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, Francois Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The highly non-linear nature of deep neural networks causes them to be
susceptible to adversarial examples and have unstable gradients which hinders
interpretability. However, existing methods to solve these issues, such as
adversarial training, are expensive and often sacrifice predictive accuracy.
  In this work, we consider curvature, which is a mathematical quantity which
encodes the degree of non-linearity. Using this, we demonstrate low-curvature
neural networks (LCNNs) that obtain drastically lower curvature than standard
models while exhibiting similar predictive performance, which leads to improved
robustness and stable gradients, with only a marginally increased training
time. To achieve this, we minimize a data-independent upper bound on the
curvature of a neural network, which decomposes overall curvature in terms of
curvatures and slopes of its constituent layers. To efficiently minimize this
bound, we introduce two novel architectural components: first, a non-linearity
called centered-softplus that is a stable variant of the softplus
non-linearity, and second, a Lipschitz-constrained batch normalization layer.
  Our experiments show that LCNNs have lower curvature, more stable gradients
and increased off-the-shelf adversarial robustness when compared to their
standard high-curvature counterparts, all without affecting predictive
performance. Our approach is easy to use and can be readily incorporated into
existing neural network models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cutting Plane Selection with Analytic Centers and Multiregression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Turner, Timo Berthold, Mathieu Besançon, Thorsten Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cutting planes are a crucial component of state-of-the-art mixed-integer
programming solvers, with the choice of which subset of cuts to add being vital
for solver performance. We propose new distance-based measures to qualify the
value of a cut by quantifying the extent to which it separates relevant parts
of the relaxed feasible set. For this purpose, we use the analytic centers of
the relaxation polytope or of its optimal face, as well as alternative optimal
solutions of the linear programming relaxation. We assess the impact of the
choice of distance measure on root node performance and throughout the whole
branch-and-bound tree, comparing our measures against those prevalent in the
literature. Finally, by a multi-output regression, we predict the relative
performance of each measure, using static features readily available before the
separation process. Our results indicate that analytic center-based methods
help to significantly reduce the number of branch-and-bound nodes needed to
explore the search space and that our multiregression approach can further
improve on any individual method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Diffusion Modeling of Long Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11495v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11495v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for video modeling based on denoising diffusion
probabilistic models that produces long-duration video completions in a variety
of realistic environments. We introduce a generative model that can at
test-time sample any arbitrary subset of video frames conditioned on any other
subset and present an architecture adapted for this purpose. Doing so allows us
to efficiently compare and optimize a variety of schedules for the order in
which frames in a long video are sampled and use selective sparse and
long-range conditioning on previously sampled frames. We demonstrate improved
video modeling over prior work on a number of datasets and sample temporally
coherent videos over 25 minutes in length. We additionally release a new video
modeling dataset and semantically meaningful metrics based on videos generated
in the CARLA autonomous driving simulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation Principal Component Analysis: base linear method for
  learning with out-of-distribution data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny M Mirkes, Jonathan Bac, Aziz Fouché, Sergey V. Stasenko, Andrei Zinovyev, Alexander N. Gorban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation is a popular paradigm in modern machine learning which aims
at tackling the problem of divergence (or shift) between the labeled training
and validation datasets (source domain) and a potentially large unlabeled
dataset (target domain). The task is to embed both datasets red into a common
space in which the source dataset is informative for training while the
divergence between source and target is minimized. The most popular domain
adaptation solutions are based on training neural networks that combine
classification and adversarial learning modules, frequently making them both
data-hungry and difficult to train. We present a method called Domain
Adaptation Principal Component Analysis (DAPCA) that identifies a linear
reduced data representation useful for solving the domain adaptation task.
DAPCA algorithm introduces positive and negative weights between pairs of data
points, and generalizes the supervised extension of principal component
analysis. DAPCA is an iterative algorithm that solves a simple quadratic
optimization problem at each iteration. The convergence of the algorithm is
guaranteed, and the number of iterations is small in practice. We validate the
suggested algorithm on previously proposed benchmarks for solving the domain
adaptation task. We also show the benefit of using DAPCA in analyzing the
single-cell omics datasets in biomedical applications. Overall, DAPCA can serve
as a practical preprocessing step in many machine learning applications leading
to reduced dataset representations, taking into account possible divergence
between source and target domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Softmax Policy Gradient Methods Can Take Exponential Time to Converge <span class="chip">COLT</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.11270v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.11270v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The softmax policy gradient (PG) method, which performs gradient ascent under
softmax policy parameterization, is arguably one of the de facto
implementations of policy optimization in modern reinforcement learning. For
$\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs),
remarkable progress has recently been achieved towards establishing global
convergence of softmax PG methods in finding a near-optimal policy. However,
prior results fall short of delineating clear dependencies of convergence rates
on salient parameters such as the cardinality of the state space $\mathcal{S}$
and the effective horizon $\frac{1}{1-\gamma}$, both of which could be
excessively large. In this paper, we deliver a pessimistic message regarding
the iteration complexity of softmax PG methods, despite assuming access to
exact gradient computation. Specifically, we demonstrate that the softmax PG
method with stepsize $\eta$ can take \[
  \frac{1}{\eta} |\mathcal{S}|^{2^{\Omega\big(\frac{1}{1-\gamma}\big)}}
~\text{iterations} \] to converge, even in the presence of a benign policy
initialization and an initial state distribution amenable to exploration (so
that the distribution mismatch coefficient is not exceedingly large). This is
accomplished by characterizing the algorithmic dynamics over a
carefully-constructed MDP containing only three actions. Our exponential lower
bound hints at the necessity of carefully adjusting update rules or enforcing
proper regularization in accelerating PG methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Mathematical Programming (Series A); also presented in
  part in Conference on Learning Theory (COLT) 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> Inference for <span class="highlight-title">Self-Supervised</span> Models <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, Nicolas Papernot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised models are increasingly prevalent in machine learning (ML)
since they reduce the need for expensively labeled data. Because of their
versatility in downstream applications, they are increasingly used as a service
exposed via public APIs. At the same time, these encoder models are
particularly vulnerable to model stealing attacks due to the high
dimensionality of vector representations they output. Yet, encoders remain
undefended: existing mitigation strategies for stealing attacks focus on
supervised learning. We introduce a new dataset inference defense, which uses
the private training set of the victim encoder model to attribute its ownership
in the event of stealing. The intuition is that the log-likelihood of an
encoder's output representations is higher on the victim's training data than
on test data if it is stolen from the victim, but not if it is independently
trained. We compute this log-likelihood using density estimation models. As
part of our evaluation, we also propose measuring the fidelity of stolen
encoders and quantifying the effectiveness of the theft detection without
involving downstream tasks; instead, we leverage mutual information and
distance measurements. Our extensive empirical results in the vision domain
demonstrate that dataset inference is a promising direction for defending
self-supervised models against model stealing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAViL: Masked Audio-Video Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Jitendra Malik, Christoph Feichtenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks. Code will be available
soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You were saying? -- Spoken Language in the V3C <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Rossetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an analysis of the distribution of spoken language in the
V3C video retrieval benchmark dataset based on automatically generated
transcripts. It finds that a large portion of the dataset is covered by spoken
language. Since language transcripts can be quickly and accurately described,
this has implications for retrieval tasks such as known-item search.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-14T00:00:00Z">2022-12-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End
  Language Modeling <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Godey, Roman Castagné, Éric de la Clergerie, Benoît Sagot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Static subword tokenization algorithms have been an essential component of
recent works on language modeling. However, their static nature results in
important flaws that degrade the models' downstream performance and robustness.
In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.
MANTa is a differentiable tokenizer trained end-to-end with the language model.
The resulting system offers a trade-off between the expressiveness of
byte-level models and the speed of models trained using subword tokenization.
In addition, our tokenizer is highly explainable since it produces an explicit
segmentation of sequences into blocks. We evaluate our pre-trained model on
several English datasets from different domains as well as on synthetic noise.
We find that MANTa improves robustness to character perturbations and
out-of-domain data. We then show that MANTa performs comparably to other models
on the general-domain GLUE benchmark. Finally, we show that it is considerably
faster than strictly byte-level models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022 Findings
  (https://aclanthology.org/2022.findings-emnlp.207/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Hang Zhang, Chen Lin, Yeyun Gong, Jian Guo, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form numerical reasoning in financial analysis aims to generate a
reasoning program to calculate the correct answer for a given question.
Previous work followed a retriever-generator framework, where the retriever
selects key facts from a long-form document, and the generator generates a
reasoning program based on retrieved facts. However, they treated all facts
equally without considering the different contributions of facts with and
without numbers. Meanwhile, the program consistency were ignored under
supervised training, resulting in lower training accuracy and diversity. To
solve these problems, we proposed APOLLO to improve the long-form numerical
reasoning framework. For the retriever, we adopt a number-aware negative
sampling strategy to enable the retriever to be more discriminative on key
numerical facts. For the generator, we design consistency-based reinforcement
learning and target program augmentation strategy based on the consistency of
program execution results. Experimental results on the FinQA and ConvFinQA
leaderboard verify the effectiveness of our proposed method, achieving the new
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Byte and Wordpiece Level Models for Massively Multilingual
  Semantic Parsing <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimo Nicosia, Francesco Piccinno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Token free approaches have been successfully applied to a series of word and
span level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece
based (mT5) sequence to sequence model on the 51 languages of the MASSIVE
multilingual semantic parsing dataset. We examine multiple experimental
settings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic
data. By leveraging a state-of-the-art label projection method for machine
translated examples, we are able to reduce the gap in exact match accuracy to
only 5 points with respect to a model trained on gold data from all the
languages. We additionally provide insights on the cross-lingual transfer of
ByT5 and show how the model compares with respect to mT5 across all parameter
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Massively Multilingual NLU 2022 Workshop Paper @ EMNLP 2022 - Winning
  approach of the MMNLU-22 Zero-Shot Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Translationese in Cross-Lingual Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaan Wang, Fandong Meng, Tingyi Zhang, Yunlong Liang, Jiarong Xu, Zhixu Li, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a document in a source language, cross-lingual summarization (CLS) aims
at generating a concise summary in a different target language. Unlike
monolingual summarization (MS), naturally occurring source-language documents
paired with target-language summaries are rare. To collect large-scale CLS
samples, existing datasets typically involve translation in their creation.
However, the translated text is distinguished from the text originally written
in that language, i.e., translationese. Though many efforts have been devoted
to CLS, none of them notice the phenomenon of translationese. In this paper, we
first confirm that the different approaches to constructing CLS datasets will
lead to different degrees of translationese. Then we design systematic
experiments to investigate how translationese affects CLS model evaluation and
performance when it appears in source documents or target summaries. In detail,
we find that (1) the translationese in documents or summaries of test sets
might lead to the discrepancy between human judgment and automatic evaluation;
(2) the translationese in training sets would harm model performance in the
real scene; (3) though machine-translated documents involve translationese,
they are very useful for building CLS systems on low-resource languages under
specific training strategies. Furthermore, we give suggestions for future CLS
research including dataset and model developments. We hope that our work could
let researchers notice the phenomenon of translationese in CLS and take it into
account in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VTCC-NLP at NL4Opt competition subtask 1: An Ensemble <span class="highlight-title">Pre-train</span>ed
  language models for Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Dung Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a combined three pre-trained language models (XLM-R, BART, and
DeBERTa-V3) as an empower of contextualized embedding for named entity
recognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th
on the leaderboard at NL4Opt competition subtask 1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Negative Style Transfer in Hybrid Dialogue System <span class="chip">AAAI-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimin Li, Qinyuan Cheng, Linyang Li, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the functionality of dialogue systems evolves, hybrid dialogue systems
that accomplish user-specific goals and participate in open-topic chitchat with
users are attracting growing attention. Existing research learns both tasks
concurrently utilizing a multi-task fusion technique but ignores the negative
transfer phenomenon induced by the unique textual style differences. Therefore,
contrastive learning based on the latent variable model is used to decouple the
various textual genres in the latent space. We devise supervised and
self-supervised positive and negative sample constructions for diverse
datasets. In addition, to capitalize on the style information contained in the
decoupled latent variables, we employ a style prefix that incorporates latent
variables further to control the generation of responses with varying styles.
We performed extensive experiments on three dialogue datasets, including a
hybrid dialogue dataset and two task-oriented dialogue datasets. The
experimental results demonstrate that our method can mitigate the negative
style transfer issue and achieves state-of-the-art performance on multiple
dialogue datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quotations, Coreference Resolution, and Sentiment Annotations in
  Croatian News Articles: An Exploratory Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jelena Sarajlić, Gaurish Thakkar, Diego Alves, Nives Mikelic Preradović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a corpus annotated for the task of direct-speech
extraction in Croatian. The paper focuses on the annotation of the quotation,
co-reference resolution, and sentiment annotation in SETimes news corpus in
Croatian and on the analysis of its language-specific differences compared to
English. From this, a list of the phenomena that require special attention when
performing these annotations is derived. The generated corpus with quotation
features annotations can be used for multiple tasks in the field of Natural
Language Processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech and Natural Language Processing Technologies for Pseudo-Pilot
  Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrutha Prasad, Juan Zuluaga-Gomez, Petr Motlicek, Saeed Sarfjoo, Iuliia Nigmatulina, Karel Vesely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a simple yet efficient repetition-based modular system
for speeding up air-traffic controllers (ATCos) training. E.g., a human pilot
is still required in EUROCONTROL's ESCAPE lite simulator (see
https://www.eurocontrol.int/simulator/escape) during ATCo training. However,
this need can be substituted by an automatic system that could act as a pilot.
In this paper, we aim to develop and integrate a pseudo-pilot agent into the
ATCo training pipeline by merging diverse artificial intelligence (AI) powered
modules. The system understands the voice communications issued by the ATCo,
and, in turn, it generates a spoken prompt that follows the pilot's phraseology
to the initial communication. Our system mainly relies on open-source AI tools
and air traffic control (ATC) databases, thus, proving its simplicity and ease
of replicability. The overall pipeline is composed of the following: (1) a
submodule that receives and pre-processes the input stream of raw audio, (2) an
automatic speech recognition (ASR) system that transforms audio into a sequence
of words; (3) a high-level ATC-related entity parser, which extracts relevant
information from the communication, i.e., callsigns and commands, and finally,
(4) a speech synthesizer submodule that generates responses based on the
high-level ATC entities previously extracted. Overall, we show that this system
could pave the way toward developing a real proof-of-concept pseudo-pilot
system. Hence, speeding up the training of ATCos while drastically reducing its
overall cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Sesar Innovation Days 2022.
  https://www.sesarju.eu/sesarinnovationdays</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building and Evaluating Universal Named-Entity Recognition English
  corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Alves, Gaurish Thakkar, Marko Tadić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the application of the Universal Named Entity framework
to generate automatically annotated corpora. By using a workflow that extracts
Wikipedia data and meta-data and DBpedia information, we generated an English
dataset which is described and evaluated. Furthermore, we conducted a set of
experiments to improve the annotations in terms of precision, recall, and
F1-measure. The final dataset is available and the established workflow can be
applied to any language with existing Wikipedia and DBpedia. As part of future
research, we intend to continue improving the annotation process and extend it
to other languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Learning for Cross-Lingual Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurish Thakkar, Nives Mikelic Preradovic, Marko Tadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a cross-lingual sentiment analysis of news articles using
zero-shot and few-shot learning. The study aims to classify the Croatian news
articles with positive, negative, and neutral sentiments using the Slovene
dataset. The system is based on a trilingual BERT-based model trained in three
languages: English, Slovene, Croatian. The paper analyses different setups
using datasets in two languages and proposes a simple multi-task model to
perform sentiment classification. The evaluation is performed using the
few-shot and zero-shot scenarios in single-task and multi-task experiments for
Croatian and Slovene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIST: a Large-Scale Annotated Resource and Neural Models for Functions
  of Modal Verbs in English Scientific Text <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Henning, Nicole Macher, Stefan Grünewald, Annemarie Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modal verbs (e.g., "can", "should", or "must") occur highly frequently in
scientific articles. Decoding their function is not straightforward: they are
often used for hedging, but they may also denote abilities and restrictions.
Understanding their meaning is important for various NLP tasks such as writing
assistance or accurate information extraction from scientific text.
  To foster research on the usage of modals in this genre, we introduce the
MIST (Modals In Scientific Text) dataset, which contains 3737 modal instances
in five scientific domains annotated for their semantic, pragmatic, or
rhetorical function. We systematically evaluate a set of competitive neural
architectures on MIST. Transfer experiments reveal that leveraging
non-scientific data is of limited benefit for modeling the distinctions in
MIST. Our corpus analysis provides evidence that scientific communities differ
in their usage of modal verbs, yet, classifiers trained on scientific data
generalize to some extent to unseen scientific domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures. Accepted to EMNLP Findings 2022; typesetting of
  this version slightly differs from conference version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainability of Text Processing and Retrieval Methods: A Critical
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Saha, Debapriyo Majumdar, Mandar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning and Machine Learning based models have become extremely popular
in text processing and information retrieval. However, the non-linear
structures present inside the networks make these models largely inscrutable. A
significant body of research has focused on increasing the transparency of
these models. This article provides a broad overview of research on the
explainability and interpretability of natural language processing and
information retrieval methods. More specifically, we survey approaches that
have been applied to explain word embeddings, sequence modeling, attention
modules, transformers, BERT, and document ranking. The concluding section
suggests some possible directions for future research on this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service
  Chatlog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zheng, Tianyu Liu, Haoran Meng, Xu Wang, Yufan Jiang, Mengliang Rao, Binghuai Lin, Zhifang Sui, Yunbo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harvesting question-answer (QA) pairs from customer service chatlog in the
wild is an efficient way to enrich the knowledge base for customer service
chatbots in the cold start or continuous integration scenarios. Prior work
attempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which
fails to integrate the incomplete utterances from the dialog context for
composite QA retrieval. In this paper, we propose N-to-N QA extraction task in
which the derived questions and corresponding answers might be separated across
different utterances. We introduce a suite of generative/discriminative tagging
based methods with end-to-end and two-stage variants that perform well on 5
customer service datasets and for the first time setup a benchmark for N-to-N
DialogQAE with utterance and session level evaluation metrics. With a deep dive
into extracted QA pairs, we find that the relations between and inside the QA
pairs can be indicators to analyze the dialogue structure, e.g. information
seeking, clarification, barge-in and elaboration. We also show that the
proposed models can adapt to different domains and languages, and reduce the
labor cost of knowledge accumulation in the real-world product dialogue
platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version; The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Similarity-Based Curriculum Learning for Image Captioning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkuan Zhang, Saku Sugawara, Akiko Aizawa, Lei Zhou, Ryohei Sasano, Koichi Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning models require the high-level generalization ability to
describe the contents of various images in words. Most existing approaches
treat the image-caption pairs equally in their training without considering the
differences in their learning difficulties. Several image captioning approaches
introduce curriculum learning methods that present training data with
increasing levels of difficulty. However, their difficulty measurements are
either based on domain-specific features or prior model training. In this
paper, we propose a simple yet efficient difficulty measurement for image
captioning using cross-modal similarity calculated by a pretrained
vision-language model. Experiments on the COCO and Flickr30k datasets show that
our proposed approach achieves superior performance and competitive convergence
speed to baselines without requiring heuristics or incurring additional
training costs. Moreover, the higher model performance on difficult examples
and unseen data also demonstrates the generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation <span class="chip">EMNLP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, John Harvill, Sunjae Yoon, Mark Hasegawa-Johnson, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word Sense Disambiguation (WSD) is an NLP task aimed at determining the
correct sense of a word in a sentence from discrete sense choices. Although
current systems have attained unprecedented performances for such tasks, the
nonuniform distribution of word senses during training generally results in
systems performing poorly on rare senses. To this end, we consider data
augmentation to increase the frequency of these least frequent senses (LFS) to
reduce the distributional bias of senses during training. We propose
Sense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that
maintains the sense of a target word. SMSMix smoothly blends two sentences
using mask prediction while preserving the relevant span determined by saliency
scores to maintain a specific word's sense. To the best of our knowledge, this
is the first attempt to apply mixup in NLP while preserving the meaning of a
specific word. With extensive experiments, we validate that our augmentation
method can effectively give more information about rare senses during training
with maintained target sense label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhrubajyoti Pathak, Sukumar Nandi, Priyankoo Sarmah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AICCSA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Prosody Representations with Unsupervised Speech
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyuan Qu, Taihao Li, Cornelius Weber, Theresa Pekarek-Rosin, Fuji Ren, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human speech can be characterized by different components, including semantic
content, speaker identity and prosodic information. Significant progress has
been made in disentangling representations for semantic content and speaker
identity in Automatic Speech Recognition (ASR) and speaker verification tasks
respectively. However, it is still an open challenging research question to
extract prosodic information because of the intrinsic association of different
attributes, such as timbre and rhythm, and because of the need for unsupervised
training schemes to achieve robust large-scale and speaker-independent ASR. The
aim of this paper is to address the disentanglement of emotional prosody from
speech based on unsupervised reconstruction. Specifically, we identify, design,
implement and integrate three crucial components in our proposed speech
reconstruction model Prosody2Vec: (1) a unit encoder that transforms speech
signals into discrete units for semantic content, (2) a pretrained speaker
verification model to generate speaker identity embeddings, and (3) a trainable
prosody encoder to learn prosody representations. We first pretrain the
Prosody2Vec representations on unlabelled emotional speech corpora, then
fine-tune the model on specific datasets to perform Speech Emotion Recognition
(SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective
evaluations on the EVC task suggest that Prosody2Vec effectively captures
general prosodic features that can be smoothly transferred to other emotional
speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the
prosody features learned by Prosody2Vec are complementary and beneficial for
the performance of widely used speech pretraining models and surpass the
state-of-the-art methods when combining Prosody2Vec with HuBERT
representations. Some audio samples can be found on our demo website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Find Someone Who: Visual Commonsense Understanding in Human-Centric
  Grounding <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan You, Rui Sun, Zhecan Wang, Kai-Wei Chang, Shih-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From a visual scene containing multiple people, human is able to distinguish
each individual given the context descriptions about what happened before,
their mental/physical states or intentions, etc. Above ability heavily relies
on human-centric commonsense knowledge and reasoning. For example, if asked to
identify the "person who needs healing" in an image, we need to first know that
they usually have injuries or suffering expressions, then find the
corresponding visual clues before finally grounding the person. We present a
new commonsense task, Human-centric Commonsense Grounding, that tests the
models' ability to ground individuals given the context descriptions about what
happened before, and their mental/physical states or intentions. We further
create a benchmark, HumanCog, a dataset with 130k grounded commonsensical
descriptions annotated on 67k images, covering diverse types of commonsense and
visual scenes. We set up a context-object-aware method as a strong baseline
that outperforms previous pre-trained and non-pretrained models. Further
analysis demonstrates that rich visual commonsense and powerful integration of
multi-modal commonsense are essential, which sheds light on future works. Data
and code will be available https://github.com/Hxyou/HumanCog.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures. EMNLP 2022-findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-train</span>ed Language Models can be Fully Zero-Shot Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we extend a pre-trained model to many language understanding tasks,
without labeled or additional unlabeled data? Pre-trained language models
(PLMs) have been effective for a wide range of NLP tasks. However, existing
approaches either require fine-tuning on downstream labeled datasets or
manually constructing proper prompts. In this paper, we propose nonparametric
prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike
previous methods, NPPrompt uses only pre-trained language models and does not
require any labeled data or additional raw corpus for further fine-tuning, nor
does it rely on humans to construct a comprehensive set of prompt label words.
We evaluate NPPrompt against previous major few-shot and zero-shot learning
methods on diverse NLP tasks: including text classification, text entailment,
similar text retrieval, and paraphrasing. Experimental results demonstrate that
our NPPrompt outperforms the previous best fully zero-shot method by big
margins, with absolute gains of 12.8% in accuracy on text classification and
18.9% on the GLUE benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReDDIT: Regret Detection and Domain Identification from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fazlourrahman Balouchzahi, Sabur Butt, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a study of regret and its expression on social
media platforms. Specifically, we present a novel dataset of Reddit texts that
have been classified into three classes: Regret by Action, Regret by Inaction,
and No Regret. We then use this dataset to investigate the language used to
express regret on Reddit and to identify the domains of text that are most
commonly associated with regret. Our findings show that Reddit users are most
likely to express regret for past actions, particularly in the domain of
relationships. We also found that deep learning models using GloVe embedding
outperformed other models in all experiments, indicating the effectiveness of
GloVe for representing the meaning and context of words in the domain of
regret. Overall, our study provides valuable insights into the nature and
prevalence of regret on social media, as well as the potential of deep learning
and word embeddings for analyzing and understanding emotional language in
online text. These findings have implications for the development of natural
language processing algorithms and the design of social media platforms that
support emotional expression and communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Detection of Contextualized Embedding Bias with Application
  to Ideology <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a fully unsupervised method to detect bias in contextualized
embeddings. The method leverages the assortative information latently encoded
by social networks and combines orthogonality regularization, structured
sparsity learning, and graph neural networks to find the embedding subspace
capturing this information. As a concrete example, we focus on the phenomenon
of ideological bias: we introduce the concept of an ideological subspace, show
how it can be found by applying our method to online discussion forums, and
present techniques to probe it. Our experiments suggest that the ideological
subspace encodes abstract evaluative semantics and reflects changes in the
political left-right spectrum during the presidency of Donald Trump.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Build-a-Bot: Teaching Conversational AI Using a <span class="highlight-title">Transformer</span>-Based Intent
  Recognition and Question Answering Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Pearce, Sharifa Alghowinem, Cynthia Breazeal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at EAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Natural Language Processing to Augment Structured Social
  Determinants of Health Data in the Electronic Health Record 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Lybarger, Nicholas J Dobbins, Ritche Long, Angad Singh, Patrick Wedgeworth, Ozlem Ozuner, Meliha Yetisgen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Social Determinants of Health (SDOH) influence personal health
outcomes and health systems interactions. Health systems capture SDOH
information through structured data and unstructured clinical notes; however,
clinical notes often contain a more comprehensive representation of several key
SDOH. The objective of this work is to assess the SDOH information gain
achievable by extracting structured semantic representations of SDOH from the
clinical narrative and combining these extracted representations with available
structured data.
  Materials and Methods: We developed a natural language processing (NLP)
information extraction model for SDOH that utilizes a deep learning entity and
relation extraction architecture. In an electronic health record (EHR) case
study, we applied the SDOH extractor to a large existing clinical data set with
over 200,000 patients and 400,000 notes and compared the extracted information
with available structured data.
  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the
EHR case study, we found 19\% of current tobacco users, 10\% of drug users, and
32\% of homeless patients only include documentation of these risk factors in
the clinical narrative.
  Conclusions: Patients who are at-risk for negative health outcomes due to
SDOH may be better served if health systems are able to identify SDOH risk
factors and associated social needs. Structured semantic representations of
text-encoded SDOH information can augment existing structured, and this more
comprehensive SDOH representation can assist health systems in identifying and
addressing social needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causes and Cures for Interference in Multilingual Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, Shruti Bhosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation models can benefit from synergy between
different language pairs, but also suffer from interference. While there is a
growing number of sophisticated methods that aim to eliminate interference, our
understanding of interference as a phenomenon is still limited. This work
identifies the main factors that contribute to interference in multilingual
machine translation. Through systematic experimentation, we find that
interference (or synergy) are primarily determined by model size, data size,
and the proportion of each language pair within the total dataset. We observe
that substantial interference occurs mainly when the model is very small with
respect to the available training data, and that using standard transformer
configurations with less than one billion parameters largely alleviates
interference and promotes synergy. Moreover, we show that tuning the sampling
temperature to control the proportion of each language pair in the data is key
to balancing the amount of interference between low and high resource language
pairs effectively, and can lead to superior performance overall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relationship Between Online Harmful Behaviors and Social Network Message
  Writing Style 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talia Sanchez Viera, Richard Khoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the relationship between an individual's writing
style and the risk that they will engage in online harmful behaviors (such as
cyberbullying). In particular, we consider whether measurable differences in
writing style relate to different personality types, as modeled by the Big-Five
personality traits and the Dark Triad traits, and can differentiate between
users who do or do not engage in harmful behaviors. We study messages from
nearly 2,500 users from two online communities (Twitter and Reddit) and find
that we can measure significant personality differences between regular and
harmful users from the writing style of as few as 100 tweets or 40 Reddit
posts, aggregate these values to distinguish between healthy and harmful
communities, and also use style attributes to predict which users will engage
in harmful behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Self-supervised</span> Learning with Contextualized Target
  Representations for Vision, Speech and Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexei Baevski, Arun Babu, Wei-Ning Hsu, Michael Auli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current self-supervised learning algorithms are often modality-specific and
require large amounts of computational resources. To address these issues, we
increase the training efficiency of data2vec, a learning objective that
generalizes across several modalities. We do not encode masked tokens, use a
fast convolutional decoder and amortize the effort to build teacher
representations. data2vec 2.0 benefits from the rich contextualized target
representations introduced in data2vec which enable a fast self-supervised
learner. Experiments on ImageNet-1K image classification show that data2vec 2.0
matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,
on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x
less time, and on GLUE natural language understanding it matches a retrained
RoBERTa model in half the time. Trading some speed for accuracy results in
ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence for Health Message Generation: Theory, Method,
  and an Empirical Study Using <span class="highlight-title">Prompt</span> Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sue Lim, Ralf Schmälzle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces and examines the potential of an AI system to generate
health awareness messages. The topic of folic acid, a vitamin that is critical
during pregnancy, served as a test case. Using prompt engineering, we generated
messages that could be used to raise awareness and compared them to retweeted
human-generated messages via computational and human evaluation methods. The
system was easy to use and prolific, and computational analyses revealed that
the AI-generated messages were on par with human-generated ones in terms of
sentiment, reading ease, and semantic content. Also, the human evaluation study
showed that AI-generated messages ranked higher in message quality and clarity.
We discuss the theoretical, practical, and ethical implications of these
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages including references, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The text-to-image model Stable Diffusion has recently become very popular.
Only weeks after its open source release, millions are experimenting with image
generation. This is due to its ease of use, since all it takes is a brief
description of the desired image to "prompt" the generative model. Rarely do
the images generated for a new prompt immediately meet the user's expectations.
Usually, an iterative refinement of the prompt ("prompt engineering") is
necessary for satisfying images. As a new perspective, we recast image prompt
engineering as interactive image retrieval - on an "infinite index". Thereby, a
prompt corresponds to a query and prompt engineering to query refinement.
Selected image-prompt pairs allow direct relevance feedback, as the model can
modify an image for the refined prompt. This is a form of one-sided interactive
retrieval, where the initiative is on the user side, whereas the server side
remains stateless. In light of an extensive literature review, we develop these
parallels in detail and apply the findings to a case study of a creative search
task on such a model. We note that the uncertainty in searching an infinite
index is virtually never-ending. We also discuss future research opportunities
related to retrieval models specialized for generative models and interactive
generative image retrieval. The application of IR technology, such as query
reformulation and relevance feedback, will contribute to improved workflows
when using generative models, while the notion of an infinite index raises new
challenges in IR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Multilingual Corpora for a Complex Named Entity Recognition and
  Classification Hierarchy using Wikipedia and DBpedia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Alves, Gaurish Thakkar, Gabriel Amaral, Tin Kuculo, Marko Tadić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-growing popularity of the field of NLP, the demand for datasets
in low resourced-languages follows suit. Following a previously established
framework, in this paper, we present the UNER dataset, a multilingual and
hierarchical parallel corpus annotated for named-entities. We describe in
detail the developed procedure necessary to create this type of dataset in any
language available on Wikipedia with DBpedia information. The three-step
procedure extracts entities from Wikipedia articles, links them to DBpedia, and
maps the DBpedia sets of classes to the UNER labels. This is followed by a
post-processing procedure that significantly increases the number of identified
entities in the final results. The paper concludes with a statistical and
qualitative analysis of the resulting dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2212.07162</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Linguistically Informed Multi-Objective <span class="highlight-title">Pre-Train</span>ing for Natural
  Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Pielka, Svetlana Schmidt, Lisa Pucknat, Rafet Sifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a linguistically enhanced combination of pre-training methods
for transformers. The pre-training objectives include POS-tagging, synset
prediction based on semantic knowledge graphs, and parent prediction based on
dependency parse trees. Our approach achieves competitive results on the
Natural Language Inference task, compared to the state of the art. Specifically
for smaller models, the method results in a significant performance boost,
emphasizing the fact that intelligent pre-training can make up for fewer
parameters and help building more efficient models. Combining POS-tagging and
synset prediction yields the overall best results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FREDA: Flexible Relation Extraction Data Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Strobl, Amine Trabelsi, Osmar Zaiane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To effectively train accurate Relation Extraction models, sufficient and
properly labeled data is required. Adequately labeled data is difficult to
obtain and annotating such data is a tricky undertaking. Previous works have
shown that either accuracy has to be sacrificed or the task is extremely
time-consuming, if done accurately. We are proposing an approach in order to
produce high-quality datasets for the task of Relation Extraction quickly.
Neural models, trained to do Relation Extraction on the created datasets,
achieve very good results and generalize well to other datasets. In our study,
we were able to annotate 10,022 sentences for 19 relations in a reasonable
amount of time, and trained a commonly used baseline model for each relation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM SAC 2023 Knowledge and Natural Language Processing
  track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-Scale Chemical Language Representations Capture Molecular
  Structure and Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09553v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09553v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models based on machine learning can enable accurate and fast molecular
property predictions, which is of interest in drug discovery and material
design. Various supervised machine learning models have demonstrated promising
performance, but the vast chemical space and the limited availability of
property labels make supervised learning challenging. Recently, unsupervised
transformer-based language models pretrained on a large unlabelled corpus have
produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, we present molecular embeddings
obtained by training an efficient transformer encoder model, MoLFormer, which
uses rotary positional embeddings. This model employs a linear attention
mechanism, coupled with highly distributed training, on SMILES sequences of 1.1
billion unlabelled molecules from the PubChem and ZINC datasets. We show that
the learned molecular representation outperforms existing baselines, including
supervised and self-supervised graph neural networks and language models, on
several downstream tasks from ten benchmark datasets. They perform
competitively on two others. Further analyses, specifically through the lens of
attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns
the spatial relationships between atoms within a molecule. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient chemical and structural information to predict various
distinct molecular properties, including quantum-chemical properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NMI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Text-based Personality Computing: Challenges and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri, Laura Boeschoten, Erik-Jan van Kesteren, Mahdi Shafiee Kamalabad, Daniel L Oberski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions towards more valid and
reliable TPC research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added acknowledgements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grammar Based Speaker Role Identification for Air Traffic Control Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrutha Prasad, Juan Zuluaga-Gomez, Petr Motlicek, Saeed Sarfjoo, Iuliia Nigmatulina, Oliver Ohneiser, Hartmut Helmke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) for air traffic control is generally
trained by pooling Air Traffic Controller (ATCO) and pilot data into one set.
This is motivated by the fact that pilot's voice communications are more scarce
than ATCOs. Due to this data imbalance and other reasons (e.g., varying
acoustic conditions), the speech from ATCOs is usually recognized more
accurately than from pilots. Automatically identifying the speaker roles is a
challenging task, especially in the case of the noisy voice recordings
collected using Very High Frequency (VHF) receivers or due to the
unavailability of the push-to-talk (PTT) signal, i.e., both audio channels are
mixed. In this work, we propose to (1) automatically segment the ATCO and pilot
data based on an intuitive approach exploiting ASR transcripts and (2)
subsequently consider an automatic recognition of ATCOs' and pilots' voice as
two separate tasks. Our work is performed on VHF audio data with high noise
levels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is
recognized to be helpful for various speech-based machine-learning tasks.
Specifically, for the speaker role identification task, the module is
represented by a simple yet efficient knowledge-based system exploiting a
grammar defined by the International Civil Aviation Organization (ICAO). The
system accepts text as the input, either manually verified annotations or
automatically generated transcripts. The developed approach provides an average
accuracy in speaker role identification of about 83%. Finally, we show that
training an acoustic model for ASR tasks separately (i.e., separate models for
ATCOs and pilots) or using a multitask approach is well suited for the noisy
data and outperforms the traditional ASR system where all data is pooled
together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Sesar Innovation Days - 2022. See
  https://www.sesarju.eu/sesarinnovationdays</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCP: <span class="highlight-title">Self-supervised</span> <span class="highlight-title">Pre-train</span>ing for Personalized Chatbots with
  Multi-level Contrastive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08753v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08753v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Zhengyi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Composition, Attention, or Both? <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Yoshida, Yohei Oseki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel architecture called Composition Attention
Grammars (CAGs) that recursively compose subtrees into a single vector
representation with a composition function, and selectively attend to previous
structural information with a self-attention mechanism. We investigate whether
these components -- the composition function and the self-attention mechanism
-- can both induce human-like syntactic generalization. Specifically, we train
language models (LMs) with and without these two components with the model
sizes carefully controlled, and evaluate their syntactic generalization
performance against six test circuits on the SyntaxGym benchmark. The results
demonstrated that the composition function and the self-attention mechanism
both play an important role to make LMs more human-like, and closer inspection
of linguistic phenomenon implied that the composition function allowed
syntactic features, but not semantic features, to percolate into subtree
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technical Report -- Competition Solution for <span class="highlight-title">Prompt</span> Tuning using
  <span class="highlight-title">Pretrain</span>ed Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang-Long Song, Wu-He Zou, Feng Li, Xiao-Lei Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning recently becomes a hot-spot in the applications of large
pretrained language models on specific downstream tasks. Regarding the Language
Model as a Service (LMaaS), black-box tuning using derivative-free optimization
(DFO) provides a novel approach to expand the practical scenarios of pretrained
models and enrich the researches of few-shot learning. In this report, we
present our solution in this competition that is based on the LMaaS scenario.
Our solution consists of several modifications to BBTv2, including multiple
label words, selection of P0, rolling update strategy, multi-task loss from MLP
classifier, and finally using the ensemble method to further improve
generalization ability. We also shared some strategies that we tried but didn't
use in the final submission for further discussion. In the end we raised a
question about the SNLI dataset and the impact on the results, as well as our
concerns about the competition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Emotion-Cause Pair Extraction via Learning to Link 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.10710v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.10710v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Song, Chen Zhang, Qiuchi Li, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion-cause pair extraction (ECPE), as an emergent natural language
processing task, aims at jointly investigating emotions and their underlying
causes in documents. It extends the previous emotion cause extraction (ECE)
task, yet without requiring a set of pre-given emotion clauses as in ECE.
Existing approaches to ECPE generally adopt a two-stage method, i.e., (1)
emotion and cause detection, and then (2) pairing the detected emotions and
causes. Such pipeline method, while intuitive, suffers from two critical
issues, including error propagation across stages that may hinder the
effectiveness, and high computational cost that would limit the practical
application of the method. To tackle these issues, we propose a multi-task
learning model that can extract emotions, causes and emotion-cause pairs
simultaneously in an end-to-end manner. Specifically, our model regards pair
extraction as a link prediction task, and learns to link from emotion clauses
to cause clauses, i.e., the links are directional. Emotion extraction and cause
extraction are incorporated into the model as auxiliary tasks, which further
boost the pair extraction. Experiments are conducted on an ECPE benchmarking
dataset. The results show that our proposed model outperforms a range of
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 5 tables, code is available at
  https://github.com/shl5133/E2EECPE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Multi-Granularity Summarization <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text summarization is a user-preference based task, i.e., for one document,
users often have different priorities for summary. As a key aspect of
customization in summarization, granularity is used to measure the semantic
coverage between the summary and source document. However, developing systems
that can generate summaries with customizable semantic coverage is still an
under-explored topic. In this paper, we propose the first unsupervised
multi-granularity summarization framework, GranuSum. We take events as the
basic semantic units of the source documents and propose to rank these events
by their salience. We also develop a model to summarize input documents with
given events as anchors and hints. By inputting different numbers of events,
GranuSum is capable of producing multi-granular summaries in an unsupervised
manner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple
summaries at different granularities for each document cluster. Experimental
results confirm the substantial superiority of GranuSum on multi-granularity
summarization over strong baselines. Further, by exploiting the event
information, GranuSum also exhibits state-of-the-art performance under the
conventional unsupervised abstractive setting. Dataset for this paper can be
found at: https://github.com/maszhongming/GranuDUC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Dmitriy Dligach, Timothy Miller, John Caskey, Brihat Sharma, Matthew M Churpek, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The meaningful use of electronic health records (EHR) continues to progress
in the digital era with clinical decision support systems augmented by
artificial intelligence. A priority in improving provider experience is to
overcome information overload and reduce the cognitive burden so fewer medical
errors and cognitive biases are introduced during patient care. One major type
of medical error is diagnostic error due to systematic or predictable errors in
judgment that rely on heuristics. The potential for clinical natural language
processing (cNLP) to model diagnostic reasoning in humans with forward
reasoning from data to diagnosis and potentially reduce the cognitive burden
and medical error has not been investigated. Existing tasks to advance the
science in cNLP have largely focused on information extraction and named entity
recognition through classification tasks. We introduce a novel suite of tasks
coined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for
developing and evaluating cNLP models with clinical diagnostic reasoning
ability. The suite includes six tasks from ten publicly available datasets
addressing clinical text understanding, medical knowledge reasoning, and
diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to
be a natural language generation framework to evaluate pre-trained language
models. Experiments with state-of-the-art pre-trained generative language
models using large general domain models and models that were continually
trained on a medical corpus demonstrate opportunities for improvement when
evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab
repository with a systematic approach to load and evaluate models for the cNLP
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-Centric Question Answering via Contrastive Learning and Invertible
  Event Transformation <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junru Lu, Xingwei Tan, Gabriele Pergola, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human reading comprehension often requires reasoning of event semantic
relations in narratives, represented by Event-centric Question-Answering (QA).
To address event-centric QA, we propose a novel QA model with contrastive
learning and invertible event transformation, call TranCLR. Our proposed model
utilizes an invertible transformation matrix to project semantic vectors of
events into a common event embedding space, trained with contrastive learning,
and thus naturally inject event semantic knowledge into mainstream QA
pipelines. The transformation matrix is fine-tuned with the annotated event
relation types between events that occurred in questions and those in answers,
using event-aware question vectors. Experimental results on the Event Semantic
Relation Reasoning (ESTER) dataset show significant improvements in both
generative and extractive settings compared to the existing strong baselines,
achieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact
Match (EM) score under the multi-answer setting. Qualitative analysis reveals
the high quality of the generated answers by TranCLR, demonstrating the
feasibility of injecting event knowledge into QA model learning. Our code and
models can be found at https://github.com/LuJunru/TranCLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Associations Between Natural Language Processing (NLP) Enriched Social
  Determinants of Health and Suicide Death among US Veterans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avijit Mitra, Richeek Pradhan, Rachel D Melamed, Kun Chen, David C Hoaglin, Katherine L Tucker, Joel I Reisman, Zhichao Yang, Weisong Liu, Jack Tsai, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Importance: Social determinants of health (SDOH) are known to be associated
with increased risk of suicidal behaviors, but few studies utilized SDOH from
unstructured electronic health record (EHR) notes.
  Objective: To investigate associations between suicide and recent SDOH,
identified using structured and unstructured data.
  Design: Nested case-control study.
  Setting: EHR data from the US Veterans Health Administration (VHA).
  Participants: 6,122,785 Veterans who received care in the US VHA between
October 1, 2010, and September 30, 2015.
  Exposures: Occurrence of SDOH over a maximum span of two years compared with
no occurrence of SDOH.
  Main Outcomes and Measures: Cases of suicide deaths were matched with 4
controls on birth year, cohort entry date, sex, and duration of follow-up. We
developed an NLP system to extract SDOH from unstructured notes. Structured
data, NLP on unstructured data, and combining them yielded seven, eight and
nine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence
intervals (CIs) were estimated using conditional logistic regression.
  Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382
person-years of follow-up (incidence rate 37.18 /100,000 person-years). Our
cohort was mostly male (92.23%) and white (76.99%). Across the six common SDOH
as covariates, NLP-extracted SDOH, on average, covered 84.38% of all SDOH
occurrences. All SDOH, measured by structured data and NLP, were significantly
associated with increased risk of suicide. The SDOH with the largest effects
was legal problems (aOR=2.67, 95% CI=2.46-2.89), followed by violence
(aOR=2.26, 95% CI=2.11-2.43). NLP-extracted and structured SDOH were also
associated with suicide.
  Conclusions and Relevance: NLP-extracted SDOH were always significantly
associated with increased risk of suicide among Veterans, suggesting the
potential of NLP in public health studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted at JAMA Network Open</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Scientific Creativity with Retrieval across Knowledge Domains <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonsu B. Kang, Sheshera Mysore, Kevin Huang, Haw-Shiuan Chang, Thorben Prein, Andrew McCallum, Aniket Kittur, Elsa Olivetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exposure to ideas in domains outside a scientist's own may benefit her in
reformulating existing research problems in novel ways and discovering new
application domains for existing solution ideas. While improved performance in
scholarly search engines can help scientists efficiently identify relevant
advances in domains they may already be familiar with, it may fall short of
helping them explore diverse ideas \textit{outside} such domains. In this paper
we explore the design of systems aimed at augmenting the end-user ability in
cross-domain exploration with flexible query specification. To this end, we
develop an exploratory search system in which end-users can select a portion of
text core to their interest from a paper abstract and retrieve papers that have
a high similarity to the user-selected core aspect but differ in terms of
domains. Furthermore, end-users can `zoom in' to specific domain clusters to
retrieve more papers from them and understand nuanced differences within the
clusters. Our case studies with scientists uncover opportunities and design
implications for systems aimed at facilitating cross-domain exploration and
inspiration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP+HCI Workshop at NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Ideological Salience and Framing in Polarized Online Groups
  with Graph Neural Networks and Structured Sparsity <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.08829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.08829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Hofmann, Xiaowen Dong, Janet B. Pierrehumbert, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing polarization of online political discourse calls for
computational tools that automatically detect and monitor ideological divides
in social media. We introduce a minimally supervised method that leverages the
network structure of online discussion forums, specifically Reddit, to detect
polarized concepts. We model polarization along the dimensions of salience and
framing, drawing upon insights from moral psychology. Our architecture combines
graph neural networks with structured sparsity learning and results in
representations for concepts and subreddits that capture temporal ideological
dynamics such as right-wing and left-wing radicalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2022 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Definition and a Test for Human-Level Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.09410v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.09410v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokgun Park, Md Ashaduzzaman Rubel Mondol, Aishwarya Pothula, Mazharul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances of AI research in many application-specific domains,
we do not know how to build a human-level artificial intelligence (HLAI). We
conjecture that learning from others' experience with the language is the
essential characteristic that distinguishes human intelligence from the rest.
Humans can update the action-value function with the verbal description as if
they experience states, actions, and corresponding rewards sequences firsthand.
In this paper, we present a classification of intelligence according to how
individual agents learn and propose a definition and a test for HLAI. The main
idea is that language acquisition without explicit rewards can be a sufficient
test for HLAI.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECON: Explicit Clothed humans Obtained from Normals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of artist-curated scans, and deep implicit functions (IF), is
enabling the creation of detailed, clothed, 3D humans from images. However,
existing methods are far from perfect. IF-based methods recover free-form
geometry but produce disembodied limbs or degenerate shapes for unseen poses or
clothes. To increase robustness for these cases, existing work uses an explicit
parametric body model to constrain surface reconstruction, but this limits the
recovery of free-form surfaces such as loose clothing that deviates from the
body. What we want is a method that combines the best properties of implicit
and explicit methods. To this end, we make two key observations: (1) current
networks are better at inferring detailed 2D maps than full-3D surfaces, and
(2) a parametric model can be seen as a "canvas" for stitching together
detailed surface patches. ECON infers high-fidelity 3D humans even in loose
clothes and challenging poses, while having realistic faces and fingers. This
goes beyond previous methods. Quantitative, evaluation of the CAPE and
Renderpeople datasets shows that ECON is more accurate than the state of the
art. Perceptual studies also show that ECON's perceived realism is better by a
large margin. Code and models are available for research purposes at
https://xiuyuliang.cn/econ
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://xiuyuliang.cn/econ Code:
  https://github.com/YuliangXiu/ECON</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Smooth Video Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Zhang, Ceyuan Yang, Yujun Shen, Yinghao Xu, Bolei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video generation requires synthesizing consistent and persistent frames with
dynamic content over time. This work investigates modeling the temporal
relations for composing video with arbitrary length, from a few frames to even
infinite, using generative adversarial networks (GANs). First, towards
composing adjacent frames, we show that the alias-free operation for single
image generation, together with adequately pre-learned knowledge, brings a
smooth frame transition without compromising the per-frame quality. Second, by
incorporating the temporal shift module (TSM), originally designed for video
understanding, into the discriminator, we manage to advance the generator in
synthesizing more consistent dynamics. Third, we develop a novel B-Spline based
motion representation to ensure temporal smoothness to achieve infinite-length
video generation. It can go beyond the frame number used in training. A
low-rank temporal modulation is also proposed to alleviate repeating contents
for long video generation. We evaluate our approach on various datasets and
show substantial improvements over video generation baselines. Code and models
will be publicly available at https://genforce.github.io/StyleSV.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Geometry-Aware Encoder for Style-Based 3D GAN Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StyleGAN has achieved great progress in 2D face reconstruction and semantic
editing via image inversion and latent editing. While studies over extending 2D
StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion
framework is still missing, limiting the applications of 3D face reconstruction
and semantic editing. In this paper, we study the challenging problem of 3D GAN
inversion where a latent code is predicted given a single face image to
faithfully recover its 3D shapes and detailed textures. The problem is
ill-posed: innumerable compositions of shape and texture could be rendered to
the current image. Furthermore, with the limited capacity of a global latent
code, 2D inversion methods cannot preserve faithful shape and texture at the
same time when applied to 3D models. To solve this problem, we devise an
effective self-training scheme to constrain the learning of inversion. The
learning is done efficiently without any real-world 2D-3D training pairs but
proxy samples generated from a 3D GAN. In addition, apart from a global latent
code that captures the coarse shape and texture information, we augment the
generation network with a local branch, where pixel-aligned features are added
to faithfully reconstruct face details. We further consider a new pipeline to
perform 3D view-consistent editing. Extensive experiments show that our method
outperforms state-of-the-art inversion methods in both shape and texture
reconstruction quality. Code and data will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An encoder-based 3D GAN inversion method. Project page:
  https://nirvanalan.github.io/projects/E3DGE/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BKinD-3D: <span class="highlight-title">Self-Supervised</span> 3D Keypoint Discovery from Multi-View Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer J. Sun, Pierre Karashchuk, Amil Dravid, Serim Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsaggelos, Bingni W. Brunton, Georgia Gkioxari, Ann Kennedy, Yisong Yue, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantifying motion in 3D is important for studying the behavior of humans and
other animals, but manual pose annotations are expensive and time-consuming to
obtain. Self-supervised keypoint discovery is a promising strategy for
estimating 3D poses without annotations. However, current keypoint discovery
approaches commonly process single 2D views and do not operate in the 3D space.
We propose a new method to perform self-supervised keypoint discovery in 3D
from multi-view videos of behaving agents, without any keypoint or bounding box
supervision in 2D or 3D. Our method uses an encoder-decoder architecture with a
3D volumetric heatmap, trained to reconstruct spatiotemporal differences across
multiple views, in addition to joint length constraints on a learned 3D
skeleton of the subject. In this way, we discover keypoints without requiring
manual supervision in videos of humans and rats, demonstrating the potential of
3D keypoint discovery for studying behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Play and Self-Describe: Policy Adaptation with Vision-Language
  Foundation Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. To adapt the policy to unseen tasks and
environments, we explore a new paradigm on leveraging the pre-trained
foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the
trained policy to a new task or a new environment, we first let the policy
self-play with randomly generated instructions to record the demonstrations.
While the execution could be wrong, we can use the pre-trained foundation
models to accurately self-describe (i.e., re-label or classify) the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
SPLAYD improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/SPLAYD/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://geyuying.github.io/SPLAYD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, Victor Adrian Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a Neural Radiance Field (NeRF) without pre-computed camera poses is
challenging. Recent advances in this direction demonstrate the possibility of
jointly optimising a NeRF and camera poses in forward-facing scenes. However,
these methods still face difficulties during dramatic camera movement. We
tackle this challenging problem by incorporating undistorted monocular depth
priors. These priors are generated by correcting scale and shift parameters
during training, with which we are then able to constrain the relative poses
between consecutive frames. This constraint is achieved using our proposed
novel loss functions. Experiments on real-world indoor and outdoor scenes show
that our method can handle challenging camera trajectories and outperforms
existing methods in terms of novel view rendering quality and pose estimation
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoqian Yang, Shikai Li, Wayne Wu, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present 3DHumanGAN, a 3D-aware generative adversarial network (GAN) that
synthesizes images of full-body humans with consistent appearances under
different view-angles and body-poses. To tackle the representational and
computational challenges in synthesizing the articulated structure of human
bodies, we propose a novel generator architecture in which a 2D convolutional
backbone is modulated by a 3D pose mapping network. The 3D pose mapping network
is formulated as a renderable implicit function conditioned on a posed 3D human
mesh. This design has several merits: i) it allows us to harness the power of
2D GANs to generate photo-realistic images; ii) it generates consistent images
under varying view-angles and specifiable poses; iii) the model can benefit
from the 3D human prior. Our model is adversarially learned from a collection
of web images needless of manual annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Compression with Product Quantized Masked Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaaeldin El-Nouby, Matthew J. Muckley, Karen Ullrich, Ivan Laptev, Jakob Verbeek, Hervé Jégou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent neural compression methods have been based on the popular hyperprior
framework. It relies on Scalar Quantization and offers a very strong
compression performance. This contrasts from recent advances in image
generation and representation learning, where Vector Quantization is more
commonly employed. In this work, we attempt to bring these lines of research
closer by revisiting vector quantization for image compression. We build upon
the VQ-VAE framework and introduce several modifications. First, we replace the
vanilla vector quantizer by a product quantizer. This intermediate solution
between vector and scalar quantization allows for a much wider set of
rate-distortion points: It implicitly defines high-quality quantizers that
would otherwise require intractably large codebooks. Second, inspired by the
success of Masked Image Modeling (MIM) in the context of self-supervised
learning and generative image models, we propose a novel conditional entropy
model which improves entropy coding by modelling the co-dependencies of the
quantized latent codes. The resulting PQ-MIM model is surprisingly effective:
its compression performance on par with recent hyperprior methods. It also
outperforms HiFiC in terms of FID and KID metrics when optimized with
perceptual losses (e.g. adversarial). Finally, since PQ-MIM is compatible with
image generation frameworks, we show qualitatively that it can operate under a
hybrid mode between compression and generation, with no further training or
finetuning. As a result, we explore the extreme compression regime where an
image is compressed into 200 bytes, i.e., less than a tweet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-Noising Diffusion: Towards Conditional Diffusion Models with
  Generative Restoration Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangfu Mei, Nithin Gopalakrishnan Nair, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion probabilistic models can model the distribution of
natural images and can generate diverse and realistic samples based on given
conditions. However, oftentimes their results can be unrealistic with
observable color shifts and textures. We believe that this issue results from
the divergence between the probabilistic distribution learned by the model and
the distribution of natural images. The delicate conditions gradually enlarge
the divergence during each sampling timestep. To address this issue, we
introduce a new method that brings the predicted samples to the training data
manifold using a pretrained unconditional diffusion model. The unconditional
model acts as a regularizer and reduces the divergence introduced by the
conditional model at each sampling step. We perform comprehensive experiments
to demonstrate the effectiveness of our approach on super-resolution,
colorization, turbulence removal, and image-deraining tasks. The improvements
obtained by our method suggest that the priors can be incorporated as a general
plugin for improving conditional diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fast Geometric Regularizer to Mitigate Event Collapse in the Contrast
  Maximization Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shintaro Shiba, Yoshimitsu Aoki, Guillermo Gallego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are emerging vision sensors and their advantages are suitable
for various applications such as autonomous robots. Contrast maximization
(CMax), which provides state-of-the-art accuracy on motion estimation using
events, may suffer from an overfitting problem called event collapse. Prior
works are computationally expensive or cannot alleviate the overfitting, which
undermines the benefits of the CMax framework. We propose a novel,
computationally efficient regularizer based on geometric principles to mitigate
event collapse. The experiments show that the proposed regularizer achieves
state-of-the-art accuracy results, while its reduced computational complexity
makes it two to four times faster than previous approaches. To the best of our
knowledge, our regularizer is the only effective solution for event collapse
without trading off runtime. We hope our work opens the door for future
applications that unlocks the advantages of event cameras.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 4 tables. Project page:
  https://github.com/tub-rip/event collapse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning useful representations for shifting tasks and distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Zhang, Léon Bottou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the dominant approach to learn representations (as a side effect of
optimizing an expected cost for a single training distribution) remain a good
approach when we are dealing with multiple distributions. Our thesis is that
such scenarios are better served by representations that are "richer" than
those obtained with a single optimization episode. This is supported by a
collection of empirical results obtained with an apparently na\"ive ensembling
technique: concatenating the representations obtained with multiple training
episodes using the same data, model, algorithm, and hyper-parameters, but
different random seeds. These independently trained networks perform similarly.
Yet, in a number of scenarios involving new distributions, the concatenated
representation performs substantially better than an equivalently sized network
trained from scratch. This proves that the representations constructed by
multiple training episodes are in fact different. Although their concatenation
carries little additional information about the training task under the
training distribution, it becomes substantially more informative when tasks or
distributions change. Meanwhile, a single training episode is unlikely to yield
such a redundant representation because the optimization process has no reason
to accumulate features that do not incrementally improve the training
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Artifacts in Real-World Video Super-Resolution Models <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recurrent structure is a prevalent framework for the task of video
super-resolution, which models the temporal dependency between frames via
hidden states. When applied to real-world scenarios with unknown and complex
degradations, hidden states tend to contain unpleasant artifacts and propagate
them to restored frames. In this circumstance, our analyses show that such
artifacts can be largely alleviated when the hidden state is replaced with a
cleaner counterpart. Based on the observations, we propose a Hidden State
Attention (HSA) module to mitigate artifacts in real-world video
super-resolution. Specifically, we first adopt various cheap filters to produce
a hidden state pool. For example, Gaussian blur filters are for smoothing
artifacts while sharpening filters are for enhancing details. To aggregate a
new hidden state that contains fewer artifacts from the hidden state pool, we
devise a Selective Cross Attention (SCA) module, in which the attention between
input features and each hidden state is calculated. Equipped with HSA, our
proposed method, namely FastRealVSR, is able to achieve 2x speedup while
obtaining better performance than Real-BasicVSR. Codes will be available at
https://github.com/TencentARC/FastRealVSR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023. Codes will be available at
  https://github.com/TencentARC/FastRealVSR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture
  of Stochastic Expert 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitong Gao, Yucong Chen, Chuyu Zhang, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equipping predicted segmentation with calibrated uncertainty is essential for
safety-critical applications. In this work, we focus on capturing the
data-inherent uncertainty (aka aleatoric uncertainty) in segmentation,
typically when ambiguities exist in input images. Due to the high-dimensional
output space and potential multiple modes in segmenting ambiguous images, it
remains challenging to predict well-calibrated uncertainty for segmentation. To
tackle this problem, we propose a novel mixture of stochastic experts (MoSE)
model, where each expert network estimates a distinct mode of the aleatoric
uncertainty and a gating network predicts the probabilities of an input image
being segmented in those modes. This yields an efficient two-level uncertainty
representation. To learn the model, we develop a Wasserstein-like loss that
directly minimizes the distribution distance between the MoSE and ground truth
annotations. The loss can easily integrate traditional segmentation quality
measures and be efficiently optimized via constraint relaxation. We validate
our method on the LIDC-IDRI dataset and a modified multimodal Cityscapes
dataset. Results demonstrate that our method achieves the state-of-the-art or
competitive performance on all metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematical model of printing-imaging channel for blind detection of
  fake copy detection patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joakim Tutt, Olga Taran, Roman Chaban, Brian Pulfer, Yury Belousov, Taras Holotyak, Slava Voloshynovskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, copy detection patterns (CDP) appear as a very promising
anti-counterfeiting technology for physical object protection. However, the
advent of deep learning as a powerful attacking tool has shown that the general
authentication schemes are unable to compete and fail against such attacks. In
this paper, we propose a new mathematical model of printing-imaging channel for
the authentication of CDP together with a new detection scheme based on it. The
results show that even deep learning created copy fakes unknown at the training
stage can be reliably authenticated based on the proposed approach and using
only digital references of CDP during authentication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at the IEEE International Workshop on Information
  Forensics and Security (WIFS) 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lambert, James Hays
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) map change detection is the task of determining when
sensor data and map data are no longer in agreement with one another due to
real-world changes. We collect the first dataset for the task, which we entitle
the Trust, but Verify (TbV) dataset, by mining thousands of hours of data from
over 9 months of autonomous vehicle fleet operations. We present learning-based
formulations for solving the problem in the bird's eye view and ego-view.
Because real map changes are infrequent and vector maps are easy to
synthetically manipulate, we lean on simulated data to train our model. Perhaps
surprisingly, we show that such models can generalize to real world
distributions. The dataset, consisting of maps and logs collected in six North
American cities, is one of the largest AV datasets to date with more than 7.8
million images. We make the data available to the public at
https://www.argoverse.org/av2.html#mapchange-link, along with code and models
at https://github.com/johnwlambert/tbv under the the CC BY-NC-SA 4.0 license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021, Track on Datasets and Benchmarks. Project page:
  https://tbv-dataset.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Child PalmID: Contactless Palmprint Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil K. Jain, Akash Godbole, Anjoo Bhatnagar, Prem Sewak Sudhish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing and least developed countries face the dire challenge of ensuring
that each child in their country receives required doses of vaccination,
adequate nutrition and proper medication. International agencies such as
UNICEF, WHO and WFP, among other organizations, strive to find innovative
solutions to determine which child has received the benefits and which have
not. Biometric recognition systems have been sought out to help solve this
problem. To that end, this report establishes a baseline accuracy of a
commercial contactless palmprint recognition system that may be deployed for
recognizing children in the age group of one to five years old. On a database
of contactless palmprint images of one thousand unique palms from 500 children,
we establish SOTA authentication accuracy of 90.85% @ FAR of 0.01%, rank-1
identification accuracy of 99.0% (closed set), and FPIR=0.01 @ FNIR=0.3 for
open-set identification using PalmMobile SDK from Armatura.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Domain Adaptive and Generalizable Semantic Segmentation with
  Class-Aware Cross-Domain <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Gong, Qin Wang, Dengxin Dai, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised sim-to-real domain adaptation (UDA) for semantic segmentation
aims to improve the real-world test performance of a model trained on simulated
data. It can save the cost of manually labeling data in real-world applications
such as robot vision and autonomous driving. Traditional UDA often assumes that
there are abundant unlabeled real-world data samples available during training
for the adaptation. However, such an assumption does not always hold in
practice owing to the collection difficulty and the scarcity of the data. Thus,
we aim to relieve this need on a large number of real data, and explore the
one-shot unsupervised sim-to-real domain adaptation (OSUDA) and generalization
(OSDG) problem, where only one real-world data sample is available. To remedy
the limited real data knowledge, we first construct the pseudo-target domain by
stylizing the simulated data with the one-shot real data. To mitigate the
sim-to-real domain gap on both the style and spatial structure level and
facilitate the sim-to-real adaptation, we further propose to use class-aware
cross-domain transformers with an intermediate domain randomization strategy to
extract the domain-invariant knowledge, from both the simulated and
pseudo-target data. We demonstrate the effectiveness of our approach for OSUDA
and OSDG on different benchmarks, outperforming the state-of-the-art methods by
a large margin, 10.87, 9.59, 13.05 and 15.91 mIoU on GTA,
SYNTHIA$\rightarrow$Cityscapes, Foggy Cityscapes, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 10 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConQueR: Query Contrast Voxel-DETR for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjin Zhu, Zhe Wang, Shaoshuai Shi, Hang Xu, Lanqing Hong, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although DETR-based 3D detectors can simplify the detection pipeline and
achieve direct sparse predictions, their performance still lags behind dense
detectors with post-processing for 3D object detection from point clouds. DETRs
usually adopt a larger number of queries than GTs (e.g., 300 queries v.s. 40
objects in Waymo) in a scene, which inevitably incur many false positives
during inference. In this paper, we propose a simple yet effective sparse 3D
detector, named Query Contrast Voxel-DETR (ConQueR), to eliminate the
challenging false positives, and achieve more accurate and sparser predictions.
We observe that most false positives are highly overlapping in local regions,
caused by the lack of explicit supervision to discriminate locally similar
queries. We thus propose a Query Contrast mechanism to explicitly enhance
queries towards their best-matched GTs over all unmatched query predictions.
This is achieved by the construction of positive and negative GT-query pairs
for each GT, and a contrastive loss to enhance positive GT-query pairs against
negative ones based on feature similarities. ConQueR closes the gap of sparse
and dense 3D detectors, and reduces up to ~60% false positives. Our
single-frame ConQueR achieves new state-of-the-art (sota) 71.6 mAPH/L2 on the
challenging Waymo Open Dataset validation set, outperforming previous sota
methods (e.g., PV-RCNN++) by over 2.0 mAPH/L2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://benjin.me/projects/2022_conquer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Robust Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training adversarially robust discriminative (i.e., softmax) classifier has
been the dominant approach to robust classification. Building on recent work on
adversarial training (AT)-based generative models, we investigate using AT to
learn unnormalized class-conditional density models and then performing
generative robust classification. Our result shows that, under the condition of
similar model capacities, the generative robust classifier achieves comparable
performance to a baseline softmax robust classifier when the test data is clean
or when the test perturbation is of limited size, and much better performance
when the test perturbation size exceeds the training perturbation size. The
generative classifier is also able to generate samples or counterfactuals that
more closely resemble the training data, suggesting that the generative
classifier can better capture the class-conditional distributions. In contrast
to standard discriminative adversarial training where advanced data
augmentation techniques are only effective when combined with weight averaging,
we find it straightforward to apply advanced data augmentation to achieve
better robustness in our approach. Our result suggests that the generative
classifier is a competitive alternative to robust classification, especially
for problems with limited number of classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContraFeat: Contrasting Deep Features for Semantic Discovery <span class="chip">AAAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinqi Zhu, Chang Xu, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StyleGAN has shown strong potential for disentangled semantic control, thanks
to its special design of multi-layer intermediate latent variables. However,
existing semantic discovery methods on StyleGAN rely on manual selection of
modified latent layers to obtain satisfactory manipulation results, which is
tedious and demanding. In this paper, we propose a model that automates this
process and achieves state-of-the-art semantic discovery performance. The model
consists of an attention-equipped navigator module and losses contrasting
deep-feature changes. We propose two model variants, with one contrasting
samples in a binary manner, and another one contrasting samples with learned
prototype variation patterns. The proposed losses are defined with pretrained
deep features, based on our assumption that the features can implicitly reveal
the desired semantic structure including consistency and orthogonality.
Additionally, we design two metrics to quantitatively evaluate the performance
of semantic discovery methods on FFHQ dataset, and also show that disentangled
representations can be derived via a simple training process. Experimentally,
our models can obtain state-of-the-art semantic discovery results without
relying on latent layer-wise manual selection, and these discovered semantics
can be used to manipulate real-world images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M-GenSeg: Domain Adaptation For Target Modality Tumor Segmentation With
  Annotation-Efficient Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malo Alefsen de Boisredon d'Assier, Eugene Vorontsov, Samuel Kadoury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated medical image segmentation using deep neural networks typically
requires substantial supervised training. However, these models fail to
generalize well across different imaging modalities. This shortcoming,
amplified by the limited availability of annotated data, has been hampering the
deployment of such methods at a larger scale across modalities. To address
these issues, we propose M-GenSeg, a new semi-supervised training strategy for
accurate cross-modality tumor segmentation on unpaired bi-modal datasets. Based
on image-level labels, a first unsupervised objective encourages the model to
perform diseased to healthy translation by disentangling tumors from the
background, which encompasses the segmentation task. Then, teaching the model
to translate between image modalities enables the synthesis of target images
from a source modality, thus leveraging the pixel-level annotations from the
source modality to enforce generalization to the target modality images. We
evaluated the performance on a brain tumor segmentation datasets composed of
four different contrast sequences from the public BraTS 2020 challenge dataset.
We report consistent improvement in Dice scores on both source and unannotated
target modalities. On all twelve distinct domain adaptation experiments, the
proposed model shows a clear improvement over state-of-the-art domain-adaptive
baselines, with absolute Dice gains on the target modality reaching 0.15.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages and 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhoMoH: Implicit Photorealistic 3D Models of Human Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Zanfir, Thiemo Alldieck, Cristian Sminchisescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PhoMoH, a neural network methodology to construct generative
models of photorealistic 3D geometry and appearance of human heads including
hair, beards, clothing and accessories. In contrast to prior work, PhoMoH
models the human head using neural fields, thus supporting complex topology.
Instead of learning a head model from scratch, we propose to augment an
existing expressive head model with new features. Concretely, we learn a highly
detailed geometry network layered on top of a mid-resolution head model
together with a detailed, local geometry-aware, and disentangled color field.
Our proposed architecture allows us to learn photorealistic human head models
from relatively little data. The learned generative geometry and appearance
networks can be sampled individually and allow the creation of diverse and
realistic human heads. Extensive experiments validate our method qualitatively
and across different metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artur Grigorev, Bernhard Thomaszewski, Michael J. Black, Otmar Hilliges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method that leverages graph neural networks, multi-level message
passing, and unsupervised training to enable real-time prediction of realistic
clothing dynamics. Whereas existing methods based on linear blend skinning must
be trained for specific garments, our method is agnostic to body shape and
applies to tight-fitting garments as well as loose, free-flowing clothing. Our
method furthermore handles changes in topology (e.g., garments with buttons or
zippers) and material properties at inference time. As one key contribution, we
propose a hierarchical message-passing scheme that efficiently propagates stiff
stretching modes while preserving local detail. We empirically show that our
method outperforms strong baselines quantitatively and that its results are
perceived as more realistic than state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGO: Recurrent Graph Optimizer For Multiple Rotation Averaging <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Li, Zhaopeng Cui, Shuaicheng Liu, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a deep recurrent Rotation Averaging Graph Optimizer
(RAGO) for Multiple Rotation Averaging (MRA). Conventional optimization-based
methods usually fail to produce accurate results due to corrupted and noisy
relative measurements. Recent learning-based approaches regard MRA as a
regression problem, while these methods are sensitive to initialization due to
the gauge freedom problem. To handle these problems, we propose a learnable
iterative graph optimizer minimizing a gauge-invariant cost function with an
edge rectification strategy to mitigate the effect of inaccurate measurements.
Our graph optimizer iteratively refines the global camera rotations by
minimizing each node's single rotation objective function. Besides, our
approach iteratively rectifies relative rotations to make them more consistent
with the current camera orientations and observed relative rotations.
Furthermore, we employ a gated recurrent unit to improve the result by tracing
the temporal information of the cost graph. Our framework is a real-time
learning-to-optimize rotation averaging graph optimizer with a tiny size
deployed for real-world applications. RAGO outperforms previous traditional and
deep methods on real-world and synthetic datasets. The code is available at
https://github.com/sfu-gruvi-3dv/RAGO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAELi -- Masked Autoencoder for Large-Scale LiDAR Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Krispel, David Schinagl, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how the inherent, but often neglected, properties of large-scale
LiDAR point clouds can be exploited for effective self-supervised
representation learning. To this end, we design a highly data-efficient feature
pre-training backbone that significantly reduces the amount of tedious 3D
annotations to train state-of-the-art object detectors. In particular, we
propose a Masked AutoEncoder (MAELi) that intuitively utilizes the sparsity of
the LiDAR point clouds in both, the encoder and the decoder, during
reconstruction. This results in more expressive and useful features, directly
applicable to downstream perception tasks, such as 3D object detection for
autonomous driving. In a novel reconstruction scheme, MAELi distinguishes
between free and occluded space and leverages a new masking strategy which
targets the LiDAR's inherent spherical projection. To demonstrate the potential
of MAELi, we pre-train one of the most widespread 3D backbones, in an
end-to-end fashion and show the merit of our fully unsupervised pre-trained
features on several 3D object detection architectures. Given only a tiny
fraction of labeled frames to fine-tune such detectors, we achieve significant
performance improvements. For example, with only $\sim800$ labeled frames,
MAELi features improve a SECOND model by +10.09APH/LEVEL 2 on Waymo Vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design-time Fashion Popularity Forecasting in VR Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos-Iordanis Papadopoulos, Christos Koutlis, Anastasios Papazoglou-Chalikias, Symeon Papadopoulos, Spiros Nikolopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to forecast the popularity of new garment designs is very
important in an industry as fast paced as fashion, both in terms of
profitability and reducing the problem of unsold inventory. Here, we attempt to
address this task in order to provide informative forecasts to fashion
designers within a virtual reality designer application that will allow them to
fine tune their creations based on current consumer preferences within an
interactive and immersive environment. To achieve this we have to deal with the
following central challenges: (1) the proposed method should not hinder the
creative process and thus it has to rely only on the garment's visual
characteristics, (2) the new garment lacks historical data from which to
extrapolate their future popularity and (3) fashion trends in general are
highly dynamical. To this end, we develop a computer vision pipeline fine tuned
on fashion imagery in order to extract relevant visual features along with the
category and attributes of the garment. We propose a hierarchical label sharing
(HLS) pipeline for automatically capturing hierarchical relations among fashion
categories and attributes. Moreover, we propose MuQAR, a Multimodal
Quasi-AutoRegressive neural network that forecasts the popularity of new
garments by combining their visual features and categorical features while an
autoregressive neural network is modelling the popularity time series of the
garment's category and attributes. Both the proposed HLS and MuQAR prove
capable of surpassing the current state-of-the-art in key benchmark datasets,
DeepFashion for image classification and VISUELLE for new garment sales
forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-based YOLO Object Detection: Proof of Concept for Forward
  Perception System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waseem Shariff, Muhammad Ali Farooq, Joe Lemley, Peter Corcoran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic vision or event vision is an advanced vision technology, where
in contrast to the visible camera that outputs pixels, the event vision
generates neuromorphic events every time there is a brightness change which
exceeds a specific threshold in the field of view (FOV). This study focuses on
leveraging neuromorphic event data for roadside object detection. This is a
proof of concept towards building artificial intelligence (AI) based pipelines
which can be used for forward perception systems for advanced vehicular
applications. The focus is on building efficient state-of-the-art object
detection networks with better inference results for fast-moving forward
perception using an event camera. In this article, the event-simulated A2D2
dataset is manually annotated and trained on two different YOLOv5 networks
(small and large variants). To further assess its robustness, single model
testing and ensemble model testing are carried out.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, ICMV conference 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Establishing a stronger baseline for lightweight contrastive models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenye Lin, Yifeng Ding, Zhixiong Cao, Hai-tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has reported a performance degradation in self-supervised
contrastive learning for specially designed efficient networks, such as
MobileNet and EfficientNet. A common practice to address this problem is to
introduce a pretrained contrastive teacher model and train the lightweight
networks with distillation signals generated by the teacher. However, it is
time and resource consuming to pretrain a teacher model when it is not
available. In this work, we aim to establish a stronger baseline for
lightweight contrastive models without using a pretrained teacher model.
Specifically, we show that the optimal recipe for efficient models is different
from that of larger models, and using the same training settings as ResNet50,
as previous research does, is inappropriate. Additionally, we observe a common
issu e in contrastive learning where either the positive or negative views can
be noisy, and propose a smoothed version of InfoNCE loss to alleviate this
problem. As a result, we successfully improve the linear evaluation results
from 36.3\% to 62.3\% for MobileNet-V3-Large and from 42.2\% to 65.8\% for
EfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with
$5\times$ fewer parameters. We hope our research will facilitate the usage of
lightweight contrastive models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully complex-valued deep learning model for visual perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddh Sikdar, Sumanth Udupa, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models operating in the complex domain are used due to their
rich representation capacity. However, most of these models are either
restricted to the first quadrant of the complex plane or project the
complex-valued data into the real domain, causing a loss of information. This
paper proposes that operating entirely in the complex domain increases the
overall performance of complex-valued models. A novel, fully complex-valued
learning scheme is proposed to train a Fully Complex-valued Convolutional
Neural Network (FC-CNN) using a newly proposed complex-valued loss function and
training strategy. Benchmarked on CIFAR-10, SVHN, and CIFAR-100, FC-CNN has a
4-10% gain compared to its real-valued counterpart, maintaining the model
complexity. With fewer parameters, it achieves comparable performance to
state-of-the-art complex-valued models on CIFAR-10 and SVHN. For the CIFAR-100
dataset, it achieves state-of-the-art performance with 25% fewer parameters.
FC-CNN shows better training efficiency and much faster convergence than all
the other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertain Facial Expression Recognition via Multi-task Assisted
  Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Xingming Zhang, Janne Kauttonen, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep models for facial expression recognition achieve high performance by
training on large-scale labeled data. However, publicly available datasets
contain uncertain facial expressions caused by ambiguous annotations or
confusing emotions, which could severely decline the robustness. Previous
studies usually follow the bias elimination method in general tasks without
considering the uncertainty problem from the perspective of different
corresponding sources. In this paper, we propose a novel method of multi-task
assisted correction in addressing uncertain facial expression recognition
called MTAC. Specifically, a confidence estimation block and a weighted
regularization module are applied to highlight solid samples and suppress
uncertain samples in every batch. In addition, two auxiliary tasks, i.e.,
action unit detection and valence-arousal measurement, are introduced to learn
semantic distributions from a data-driven AU graph and mitigate category
imbalance based on latent dependencies between discrete and continuous
emotions, respectively. Moreover, a re-labeling strategy guided by
feature-level similarity constraint further generates new labels for identified
uncertain samples to promote model learning. The proposed method can flexibly
combine with existing frameworks in a fully-supervised or weakly-supervised
manner. Experiments on RAF-DB, AffectNet, and AffWild2 datasets demonstrate
that the MTAC obtains substantial improvements over baselines when facing
synthetic and real uncertainties and outperforms the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 5 tables. arXiv admin note: text overlap with
  arXiv:2204.11053</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducible scaling laws for contrastive language-image learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural networks has led to remarkable performance across a wide
range of tasks. Moreover, performance often follows reliable scaling laws as a
function of training set size, model size, and compute, which offers valuable
guidance as large-scale experiments are becoming increasingly expensive.
However, previous work on scaling laws has primarily used private data \&
models or focused on uni-modal language or vision learning. To address these
limitations, we investigate scaling laws for contrastive language-image
pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP
repository. Our large-scale experiments involve models trained on up to two
billion image-text pairs and identify power law scaling for multiple downstream
tasks including zero-shot classification, retrieval, linear probing, and
end-to-end fine-tuning. We find that the training distribution plays a key role
in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling
behavior despite identical model architectures and similar training recipes. We
open-source our evaluation workflow and all models, including the largest
public CLIP models, to ensure reproducibility and make scaling laws research
more accessible. Source code and instructions to reproduce this study will be
available at https://github.com/LAION-AI/scaling-laws-openclip
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blood Oxygen Saturation Estimation from Facial Video via DC and AC
  components of Spatio-temporal Map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Akamatsu, Yoshifumi Onishi, Hitoshi Imaoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Peripheral blood oxygen saturation (SpO2), an indicator of oxygen levels in
the blood, is one of the most important physiological parameters. Although SpO2
is usually measured using a pulse oximeter, non-contact SpO2 estimation methods
from facial or hand videos have been attracting attention in recent years. In
this paper, we propose an SpO2 estimation method from facial videos based on
convolutional neural networks (CNN). Our method constructs CNN models that
consider the direct current (DC) and alternating current (AC) components
extracted from the RGB signals of facial videos, which are important in the
principle of SpO2 estimation. Specifically, we extract the DC and AC components
from the spatio-temporal map using filtering processes and train CNN models to
predict SpO2 from these components. We also propose an end-to-end model that
predicts SpO2 directly from the spatio-temporal map by extracting the DC and AC
components via convolutional layers. Experiments using facial videos and SpO2
data from 50 subjects demonstrate that the proposed method achieves a better
estimation performance than current state-of-the-art SpO2 estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial intelligence-driven digital twin of a modern house
  demonstrated in virtual reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Mohammed Elfarri, Adil Rasheed, Omer San
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A digital twin is defined as a virtual representation of a physical asset
enabled through data and simulators for real-time prediction, optimization,
monitoring, controlling, and improved decision-making. Unfortunately, the term
remains vague and says little about its capability. Recently, the concept of
capability level has been introduced to address this issue. Based on its
capability, the concept states that a digital twin can be categorized on a
scale from zero to five, referred to as standalone, descriptive, diagnostic,
predictive, prescriptive, and autonomous, respectively. The current work
introduces the concept in the context of the built environment. It demonstrates
the concept by using a modern house as a use case. The house is equipped with
an array of sensors that collect timeseries data regarding the internal state
of the house. Together with physics-based and data-driven models, these data
are used to develop digital twins at different capability levels demonstrated
in virtual reality. The work, in addition to presenting a blueprint for
developing digital twins, also provided future research directions to enhance
the technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization by Learning and Removing Domain-specific Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ding, Lei Wang, Bin Liang, Shuming Liang, Yang Wang, Fang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) suffer from domain shift when the test dataset
follows a distribution different from the training dataset. Domain
generalization aims to tackle this issue by learning a model that can
generalize to unseen domains. In this paper, we propose a new approach that
aims to explicitly remove domain-specific features for domain generalization.
Following this approach, we propose a novel framework called Learning and
Removing Domain-specific features for Generalization (LRDG) that learns a
domain-invariant model by tactically removing domain-specific features from the
input images. Specifically, we design a classifier to effectively learn the
domain-specific features for each source domain, respectively. We then develop
an encoder-decoder network to map each input image into a new image space where
the learned domain-specific features are removed. With the images output by the
encoder-decoder network, another classifier is designed to learn the
domain-invariant features to conduct image classification. Extensive
experiments demonstrate that our framework achieves superior performance
compared with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Sketching of Mannequin Poses <span class="chip">3DV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gizem Unlu, Mohamed Sayed, Gabriel Brostow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It can be easy and even fun to sketch humans in different poses. In contrast,
creating those same poses on a 3D graphics "mannequin" is comparatively
tedious. Yet 3D body poses are necessary for various downstream applications.
We seek to preserve the convenience of 2D sketching while giving users of
different skill levels the flexibility to accurately and more quickly
pose\slash refine a 3D mannequin.
  At the core of the interactive system, we propose a machine-learning model
for inferring the 3D pose of a CG mannequin from sketches of humans drawn in a
cylinder-person style. Training such a model is challenging because of artist
variability, a lack of sketch training data with corresponding ground truth 3D
poses, and the high dimensionality of human pose-space. Our unique approach to
synthesizing vector graphics training data underpins our integrated
ML-and-kinematics system. We validate the system by tightly coupling it with a
user interface, and by performing a user study, in addition to quantitative
comparisons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and published at 3DV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLIP: Noise-robust Language-Image <span class="highlight-title">Pre-train</span>ing <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runhui Huang, Yanxin Long, Jianhua Han, Hang Xu, Xiwen Liang, Chunjing Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale cross-modal pre-training paradigms have recently shown ubiquitous
success on a wide range of downstream tasks, e.g., zero-shot classification,
retrieval and image captioning. However, their successes highly rely on the
scale and quality of web-crawled data that naturally contain incomplete and
noisy information (e.g., wrong or irrelevant content). Existing works either
design manual rules to clean data or generate pseudo-targets as auxiliary
signals for reducing noise impact, which do not explicitly tackle both the
incorrect and incomplete challenges simultaneously. In this paper, to
automatically mitigate the impact of noise by solely mining over existing data,
we propose a principled Noise-robust Language-Image Pre-training framework
(NLIP) to stabilize pre-training via two schemes: noise-harmonization and
noise-completion. First, in noise-harmonization scheme, NLIP estimates the
noise probability of each pair according to the memorization effect of
cross-modal transformers, then adopts noise-adaptive regularization to
harmonize the cross-modal alignments with varying degrees. Second, in
noise-completion scheme, to enrich the missing object information of text, NLIP
injects a concept-conditioned cross-modal decoder to obtain semantic-consistent
synthetic captions to complete noisy ones, which uses the retrieved visual
concepts (i.e., objects' names) for the corresponding image to guide captioning
generation. By collaboratively optimizing noise-harmonization and
noise-completion schemes, our NLIP can alleviate the common noise effects
during image-text pre-training in a more efficient way. Extensive experiments
show the significant performance improvements of our NLIP using only 26M data
over existing pre-trained models (e.g., CLIP, FILIP and BLIP) on 12 zero-shot
classification datasets, MSCOCO image captioning and zero-shot image-text
retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Complex-valued Fully Convolutional Multi-feature Fusion Network
  (FC2MFN) for Building Segmentation of InSAR images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddh Sikdar, Sumanth Udupa, Suresh Sundaram, Narasimhan Sundararajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building segmentation in high-resolution InSAR images is a challenging task
that can be useful for large-scale surveillance. Although complex-valued deep
learning networks perform better than their real-valued counterparts for
complex-valued SAR data, phase information is not retained throughout the
network, which causes a loss of information. This paper proposes a Fully
Complex-valued, Fully Convolutional Multi-feature Fusion Network(FC2MFN) for
building semantic segmentation on InSAR images using a novel, fully
complex-valued learning scheme. The network learns multi-scale features,
performs multi-feature fusion, and has a complex-valued output. For the
particularity of complex-valued InSAR data, a new complex-valued pooling layer
is proposed that compares complex numbers considering their magnitude and
phase. This helps the network retain the phase information even through the
pooling layer. Experimental results on the simulated InSAR dataset show that
FC2MFN achieves better results compared to other state-of-the-art methods in
terms of segmentation performance and model complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Symposium Series On Computational
  Intelligence 2022, 8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel state connection strategy for quantum computing to represent and
  compress digital images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ershadul Haque, Manoranjan Paul, Tanmoy Debnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum image processing draws a lot of attention due to faster data
computation and storage compared to classical data processing systems.
Converting classical image data into the quantum domain and state label
preparation complexity is still a challenging issue. The existing techniques
normally connect the pixel values and the state position directly. Recently,
the EFRQI (efficient flexible representation of the quantum image) approach
uses an auxiliary qubit that connects the pixel-representing qubits to the
state position qubits via Toffoli gates to reduce state connection. Due to the
twice use of Toffoli gates for each pixel connection still it requires a
significant number of bits to connect each pixel value. In this paper, we
propose a new SCMFRQI (state connection modification FRQI) approach for further
reducing the required bits by modifying the state connection using a reset gate
rather than repeating the use of the same Toffoli gate connection as a reset
gate. Moreover, unlike other existing methods, we compress images using
block-level for further reduction of required qubits. The experimental results
confirm that the proposed method outperforms the existing methods in terms of
both image representation and compression points of view.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Similarity-Based Curriculum Learning for Image Captioning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkuan Zhang, Saku Sugawara, Akiko Aizawa, Lei Zhou, Ryohei Sasano, Koichi Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning models require the high-level generalization ability to
describe the contents of various images in words. Most existing approaches
treat the image-caption pairs equally in their training without considering the
differences in their learning difficulties. Several image captioning approaches
introduce curriculum learning methods that present training data with
increasing levels of difficulty. However, their difficulty measurements are
either based on domain-specific features or prior model training. In this
paper, we propose a simple yet efficient difficulty measurement for image
captioning using cross-modal similarity calculated by a pretrained
vision-language model. Experiments on the COCO and Flickr30k datasets show that
our proposed approach achieves superior performance and competitive convergence
speed to baselines without requiring heuristics or incurring additional
training costs. Moreover, the higher model performance on difficult examples
and unseen data also demonstrates the generalization ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Negative Correlation Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhang, Qibin Hou, Yun Liu, Jia-Wang Bian, Xun Xu, Joey Tianyi Zhou, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble learning serves as a straightforward way to improve the performance
of almost any machine learning algorithm. Existing deep ensemble methods
usually naively train many different models and then aggregate their
predictions. This is not optimal in our view from two aspects: i) Naively
training multiple models adds much more computational burden, especially in the
deep learning era; ii) Purely optimizing each base model without considering
their interactions limits the diversity of ensemble and performance gains. We
tackle these issues by proposing deep negative correlation classification
(DNCC), in which the accuracy and diversity trade-off is systematically
controlled by decomposing the loss function seamlessly into individual accuracy
and the correlation between individual models and the ensemble. DNCC yields a
deep classification ensemble where the individual estimator is both accurate
and negatively correlated. Thanks to the optimized diversities, DNCC works well
even when utilizing a shared network backbone, which significantly improves its
efficiency when compared with most existing ensemble systems. Extensive
experiments on multiple benchmark datasets and network structures demonstrate
the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Warped Planar Object Detection Network For Automatic License
  Plate Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Dinh Tra, Nguyen Cong Tri, Phan Duy Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to improve the Warping Planer Object Detection Network
(WPOD-Net) using feature engineering to increase accuracy. What problems are
solved using the Warping Object Detection Network using feature engineering?
More specifically, we think that it makes sense to add knowledge about edges in
the image to enhance the information for determining the license plate contour
of the original WPOD-Net model. The Sobel filter has been selected
experimentally and acts as a Convolutional Neural Network layer, the edge
information is combined with the old information of the original network to
create the final embedding vector. The proposed model was compared with the
original model on a set of data that we collected for evaluation. The results
are evaluated through the Quadrilateral Intersection over Union value and
demonstrate that the model has a significant improvement in performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Wen Dong, Naoya Takahashi, Yuki Mitsufuji, Julian McAuley, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen progress beyond domain-specific sound separation for
speech or music towards universal sound separation for arbitrary sounds. Prior
work on universal sound separation has investigated separating a target sound
out of an audio mixture given a text query. Such text-queried sound separation
systems provide a natural and scalable interface for specifying arbitrary
target sounds. However, supervised text-queried sound separation systems
require costly labeled audio-text pairs for training. Moreover, the audio
provided in existing datasets is often recorded in a controlled environment,
causing a considerable generalization gap to noisy audio in the wild. In this
work, we aim to approach text-queried universal sound separation by using only
unlabeled data. We propose to leverage the visual modality as a bridge to learn
the desired audio-textual correspondence. The proposed CLIPSep model first
encodes the input query into a query vector using the contrastive
language-image pretraining (CLIP) model, and the query vector is then used to
condition an audio separation model to separate out the target sound. While the
model is trained on image-audio pairs extracted from unlabeled videos, at test
time we can instead query the model with text inputs in a zero-shot setting,
thanks to the joint language-image embedding learned by the CLIP model.
Further, videos in the wild often contain off-screen sounds and background
noise that may hinder the model from learning the desired audio-textual
correspondence. To address this problem, we further propose an approach called
noise invariant training for training a query-based sound separation model on
noisy data. Experimental results show that the proposed models successfully
learn text-queried universal sound separation using only noisy unlabeled
videos, even achieving competitive performance against a supervised model in
some settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception
  for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengwei Bai, Guoyuan Wu, Matthew J. Barth, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing the latest advances in Artificial Intelligence (AI), the computer
vision community is now witnessing an unprecedented evolution in all kinds of
perception tasks, particularly in object detection. Based on multiple spatially
separated perception nodes, Cooperative Perception (CP) has emerged to
significantly advance the perception of automated driving. However, current
cooperative object detection methods mainly focus on ego-vehicle efficiency
without considering the practical issues of system-wide costs. In this paper,
we introduce VINet, a unified deep learning-based CP network for scalable,
lightweight, and heterogeneous cooperative 3D object detection. VINet is the
first CP method designed from the standpoint of large-scale system-level
implementation and can be divided into three main phases: 1) Global
Pre-Processing and Lightweight Feature Extraction which prepare the data into
global style and extract features for cooperation in a lightweight manner; 2)
Two-Stream Fusion which fuses the features from scalable and heterogeneous
perception nodes; and 3) Central Feature Backbone and 3D Detection Head which
further process the fused features and generate cooperative detection results.
A cooperative perception platform is designed and developed for CP dataset
acquisition and several baselines are compared during the experiments. The
experimental analysis shows that VINet can achieve remarkable improvements for
pedestrians and cars with 2x less system-wide computational costs and 12x less
system-wide communicational costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Artificial Intelligence in Retinal Imaging for the detection
  of Systemic Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayushi Raj Bhatt, Rajkumar Vaghashiya, Meghna Kulkarni, Dr Prakash Kamaraj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (AI) in the form of an interpretable and
semiautomatic approach to stage grading ocular pathologies such as Diabetic
retinopathy, Hypertensive retinopathy, and other retinopathies on the backdrop
of major systemic diseases. The experimental study aims to evaluate an
explainable staged grading process without using deep Convolutional Neural
Networks (CNNs) directly. Many current CNN-based deep neural networks used for
diagnosing retinal disorders might have appreciable performance but fail to
pinpoint the basis driving their decisions. To improve these decisions'
transparency, we have proposed a clinician-in-the-loop assisted intelligent
workflow that performs a retinal vascular assessment on the fundus images to
derive quantifiable and descriptive parameters. The retinal vessel parameters
meta-data serve as hyper-parameters for better interpretation and
explainability of decisions. The semiautomatic methodology aims to have a
federated approach to AI in healthcare applications with more inputs and
interpretations from clinicians. The baseline process involved in the machine
learning pipeline through image processing techniques for optic disc detection,
vessel segmentation, and arteriole/venule identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-branch Cross-Patch Attention Learning for Group Affect Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongxia Xie, Ming-Xian Lee, Tzu-Jui Chen, Hung-Jen Chen, Hou-I Liu, Hong-Han Shuai, Wen-Huang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group affect refers to the subjective emotion that is evoked by an external
stimulus in a group, which is an important factor that shapes group behavior
and outcomes. Recognizing group affect involves identifying important
individuals and salient objects among a crowd that can evoke emotions. Most of
the existing methods are proposed to detect faces and objects using pre-trained
detectors and summarize the results into group emotions by specific rules.
However, such affective region selection mechanisms are heuristic and
susceptible to imperfect faces and objects from the pre-trained detectors.
Moreover, faces and objects on group-level images are often contextually
relevant. There is still an open question about how important faces and objects
can be interacted with. In this work, we incorporate the psychological concept
called Most Important Person (MIP). It represents the most noteworthy face in
the crowd and has an affective semantic meaning. We propose the Dual-branch
Cross-Patch Attention Transformer (DCAT) which uses global image and MIP
together as inputs. Specifically, we first learn the informative facial regions
produced by the MIP and the global context separately. Then, the Cross-Patch
Attention module is proposed to fuse the features of MIP and global context
together to complement each other. With parameters less than 10x, the proposed
DCAT outperforms state-of-the-art methods on two datasets of group valence
prediction, GAF 3.0 and GroupEmoW datasets. Moreover, our proposed model can be
transferred to another group affect task, group cohesion, and shows comparable
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Significantly improving zero-shot X-ray pathology classification via
  fine-tuning <span class="highlight-title">pre-train</span>ed image-text encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongseong Jang, Daeun Kyung, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been successfully adopted to diverse domains
including pathology classification based on medical images. However,
large-scale and high-quality data to train powerful neural networks are rare in
the medical domain as the labeling must be done by qualified experts.
Researchers recently tackled this problem with some success by taking advantage
of models pre-trained on large-scale general domain data. Specifically,
researchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it
with chest X-ray images and paired reports to perform zero-shot pathology
classification, thus completely removing the need for pathology-annotated
images to train a classification model. Existing studies, however, fine-tuned
the pre-trained model with the same contrastive learning objective, and failed
to exploit the multi-labeled nature of medical image-report pairs. In this
paper, we propose a new fine-tuning strategy based on sentence sampling and
positive-pair loss relaxation for improving the downstream zero-shot pathology
classification performance, which can be applied to any pre-trained contrastive
image-text encoders. Our method consistently showed dramatically improved
zero-shot pathology classification performance on four different chest X-ray
datasets and 3 different pre-trained models (5.77% average AUROC increase). In
particular, fine-tuning CLIP with our method showed much comparable or
marginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1
score and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent
diseases from the CheXpert dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PD-Quant: Post-Training Quantization based on Prediction Difference
  Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang, Wenyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a neural network compression technique, post-training quantization (PTQ)
transforms a pre-trained model into a quantized model using a lower-precision
data type. However, the prediction accuracy will decrease because of the
quantization noise, especially in extremely low-bit settings. How to determine
the appropriate quantization parameters (e.g., scaling factors and rounding of
weights) is the main problem facing now. Many existing methods determine the
quantization parameters by minimizing the distance between features before and
after quantization. Using this distance as the metric to optimize the
quantization parameters only considers local information. We analyze the
problem of minimizing local metrics and indicate that it would not result in
optimal quantization parameters. Furthermore, the quantized model suffers from
overfitting due to the small number of calibration samples in PTQ. In this
paper, we propose PD-Quant to solve the problems. PD-Quant uses the information
of differences between network prediction before and after quantization to
determine the quantization parameters. To mitigate the overfitting problem,
PD-Quant adjusts the distribution of activations in PTQ. Experiments show that
PD-Quant leads to better quantization parameters and improves the prediction
accuracy of quantized models, especially in low-bit settings. For example,
PD-Quant pushes the accuracy of ResNet-18 up to 53.08% and RegNetX-600MF up to
40.92% in weight 2-bit activation 2-bit. The code will be released at
https://github.com/hustvl/PD-Quant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shared Coupling-bridge for Weakly Supervised Local Feature Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Sun, Jiewen Zhu, Luping Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse local feature extraction is usually believed to be of important
significance in typical vision tasks such as simultaneous localization and
mapping, image matching and 3D reconstruction. At present, it still has some
deficiencies needing further improvement, mainly including the discrimination
power of extracted local descriptors, the localization accuracy of detected
keypoints, and the efficiency of local feature learning. This paper focuses on
promoting the currently popular sparse local feature learning with camera pose
supervision. Therefore, it pertinently proposes a Shared Coupling-bridge scheme
with four light-weight yet effective improvements for weakly-supervised local
feature (SCFeat) learning. It mainly contains: i) the
\emph{Feature-Fusion-ResUNet Backbone} (F2R-Backbone) for local descriptors
learning, ii) a shared coupling-bridge normalization to improve the decoupling
training of description network and detection network, iii) an improved
detection network with peakiness measurement to detect keypoints and iv) the
fundamental matrix error as a reward factor to further optimize feature
detection training. Extensive experiments prove that our SCFeat improvement is
effective. It could often obtain a state-of-the-art performance on classic
image matching and visual localization. In terms of 3D reconstruction, it could
still achieve competitive results. For sharing and communication, our source
codes are available at https://github.com/sunjiayuanro/SCFeat.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Neuron Morphology Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Jiang, Michael Goebel, Cezar Borba, William Smith, B. S. Manjunath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding an accurate representation of neuron
shapes, extracting sub-cellular features, and classifying neurons based on
neuron shapes. In neuroscience research, the skeleton representation is often
used as a compact and abstract representation of neuron shapes. However,
existing methods are limited to getting and analyzing "curve" skeletons which
can only be applied for tubular shapes. This paper presents a 3D neuron
morphology analysis method for more general and complex neuron shapes. First,
we introduce the concept of skeleton mesh to represent general neuron shapes
and propose a novel method for computing mesh representations from 3D surface
point clouds. A skeleton graph is then obtained from skeleton mesh and is used
to extract sub-cellular features. Finally, an unsupervised learning method is
used to embed the skeleton graph for neuron classification. Extensive
experiment results are provided and demonstrate the robustness of our method to
analyze neuron morphology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Domain Fusion for Multi-modal Aerial View Object
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Udupa, Aniruddh Sikdar, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection and classification using aerial images is a challenging task
as the information regarding targets are not abundant. Synthetic Aperture
Radar(SAR) images can be used for Automatic Target Recognition(ATR) systems as
it can operate in all-weather conditions and in low light settings. But, SAR
images contain salt and pepper noise(speckle noise) that cause hindrance for
the deep learning models to extract meaningful features. Using just aerial view
Electro-optical(EO) images for ATR systems may also not result in high accuracy
as these images are of low resolution and also do not provide ample information
in extreme weather conditions. Therefore, information from multiple sensors can
be used to enhance the performance of Automatic Target Recognition(ATR)
systems. In this paper, we explore a methodology to use both EO and SAR sensor
information to effectively improve the performance of the ATR systems by
handling the shortcomings of each of the sensors. A novel Multi-Modal Domain
Fusion(MDF) network is proposed to learn the domain invariant features from
multi-modal data and use it to accurately classify the aerial view objects. The
proposed MDF network achieves top-10 performance in the Track-1 with an
accuracy of 25.3 % and top-5 performance in Track-2 with an accuracy of 34.26 %
in the test phase on the PBVS MAVOC Challenge dataset [18].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving group robustness under noisy labels using predictive
  uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongpin Oh, Dae Lee, Jeunghyun Byun, Bonggun Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard empirical risk minimization (ERM) can underperform on certain
minority groups (i.e., waterbirds in lands or landbirds in water) due to the
spurious correlation between the input and its label. Several studies have
improved the worst-group accuracy by focusing on the high-loss samples. The
hypothesis behind this is that such high-loss samples are
\textit{spurious-cue-free} (SCF) samples. However, these approaches can be
problematic since the high-loss samples may also be samples with noisy labels
in the real-world scenarios. To resolve this issue, we utilize the predictive
uncertainty of a model to improve the worst-group accuracy under noisy labels.
To motivate this, we theoretically show that the high-uncertainty samples are
the SCF samples in the binary classification problem. This theoretical result
implies that the predictive uncertainty is an adequate indicator to identify
SCF samples in a noisy label setting. Motivated from this, we propose a novel
ENtropy based Debiasing (END) framework that prevents models from learning the
spurious cues while being robust to the noisy labels. In the END framework, we
first train the \textit{identification model} to obtain the SCF samples from a
training set using its predictive uncertainty. Then, another model is trained
on the dataset augmented with an oversampled SCF set. The experimental results
show that our END framework outperforms other strong baselines on several
real-world benchmarks that consider both the noisy labels and the
spurious-cues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaptation for Automated Knee Osteoarthritis
  Phenotype Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junru Zhong, Yongcheng Yao, Donal G. Cahill, Fan Xiao, Siyue Li, Jack Lee, Kevin Ki-Wai Ho, Michael Tim-Yun Ong, James F. Griffith, Weitian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: The aim of this study was to demonstrate the utility of unsupervised
domain adaptation (UDA) in automated knee osteoarthritis (OA) phenotype
classification using a small dataset (n=50). Materials and Methods: For this
retrospective study, we collected 3,166 three-dimensional (3D) double-echo
steady-state magnetic resonance (MR) images from the Osteoarthritis Initiative
dataset and 50 3D turbo/fast spin-echo MR images from our institute (in 2020
and 2021) as the source and target datasets, respectively. For each patient,
the degree of knee OA was initially graded according to the MRI Osteoarthritis
Knee Score (MOAKS) before being converted to binary OA phenotype labels. The
proposed UDA pipeline included (a) pre-processing, which involved automatic
segmentation and region-of-interest cropping; (b) source classifier training,
which involved pre-training phenotype classifiers on the source dataset; (c)
target encoder adaptation, which involved unsupervised adaption of the source
encoder to the target encoder and (d) target classifier validation, which
involved statistical analysis of the target classification performance
evaluated by the area under the receiver operating characteristic curve
(AUROC), sensitivity, specificity and accuracy. Additionally, a classifier was
trained without UDA for comparison. Results: The target classifier trained with
UDA achieved improved AUROC, sensitivity, specificity and accuracy for both
knee OA phenotypes compared with the classifier trained without UDA.
Conclusion: The proposed UDA approach improves the performance of automated
knee OA phenotype classification for small target datasets by utilising a
large, high-quality source dataset for training. The results successfully
demonstrated the advantages of the UDA approach in classification on small
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Junru Zhong and Yongcheng Yao share the same contribution. 17 pages,
  4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Delineation in Satellite Images <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuocheng Shang, Ahmed Eldawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is being widely applied to analyze satellite data with
problems such as classification and feature detection. Unlike traditional image
processing algorithms, geospatial applications need to convert the detected
objects from a raster form to a geospatial vector form to further analyze it.
This gem delivers a simple and light-weight algorithm for delineating the
pixels that are marked by ML algorithms to extract geospatial objects from
satellite images. The proposed algorithm is exact and users can further apply
simplification and approximation based on the application needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 4 Figures, 1 Table, to be submitted to the 4th ACM
  SIGSPATIAL International Workshop on Spatial Gems (SpatialGems 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Zero-Shot Adversarial Robustness for Large-Scale Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, Carl Vondrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained large-scale vision-language models like CLIP have exhibited strong
generalization over unseen tasks. Yet imperceptible adversarial perturbations
can significantly reduce CLIP's performance on new tasks. In this work, we
identify and explore the problem of \emph{adapting large-scale models for
zero-shot adversarial robustness}. We first identify two key factors during
model adaption -- training losses and adaptation methods -- that affect the
model's zero-shot adversarial robustness. We then propose a text-guided
contrastive adversarial training loss, which aligns the text embeddings and the
adversarial visual features with contrastive learning on a small set of
training data. We apply this training loss to two adaption methods, model
finetuning and visual prompt tuning. We find that visual prompt tuning is more
effective in the absence of texts, while finetuning wins in the existence of
text guidance. Overall, our approach significantly improves the zero-shot
adversarial robustness over CLIP, seeing an average improvement of over 31
points over ImageNet and 15 zero-shot datasets. We hope this work can shed
light on understanding the zero-shot adversarial robustness of large-scale
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Video Anomaly Detection without Target Domain Adaptation <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Aich, Kuan-Chuan Peng, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume
that at least few task-relevant target domain training data are available for
adaptation from the source to the target domain. However, this requires
laborious model-tuning by the end-user who may prefer to have a system that
works ``out-of-the-box." To address such practical scenarios, we identify a
novel target domain (inference-time) VAD task where no target domain training
data are available. To this end, we propose a new `Zero-shot Cross-domain Video
Anomaly Detection (zxvad)' framework that includes a future-frame prediction
generative model setup. Different from prior future-frame prediction models,
our model uses a novel Normalcy Classifier module to learn the features of
normal event videos by learning how such features are different ``relatively"
to features in pseudo-abnormal examples. A novel Untrained Convolutional Neural
Network based Anomaly Synthesis module crafts these pseudo-abnormal examples by
adding foreign objects in normal video frames with no extra training cost. With
our novel relative normalcy feature learning strategy, zxvad generalizes and
learns to distinguish between normal and abnormal frames in a new target domain
without adaptation during inference. Through evaluations on common datasets, we
show that zxvad outperforms the state-of-the-art (SOTA), regardless of whether
task-relevant (i.e., VAD) source training data are available or not. Lastly,
zxvad also beats the SOTA methods in inference-time efficiency metrics
including the model size, total parameters, GPU energy consumption, and GMACs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2023; Includes Supplementary Material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Find Someone Who: Visual Commonsense Understanding in Human-Centric
  Grounding <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan You, Rui Sun, Zhecan Wang, Kai-Wei Chang, Shih-Fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From a visual scene containing multiple people, human is able to distinguish
each individual given the context descriptions about what happened before,
their mental/physical states or intentions, etc. Above ability heavily relies
on human-centric commonsense knowledge and reasoning. For example, if asked to
identify the "person who needs healing" in an image, we need to first know that
they usually have injuries or suffering expressions, then find the
corresponding visual clues before finally grounding the person. We present a
new commonsense task, Human-centric Commonsense Grounding, that tests the
models' ability to ground individuals given the context descriptions about what
happened before, and their mental/physical states or intentions. We further
create a benchmark, HumanCog, a dataset with 130k grounded commonsensical
descriptions annotated on 67k images, covering diverse types of commonsense and
visual scenes. We set up a context-object-aware method as a strong baseline
that outperforms previous pre-trained and non-pretrained models. Further
analysis demonstrates that rich visual commonsense and powerful integration of
multi-modal commonsense are essential, which sheds light on future works. Data
and code will be available https://github.com/Hxyou/HumanCog.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures. EMNLP 2022-findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localizing Objects in 3D from Egocentric Videos with Visual Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjie Mai, Abdullah Hamdi, Silvio Giancola, Chen Zhao, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advances in video and 3D understanding, novel 4D
spatio-temporal challenges fusing both concepts have emerged. Towards this
direction, the Ego4D Episodic Memory Benchmark proposed a task for Visual
Queries with 3D Localization (VQ3D). Given an egocentric video clip and an
image crop depicting a query object, the goal is to localize the 3D position of
the center of that query object with respect to the camera pose of a query
frame. Current methods tackle the problem of VQ3D by lifting the 2D
localization results of the sister task Visual Queries with 2D Localization
(VQ2D) into a 3D reconstruction. Yet, we point out that the low number of
Queries with Poses (QwP) from previous VQ3D methods severally hinders their
overall success rate and highlights the need for further effort in 3D modeling
to tackle the VQ3D task. In this work, we formalize a pipeline that better
entangles 3D multiview geometry with 2D object retrieval from egocentric
videos. We estimate more robust camera poses, leading to more successful object
queries and substantially improved VQ3D performance. In practice, our method
reaches a top-1 overall success rate of 86.36% on the Ego4D Episodic Memory
Benchmark VQ3D, a 10x improvement over the previous state-of-the-art. In
addition, we provide a complete empirical study highlighting the remaining
challenges in VQ3D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Active Solution for Two-Dimensional Face Presentation Attack
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matineh Pooshideh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identity authentication is the process of verifying one's identity. There are
several identity authentication methods, among which biometric authentication
is of utmost importance. Facial recognition is a sort of biometric
authentication with various applications, such as unlocking mobile phones and
accessing bank accounts. However, presentation attacks pose the greatest threat
to facial recognition. A presentation attack is an attempt to present a
non-live face, such as a photo, video, mask, and makeup, to the camera.
Presentation attack detection is a countermeasure that attempts to identify
between a genuine user and a presentation attack. Several industries, such as
financial services, healthcare, and education, use biometric authentication
services on various devices. This illustrates the significance of presentation
attack detection as the verification step. In this paper, we study
state-of-the-art to cover the challenges and solutions related to presentation
attack detection in a single place. We identify and classify different
presentation attack types and identify the state-of-the-art methods that could
be used to detect each of them. We compare the state-of-the-art literature
regarding attack types, evaluation metrics, accuracy, and datasets and discuss
research and industry challenges of presentation attack detection. Most
presentation attack detection approaches rely on extensive data training and
quality, making them difficult to implement. We introduce an efficient active
presentation attack detection approach that overcomes weaknesses in the
existing literature. The proposed approach does not require training data, is
CPU-light, can process low-quality images, has been tested with users of
various ages and is shown to be user-friendly and highly robust to
2-dimensional presentation attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we make virtual characters in a scene interact with their surrounding
objects through simple instructions? Is it possible to synthesize such motion
plausibly with a diverse set of objects and instructions? Inspired by these
questions, we present the first framework to synthesize the full-body motion of
virtual human characters performing specified actions with 3D objects placed
within their reach. Our system takes as input textual instructions specifying
the objects and the associated intentions of the virtual characters and outputs
diverse sequences of full-body motions. This is in contrast to existing work,
where full-body action synthesis methods generally do not consider object
interactions, and human-object interaction methods focus mainly on synthesizing
hand or finger movements for grasping objects. We accomplish our objective by
designing an intent-driven full-body motion generator, which uses a pair of
decoupled conditional variational autoencoders (CVAE) to learn the motion of
the body parts in an autoregressive manner. We also optimize for the positions
of the objects with six degrees of freedom (6DoF) such that they plausibly fit
within the hands of the synthesized characters. We compare our proposed method
with the existing methods of motion synthesis and establish a new and stronger
state-of-the-art for the task of intent-driven motion synthesis. Through a user
study, we further show that our synthesized full-body motions appear more
realistic to the participants in more than 80% of scenarios compared to the
current state-of-the-art methods, and are perceived to be as good as the ground
truth on several occasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plastic Contaminant Detection in Aerial Imagery of Cotton Fields with
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pappu Kumar Yadav, J. Alex Thomasson, Robert G. Hardin, Stephen W. Searcy, Ulisses Braga-Neto, Sorin C. Popescu, Roberto Rodriguez, Daniel E Martin, Juan Enciso, Karem Meza, Emma L. White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plastic shopping bags that get carried away from the side of roads and
tangled on cotton plants can end up at cotton gins if not removed before the
harvest. Such bags may not only cause problem in the ginning process but might
also get embodied in cotton fibers reducing its quality and marketable value.
Therefore, it is required to detect, locate, and remove the bags before cotton
is harvested. Manually detecting and locating these bags in cotton fields is
labor intensive, time-consuming and a costly process. To solve these
challenges, we present application of four variants of YOLOv5 (YOLOv5s,
YOLOv5m, YOLOv5l and YOLOv5x) for detecting plastic shopping bags using
Unmanned Aircraft Systems (UAS)-acquired RGB (Red, Green, and Blue) images. We
also show fixed effect model tests of color of plastic bags as well as
YOLOv5-variant on average precision (AP), mean average precision (mAP@50) and
accuracy. In addition, we also demonstrate the effect of height of plastic bags
on the detection accuracy. It was found that color of bags had significant
effect (p < 0.001) on accuracy across all the four variants while it did not
show any significant effect on the AP with YOLOv5m (p = 0.10) and YOLOv5x (p =
0.35) at 95% confidence level. Similarly, YOLOv5-variant did not show any
significant effect on the AP (p = 0.11) and accuracy (p = 0.73) of white bags,
but it had significant effects on the AP (p = 0.03) and accuracy (p = 0.02) of
brown bags including on the mAP@50 (p = 0.01) and inference speed (p < 0.0001).
Additionally, height of plastic bags had significant effect (p < 0.0001) on
overall detection accuracy. The findings reported in this paper can be useful
in speeding up removal of plastic bags from cotton fields before harvest and
thereby reducing the amount of contaminants that end up at cotton gins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Probabilistic Models beat GANs on Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustav Müller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Deep Learning applications critically depends on the quality
and scale of the underlying training data. Generative adversarial networks
(GANs) can generate arbitrary large datasets, but diversity and fidelity are
limited, which has recently been addressed by denoising diffusion probabilistic
models (DDPMs) whose superiority has been demonstrated on natural images. In
this study, we propose Medfusion, a conditional latent DDPM for medical images.
We compare our DDPM-based model against GAN-based models, which constitute the
current state-of-the-art in the medical domain. Medfusion was trained and
compared with (i) StyleGan-3 on n=101,442 images from the AIROGS challenge
dataset to generate fundoscopies with and without glaucoma, (ii) ProGAN on
n=191,027 from the CheXpert dataset to generate radiographs with and without
cardiomegaly and (iii) wGAN on n=19,557 images from the CRCMS dataset to
generate histopathological images with and without microsatellite stability. In
the AIROGS, CRMCS, and CheXpert datasets, Medfusion achieved lower (=better)
FID than the GANs (11.63 versus 20.43, 30.03 versus 49.26, and 17.28 versus
84.31). Also, fidelity (precision) and diversity (recall) were higher (=better)
for Medfusion in all three datasets. Our study shows that DDPM are a superior
alternative to GANs for image synthesis in the medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards fully automated deep-learning-based brain tumor segmentation: is
  brain extraction still necessary? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Machado Pacheco, Guilherme de Souza e Cassia, Danilo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art brain tumor segmentation is based on deep learning models
applied to multi-modal MRIs. Currently, these models are trained on images
after a preprocessing stage that involves registration, interpolation, brain
extraction (BE, also known as skull-stripping) and manual correction by an
expert. However, for clinical practice, this last step is tedious and
time-consuming and, therefore, not always feasible, resulting in
skull-stripping faults that can negatively impact the tumor segmentation
quality. Still, the extent of this impact has never been measured for any of
the many different BE methods available. In this work, we propose an automatic
brain tumor segmentation pipeline and evaluate its performance with multiple BE
methods. Our experiments show that the choice of a BE method can compromise up
to 15.7% of the tumor segmentation performance. Moreover, we propose training
and testing tumor segmentation models on non-skull-stripped images, effectively
discarding the BE step from the pipeline. Our results show that this approach
leads to a competitive performance at a fraction of the time. We conclude that,
in contrast to the current paradigm, training tumor segmentation models on
non-skull-stripped images can be the best option when high performance in
clinical practice is desired.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAIF: Sparse Adversarial and Interpretable Attack Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps, Jennifer Dy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks hamper the decision-making ability of neural networks by
perturbing the input signal. The addition of calculated small distortion to
images, for instance, can deceive a well-trained image classification network.
In this work, we propose a novel attack technique called Sparse Adversarial and
Interpretable Attack Framework (SAIF). Specifically, we design imperceptible
attacks that contain low-magnitude perturbations at a small number of pixels
and leverage these sparse attacks to reveal the vulnerability of classifiers.
We use the Frank-Wolfe (conditional gradient) algorithm to simultaneously
optimize the attack perturbations for bounded magnitude and sparsity with
$O(1/\sqrt{T})$ convergence. Empirical results show that SAIF computes highly
imperceptible and interpretable adversarial examples, and outperforms
state-of-the-art sparse attack methods on the ImageNet dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The text-to-image model Stable Diffusion has recently become very popular.
Only weeks after its open source release, millions are experimenting with image
generation. This is due to its ease of use, since all it takes is a brief
description of the desired image to "prompt" the generative model. Rarely do
the images generated for a new prompt immediately meet the user's expectations.
Usually, an iterative refinement of the prompt ("prompt engineering") is
necessary for satisfying images. As a new perspective, we recast image prompt
engineering as interactive image retrieval - on an "infinite index". Thereby, a
prompt corresponds to a query and prompt engineering to query refinement.
Selected image-prompt pairs allow direct relevance feedback, as the model can
modify an image for the refined prompt. This is a form of one-sided interactive
retrieval, where the initiative is on the user side, whereas the server side
remains stateless. In light of an extensive literature review, we develop these
parallels in detail and apply the findings to a case study of a creative search
task on such a model. We note that the uncertainty in searching an infinite
index is virtually never-ending. We also discuss future research opportunities
related to retrieval models specialized for generative models and interactive
generative image retrieval. The application of IR technology, such as query
reformulation and relevance feedback, will contribute to improved workflows
when using generative models, while the notion of an infinite index raises new
challenges in IR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RTMDet: An Empirical Study of Designing Real-Time Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to design an efficient real-time object detector that
exceeds the YOLO series and is easily extensible for many object recognition
tasks such as instance segmentation and rotated object detection. To obtain a
more efficient model architecture, we explore an architecture that has
compatible capacities in the backbone and neck, constructed by a basic building
block that consists of large-kernel depth-wise convolutions. We further
introduce soft labels when calculating matching costs in the dynamic label
assignment to improve accuracy. Together with better training techniques, the
resulting object detector, named RTMDet, achieves 52.8% AP on COCO with 300+
FPS on an NVIDIA 3090 GPU, outperforming the current mainstream industrial
detectors. RTMDet achieves the best parameter-accuracy trade-off with
tiny/small/medium/large/extra-large model sizes for various application
scenarios, and obtains new state-of-the-art performance on real-time instance
segmentation and rotated object detection. We hope the experimental results can
provide new insights into designing versatile real-time object detectors for
many object recognition tasks. Code and models are released at
https://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergent Data-driven Regularizations for CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Kabri, Alexander Auras, Danilo Riccio, Hartmut Bauermeister, Martin Benning, Michael Moeller, Martin Burger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of images from their corresponding noisy Radon transform
is a typical example of an ill-posed linear inverse problem as arising in the
application of computerized tomography (CT). As the (na\"{\i}ve) solution does
not depend on the measured data continuously, regularization is needed to
re-establish a continuous dependence. In this work, we investigate simple, but
yet still provably convergent approaches to learning linear regularization
methods from data. More specifically, we analyze two approaches: One generic
linear regularization that learns how to manipulate the singular values of the
linear operator in an extension of [1], and one tailored approach in the
Fourier domain that is specific to CT-reconstruction. We prove that such
approaches become convergent regularization methods as well as the fact that
the reconstructions they provide are typically much smoother than the training
data they were trained on. Finally, we compare the spectral as well as the
Fourier-based approaches for CT-reconstruction numerically, discuss their
advantages and disadvantages and investigate the effect of discretization
errors at different resolutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projection-Domain Self-Supervision for Volumetric Helical CT
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onni Kosomaa, Samuli Laine, Tero Karras, Miika Aittala, Jaakko Lehtinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a deep learning method for three-dimensional reconstruction in
low-dose helical cone-beam computed tomography. We reconstruct the volume
directly, i.e., not from 2D slices, guaranteeing consistency along all axes. In
a crucial step beyond prior work, we train our model in a self-supervised
manner in the projection domain using noisy 2D projection data, without relying
on 3D reference data or the output of a reference reconstruction method. This
means the fidelity of our results is not limited by the quality and
availability of such data. We evaluate our method on real helical cone-beam
projections and simulated phantoms. Our reconstructions are sharper and less
noisy than those of previous methods, and several decibels better in
quantitative PSNR measurements. When applied to full-dose data, our method
produces high-quality results orders of magnitude faster than iterative
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sparse Training via More Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyi Huang, Bowen Lei, Dongkuan Xu, Hongwu Peng, Yue Sun, Mimi Xie, Caiwen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-parameterization of deep neural networks (DNNs) has shown high
prediction accuracy for many applications. Although effective, the large number
of parameters hinders its popularity on resource-limited devices and has an
outsize environmental impact. Sparse training (using a fixed number of nonzero
weights in each iteration) could significantly mitigate the training costs by
reducing the model size. However, existing sparse training methods mainly use
either random-based or greedy-based drop-and-grow strategies, resulting in
local minimal and low accuracy. In this work, we consider the dynamic sparse
training as a sparse connectivity search problem and design an exploitation and
exploration acquisition function to escape from local optima and saddle points.
We further design an acquisition function and provide the theoretical
guarantees for the proposed method and clarify its convergence property.
Experimental results show that sparse models (up to 98\% sparsity) obtained by
our proposed method outperform the SOTA sparse training methods on a wide
variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10,
ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models.
On ResNet-50 / ImageNet, the proposed method has up to 8.2\% accuracy
improvement compared to SOTA sparse training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REPAIR: REnormalizing Permuted Activations for Interpolation Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, Behnam Neyshabur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmentation-guided Domain Adaptation for Efficient Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Märkert, Martin Sunkel, Anselm Haselhoff, Stefan Rudolph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complete depth information and efficient estimators have become vital
ingredients in scene understanding for automated driving tasks. A major problem
for LiDAR-based depth completion is the inefficient utilization of convolutions
due to the lack of coherent information as provided by the sparse nature of
uncorrelated LiDAR point clouds, which often leads to complex and
resource-demanding networks. The problem is reinforced by the expensive
aquisition of depth data for supervised training. In this work, we propose an
efficient depth completion model based on a vgg05-like CNN architecture and
propose a semi-supervised domain adaptation approach to transfer knowledge from
synthetic to real world data to improve data-efficiency and reduce the need for
a large database. In order to boost spatial coherence, we guide the learning
process using segmentations as additional source of information. The efficiency
and accuracy of our approach is evaluated on the KITTI dataset. Our approach
improves on previous efficient and low parameter state of the art approaches
while having a noticeably lower computational footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TreEnhance: A Tree Search Method For Low-Light Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Cotogni, Claudio Cusano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present TreEnhance, an automatic method for low-light image
enhancement capable of improving the quality of digital images. The method
combines tree search theory, and in particular the Monte Carlo Tree Search
(MCTS) algorithm, with deep reinforcement learning. Given as input a low-light
image, TreEnhance produces as output its enhanced version together with the
sequence of image editing operations used to obtain it. During the training
phase, the method repeatedly alternates two main phases: a generation phase,
where a modified version of MCTS explores the space of image editing operations
and selects the most promising sequence, and an optimization phase, where the
parameters of a neural network, implementing the enhancement policy, are
updated.
  Two different inference solutions are proposed for the enhancement of new
images: one is based on MCTS and is more accurate but more time and memory
consuming; the other directly applies the learned policy and is faster but
slightly less precise. As a further contribution, we propose a guided search
strategy that "reverses" the enhancement procedure that a photo editor applied
to a given input image. Unlike other methods from the state of the art,
TreEnhance does not pose any constraint on the image resolution and can be used
in a variety of scenarios with minimal tuning. We tested the method on two
datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good
results from both a qualitative and a quantitative point of view.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S^2-<span class="highlight-title">Transformer</span> for Mask-Aware Hyperspectral Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiamian Wang, Kunpeng Li, Yulun Zhang, Xin Yuan, Zhiqiang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technology of hyperspectral imaging (HSI) records the visual information
upon long-range-distributed spectral wavelengths. A representative
hyperspectral image acquisition procedure conducts a 3D-to-2D encoding by the
coded aperture snapshot spectral imager (CASSI) and requires a software decoder
for the 3D signal reconstruction. By observing this physical encoding
procedure, two major challenges stand in the way of a high-fidelity
reconstruction. (i) To obtain 2D measurements, CASSI dislocates multiple
channels by disperser-titling and squeezes them onto the same spatial region,
yielding an entangled data loss. (ii) The physical coded aperture leads to a
masked data loss by selectively blocking the pixel-wise light exposure. To
tackle these challenges, we propose a spatial-spectral (S^2-) Transformer
network with a mask-aware learning strategy. First, we simultaneously leverage
spatial and spectral attention modeling to disentangle the blended information
in the 2D measurement along both two dimensions. A series of Transformer
structures are systematically designed to fully investigate the spatial and
spectral informative properties of the hyperspectral data. Second, the masked
pixels will induce higher prediction difficulty and should be treated
differently from unmasked ones. Thereby, we adaptively prioritize the loss
penalty attributing to the mask structure by inferring the pixel-wise
reconstruction difficulty upon the mask-encoded prediction. We theoretically
discusses the distinct convergence tendencies between masked/unmasked regions
of the proposed learning strategy. Extensive experiments demonstrates that the
proposed method achieves superior reconstruction performance. Additionally, we
empirically elaborate the behaviour of spatial and spectral attentions under
the proposed architecture, and comprehensively examine the impact of the
mask-aware learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 16 figures, 6 tables, Code:
  https://github.com/Jiamian-Wang/S2-transformer-HSI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation
  techniques for Vestibular Schwannoma and Cochlea Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.02831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.02831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reuben Dorent, Aaron Kujawa, Marina Ivory, Spyridon Bakas, Nicola Rieke, Samuel Joutard, Ben Glocker, Jorge Cardoso, Marc Modat, Kayhan Batmanghelich, Arseniy Belkov, Maria Baldeon Calisto, Jae Won Choi, Benoit M. Dawant, Hexin Dong, Sergio Escalera, Yubo Fan, Lasse Hansen, Mattias P. Heinrich, Smriti Joshi, Victoriya Kashtanova, Hyeon Gyu Kim, Satoshi Kondo, Christian N. Kruse, Susana K. Lai-Yuen, Hao Li, Han Liu, Buntheng Ly, Ipek Oguz, Hyungseob Shin, Boris Shirokikh, Zixian Su, Guotai Wang, Jianghao Wu, Yanwu Xu, Kai Yao, Li Zhang, Sebastien Ourselin, Jonathan Shapey, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Adaptation (DA) has recently raised strong interests in the medical
imaging community. While a large variety of DA techniques has been proposed for
image segmentation, most of these techniques have been validated either on
private datasets or on small publicly available datasets. Moreover, these
datasets mostly addressed single-class problems. To tackle these limitations,
the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in
conjunction with the 24th International Conference on Medical Image Computing
and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large
and multi-class benchmark for unsupervised cross-modality DA. The challenge's
goal is to segment two key brain structures involved in the follow-up and
treatment planning of vestibular schwannoma (VS): the VS and the cochleas.
Currently, the diagnosis and surveillance in patients with VS are performed
using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in
using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,
we created an unsupervised cross-modality segmentation benchmark. The training
set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).
The aim was to automatically perform unilateral VS and bilateral cochlea
segmentation on hrT2 as provided in the testing set (N=137). A total of 16
teams submitted their algorithm for the evaluation phase. The level of
performance reached by the top-performing teams is strikingly high (best median
Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -
VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an
image-to-image translation approach to transform the source-domain images into
pseudo-target-domain images. A segmentation network was then trained using
these generated images and the manual annotations provided for the source
image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSNet: a simple yet efficient network with dual-stream attention for
  lesion segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lesion segmentation requires both speed and accuracy. In this paper, we
propose a simple yet efficient network DSNet, which consists of a encoder based
on Transformer and a convolutional neural network(CNN)-based distinct pyramid
decoder containing three dual-stream attention (DSA) modules. Specifically, the
DSA module fuses features from two adjacent levels through the false positive
stream attention (FPSA) branch and the false negative stream attention (FNSA)
branch to obtain features with diversified contextual information. We compare
our method with various state-of-the-art (SOTA) lesion segmentation methods
with several public datasets, including CVC-ClinicDB, Kvasir-SEG, and ISIC-2018
Task 1. The experimental results show that our method achieves SOTA performance
in terms of mean Dice coefficient (mDice) and mean Intersection over Union
(mIoU) with low model complexity and memory consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSMTL++: Revisiting <span class="highlight-title">Self-Supervised</span> Multi-Task Learning for Video
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Barbalau, Radu Tudor Ionescu, Mariana-Iuliana Georgescu, Jacob Dueholm, Bharathkumar Ramachandra, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moeslund, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A self-supervised multi-task learning (SSMTL) framework for video anomaly
detection was recently introduced in literature. Due to its highly accurate
results, the method attracted the attention of many researchers. In this work,
we revisit the self-supervised multi-task learning framework, proposing several
updates to the original method. First, we study various detection methods, e.g.
based on detecting high-motion regions using optical flow or background
subtraction, since we believe the currently used pre-trained YOLOv3 is
suboptimal, e.g. objects in motion or objects from unknown classes are never
detected. Second, we modernize the 3D convolutional backbone by introducing
multi-head self-attention modules, inspired by the recent success of vision
transformers. As such, we alternatively introduce both 2D and 3D convolutional
vision transformer (CvT) blocks. Third, in our attempt to further improve the
model, we study additional self-supervised learning tasks, such as predicting
segmentation maps through knowledge distillation, solving jigsaw puzzles,
estimating body pose through knowledge distillation, predicting masked regions
(inpainting), and adversarial learning with pseudo-anomalies. We conduct
experiments to assess the performance impact of the introduced changes. Upon
finding more promising configurations of the framework, dubbed SSMTL++v1 and
SSMTL++v2, we extend our preliminary experiments to more data sets,
demonstrating that our performance gains are consistent across all data sets.
In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the
state-of-the-art performance bar to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration at Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot
  Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to be generally useful, they must be able to find arbitrary
objects described by people (i.e., be language-driven) even without expensive
navigation training on in-domain data (i.e., perform zero-shot inference). We
explore these capabilities in a unified setting: language-driven zero-shot
object navigation (L-ZSON). Inspired by the recent success of open-vocabulary
models for image classification, we investigate a straightforward framework,
CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without
fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark,
which considers finding uncommon objects, objects described by spatial and
appearance attributes, and hidden objects described relative to visible
objects. We conduct an in-depth empirical study by directly deploying 21 CoW
baselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k
navigation episodes and find that (1) CoW baselines often struggle to leverage
language descriptions, but are proficient at finding uncommon objects. (2) A
simple CoW, with CLIP-based object localization and classical exploration --
and no additional training -- matches the navigation efficiency of a
state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This
same CoW provides a 15.6 percentage point improvement in success over a
state-of-the-art RoboTHOR ZSON model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepIPC: Deeply Integrated Perception and Control for an Autonomous
  Vehicle in Real Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Natan, Jun Miura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DeepIPC, an end-to-end autonomous driving model that handles both
perception and control tasks in driving a vehicle. The model consists of two
main parts, perception and controller modules. The perception module takes an
RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic
mapping along with providing their encoded features. Meanwhile, the controller
module processes these features with the measurement of GNSS locations and
angular speed to estimate waypoints that come with latent features. Then, two
different agents are used to translate waypoints and latent features into a set
of navigational controls to drive the vehicle. The model is evaluated by
predicting driving records and performing automated driving under various
conditions in real environments. The experimental results show that DeepIPC
achieves the best drivability and multi-task performance even with fewer
parameters compared to the other models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testing Human Ability To Detect Deepfake Images of Human Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergi D. Bray, Shane D. Johnson, Bennett Kleinberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes are computationally-created entities that falsely represent
reality. They can take image, video, and audio modalities, and pose a threat to
many areas of systems and societies, comprising a topic of interest to various
aspects of cybersecurity and cybersafety. In 2020 a workshop consulting AI
experts from academia, policing, government, the private sector, and state
security agencies ranked deepfakes as the most serious AI threat. These experts
noted that since fake material can propagate through many uncontrolled routes,
changes in citizen behaviour may be the only effective defence. This study aims
to assess human ability to identify image deepfakes of human faces
(StyleGAN2:FFHQ) from nondeepfake images (FFHQ), and to assess the
effectiveness of simple interventions intended to improve detection accuracy.
Using an online survey, 280 participants were randomly allocated to one of four
groups: a control group, and 3 assistance interventions. Each participant was
shown a sequence of 20 images randomly selected from a pool of 50 deepfake and
50 real images of human faces. Participants were asked if each image was
AI-generated or not, to report their confidence, and to describe the reasoning
behind each response. Overall detection accuracy was only just above chance and
none of the interventions significantly improved this. Participants' confidence
in their answers was high and unrelated to accuracy. Assessing the results on a
per-image basis reveals participants consistently found certain images harder
to label correctly, but reported similarly high confidence regardless of the
image. Thus, although participant accuracy was 62% overall, this accuracy
across images ranged quite evenly between 85% and 30%, with an accuracy of
below 50% for one in every five images. We interpret the findings as suggesting
that there is a need for an urgent call to action to address this threat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZippyPoint: Fast Interest Point Detection, Description, and Matching
  through Mixed Precision Discretization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menelaos Kanakis, Simon Maurer, Matteo Spallanzani, Ajad Chhatkuli, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient detection and description of geometric regions in images is a
prerequisite in visual systems for localization and mapping. Such systems still
rely on traditional hand-crafted methods for efficient generation of
lightweight descriptors, a common limitation of the more powerful neural
network models that come with high compute and specific hardware requirements.
In this paper, we focus on the adaptations required by detection and
description neural networks to enable their use in computationally limited
platforms such as robots, mobile, and augmented reality devices. To that end,
we investigate and adapt network quantization techniques to accelerate
inference and enable its use on compute limited platforms. In addition, we
revisit common practices in descriptor quantization and propose the use of a
binary descriptor normalization layer, enabling the generation of distinctive
binary descriptors with a constant number of ones. ZippyPoint, our efficient
quantized network with binary descriptors, improves the network runtime speed,
the descriptor matching speed, and the 3D model size, by at least an order of
magnitude when compared to full-precision counterparts. These improvements come
at a minor performance degradation as evaluated on the tasks of homography
estimation, visual localization, and map-free visual relocalization. Code and
trained models will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brazilian Data at Risk in the Age of AI? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoni F. da S. Teixeira, Rafael B. Januzi, Fabio A. Faria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in image processing and analysis as well as machine learning
techniques have contributed to the use of biometric recognition systems in
daily people tasks. These tasks range from simple access to mobile devices to
tagging friends in photos shared on social networks and complex financial
operations on self-service devices for banking transactions. In China, the use
of these systems goes beyond personal use becoming a country's government
policy with the objective of monitoring the behavior of its population. On July
05th 2021, the Brazilian government announced acquisition of a biometric
recognition system to be used nationwide. In the opposite direction to China,
Europe and some American cities have already started the discussion about the
legality of using biometric systems in public places, even banning this
practice in their territory. In order to open a deeper discussion about the
risks and legality of using these systems, this work exposes the
vulnerabilities of biometric recognition systems, focusing its efforts on the
face modality. Furthermore, it shows how it is possible to fool a biometric
system through a well-known presentation attack approach in the literature
called morphing. Finally, a list of ten concerns was created to start the
discussion about the security of citizen data and data privacy law in the Age
of Artificial Intelligence (AI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages in Portuguese and 5 figures, Top 3 among the best papers at
  the ENIAC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-signed prioritized neural fitting for surface reconstruction from
  unoriented point clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.06715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.06715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runsong Zhu, Di Kang, Ka-Hei Hui, Yue Qian, Xuefei Zhe, Zhen Dong, Linchao Bao, Pheng-Ann Heng, Chi-Wing Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D geometry from \emph{unoriented} point clouds can benefit
many downstream tasks. Recent shape modeling methods mostly adopt implicit
neural representation to fit a signed distance field (SDF) and optimize the
network by \emph{unsigned} supervision. However, these methods occasionally
have difficulty in finding the coarse shape for complicated objects, especially
suffering from the ``ghost'' surfaces (\ie, fake surfaces that should not
exist). To guide the network quickly fit the coarse shape, we propose to
utilize the signed supervision in regions that are obviously outside the object
and can be easily determined, resulting in our semi-signed supervision. To
better recover high-fidelity details, a novel importance sampling based on
tracked region losses and a progressive positional encoding (PE) prioritize the
optimization towards underfitting and complicated regions. Specifically, we
voxelize and partition the object space into \emph{sign-known} and
\emph{sign-uncertain} regions, in which different supervisions are applied.
Besides, we adaptively adjust the sampling rate of each voxel according to the
tracked reconstruction loss, so that the network can focus more on the
complicated under-fitting regions. To this end, we propose our semi-signed
prioritized (SSP) neural fitting, and conduct extensive experiments to
demonstrate that SSP achieves state-of-the-art performance on multiple datasets
including the ABC subset and various challenging data. The code will be
released upon the publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreeSeg: Free Mask from Interpretable Contrastive Language-Image
  <span class="highlight-title">Pretrain</span>ing for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Huifeng Yao, Hualiang Wang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully supervised semantic segmentation learns from dense masks, which
requires heavy annotation cost for closed set. In this paper, we use natural
language as supervision without any pixel-level annotation for open world
segmentation. We call the proposed framework as FreeSeg, where the mask is
freely available from raw feature map of pretraining model. Compared with
zero-shot or openset segmentation, FreeSeg doesn't require any annotated masks,
and it widely predicts categories beyond class-agnostic unsupervised
segmentation. Specifically, FreeSeg obtains free mask from Image-Text
Similarity Map (ITSM) of Interpretable Contrastive Language-Image Pretraining
(ICLIP). And our core improvements are the smoothed min pooling for dense
ICLIP, with the partial label and pixel strategies for segmentation.
Furthermore, FreeSeg is very straight forward without complex design like
grouping, clustering or retrieval. Besides the simplicity, the performances of
FreeSeg surpass previous state-of-the-art at large margins, e.g. 13.4% higher
at mIoU on VOC dataset in the same settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper contains some immature results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial
  Expression Recognition <span class="chip">ICPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.11053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.11053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Xingming Zhang, Janne Kauttonen, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality annotated images are significant to deep facial expression
recognition (FER) methods. However, uncertain labels, mostly existing in
large-scale public datasets, often mislead the training process. In this paper,
we achieve uncertain label correction of facial expressions using auxiliary
action unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization
module is introduced to highlight valid samples and suppress category imbalance
in every batch. Based on the latent dependency between emotions and AUs, an
auxiliary branch using graph convolutional layers is added to extract the
semantic information from graph topologies. Finally, a re-labeling strategy
corrects the ambiguous annotations by comparing their feature similarities with
semantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57%
accuracy on RAF-DB and AffectNet datasets, respectively, outperforming the
baseline and state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, accecpted by ICPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Recognition using Region Creep 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1909.10811v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1909.10811v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kieran Greer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a new type of auto-associative image classifier that
uses a shallow architecture with a very quick learning phase. The image is
parsed into smaller areas and each area is saved directly for a region, along
with the related output category. When a new image is presented, a direct match
with each region is made and the best matching areas returned. Each area stores
a list of the categories it belongs to, where there is a one-to-many relation
between the input region and the output categories. The image classification
process sums the category lists to return a preferred category for the whole
image. These areas can overlap with each other and when moving from a region to
its neighbours, there is likely to be only small changes in the area image
part. It would therefore be possible to guess what the best image area is for
one region by cumulating the results of its neighbours. This associative
feature is being called 'Region Creep' and the cumulated region can be compared
with train cases instead, when a suitable match is not found. Rules can be
included and state that: if one set of pixels are present, another set should
either be removed or should also be present, where this is across the whole
image. The memory problems with a traditional auto-associative network may be
less with this version and tests on a set of hand-written numbers have produced
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Statistical Significance of Neural Network-based Image
  Segmentation by Selective Inference <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.01823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.01823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vo Nguyen Le Duy, Shogo Iwazaki, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although a vast body of literature relates to image segmentation methods that
use deep neural networks (DNNs), less attention has been paid to assessing the
statistical reliability of segmentation results. In this study, we interpret
the segmentation results as hypotheses driven by DNN (called DNN-driven
hypotheses) and propose a method by which to quantify the reliability of these
hypotheses within a statistical hypothesis testing framework. Specifically, we
consider a statistical hypothesis test for the difference between the object
and background regions. This problem is challenging, as the difference would be
falsely large because of the adaptation of the DNN to the data. To overcome
this difficulty, we introduce a conditional selective inference (SI) framework
-- a new statistical inference framework for data-driven hypotheses that has
recently received considerable attention -- to compute exact (non-asymptotic)
valid p-values for the segmentation results. To use the conditional SI
framework for DNN-based segmentation, we develop a new SI algorithm based on
the homotopy method, which enables us to derive the exact (non-asymptotic)
sampling distribution of DNN-driven hypothesis. We conduct experiments on both
synthetic and real-world datasets, through which we offer evidence that our
proposed method can successfully control the false positive rate, has good
performance in terms of computational efficiency, and provides good results
when applied to medical image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Attention Network for Few-Shot Object Detection via
  Meta-Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwoo Park, Jong-Min Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection (FSOD) aims to classify and detect few images of
novel categories. Existing meta-learning methods insufficiently exploit
features between support and query images owing to structural limitations. We
propose a hierarchical attention network with sequentially large receptive
fields to fully exploit the query and support images. In addition,
meta-learning does not distinguish the categories well because it determines
whether the support and query images match. In other words, metric-based
learning for classification is ineffective because it does not work directly.
Thus, we propose a contrastive learning method called meta-contrastive
learning, which directly helps achieve the purpose of the meta-learning
strategy. Finally, we establish a new state-of-the-art network, by realizing
significant margins. Our method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP
improvements for 1-30 shots object detection on COCO dataset. Our code is
available at: https://github.com/infinity7428/hANMCL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-fabrication Targeted Attack for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchong Zhang, Changfeng Sun, Haoliang Han, Hang Wang, Hongbin Sun, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent researches show that the deep learning based object detection is
vulnerable to adversarial examples. Generally, the adversarial attack for
object detection contains targeted attack and untargeted attack. According to
our detailed investigations, the research on the former is relatively fewer
than the latter and all the existing methods for the targeted attack follow the
same mode, i.e., the object-mislabeling mode that misleads detectors to
mislabel the detected object as a specific wrong label. However, this mode has
limited attack success rate, universal and generalization performances. In this
paper, we propose a new object-fabrication targeted attack mode which can
mislead detectors to `fabricate' extra false objects with specific target
labels. Furthermore, we design a dual attention based targeted feature space
attack method to implement the proposed targeted attack mode. The attack
performances of the proposed mode and method are evaluated on MS COCO and
BDD100K datasets using FasterRCNN and YOLOv5. Evaluation results demonstrate
that, the proposed object-fabrication targeted attack mode and the
corresponding targeted feature space attack method show significant
improvements in terms of image-specific attack, universal performance and
generalization capability, compared with the previous targeted attack for
object detection. Code will be made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Correlation Mining Network for Person Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Wang, Xingqun Qi, Kun Yuan, Muyi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person image generation aims to perform non-rigid deformation on source
images, which generally requires unaligned data pairs for training. Recently,
self-supervised methods express great prospects in this task by merging the
disentangled representations for self-reconstruction. However, such methods
fail to exploit the spatial correlation between the disentangled features. In
this paper, we propose a Self-supervised Correlation Mining Network (SCM-Net)
to rearrange the source images in the feature space, in which two collaborative
modules are integrated, Decomposed Style Encoder (DSE) and Correlation Mining
Module (CMM). Specifically, the DSE first creates unaligned pairs at the
feature level. Then, the CMM establishes the spatial correlation field for
feature rearrangement. Eventually, a translation module transforms the
rearranged features to realistic results. Meanwhile, for improving the fidelity
of cross-scale pose transformation, we propose a graph based Body Structure
Retaining Loss (BSR Loss) to preserve reasonable body structures on half body
to full body generation. Extensive experiments conducted on DeepFashion dataset
demonstrate the superiority of our method compared with other supervised and
unsupervised approaches. Furthermore, satisfactory results on face generation
show the versatility of our method in other deformation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RetiFluidNet: A Self-Adaptive and Multi-Attention Deep Convolutional
  Network for Retinal OCT Fluid Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Rasti, Armin Biglari, Mohammad Rezapourian, Ziyun Yang, Sina Farsiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical coherence tomography (OCT) helps ophthalmologists assess macular
edema, accumulation of fluids, and lesions at microscopic resolution.
Quantification of retinal fluids is necessary for OCT-guided treatment
management, which relies on a precise image segmentation step. As manual
analysis of retinal fluids is a time-consuming, subjective, and error-prone
task, there is increasing demand for fast and robust automatic solutions. In
this study, a new convolutional neural architecture named RetiFluidNet is
proposed for multi-class retinal fluid segmentation. The model benefits from
hierarchical representation learning of textural, contextual, and edge features
using a new self-adaptive dual-attention (SDA) module, multiple self-adaptive
attention-based skip connections (SASC), and a novel multi-scale deep self
supervision learning (DSL) scheme. The attention mechanism in the proposed SDA
module enables the model to automatically extract deformation-aware
representations at different levels, and the introduced SASC paths further
consider spatial-channel interdependencies for concatenation of counterpart
encoder and decoder units, which improve representational capability.
RetiFluidNet is also optimized using a joint loss function comprising a
weighted version of dice overlap and edge-preserved connectivity-based losses,
where several hierarchical stages of multi-scale local losses are integrated
into the optimization process. The model is validated based on three publicly
available datasets: RETOUCH, OPTIMA, and DUKE, with comparisons against several
baselines. Experimental results on the datasets prove the effectiveness of the
proposed model in retinal OCT fluid segmentation and reveal that the suggested
method is more effective than existing state-of-the-art fluid segmentation
algorithms in adapting to retinal OCT scans recorded by various image scanning
instruments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Early Access Version, IEEE Transactions on Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological Network: How Far Can We Go with Morphological Neurons? <span class="chip">BMVC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1901.00109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1901.00109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Mondal, Sanchayan Santra, Soumendu Sundar Mukherjee, Bhabatosh Chanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Morphological neurons, that is morphological operators such as dilation and
erosion with learnable structuring elements, have intrigued researchers for
quite some time because of the power these operators bring to the table despite
their simplicity. These operators are known to be powerful nonlinear tools, but
for a given problem coming up with a sequence of operations and their
structuring element is a non-trivial task. So, the existing works have mainly
focused on this part of the problem without delving deep into their
applicability as generic operators. A few works have tried to utilize
morphological neurons as a part of classification (and regression) networks
when the input is a feature vector. However, these methods mainly focus on a
specific problem, without going into generic theoretical analysis. In this
work, we have theoretically analyzed morphological neurons and have shown that
these are far more powerful than previously anticipated. Our proposed
morphological block, containing dilation and erosion followed by their linear
combination, represents a sum of hinge functions. Existing works show that
hinge functions perform quite well in classification and regression problems.
Two morphological blocks can even approximate any continuous function. However,
to facilitate the theoretical analysis that we have done in this paper, we have
restricted ourselves to the 1D version of the operators, where the structuring
element operates on the whole input. Experimental evaluations also indicate the
effectiveness of networks built with morphological neurons, over similarly
structured neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BMVC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Liu, Noam Aigerman, Vladimir G. Kim, Rana Hanocka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural technique for learning to select a local sub-region
around a point which can be used for mesh parameterization. The motivation for
our framework is driven by interactive workflows used for decaling, texturing,
or painting on surfaces. Our key idea is to incorporate segmentation
probabilities as weights of a classical parameterization method, implemented as
a novel differentiable parameterization layer within a neural network
framework. We train a segmentation network to select 3D regions that are
parameterized into 2D and penalized by the resulting distortion, giving rise to
segmentations which are distortion-aware. Following training, a user can use
our system to interactively select a point on the mesh and obtain a large,
meaningful region around the selection which induces a low-distortion
parameterization. Our code and project page are currently available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://threedle.github.io/DA-Wand/ Code:
  https://github.com/threedle/DA-Wand</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Learning with Global Relatedness Decoupled-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.05583v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.05583v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhou, Yanrong Guo, Shijie Hao, Richang Hong, Zhengjun Zha, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success that metric learning based approaches have achieved in
few-shot learning, recent works reveal the ineffectiveness of their episodic
training mode. In this paper, we point out two potential reasons for this
problem: 1) the random episodic labels can only provide limited supervision
information, while the relatedness information between the query and support
samples is not fully exploited; 2) the meta-learner is usually constrained by
the limited contextual information of the local episode. To overcome these
problems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)
method using the global category knowledge and the Relatedness
Decoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts
quickly by imitating the habit of humans, i.e. learning from the deep knowledge
distilled from the teacher. More specifically, we first train a global learner
on the entire base subset using category labels as supervision to leverage the
global context information of the categories. Then, the well-trained global
learner is used to simulate the query-support relatedness in global
dependencies. Finally, the distilled global query-support relatedness is
explicitly used to train the meta-learner using the RDD strategy, with the goal
of making the meta-learner more discriminative. The RDD strategy aims to
decouple the dense query-support relatedness into the groups of sparse
decoupled relatedness. Moreover, only the relatedness of a single support
sample with other query samples is considered in each group. By distilling the
sparse decoupled relatedness group by group, sharper relatedness can be
effectively distilled to the meta-learner, thereby facilitating the learning of
a discriminative meta-learner. We conduct extensive experiments on the
miniImagenet and CIFAR-FS datasets, which show the state-of-the-art performance
of our GRDD method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SA-DPSGD: Differentially Private Stochastic Gradient Descent based on
  Simulated Annealing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Fu, Zhili Chen, XinPeng Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differential privacy (DP) provides a formal privacy guarantee that prevents
adversaries with access to machine learning models from extracting information
about individual training points. Differentially private stochastic gradient
descent (DPSGD) is the most popular training method with differential privacy
in image recognition. However, existing DPSGD schemes lead to significant
performance degradation, which prevents the application of differential
privacy. In this paper, we propose a simulated annealing-based differentially
private stochastic gradient descent scheme (SA-DPSGD) which accepts a candidate
update with a probability that depends both on the update quality and on the
number of iterations. Through this random update screening, we make the
differentially private gradient descent proceed in the right direction in each
iteration, and result in a more accurate model finally. In our experiments,
under the same hyperparameters, our scheme achieves test accuracies 98.35%,
87.41% and 60.92% on datasets MNIST, FashionMNIST and CIFAR10, respectively,
compared to the state-of-the-art result of 98.12%, 86.33% and 59.34%. Under the
freely adjusted hyperparameters, our scheme achieves even higher accuracies,
98.89%, 88.50% and 64.17%. We believe that our method has a great contribution
for closing the accuracy gap between private and non-private image
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Frequency Space Diffusion Models for Accelerated MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Hairong Zheng, Dong Liang, Yanjie Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models with continuous stochastic differential equations (SDEs)
have shown superior performances in image generation. It can be used as a deep
generative prior to solve the inverse problem in MR reconstruction. However,
the existing VP-SDE can be treated as maximizing the energy of the MR image to
be reconstructed and may lead to SDE sequence divergence. The VE-SDE based MR
reconstruction is not consistent with actual diffusion process. In addition,
both VE- and VP-SDEs-based models suffer from a time-consuming sampling
procedure, resulting long reconstruction time. In this study, a new SDE
focusing on the diffusion process in high-frequency space is designed
specifically for robust MR reconstruction based on diffusion models.
Experiments on the publicly fastMRI dataset show that HFS-SDE based
reconstruction method outperforms the parallel imaging, supervised deep
learning, and existing VE- and VP-SDEs-based methods in terms of reconstruction
accuracy. It also improves the stability of MR reconstruction and accelerates
sampling procedure of reverse diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-shot recognition of any material anywhere using contrastive learning
  with physics-based rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel S. Drehwald, Sagi Eppel, Jolina Li, Han Hao, Alan Aspuru-Guzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MatSim: a synthetic dataset, a benchmark, and a method for
computer vision based recognition of similarities and transitions between
materials and textures, focusing on identifying any material under any
conditions using one or a few examples (one-shot learning). The visual
recognition of materials is essential to everything from examining food while
cooking to inspecting agriculture, chemistry, and industrial products. In this
work, we utilize giant repositories used by computer graphics artists to
generate a new CGI dataset for material similarity. We use physics-based
rendering (PBR) repositories for visual material simulation, assign these
materials random 3D objects, and render images with a vast range of backgrounds
and illumination conditions (HDRI). We add a gradual transition between
materials to support applications with a smooth transition between states (like
gradually cooked food). We also render materials inside transparent containers
to support beverage and chemistry lab use cases. We then train a contrastive
learning network to generate a descriptor that identifies unfamiliar materials
using a single image. We also present a new benchmark for a few-shot material
recognition that contains a wide range of real-world examples, including the
state of a chemical reaction, rotten/fresh fruits, states of food, different
types of construction materials, types of ground, and many other use cases
involving material states, transitions and subclasses. We show that a network
trained on the MatSim synthetic dataset outperforms state-of-the-art models
like Clip on the benchmark, despite being tested on material classes that were
not seen during training. The dataset, benchmark, code and trained models are
available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>for associated code and dataset, see
  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or
  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX
  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Lake: a Lakehouse for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasun Hambardzumyan, Abhinav Tuli, Levon Ghukasyan, Fariz Rahman, Hrant Topchyan, David Isayan, Mark McQuade, Mikayel Harutyunyan, Tatevik Hakobyan, Ivo Stranic, Davit Buniatyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional data lakes provide critical data infrastructure for analytical
workloads by enabling time travel, running SQL queries, ingesting data with
ACID transactions, and visualizing petabyte-scale datasets on cloud storage.
They allow organizations to break down data silos, unlock data-driven
decision-making, improve operational efficiency, and reduce costs. However, as
deep learning usage increases, traditional data lakes are not well-designed for
applications such as natural language processing (NLP), audio processing,
computer vision, and applications involving non-tabular datasets. This paper
presents Deep Lake, an open-source lakehouse for deep learning applications
developed at Activeloop. Deep Lake maintains the benefits of a vanilla data
lake with one key difference: it stores complex data, such as images, videos,
annotations, as well as tabular data, in the form of tensors and rapidly
streams the data over the network to (a) Tensor Query Language, (b) in-browser
visualization engine, or (c) deep learning frameworks without sacrificing GPU
utilization. Datasets stored in Deep Lake can be accessed from PyTorch,
TensorFlow, JAX, and integrate with numerous MLOps tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Localization supervision of chest x-ray classifiers using label-specific
  eye-tracking annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have been successfully applied to chest
x-ray (CXR) images. Moreover, annotated bounding boxes have been shown to
improve the interpretability of a CNN in terms of localizing abnormalities.
However, only a few relatively small CXR datasets containing bounding boxes are
available, and collecting them is very costly. Opportunely, eye-tracking (ET)
data can be collected in a non-intrusive way during the clinical workflow of a
radiologist. We use ET data recorded from radiologists while dictating CXR
reports to train CNNs. We extract snippets from the ET data by associating them
with the dictation of keywords and use them to supervise the localization of
specific abnormalities. We show that this method improves a model's
interpretability without impacting its image-level classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Unpaired Multi-modal Medical Image Segmentation via Learning
  Structured Semantic Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Yang, Ye Zhu, Ruimao Zhang, Chaoqun Wang, Zhen Li, Xiang Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating multi-modal data to improve medical image analysis has received
great attention recently. However, due to the modal discrepancy, how to use a
single model to process the data from multiple modalities is still an open
issue. In this paper, we propose a novel scheme to achieve better pixel-level
segmentation for unpaired multi-modal medical images. Different from previous
methods which adopted both modality-specific and modality-shared modules to
accommodate the appearance variance of different modalities while extracting
the common semantic information, our method is based on a single Transformer
with a carefully designed External Attention Module (EAM) to learn the
structured semantic consistency (i.e. semantic class representations and their
correlations) between modalities in the training phase. In practice, the
above-mentioned structured semantic consistency across modalities can be
progressively achieved by implementing the consistency regularization at the
modality-level and image-level respectively. The proposed EAMs are adopted to
learn the semantic consistency for different scale representations and can be
discarded once the model is optimized. Therefore, during the testing phase, we
only need to maintain one Transformer for all modal predictions, which nicely
balances the model's ease of use and simplicity. To demonstrate the
effectiveness of the proposed method, we conduct the experiments on two medical
image segmentation scenarios: (1) cardiac structure segmentation, and (2)
abdominal multi-organ segmentation. Extensive results show that the proposed
method outperforms the state-of-the-art methods by a wide margin, and even
achieves competitive performance with extremely limited training samples (e.g.,
1 or 3 annotated CT or MRI images) for one specific modality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Flow Prediction via Variational Bayesian Inference-based
  Encoder-Decoder Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlei Kong, Xiaomeng Fan, Xue-Bo Jin, Min Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate traffic flow prediction, a hotspot for intelligent transportation
research, is the prerequisite for mastering traffic and making travel plans.
The speed of traffic flow can be affected by roads condition, weather,
holidays, etc. Furthermore, the sensors to catch the information about traffic
flow will be interfered with by environmental factors such as illumination,
collection time, occlusion, etc. Therefore, the traffic flow in the practical
transportation system is complicated, uncertain, and challenging to predict
accurately. This paper proposes a deep encoder-decoder prediction framework
based on variational Bayesian inference. A Bayesian neural network is
constructed by combining variational inference with gated recurrent units (GRU)
and used as the deep neural network unit of the encoder-decoder framework to
mine the intrinsic dynamics of traffic flow. Then, the variational inference is
introduced into the multi-head attention mechanism to avoid noise-induced
deterioration of prediction accuracy. The proposed model achieves superior
prediction performance on the Guangzhou urban traffic flow dataset over the
benchmarks, particularly when the long-term prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Researchers Could Obtain Quick and Cheap User Feedback on their
  Algorithms Without Having to Operate their Own Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Eichinger, Ananta Lamichhane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of recommendation algorithms are evaluated on the basis of
historic benchmark datasets. Evaluation on historic benchmark datasets is quick
and cheap to conduct, yet excludes the viewpoint of users who actually consume
recommendations. User feedback is seldom collected, since it requires access to
an operational recommender system. Establishing and maintaining an operational
recommender system imposes a timely and financial burden that a majority of
researchers cannot shoulder. We aim to reduce this burden in order to promote
widespread user-centric evaluations of recommendation algorithms, in particular
for novice researchers in the field. We present work in progress on an
evaluation tool that implements a novel paradigm that enables user-centric
evaluations of recommendation algorithms without access to an operational
recommender system. Finally, we sketch the experiments we plan to conduct with
the help of the evaluation tool.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainability of Text Processing and Retrieval Methods: A Critical
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Saha, Debapriyo Majumdar, Mandar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning and Machine Learning based models have become extremely popular
in text processing and information retrieval. However, the non-linear
structures present inside the networks make these models largely inscrutable. A
significant body of research has focused on increasing the transparency of
these models. This article provides a broad overview of research on the
explainability and interpretability of natural language processing and
information retrieval methods. More specifically, we survey approaches that
have been applied to explain word embeddings, sequence modeling, attention
modules, transformers, BERT, and document ranking. The concluding section
suggests some possible directions for future research on this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service
  Chatlog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zheng, Tianyu Liu, Haoran Meng, Xu Wang, Yufan Jiang, Mengliang Rao, Binghuai Lin, Zhifang Sui, Yunbo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harvesting question-answer (QA) pairs from customer service chatlog in the
wild is an efficient way to enrich the knowledge base for customer service
chatbots in the cold start or continuous integration scenarios. Prior work
attempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which
fails to integrate the incomplete utterances from the dialog context for
composite QA retrieval. In this paper, we propose N-to-N QA extraction task in
which the derived questions and corresponding answers might be separated across
different utterances. We introduce a suite of generative/discriminative tagging
based methods with end-to-end and two-stage variants that perform well on 5
customer service datasets and for the first time setup a benchmark for N-to-N
DialogQAE with utterance and session level evaluation metrics. With a deep dive
into extracted QA pairs, we find that the relations between and inside the QA
pairs can be indicators to analyze the dialogue structure, e.g. information
seeking, clarification, barge-in and elaboration. We also show that the
proposed models can adapt to different domains and languages, and reduce the
labor cost of knowledge accumulation in the real-world product dialogue
platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version; The first three authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Infinite Index: Information Retrieval on Generative Text-To-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The text-to-image model Stable Diffusion has recently become very popular.
Only weeks after its open source release, millions are experimenting with image
generation. This is due to its ease of use, since all it takes is a brief
description of the desired image to "prompt" the generative model. Rarely do
the images generated for a new prompt immediately meet the user's expectations.
Usually, an iterative refinement of the prompt ("prompt engineering") is
necessary for satisfying images. As a new perspective, we recast image prompt
engineering as interactive image retrieval - on an "infinite index". Thereby, a
prompt corresponds to a query and prompt engineering to query refinement.
Selected image-prompt pairs allow direct relevance feedback, as the model can
modify an image for the refined prompt. This is a form of one-sided interactive
retrieval, where the initiative is on the user side, whereas the server side
remains stateless. In light of an extensive literature review, we develop these
parallels in detail and apply the findings to a case study of a creative search
task on such a model. We note that the uncertainty in searching an infinite
index is virtually never-ending. We also discuss future research opportunities
related to retrieval models specialized for generative models and interactive
generative image retrieval. The application of IR technology, such as query
reformulation and relevance feedback, will contribute to improved workflows
when using generative models, while the notion of an infinite index raises new
challenges in IR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CHIIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PrivateRec: Differentially Private Training and Serving for Federated
  News Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Yanlin Wang, Yang Cao, Lingjuan Lyu, Weike Pan, Yun Chen, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting and training over sensitive personal data raise severe privacy
concerns in personalized recommendation systems, and federated learning can
potentially alleviate the problem by training models over decentralized user
data.However, a theoretically private solution in both the training and serving
stages of federated recommendation is essential but still lacking.Furthermore,
naively applying differential privacy (DP) to the two stages in federated
recommendation would fail to achieve a satisfactory trade-off between privacy
and utility due to the high-dimensional characteristics of model gradients and
hidden representations.In this work, we propose a federated news recommendation
method for achieving a better utility in model training and online serving
under a DP guarantee.We first clarify the DP definition over behavior data for
each round in the life-circle of federated recommendation systems.Next, we
propose a privacy-preserving online serving mechanism under this definition
based on the idea of decomposing user embeddings with public basic vectors and
perturbing the lower-dimensional combination coefficients. We apply a random
behavior padding mechanism to reduce the required noise intensity for better
utility. Besides, we design a federated recommendation model training method,
which can generate effective and public basic vectors for serving while
providing DP for training participants. We avoid the dimension-dependent noise
for large models via label permutation and differentially private attention
modules. Experiments on real-world news recommendation datasets validate that
our method achieves superior utility under a DP guarantee in both training and
serving of federated news recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Scientific Creativity with Retrieval across Knowledge Domains <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonsu B. Kang, Sheshera Mysore, Kevin Huang, Haw-Shiuan Chang, Thorben Prein, Andrew McCallum, Aniket Kittur, Elsa Olivetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exposure to ideas in domains outside a scientist's own may benefit her in
reformulating existing research problems in novel ways and discovering new
application domains for existing solution ideas. While improved performance in
scholarly search engines can help scientists efficiently identify relevant
advances in domains they may already be familiar with, it may fall short of
helping them explore diverse ideas \textit{outside} such domains. In this paper
we explore the design of systems aimed at augmenting the end-user ability in
cross-domain exploration with flexible query specification. To this end, we
develop an exploratory search system in which end-users can select a portion of
text core to their interest from a paper abstract and retrieve papers that have
a high similarity to the user-selected core aspect but differ in terms of
domains. Furthermore, end-users can `zoom in' to specific domain clusters to
retrieve more papers from them and understand nuanced differences within the
clusters. Our case studies with scientists uncover opportunities and design
implications for systems aimed at facilitating cross-domain exploration and
inspiration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NLP+HCI Workshop at NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating 2+1D Lattice Quantum Electrodynamics at Finite Density with
  Neural Flow Wavefunctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Di Luo, Kaiwen Hu, Bryan K. Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a neural flow wavefunction, Gauge-Fermion FlowNet, and use it to
simulate 2+1D lattice compact quantum electrodynamics with finite density
dynamical fermions. The gauge field is represented by a neural network which
parameterizes a discretized flow-based transformation of the amplitude while
the fermionic sign structure is represented by a neural net backflow. This
approach directly represents the $U(1)$ degree of freedom without any
truncation, obeys Guass's law by construction, samples autoregressively
avoiding any equilibration time, and variationally simulates Gauge-Fermion
systems with sign problems accurately. In this model, we investigate
confinement and string breaking phenomena in different fermion density and
hopping regimes. We study the phase transition from the charge crystal phase to
the vacuum phase at zero density, and observe the phase seperation and the net
charge penetration blocking effect under magnetic interaction at finite
density. In addition, we investigate a magnetic phase transition due to the
competition effect between the kinetic energy of fermions and the magnetic
energy of the gauge field. With our method, we further note potential
differences on the order of the phase transitions between a continuous $U(1)$
system and one with finite truncation. Our state-of-the-art neural network
approach opens up new possibilities to study different gauge theories coupled
to dynamical matter in higher dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Over-the-Air FedGradNorm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cemil Vahapoglu, Matin Mortaheb, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) is a learning paradigm to learn multiple related
tasks simultaneously with a single shared network where each task has a
distinct personalized header network for fine-tuning. MTL can be integrated
into a federated learning (FL) setting if tasks are distributed across clients
and clients have a single shared network, leading to personalized federated
learning (PFL). To cope with statistical heterogeneity in the federated setting
across clients which can significantly degrade the learning performance, we use
a distributed dynamic weighting approach. To perform the communication between
the remote parameter server (PS) and the clients efficiently over the noisy
channel in a power and bandwidth-limited regime, we utilize over-the-air (OTA)
aggregation and hierarchical federated learning (HFL). Thus, we propose
hierarchical over-the-air (HOTA) PFL with a dynamic weighting strategy which we
call HOTA-FedGradNorm. Our algorithm considers the channel conditions during
the dynamic weight selection process. We conduct experiments on a wireless
communication system dataset (RadComDynamic). The experimental results
demonstrate that the training speed with HOTA-FedGradNorm is faster compared to
the algorithms with a naive static equal weighting strategy. In addition,
HOTA-FedGradNorm provides robustness against the negative channel effects by
compensating for the channel conditions during the dynamic weight selection
process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Domain Transfer via Semantic Skill Imitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Pertsch, Ruta Desai, Vikash Kumar, Franziska Meier, Joseph J. Lim, Dhruv Batra, Akshara Rai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach for semantic imitation, which uses demonstrations from
a source domain, e.g. human videos, to accelerate reinforcement learning (RL)
in a different target domain, e.g. a robotic manipulator in a simulated
kitchen. Instead of imitating low-level actions like joint velocities, our
approach imitates the sequence of demonstrated semantic skills like "opening
the microwave" or "turning on the stove". This allows us to transfer
demonstrations across environments (e.g. real-world to simulated kitchen) and
agent embodiments (e.g. bimanual human demonstration to robotic arm). We
evaluate on three challenging cross-domain learning problems and match the
performance of demonstration-accelerated RL approaches that require in-domain
demonstrations. In a simulated kitchen environment, our approach learns
long-horizon robot manipulation tasks, using less than 3 minutes of human video
demonstrations from a real-world kitchen. This enables scaling robot learning
via the reuse of demonstrations, e.g. collected as human videos, for learning
in any number of target domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://kpertsch.github.io/star</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Play and Self-Describe: Policy Adaptation with Vision-Language
  Foundation Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress on vision-language foundation models have brought significant
advancement to building general-purpose robots. By using the pre-trained models
to encode the scene and instructions as inputs for decision making, the
instruction-conditioned policy can generalize across different objects and
tasks. While this is encouraging, the policy still fails in most cases given an
unseen task or environment. To adapt the policy to unseen tasks and
environments, we explore a new paradigm on leveraging the pre-trained
foundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the
trained policy to a new task or a new environment, we first let the policy
self-play with randomly generated instructions to record the demonstrations.
While the execution could be wrong, we can use the pre-trained foundation
models to accurately self-describe (i.e., re-label or classify) the
demonstrations. This automatically provides new pairs of
demonstration-instruction data for policy fine-tuning. We evaluate our method
on a broad range of experiments with the focus on generalization on unseen
objects, unseen tasks, unseen environments, and sim-to-real transfer. We show
SPLAYD improves baselines by a large margin in all cases. Our project page is
available at https://geyuying.github.io/SPLAYD/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://geyuying.github.io/SPLAYD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Strategies for Cooperative Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Ibrahim, Ammar Fayad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adequate strategizing of agents behaviors is essential to solving cooperative
MARL problems. One intuitively beneficial yet uncommon method in this domain is
predicting agents future behaviors and planning accordingly. Leveraging this
point, we propose a two-level hierarchical architecture that combines a novel
information-theoretic objective with a trajectory prediction model to learn a
strategy. To this end, we introduce a latent policy that learns two types of
latent strategies: individual $z_A$, and relational $z_R$ using a modified
Graph Attention Network module to extract interaction features. We encourage
each agent to behave according to the strategy by conditioning its local $Q$
functions on $z_A$, and we further equip agents with a shared $Q$ function that
conditions on $z_R$. Additionally, we introduce two regularizers to allow
predicted trajectories to be accurate and rewarding. Empirical results on
Google Research Football (GRF) and StarCraft (SC) II micromanagement tasks show
that our method establishes a new state of the art being, to the best of our
knowledge, the first MARL algorithm to solve all super hard SC II scenarios as
well as the GRF full game with a win rate higher than $95\%$, thus
outperforming all existing methods. Videos and brief overview of the methods
and results are available at:
https://sites.google.com/view/hier-strats-marl/home.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Kernelized Independence Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Podkopaev, Patrick Blöbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Independence testing is a fundamental and classical statistical problem that
has been extensively studied in the batch setting when one fixes the sample
size before collecting data. However, practitioners often prefer procedures
that adapt to the complexity of a problem at hand instead of setting sample
size in advance. Ideally, such procedures should (a) allow stopping earlier on
easy tasks (and later on harder tasks), hence making better use of available
resources, and (b) continuously monitor the data and efficiently incorporate
statistical evidence after collecting new data, while controlling the false
alarm rate. It is well known that classical batch tests are not tailored for
streaming data settings, since valid inference after data peeking requires
correcting for multiple testing, but such corrections generally result in low
power. In this paper, we design sequential kernelized independence tests
(SKITs) that overcome such shortcomings based on the principle of testing by
betting. We exemplify our broad framework using bets inspired by kernelized
dependence measures such as the Hilbert-Schmidt independence criterion (HSIC)
and the constrained-covariance criterion (COCO). Importantly, we also
generalize the framework to non-i.i.d. time-varying settings, for which there
exist no batch tests. We demonstrate the power of our approaches on both
simulated and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstruction of Multivariate Sparse Signals from Mismatched Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taulant Koka, Michael Muma, Benjamín Béjar Haro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Erroneous correspondences between samples and their respective channel or
target commonly arise in several real-world applications. For instance,
whole-brain calcium imaging of freely moving organisms, multiple target
tracking or multi-person contactless vital sign monitoring may be severely
affected by mismatched sample-channel assignments. To systematically address
this fundamental problem, we pose it as a signal reconstruction problem where
we have lost correspondences between the samples and their respective channels.
We show that under the assumption that the signals of interest admit a sparse
representation over an overcomplete dictionary, unique signal recovery is
possible. Our derivations reveal that the problem is equivalent to a structured
unlabeled sensing problem without precise knowledge of the sensing matrix.
Unfortunately, existing methods are neither robust to errors in the regressors
nor do they exploit the structure of the problem. Therefore, we propose a novel
robust two-step approach for the reconstruction of shuffled sparse signals. The
performance and robustness of the proposed approach is illustrated in an
application of whole-brain calcium imaging in computational neuroscience. The
proposed framework can be generalized to sparse signal representations other
than the ones considered in this work to be applied in a variety of real-world
problems with imprecise measurement or channel assignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, submitted to Transactions on Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Invariant Subspaces of Koopman Operators--Part 2: Heterogeneous
  Dictionary Mixing to Approximate Subspace Invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles A. Johnson, Shara Balakrishnan, Enoch Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work builds on the models and concepts presented in part 1 to learn
approximate dictionary representations of Koopman operators from data. Part I
of this paper presented a methodology for arguing the subspace invariance of a
Koopman dictionary. This methodology was demonstrated on the state-inclusive
logistic lifting (SILL) basis. This is an affine basis augmented with
conjunctive logistic functions. The SILL dictionary's nonlinear functions are
homogeneous, a norm in data-driven dictionary learning of Koopman operators. In
this paper, we discover that structured mixing of heterogeneous dictionary
functions drawn from different classes of nonlinear functions achieve the same
accuracy and dimensional scaling as the deep-learning-based deepDMD algorithm.
We specifically show this by building a heterogeneous dictionary comprised of
SILL functions and conjunctive radial basis functions (RBFs). This mixed
dictionary achieves the same accuracy and dimensional scaling as deepDMD with
an order of magnitude reduction in parameters, while maintaining geometric
interpretability. These results strengthen the viability of dictionary-based
Koopman models to solving high-dimensional nonlinear learning problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:2206.13585</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Post-hoc Uncertainty Learning using a Dirichlet Meta-Model <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maohao Shen, Yuheng Bu, Prasanna Sattigeri, Soumya Ghosh, Subhro Das, Gregory Wornell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is known that neural networks have the problem of being over-confident
when directly using the output label distribution to generate uncertainty
measures. Existing methods mainly resolve this issue by retraining the entire
model to impose the uncertainty quantification capability so that the learned
model can achieve desired performance in accuracy and uncertainty prediction
simultaneously. However, training the model from scratch is computationally
expensive and may not be feasible in many situations. In this work, we consider
a more practical post-hoc uncertainty learning setting, where a well-trained
base model is given, and we focus on the uncertainty quantification task at the
second stage of training. We propose a novel Bayesian meta-model to augment
pre-trained models with better uncertainty quantification abilities, which is
effective and computationally efficient. Our proposed method requires no
additional training data and is flexible enough to quantify different
uncertainties and easily adapt to different application settings, including
out-of-domain data detection, misclassification detection, and trustworthy
transfer learning. We demonstrate our proposed meta-model approach's
flexibility and superior empirical performance on these applications over
multiple representative image classification benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Invariant Subspaces of Koopman Operators--Part 1: A Methodology
  for Demonstrating a Dictionary's Approximate Subspace Invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles A. Johnson, Shara Balakrishnan, Enoch Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Koopman operators model nonlinear dynamics as a linear dynamic system acting
on a nonlinear function as the state. This nonstandard state is often called a
Koopman observable and is usually approximated numerically by a superposition
of functions drawn from a dictionary. In a widely used algorithm, Extended
Dynamic Mode Decomposition, the dictionary functions are drawn from a fixed
class of functions. Recently, deep learning combined with EDMD has been used to
learn novel dictionary functions in an algorithm called deep dynamic mode
decomposition (deepDMD). The learned representation both (1) accurately models
and (2) scales well with the dimension of the original nonlinear system. In
this paper we analyze the learned dictionaries from deepDMD and explore the
theoretical basis for their strong performance. We explore State-Inclusive
Logistic Lifting (SILL) dictionary functions to approximate Koopman
observables. Error analysis of these dictionary functions show they satisfy a
property of subspace approximation, which we define as uniform finite
approximate closure. Our results provide a hypothesis to explain the success of
deep neural networks in learning numerical approximations to Koopman operators.
Part 2 of this paper will extend this explanation by demonstrating the subspace
invariant of heterogeneous dictionaries and presenting a head-to-head numerical
comparison of deepDMD and low-parameter heterogeneous dictionary learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2206.13585</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scheduling and Aggregation Design for Asynchronous Federated Learning
  over Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chung-Hsuan Hu, Zheng Chen, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a collaborative machine learning (ML) framework
that combines on-device training and server-based aggregation to train a common
ML model among distributed agents. In this work, we propose an asynchronous FL
design with periodic aggregation to tackle the straggler issue in FL systems.
Considering limited wireless communication resources, we investigate the effect
of different scheduling policies and aggregation designs on the convergence
performance. Driven by the importance of reducing the bias and variance of the
aggregated model updates, we propose a scheduling policy that jointly considers
the channel quality and training data representation of user devices. The
effectiveness of our channel-aware data-importance-based scheduling policy,
compared with state-of-the-art methods proposed for synchronous FL, is
validated through simulations. Moreover, we show that an "age-aware"
aggregation weighting design can significantly improve the learning performance
in an asynchronous FL setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by JSAC special issue: Communication-Efficient Distributed
  Learning over Networks; to appear</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lorentz Group Equivariant Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichun Hao, Raghav Kansal, Javier Duarte, Nadezda Chernyavskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant work recently in developing machine learning
models in high energy physics (HEP), for tasks such as classification,
simulation, and anomaly detection. Typically, these models are adapted from
those designed for datasets in computer vision or natural language processing
without necessarily incorporating inductive biases suited to HEP data, such as
respecting its inherent symmetries. Such inductive biases can make the model
more performant and interpretable, and reduce the amount of training data
needed. To that end, we develop the Lorentz group autoencoder (LGAE), an
autoencoder model equivariant with respect to the proper, orthochronous Lorentz
group $\mathrm{SO}^+(3,1)$, with a latent space living in the representations
of the group. We present our architecture and several experimental results on
jets at the LHC and find it significantly outperforms a non-Lorentz-equivariant
graph neural network baseline on compression and reconstruction, and anomaly
detection. We also demonstrate the advantage of such an equivariant model in
analyzing the latent space of the autoencoder, which can have a significant
impact on the explainability of anomalies found by such black-box machine
learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 4 tables, and a 2 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning useful representations for shifting tasks and distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Zhang, Léon Bottou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the dominant approach to learn representations (as a side effect of
optimizing an expected cost for a single training distribution) remain a good
approach when we are dealing with multiple distributions. Our thesis is that
such scenarios are better served by representations that are "richer" than
those obtained with a single optimization episode. This is supported by a
collection of empirical results obtained with an apparently na\"ive ensembling
technique: concatenating the representations obtained with multiple training
episodes using the same data, model, algorithm, and hyper-parameters, but
different random seeds. These independently trained networks perform similarly.
Yet, in a number of scenarios involving new distributions, the concatenated
representation performs substantially better than an equivalently sized network
trained from scratch. This proves that the representations constructed by
multiple training episodes are in fact different. Although their concatenation
carries little additional information about the training task under the
training distribution, it becomes substantially more informative when tasks or
distributions change. Meanwhile, a single training episode is unlikely to yield
such a redundant representation because the optimization process has no reason
to accumulate features that do not incrementally improve the training
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility
  on Demand Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Enders, James Harrison, Marco Pavone, Maximilian Schiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the sequential decision-making problem of making proactive
request assignment and rejection decisions for a profit-maximizing operator of
an autonomous mobility on demand system. We formalize this problem as a Markov
decision process and propose a novel combination of multi-agent Soft
Actor-Critic and weighted bipartite matching to obtain an anticipative control
policy. Thereby, we factorize the operator's otherwise intractable action
space, but still obtain a globally coordinated decision. Experiments based on
real-world taxi data show that our method outperforms state of the art
benchmarks with respect to performance, stability, and computational
tractability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, extended version of paper submitted to the 5th
  Learning for Dynamics & Control Conference (L4DC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian data fusion with shared priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Tales Imbiriba, Victor Elvira, Pau Closas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of data and knowledge from several sources is known as data
fusion. When data is available in a distributed fashion or when different
sensors are used to infer a quantity of interest, data fusion becomes
essential. In Bayesian settings, a priori information of the unknown quantities
is available and, possibly, shared among the distributed estimators. When the
local estimates are fused, such prior might be overused unless it is accounted
for. This paper explores the effects of shared priors in Bayesian data fusion
contexts, providing fusion rules and analysis to understand the performance of
such fusion as a function of the number of collaborative agents and the
uncertainty of the priors. Analytical results are corroborated through
experiments in a variety of estimation and classification problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximal Initial Learning Rates in Deep ReLU Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Iyer, Boris Hanin, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training a neural network requires choosing a suitable learning rate,
involving a trade-off between speed and effectiveness of convergence. While
there has been considerable theoretical and empirical analysis of how large the
learning rate can be, most prior work focuses only on late-stage training. In
this work, we introduce the maximal initial learning rate $\eta^{\ast}$ - the
largest learning rate at which a randomly initialized neural network can
successfully begin training and achieve (at least) a given threshold accuracy.
Using a simple approach to estimate $\eta^{\ast}$, we observe that in
constant-width fully-connected ReLU networks, $\eta^{\ast}$ demonstrates
different behavior to the maximum learning rate later in training.
Specifically, we find that $\eta^{\ast}$ is well predicted as a power of
$(\text{depth} \times \text{width})$, provided that (i) the width of the
network is sufficiently large compared to the depth, and (ii) the input layer
of the network is trained at a relatively small learning rate. We further
analyze the relationship between $\eta^{\ast}$ and the sharpness $\lambda_{1}$
of the network at initialization, indicating that they are closely though not
inversely related. We formally prove bounds for $\lambda_{1}$ in terms of
$(\text{depth} \times \text{width})$ that align with our empirical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Robust Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training adversarially robust discriminative (i.e., softmax) classifier has
been the dominant approach to robust classification. Building on recent work on
adversarial training (AT)-based generative models, we investigate using AT to
learn unnormalized class-conditional density models and then performing
generative robust classification. Our result shows that, under the condition of
similar model capacities, the generative robust classifier achieves comparable
performance to a baseline softmax robust classifier when the test data is clean
or when the test perturbation is of limited size, and much better performance
when the test perturbation size exceeds the training perturbation size. The
generative classifier is also able to generate samples or counterfactuals that
more closely resemble the training data, suggesting that the generative
classifier can better capture the class-conditional distributions. In contrast
to standard discriminative adversarial training where advanced data
augmentation techniques are only effective when combined with weight averaging,
we find it straightforward to apply advanced data augmentation to achieve
better robustness in our approach. Our result suggests that the generative
classifier is a competitive alternative to robust classification, especially
for problems with limited number of classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Directional Direct Feedback Alignment: Estimating Backpropagation Paths
  for Efficient Learning on Neural Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bacho, Dominique Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The error Backpropagation algorithm (BP) is a key method for training deep
neural networks. While performant, it is also resource-demanding in terms of
computation, memory usage and energy. This makes it unsuitable for online
learning on edge devices that require a high processing rate and low energy
consumption. More importantly, BP does not take advantage of the parallelism
and local characteristics offered by dedicated neural processors. There is
therefore a demand for alternative algorithms to BP that could improve the
latency, memory requirements, and energy footprint of neural networks on
hardware. In this work, we propose a novel method based on Direct Feedback
Alignment (DFA) which uses Forward-Mode Automatic Differentiation to estimate
backpropagation paths and learn feedback connections in an online manner. We
experimentally show that Directional DFA achieves performances that are closer
to BP than other feedback methods on several benchmark datasets and
architectures while benefiting from the locality and parallelization
characteristics of DFA. Moreover, we show that, unlike other feedback learning
algorithms, our method provides stable learning for convolution layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backdoor Mitigation in Deep Neural Networks via Strategic Retraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Dhonthi, Ernst Moritz Hahn, Vahid Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNN) are becoming increasingly more important in
assisted and automated driving. Using such entities which are obtained using
machine learning is inevitable: tasks such as recognizing traffic signs cannot
be developed reasonably using traditional software development methods. DNN
however do have the problem that they are mostly black boxes and therefore hard
to understand and debug. One particular problem is that they are prone to
hidden backdoors. This means that the DNN misclassifies its input, because it
considers properties that should not be decisive for the output. Backdoors may
either be introduced by malicious attackers or by inappropriate training. In
any case, detecting and removing them is important in the automotive area, as
they might lead to safety violations with potentially severe consequences. In
this paper, we introduce a novel method to remove backdoors. Our method works
for both intentional as well as unintentional backdoors. We also do not require
prior knowledge about the shape or distribution of backdoors. Experimental
evidence shows that our method performs well on several medium-sized examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages, 7 Tables, 4 Figures. Accepted at the International
  Symposium of Formal Methods 2023 (FM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Hang Zhang, Chen Lin, Yeyun Gong, Jian Guo, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-form numerical reasoning in financial analysis aims to generate a
reasoning program to calculate the correct answer for a given question.
Previous work followed a retriever-generator framework, where the retriever
selects key facts from a long-form document, and the generator generates a
reasoning program based on retrieved facts. However, they treated all facts
equally without considering the different contributions of facts with and
without numbers. Meanwhile, the program consistency were ignored under
supervised training, resulting in lower training accuracy and diversity. To
solve these problems, we proposed APOLLO to improve the long-form numerical
reasoning framework. For the retriever, we adopt a number-aware negative
sampling strategy to enable the retriever to be more discriminative on key
numerical facts. For the generator, we design consistency-based reinforcement
learning and target program augmentation strategy based on the consistency of
program execution results. Experimental results on the FinQA and ConvFinQA
leaderboard verify the effectiveness of our proposed method, achieving the new
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cutting Plane Selection with Analytic Centers and Multiregression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Turner, Timo Berthold, Mathieu Besançon, Thorsten Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cutting planes are a crucial component of state-of-the-art mixed-integer
programming solvers, with the choice of which subset of cuts to add being vital
for solver performance. We propose new distance-based measures to qualify the
value of a cut by quantifying the extent to which it separates relevant parts
of the relaxed feasible set. For this purpose, we use the analytic centers of
the relaxation polytope or of its optimal face, as well as alternative optimal
solutions of the linear programming relaxation. We assess the impact of the
choice of distance measure on root node performance and throughout the whole
branch-and-bound tree, comparing our measures against those prevalent in the
literature. Finally, by a multi-output regression, we predict the relative
performance of each measure, using static features readily available before the
separation process. Our results indicate that analytic center-based methods
help to significantly reduce the number of branch-and-bound nodes needed to
explore the search space and that our multiregression approach can further
improve on any individual method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedSkip: Combatting Statistical Heterogeneity with Federated Skip
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqing Fan, Yanfeng Wang, Jiangchao Yao, Lingjuan Lyu, Ya Zhang, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The statistical heterogeneity of the non-independent and identically
distributed (non-IID) data in local clients significantly limits the
performance of federated learning. Previous attempts like FedProx, SCAFFOLD,
MOON, FedNova and FedDyn resort to an optimization perspective, which requires
an auxiliary term or re-weights local updates to calibrate the learning bias or
the objective inconsistency. However, in addition to previous explorations for
improvement in federated averaging, our analysis shows that another critical
bottleneck is the poorer optima of client models in more heterogeneous
conditions. We thus introduce a data-driven approach called FedSkip to improve
the client optima by periodically skipping federated averaging and scattering
local models to the cross devices. We provide theoretical analysis of the
possible benefit from FedSkip and conduct extensive experiments on a range of
datasets to demonstrate that FedSkip achieves much higher accuracy, better
aggregation efficiency and competing communication efficiency. Source code is
available at: https://github.com/MediaBrain-SJTU/FedSkip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Scoccola, Hitesh Gakhar, Johnathan Bush, Nikolas Schonsheck, Tatum Rask, Ling Zhou, Jose A. Perea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The circular coordinates algorithm of de Silva, Morozov, and
Vejdemo-Johansson takes as input a dataset together with a cohomology class
representing a $1$-dimensional hole in the data; the output is a map from the
data into the circle that captures this hole, and that is of minimum energy in
a suitable sense. However, when applied to several cohomology classes, the
output circle-valued maps can be "geometrically correlated" even if the chosen
cohomology classes are linearly independent. It is shown in the original work
that less correlated maps can be obtained with suitable integer linear
combinations of the cohomology classes, with the linear combinations being
chosen by inspection. In this paper, we identify a formal notion of geometric
correlation between circle-valued maps which, in the Riemannian manifold case,
corresponds to the Dirichlet form, a bilinear form derived from the Dirichlet
energy. We describe a systematic procedure for constructing low energy
torus-valued maps on data, starting from a set of linearly independent
cohomology classes. We showcase our procedure with computational examples. Our
main algorithm is based on the Lenstra--Lenstra--Lov\'asz algorithm from
computational number theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traffic Flow Prediction via Variational Bayesian Inference-based
  Encoder-Decoder Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlei Kong, Xiaomeng Fan, Xue-Bo Jin, Min Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate traffic flow prediction, a hotspot for intelligent transportation
research, is the prerequisite for mastering traffic and making travel plans.
The speed of traffic flow can be affected by roads condition, weather,
holidays, etc. Furthermore, the sensors to catch the information about traffic
flow will be interfered with by environmental factors such as illumination,
collection time, occlusion, etc. Therefore, the traffic flow in the practical
transportation system is complicated, uncertain, and challenging to predict
accurately. This paper proposes a deep encoder-decoder prediction framework
based on variational Bayesian inference. A Bayesian neural network is
constructed by combining variational inference with gated recurrent units (GRU)
and used as the deep neural network unit of the encoder-decoder framework to
mine the intrinsic dynamics of traffic flow. Then, the variational inference is
introduced into the multi-head attention mechanism to avoid noise-induced
deterioration of prediction accuracy. The proposed model achieves superior
prediction performance on the Guangzhou urban traffic flow dataset over the
benchmarks, particularly when the long-term prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAGS Framework for Comparative Analysis of Federated Learning
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahnaf Hannan Lodhi, Barış Akgün, Öznur Özkasap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has become a key choice for distributed machine
learning. Initially focused on centralized aggregation, recent works in FL have
emphasized greater decentralization to adapt to the highly heterogeneous
network edge. Among these, Hierarchical, Device-to-Device and Gossip Federated
Learning (HFL, D2DFL \& GFL respectively) can be considered as foundational FL
algorithms employing fundamental aggregation strategies. A number of FL
algorithms were subsequently proposed employing multiple fundamental
aggregation schemes jointly. Existing research, however, subjects the FL
algorithms to varied conditions and gauges the performance of these algorithms
mainly against Federated Averaging (FedAvg) only. This work consolidates the FL
landscape and offers an objective analysis of the major FL algorithms through a
comprehensive cross-evaluation for a wide range of operating conditions. In
addition to the three foundational FL algorithms, this work also analyzes six
derived algorithms. To enable a uniform assessment, a multi-FL framework named
FLAGS: Federated Learning AlGorithms Simulation has been developed for rapid
configuration of multiple FL algorithms. Our experiments indicate that fully
decentralized FL algorithms achieve comparable accuracy under multiple
operating conditions, including asynchronous aggregation and the presence of
stragglers. Furthermore, decentralized FL can also operate in noisy
environments and with a comparably higher local update rate. However, the
impact of extremely skewed data distributions on decentralized FL is much more
adverse than on centralized variants. The results indicate that it may not be
necessary to restrict the devices to a single FL algorithm; rather, multi-FL
nodes may operate with greater efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 10 figures. Accepted for publication in Elsevier 'Internet
  of Things'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech and Natural Language Processing Technologies for Pseudo-Pilot
  Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrutha Prasad, Juan Zuluaga-Gomez, Petr Motlicek, Saeed Sarfjoo, Iuliia Nigmatulina, Karel Vesely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a simple yet efficient repetition-based modular system
for speeding up air-traffic controllers (ATCos) training. E.g., a human pilot
is still required in EUROCONTROL's ESCAPE lite simulator (see
https://www.eurocontrol.int/simulator/escape) during ATCo training. However,
this need can be substituted by an automatic system that could act as a pilot.
In this paper, we aim to develop and integrate a pseudo-pilot agent into the
ATCo training pipeline by merging diverse artificial intelligence (AI) powered
modules. The system understands the voice communications issued by the ATCo,
and, in turn, it generates a spoken prompt that follows the pilot's phraseology
to the initial communication. Our system mainly relies on open-source AI tools
and air traffic control (ATC) databases, thus, proving its simplicity and ease
of replicability. The overall pipeline is composed of the following: (1) a
submodule that receives and pre-processes the input stream of raw audio, (2) an
automatic speech recognition (ASR) system that transforms audio into a sequence
of words; (3) a high-level ATC-related entity parser, which extracts relevant
information from the communication, i.e., callsigns and commands, and finally,
(4) a speech synthesizer submodule that generates responses based on the
high-level ATC entities previously extracted. Overall, we show that this system
could pave the way toward developing a real proof-of-concept pseudo-pilot
system. Hence, speeding up the training of ATCos while drastically reducing its
overall cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Sesar Innovation Days 2022.
  https://www.sesarju.eu/sesarinnovationdays</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Establishing a stronger baseline for lightweight contrastive models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenye Lin, Yifeng Ding, Zhixiong Cao, Hai-tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has reported a performance degradation in self-supervised
contrastive learning for specially designed efficient networks, such as
MobileNet and EfficientNet. A common practice to address this problem is to
introduce a pretrained contrastive teacher model and train the lightweight
networks with distillation signals generated by the teacher. However, it is
time and resource consuming to pretrain a teacher model when it is not
available. In this work, we aim to establish a stronger baseline for
lightweight contrastive models without using a pretrained teacher model.
Specifically, we show that the optimal recipe for efficient models is different
from that of larger models, and using the same training settings as ResNet50,
as previous research does, is inappropriate. Additionally, we observe a common
issu e in contrastive learning where either the positive or negative views can
be noisy, and propose a smoothed version of InfoNCE loss to alleviate this
problem. As a result, we successfully improve the linear evaluation results
from 36.3\% to 62.3\% for MobileNet-V3-Large and from 42.2\% to 65.8\% for
EfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with
$5\times$ fewer parameters. We hope our research will facilitate the usage of
lightweight contrastive models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducible scaling laws for contrastive language-image learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural networks has led to remarkable performance across a wide
range of tasks. Moreover, performance often follows reliable scaling laws as a
function of training set size, model size, and compute, which offers valuable
guidance as large-scale experiments are becoming increasingly expensive.
However, previous work on scaling laws has primarily used private data \&
models or focused on uni-modal language or vision learning. To address these
limitations, we investigate scaling laws for contrastive language-image
pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP
repository. Our large-scale experiments involve models trained on up to two
billion image-text pairs and identify power law scaling for multiple downstream
tasks including zero-shot classification, retrieval, linear probing, and
end-to-end fine-tuning. We find that the training distribution plays a key role
in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling
behavior despite identical model architectures and similar training recipes. We
open-source our evaluation workflow and all models, including the largest
public CLIP models, to ensure reproducibility and make scaling laws research
more accessible. Source code and instructions to reproduce this study will be
available at https://github.com/LAION-AI/scaling-laws-openclip
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Optimal Estimation of Time Offset Synchronization with
  Temperature Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maurizio Mongelli, Stefano Scanzio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper addresses the problem of time offset synchronization in the
presence of temperature variations, which lead to a non-Gaussian environment.
In this context, regular Kalman filtering reveals to be suboptimal. A
functional optimization approach is developed in order to approximate optimal
estimation of the clock offset between master and slave. A numerical
approximation is provided to this aim, based on regular neural network
training. Other heuristics are provided as well, based on spline regression. An
extensive performance evaluation highlights the benefits of the proposed
techniques, which can be easily generalized to several clock synchronization
protocols and operating environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards mapping the contemporary art world with ArtLM: an art-specific
  NLP model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinkai Chen, Mohamed El-Mennaoui, Antoine Fosset, Amine Rebei, Haoyang Cao, Christy Eóin O'Beirne, Sasha Shevchenko, Mathieu Rosenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an increasing amount of data in the art world, discovering artists and
artworks suitable to collectors' tastes becomes a challenge. It is no longer
enough to use visual information, as contextual information about the artist
has become just as important in contemporary art. In this work, we present a
generic Natural Language Processing framework (called ArtLM) to discover the
connections among contemporary artists based on their biographies. In this
approach, we first continue to pre-train the existing general English language
models with a large amount of unlabelled art-related data. We then fine-tune
this new pre-trained model with our biography pair dataset manually annotated
by a team of professionals in the art industry. With extensive experiments, we
demonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and
outperforms other baseline models. We also provide a visualisation and a
qualitative analysis of the artist network built from ArtLM's outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning in System Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Antonio Martin H., Oscar Fernandez Vicente, Sergio Perez, Anas Belfadil, Cristina Ibanez-Llano, Freddy Jose Perozo Rondon, Jose Javier Valle, Javier Arechalde Pelaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System identification, also known as learning forward models, transfer
functions, system dynamics, etc., has a long tradition both in science and
engineering in different fields. Particularly, it is a recurring theme in
Reinforcement Learning research, where forward models approximate the state
transition function of a Markov Decision Process by learning a mapping function
from current state and action to the next state. This problem is commonly
defined as a Supervised Learning problem in a direct way. This common approach
faces several difficulties due to the inherent complexities of the dynamics to
learn, for example, delayed effects, high non-linearity, non-stationarity,
partial observability and, more important, error accumulation when using
bootstrapped predictions (predictions based on past predictions), over large
time horizons. Here we explore the use of Reinforcement Learning in this
problem. We elaborate on why and how this problem fits naturally and sound as a
Reinforcement Learning problem, and present some experimental results that
demonstrate RL is a promising technique to solve these kind of problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Neurips Deep Reinforcement Learning Workshop 2022:
  https://openreview.net/forum?id=fGcbpWQIJZV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification for Deep Neural Networks: An Empirical
  Comparison and Usage Guidelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Weiss, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNN) are increasingly used as components of larger
software systems that need to process complex data, such as images, written
texts, audio/video signals. DNN predictions cannot be assumed to be always
correct for several reasons, among which the huge input space that is dealt
with, the ambiguity of some inputs data, as well as the intrinsic properties of
learning algorithms, which can provide only statistical warranties. Hence,
developers have to cope with some residual error probability. An architectural
pattern commonly adopted to manage failure-prone components is the supervisor,
an additional component that can estimate the reliability of the predictions
made by untrusted (e.g., DNN) components and can activate an automated healing
procedure when these are likely to fail, ensuring that the Deep Learning based
System (DLS) does not cause damages, despite its main functionality being
suspended.
  In this paper, we consider DLS that implement a supervisor by means of
uncertainty estimation. After overviewing the main approaches to uncertainty
estimation and discussing their pros and cons, we motivate the need for a
specific empirical assessment method that can deal with the experimental
setting in which supervisors are used, where the accuracy of the DNN matters
only as long as the supervisor lets the DLS continue to operate. Then we
present a large empirical study conducted to compare the alternative approaches
to uncertainty estimation. We distilled a set of guidelines for developers that
are useful to incorporate a supervisor based on uncertainty monitoring into a
DLS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Software: Testing,
  Verification and Reliability. arXiv admin note: substantial text overlap with
  arXiv:2102.00902</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplification of Forest Classifiers and Regressors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuyoshi Nakamura, Kento Sakurada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of sharing as many branching conditions of a given
forest classifier or regressor as possible while keeping classification
performance. As a constraint for preventing from accuracy degradation, we first
consider the one that the decision paths of all the given feature vectors must
not change. For a branching condition that a value of a certain feature is at
most a given threshold, the set of values satisfying such constraint can be
represented as an interval. Thus, the problem is reduced to the problem of
finding the minimum set intersecting all the constraint-satisfying intervals
for each set of branching conditions on the same feature. We propose an
algorithm for the original problem using an algorithm solving this problem
efficiently. The constraint is relaxed later to promote further sharing of
branching conditions by allowing decision path change of a certain ratio of the
given feature vectors or allowing a certain number of non-intersected
constraint-satisfying intervals. We also extended our algorithm for both the
relaxations. The effectiveness of our method is demonstrated through
comprehensive experiments using 21 datasets (13 classification and 8 regression
datasets in UCI machine learning repository) and 4 classifiers/regressors
(random forest, extremely randomized trees, AdaBoost and gradient boosting).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization by Learning and Removing Domain-specific Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ding, Lei Wang, Bin Liang, Shuming Liang, Yang Wang, Fang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) suffer from domain shift when the test dataset
follows a distribution different from the training dataset. Domain
generalization aims to tackle this issue by learning a model that can
generalize to unseen domains. In this paper, we propose a new approach that
aims to explicitly remove domain-specific features for domain generalization.
Following this approach, we propose a novel framework called Learning and
Removing Domain-specific features for Generalization (LRDG) that learns a
domain-invariant model by tactically removing domain-specific features from the
input images. Specifically, we design a classifier to effectively learn the
domain-specific features for each source domain, respectively. We then develop
an encoder-decoder network to map each input image into a new image space where
the learned domain-specific features are removed. With the images output by the
encoder-decoder network, another classifier is designed to learn the
domain-invariant features to conduct image classification. Extensive
experiments demonstrate that our framework achieves superior performance
compared with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation <span class="chip">EMNLP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, John Harvill, Sunjae Yoon, Mark Hasegawa-Johnson, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word Sense Disambiguation (WSD) is an NLP task aimed at determining the
correct sense of a word in a sentence from discrete sense choices. Although
current systems have attained unprecedented performances for such tasks, the
nonuniform distribution of word senses during training generally results in
systems performing poorly on rare senses. To this end, we consider data
augmentation to increase the frequency of these least frequent senses (LFS) to
reduce the distributional bias of senses during training. We propose
Sense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that
maintains the sense of a target word. SMSMix smoothly blends two sentences
using mask prediction while preserving the relevant span determined by saliency
scores to maintain a specific word's sense. To the best of our knowledge, this
is the first attempt to apply mixup in NLP while preserving the meaning of a
specific word. With extensive experiments, we validate that our augmentation
method can effectively give more information about rare senses during training
with maintained target sense label.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Negative Correlation Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Zhang, Qibin Hou, Yun Liu, Jia-Wang Bian, Xun Xu, Joey Tianyi Zhou, Ce Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensemble learning serves as a straightforward way to improve the performance
of almost any machine learning algorithm. Existing deep ensemble methods
usually naively train many different models and then aggregate their
predictions. This is not optimal in our view from two aspects: i) Naively
training multiple models adds much more computational burden, especially in the
deep learning era; ii) Purely optimizing each base model without considering
their interactions limits the diversity of ensemble and performance gains. We
tackle these issues by proposing deep negative correlation classification
(DNCC), in which the accuracy and diversity trade-off is systematically
controlled by decomposing the loss function seamlessly into individual accuracy
and the correlation between individual models and the ensemble. DNCC yields a
deep classification ensemble where the individual estimator is both accurate
and negatively correlated. Thanks to the optimized diversities, DNCC works well
even when utilizing a shared network backbone, which significantly improves its
efficiency when compared with most existing ensemble systems. Extensive
experiments on multiple benchmark datasets and network structures demonstrate
the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-enabled exploration of Instagram profiles predicts soft skills and
  personality traits to empower hiring decisions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mercedeh Harirchian, Fereshteh Amin, Saeed Rouhani, Aref Aligholipour, Vahid Amiri Lord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It does not matter whether it is a job interview with Tech Giants, Wall
Street firms, or a small startup; all candidates want to demonstrate their best
selves or even present themselves better than they really are. Meanwhile,
recruiters want to know the candidates' authentic selves and detect soft skills
that prove an expert candidate would be a great fit in any company. Recruiters
worldwide usually struggle to find employees with the highest level of these
skills. Digital footprints can assist recruiters in this process by providing
candidates' unique set of online activities, while social media delivers one of
the largest digital footprints to track people. In this study, for the first
time, we show that a wide range of behavioral competencies consisting of 16
in-demand soft skills can be automatically predicted from Instagram profiles
based on the following lists and other quantitative features using machine
learning algorithms. We also provide predictions on Big Five personality
traits. Models were built based on a sample of 400 Iranian volunteer users who
answered an online questionnaire and provided their Instagram usernames which
allowed us to crawl the public profiles. We applied several machine learning
algorithms to the uniformed data. Deep learning models mostly outperformed by
demonstrating 70% and 69% average Accuracy in two-level and three-level
classifications respectively. Creating a large pool of people with the highest
level of soft skills, and making more accurate evaluations of job candidates is
possible with the application of AI on social media user-generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Artificial Intelligence in Retinal Imaging for the detection
  of Systemic Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayushi Raj Bhatt, Rajkumar Vaghashiya, Meghna Kulkarni, Dr Prakash Kamaraj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (AI) in the form of an interpretable and
semiautomatic approach to stage grading ocular pathologies such as Diabetic
retinopathy, Hypertensive retinopathy, and other retinopathies on the backdrop
of major systemic diseases. The experimental study aims to evaluate an
explainable staged grading process without using deep Convolutional Neural
Networks (CNNs) directly. Many current CNN-based deep neural networks used for
diagnosing retinal disorders might have appreciable performance but fail to
pinpoint the basis driving their decisions. To improve these decisions'
transparency, we have proposed a clinician-in-the-loop assisted intelligent
workflow that performs a retinal vascular assessment on the fundus images to
derive quantifiable and descriptive parameters. The retinal vessel parameters
meta-data serve as hyper-parameters for better interpretation and
explainability of decisions. The semiautomatic methodology aims to have a
federated approach to AI in healthcare applications with more inputs and
interpretations from clinicians. The baseline process involved in the machine
learning pipeline through image processing techniques for optic disc detection,
vessel segmentation, and arteriole/venule identification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Probability of Necessity and Sufficiency of Explaining Graph
  Neural Networks: A Lower Bound Optimization Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichu Cai, Yuxuan Zhu, Xuexin Chen, Yuan Fang, Min Wu, Jie Qiao, Zhifeng Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability of Graph Neural Networks (GNNs) is critical to various GNN
applications but remains an open challenge. A convincing explanation should be
both necessary and sufficient simultaneously. However, existing GNN explaining
approaches focus on only one of the two aspects, necessity or sufficiency, or a
trade-off between the two. To search for the most necessary and sufficient
explanation, the Probability of Necessity and Sufficiency (PNS) can be applied
since it can mathematically quantify the necessity and sufficiency of an
explanation. Nevertheless, the difficulty of obtaining PNS due to
non-monotonicity and the challenge of counterfactual estimation limits its wide
use. To address the non-identifiability of PNS, we resort to a lower bound of
PNS that can be optimized via counterfactual estimation, and propose Necessary
and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound.
Specifically, we employ nearest neighbor matching to generate counterfactual
samples for the features, which is different from the random perturbation. In
particular, NSEG combines the edges and node features to generate an
explanation, where the common edge explanation is a special case of the
combined explanation. Empirical study shows that NSEG achieves excellent
performance in generating the most necessary and sufficient explanations among
a series of state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Significantly improving zero-shot X-ray pathology classification via
  fine-tuning <span class="highlight-title">pre-train</span>ed image-text encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongseong Jang, Daeun Kyung, Seung Hwan Kim, Honglak Lee, Kyunghoon Bae, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been successfully adopted to diverse domains
including pathology classification based on medical images. However,
large-scale and high-quality data to train powerful neural networks are rare in
the medical domain as the labeling must be done by qualified experts.
Researchers recently tackled this problem with some success by taking advantage
of models pre-trained on large-scale general domain data. Specifically,
researchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it
with chest X-ray images and paired reports to perform zero-shot pathology
classification, thus completely removing the need for pathology-annotated
images to train a classification model. Existing studies, however, fine-tuned
the pre-trained model with the same contrastive learning objective, and failed
to exploit the multi-labeled nature of medical image-report pairs. In this
paper, we propose a new fine-tuning strategy based on sentence sampling and
positive-pair loss relaxation for improving the downstream zero-shot pathology
classification performance, which can be applied to any pre-trained contrastive
image-text encoders. Our method consistently showed dramatically improved
zero-shot pathology classification performance on four different chest X-ray
datasets and 3 different pre-trained models (5.77% average AUROC increase). In
particular, fine-tuning CLIP with our method showed much comparable or
marginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1
score and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent
diseases from the CheXpert dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhrubajyoti Pathak, Sukumar Nandi, Priyankoo Sarmah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).
It is a well-studied topic in several resource-rich languages. However, the
development of computational linguistic resources is still in its infancy
despite the existence of numerous languages that are historically and literary
rich. Assamese, an Indian scheduled language, spoken by more than 25 million
people, falls under this category. In this paper, we present a Deep Learning
(DL)-based POS tagger for Assamese. The development process is divided into two
stages. In the first phase, several pre-trained word embeddings are employed to
train several tagging models. This allows us to evaluate the performance of the
word embeddings in the POS tagging task. The top-performing model from the
first phase is employed to annotate another set of new sentences. In the second
phase, the model is trained further using the fresh dataset. Finally, we attain
a tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for
further study on DL-based Assamese POS tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AICCSA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xumeng Gong, Cheng Yang, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning (CL), which can extract the information shared between
different contrastive views, has become a popular paradigm for vision
representation learning. Inspired by the success in computer vision, recent
work introduces CL into graph modeling, dubbed as graph contrastive learning
(GCL). However, generating contrastive views in graphs is more challenging than
that in images, since we have little prior knowledge on how to significantly
augment a graph without changing its labels. We argue that typical data
augmentation techniques (e.g., edge dropping) in GCL cannot generate diverse
enough contrastive views to filter out noises. Moreover, previous GCL methods
employ two view encoders with exactly the same neural architecture and tied
parameters, which further harms the diversity of augmented views. To address
this limitation, we propose a novel paradigm named model augmented GCL
(MA-GCL), which will focus on manipulating the architectures of view encoders
instead of perturbing graph inputs. Specifically, we present three
easy-to-implement model augmentation tricks for GCL, namely asymmetric, random
and shuffling, which can respectively help alleviate high- frequency noises,
enrich training instances and bring safer augmentations. All three tricks are
compatible with typical data augmentations. Experimental results show that
MA-GCL can achieve state-of-the-art performance on node classification
benchmarks by applying the three tricks on a simple base model. Extensive
studies also validate our motivation and the effectiveness of each trick.
(Code, data and appendix are available at https://github.com/GXM1141/MA-GCL. )
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving group robustness under noisy labels using predictive
  uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongpin Oh, Dae Lee, Jeunghyun Byun, Bonggun Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard empirical risk minimization (ERM) can underperform on certain
minority groups (i.e., waterbirds in lands or landbirds in water) due to the
spurious correlation between the input and its label. Several studies have
improved the worst-group accuracy by focusing on the high-loss samples. The
hypothesis behind this is that such high-loss samples are
\textit{spurious-cue-free} (SCF) samples. However, these approaches can be
problematic since the high-loss samples may also be samples with noisy labels
in the real-world scenarios. To resolve this issue, we utilize the predictive
uncertainty of a model to improve the worst-group accuracy under noisy labels.
To motivate this, we theoretically show that the high-uncertainty samples are
the SCF samples in the binary classification problem. This theoretical result
implies that the predictive uncertainty is an adequate indicator to identify
SCF samples in a noisy label setting. Motivated from this, we propose a novel
ENtropy based Debiasing (END) framework that prevents models from learning the
spurious cues while being robust to the noisy labels. In the END framework, we
first train the \textit{identification model} to obtain the SCF samples from a
training set using its predictive uncertainty. Then, another model is trained
on the dataset augmented with an oversampled SCF set. The experimental results
show that our END framework outperforms other strong baselines on several
real-world benchmarks that consider both the noisy labels and the
spurious-cues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning and Predicting Multimodal Vehicle Action Distributions in a
  Unified Probabilistic Model Without Labels <span class="chip">ICRA 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Richter, Patrick R. Barragán, Sertac Karaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified probabilistic model that learns a representative set of
discrete vehicle actions and predicts the probability of each action given a
particular scenario. Our model also enables us to estimate the distribution
over continuous trajectories conditioned on a scenario, representing what each
discrete action would look like if executed in that scenario. While our primary
objective is to learn representative action sets, these capabilities combine to
produce accurate multimodal trajectory predictions as a byproduct. Although our
learned action representations closely resemble semantically meaningful
categories (e.g., "go straight", "turn left", etc.), our method is entirely
self-supervised and does not utilize any manually generated labels or
categories. Our method builds upon recent advances in variational inference and
deep unsupervised clustering, resulting in full distribution estimates based on
deterministic model evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Fresh Perspectives on the Future of Autonomous
  Driving workshop, ICRA 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety Correction from Baseline: Towards the Risk-aware Policy in
  Robotics via Dual-agent Reinforcement Learning <span class="chip">IROS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linrui Zhang, Zichen Yan, Li Shen, Shoujie Li, Xueqian Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a risk-aware policy is essential but rather challenging in
unstructured robotic tasks. Safe reinforcement learning methods open up new
possibilities to tackle this problem. However, the conservative policy updates
make it intractable to achieve sufficient exploration and desirable performance
in complex, sample-expensive environments. In this paper, we propose a
dual-agent safe reinforcement learning strategy consisting of a baseline and a
safe agent. Such a decoupled framework enables high flexibility, data
efficiency and risk-awareness for RL-based control. Concretely, the baseline
agent is responsible for maximizing rewards under standard RL settings. Thus,
it is compatible with off-the-shelf training techniques of unconstrained
optimization, exploration and exploitation. On the other hand, the safe agent
mimics the baseline agent for policy improvement and learns to fulfill safety
constraints via off-policy RL tuning. In contrast to training from scratch,
safe policy correction requires significantly fewer interactions to obtain a
near-optimal policy. The dual policies can be optimized synchronously via a
shared replay buffer, or leveraging the pre-trained model or the
non-learning-based controller as a fixed baseline agent. Experimental results
show that our approach can learn feasible skills without prior knowledge as
well as deriving risk-averse counterparts from pre-trained unsafe policies. The
proposed method outperforms the state-of-the-art safe RL algorithms on
difficult robot locomotion and manipulation tasks with respect to both safety
constraint satisfaction and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2022 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Exploration in Resource-Restricted Reinforcement Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihai Wang, Taoxing Pan, Qi Zhou, Jie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications of reinforcement learning (RL), performing
actions requires consuming certain types of resources that are
non-replenishable in each episode. Typical applications include robotic control
with limited energy and video games with consumable items. In tasks with
non-replenishable resources, we observe that popular RL methods such as soft
actor critic suffer from poor sample efficiency. The major reason is that, they
tend to exhaust resources fast and thus the subsequent exploration is severely
restricted due to the absence of resources. To address this challenge, we first
formalize the aforementioned problem as a resource-restricted reinforcement
learning, and then propose a novel resource-aware exploration bonus (RAEB) to
make reasonable usage of resources. An appealing feature of RAEB is that, it
can significantly reduce unnecessary resource-consuming trials while
effectively encouraging the agent to explore unvisited states. Experiments
demonstrate that the proposed RAEB significantly outperforms state-of-the-art
exploration strategies in resource-restricted reinforcement learning
environments, improving the sample efficiency by up to an order of magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Particle-Based Score Estimation for State Space Model Learning in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angad Singh, Omar Makhlouf, Maximilian Igl, Joao Messias, Arnaud Doucet, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object state estimation is a fundamental problem for robotic
applications where a robot must interact with other moving objects. Typically,
other objects' relevant state features are not directly observable, and must
instead be inferred from observations. Particle filtering can perform such
inference given approximate transition and observation models. However, these
models are often unknown a priori, yielding a difficult parameter estimation
problem since observations jointly carry transition and observation noise. In
this work, we consider learning maximum-likelihood parameters using particle
methods. Recent methods addressing this problem typically differentiate through
time in a particle filter, which requires workarounds to the non-differentiable
resampling step, that yield biased or high variance gradient estimates. By
contrast, we exploit Fisher's identity to obtain a particle-based approximation
of the score function (the gradient of the log likelihood) that yields a low
variance estimate while only requiring stepwise differentiation through the
transition and observation models. We apply our method to real data collected
from autonomous vehicles (AVs) and show that it learns better models than
existing techniques and is more stable in training, yielding an effective
smoother for tracking the trajectories of vehicles around an AV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Agent's Decision-making in a Hierarchical Reinforcement
  Learning Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Muñoz, Ernesto Portugal, Angel Ayala, Bruno Fernandes, Francisco Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning is a machine learning approach based on behavioral
psychology. It is focused on learning agents that can acquire knowledge and
learn to carry out new tasks by interacting with the environment. However, a
problem occurs when reinforcement learning is used in critical contexts where
the users of the system need to have more information and reliability for the
actions executed by an agent. In this regard, explainable reinforcement
learning seeks to provide to an agent in training with methods in order to
explain its behavior in such a way that users with no experience in machine
learning could understand the agent's behavior. One of these is the
memory-based explainable reinforcement learning method that is used to compute
probabilities of success for each state-action pair using an episodic memory.
In this work, we propose to make use of the memory-based explainable
reinforcement learning method in a hierarchical environment composed of
sub-tasks that need to be first addressed to solve a more complex task. The end
goal is to verify if it is possible to provide to the agent the ability to
explain its actions in the global task as well as in the sub-tasks. The results
obtained showed that it is possible to use the memory-based method in
hierarchical environments with high-level tasks and compute the probabilities
of success to be used as a basis for explaining the agent's behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error-Aware B-PINNs: Improving Uncertainty Quantification in Bayesian
  Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Graf, Pablo Flores, Pavlos Protopapas, Karim Pichara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) are gaining popularity as a method
for solving differential equations. While being more feasible in some contexts
than the classical numerical techniques, PINNs still lack credibility. A remedy
for that can be found in Uncertainty Quantification (UQ) which is just
beginning to emerge in the context of PINNs. Assessing how well the trained
PINN complies with imposed differential equation is the key to tackling
uncertainty, yet there is lack of comprehensive methodology for this task. We
propose a framework for UQ in Bayesian PINNs (B-PINNs) that incorporates the
discrepancy between the B-PINN solution and the unknown true solution. We
exploit recent results on error bounds for PINNs on linear dynamical systems
and demonstrate the predictive uncertainty on a class of linear ODEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain
  Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Fu, Zesen Zhuang, Luyao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain has empowered computer systems to be more secure using a
distributed network. However, the current blockchain design suffers from
fairness issues in transaction ordering. Miners are able to reorder
transactions to generate profits, the so-called miner extractable value (MEV).
Existing research recognizes MEV as a severe security issue and proposes
potential solutions, including prominent Flashbots. However, previous studies
have mostly analyzed blockchain data, which might not capture the impacts of
MEV in a much broader AI society. Thus, in this research, we applied natural
language processing (NLP) methods to comprehensively analyze topics in tweets
on MEV. We collected more than 20000 tweets with \#MEV and \#Flashbots hashtags
and analyzed their topics. Our results show that the tweets discussed profound
topics of ethical concern, including security, equity, emotional sentiments,
and the desire for solutions to MEV. We also identify the co-movements of MEV
activities on blockchain and social media platforms. Our study contributes to
the literature at the interface of blockchain security, MEV solutions, and AI
ethics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we make virtual characters in a scene interact with their surrounding
objects through simple instructions? Is it possible to synthesize such motion
plausibly with a diverse set of objects and instructions? Inspired by these
questions, we present the first framework to synthesize the full-body motion of
virtual human characters performing specified actions with 3D objects placed
within their reach. Our system takes as input textual instructions specifying
the objects and the associated intentions of the virtual characters and outputs
diverse sequences of full-body motions. This is in contrast to existing work,
where full-body action synthesis methods generally do not consider object
interactions, and human-object interaction methods focus mainly on synthesizing
hand or finger movements for grasping objects. We accomplish our objective by
designing an intent-driven full-body motion generator, which uses a pair of
decoupled conditional variational autoencoders (CVAE) to learn the motion of
the body parts in an autoregressive manner. We also optimize for the positions
of the objects with six degrees of freedom (6DoF) such that they plausibly fit
within the hands of the synthesized characters. We compare our proposed method
with the existing methods of motion synthesis and establish a new and stronger
state-of-the-art for the task of intent-driven motion synthesis. Through a user
study, we further show that our synthesized full-body motions appear more
realistic to the participants in more than 80% of scenarios compared to the
current state-of-the-art methods, and are perceived to be as good as the ground
truth on several occasions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative structured normalizing flow Gaussian processes applied to
  spectroscopic data <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Klein, Nishant Panda, Patrick Gasda, Diane Oyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel generative model for mapping inputs to
structured, high-dimensional outputs using structured conditional normalizing
flows and Gaussian process regression. The model is motivated by the need to
characterize uncertainty in the input/output relationship when making
inferences on new data. In particular, in the physical sciences, limited
training data may not adequately characterize future observed data; it is
critical that models adequately indicate uncertainty, particularly when they
may be asked to extrapolate. In our proposed model, structured conditional
normalizing flows provide parsimonious latent representations that relate to
the inputs through a Gaussian process, providing exact likelihood calculations
and uncertainty that naturally increases away from the training data inputs. We
demonstrate the methodology on laser-induced breakdown spectroscopy data from
the ChemCam instrument onboard the Mars rover Curiosity. ChemCam was designed
to recover the chemical composition of rock and soil samples by measuring the
spectral properties of plasma atomic emissions induced by a laser pulse. We
show that our model can generate realistic spectra conditional on a given
chemical composition and that we can use the model to perform uncertainty
quantification of chemical compositions for new observed spectra. Based on our
results, we anticipate that our proposed modeling approach may be useful in
other scientific domains with high-dimensional, complex structure where it is
important to quantify predictive uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best paper award, 1st Annual AAAI Workshop on AI to Accelerate
  Science and Engineering (AI2ASE), February 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Reachability Analysis of Neural Network-Controlled Systems via
  Adaptive Polytopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Entesari, Mahyar Fazlyab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-approximating the reachable sets of dynamical systems is a fundamental
problem in safety verification and robust control synthesis. The representation
of these sets is a key factor that affects the computational complexity and the
approximation error. In this paper, we develop a new approach for
over-approximating the reachable sets of neural network dynamical systems using
adaptive template polytopes. We use the singular value decomposition of linear
layers along with the shape of the activation functions to adapt the geometry
of the polytopes at each time step to the geometry of the true reachable sets.
We then propose a branch-and-bound method to compute accurate
over-approximations of the reachable sets by the inferred templates. We
illustrate the utility of the proposed approach in the reachability analysis of
linear systems driven by neural network controllers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Maximum Inner Product Search in High Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Tiwari, Ryan Kang, Je-Yong Lee, Luke Lee, Chris Piech, Sebastian Thrun, Ilan Shomorony, Martin Jinye Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximum Inner Product Search (MIPS) is a popular problem in the machine
learning literature due to its applicability in a wide array of applications,
such as recommender systems. In high-dimensional settings, however, MIPS
queries can become computationally expensive as most existing solutions do not
scale well with data dimensionality. In this work, we present a
state-of-the-art algorithm for the MIPS problem in high dimensions, dubbed
BanditMIPS. BanditMIPS is a randomized algorithm that borrows techniques from
multi-armed bandits to reduce the MIPS problem to a best-arm identification
problem. BanditMIPS reduces the complexity of state-of-the-art algorithms from
$O(\sqrt{d})$ to $O(\text{log}d)$, where $d$ is the dimension of the problem
data vectors. On high-dimensional real-world datasets, BanditMIPS runs
approximately 12 times faster than existing approaches and returns the same
solution. BanditMIPS requires no preprocessing of the data and includes a
hyperparameter that practitioners may use to trade off accuracy and runtime. We
also propose a variant of our algorithm, named BanditMIPS-$\alpha$, which
employs non-uniform sampling across the data dimensions to provide further
speedups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReDDIT: Regret Detection and Domain Identification from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fazlourrahman Balouchzahi, Sabur Butt, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a study of regret and its expression on social
media platforms. Specifically, we present a novel dataset of Reddit texts that
have been classified into three classes: Regret by Action, Regret by Inaction,
and No Regret. We then use this dataset to investigate the language used to
express regret on Reddit and to identify the domains of text that are most
commonly associated with regret. Our findings show that Reddit users are most
likely to express regret for past actions, particularly in the domain of
relationships. We also found that deep learning models using GloVe embedding
outperformed other models in all experiments, indicating the effectiveness of
GloVe for representing the meaning and context of words in the domain of
regret. Overall, our study provides valuable insights into the nature and
prevalence of regret on social media, as well as the potential of deep learning
and word embeddings for analyzing and understanding emotional language in
online text. These findings have implications for the development of natural
language processing algorithms and the design of social media platforms that
support emotional expression and communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Policy Optimization in Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Masudur Rahman, Yexiang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The policy gradient method enjoys the simplicity of the objective where the
agent optimizes the cumulative reward directly. Moreover, in the continuous
action domain, parameterized distribution of action distribution allows easy
control of exploration, resulting from the variance of the representing
distribution. Entropy can play an essential role in policy optimization by
selecting the stochastic policy, which eventually helps better explore the
environment in reinforcement learning (RL). However, the stochasticity often
reduces as the training progresses; thus, the policy becomes less exploratory.
Additionally, certain parametric distributions might only work for some
environments and require extensive hyperparameter tuning. This paper aims to
mitigate these issues. In particular, we propose an algorithm called Robust
Policy Optimization (RPO), which leverages a perturbed distribution. We
hypothesize that our method encourages high-entropy actions and provides a way
to represent the action space better. We further provide empirical evidence to
verify our hypothesis. We evaluated our methods on various continuous control
tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed
that in many settings, RPO increases the policy entropy early in training and
then maintains a certain level of entropy throughout the training period.
Eventually, our agent RPO shows consistently improved performance compared to
PPO and other techniques: entropy regularization, different distributions, and
data augmentation. Furthermore, in several settings, our method stays robust in
performance, while other baseline mechanisms fail to improve and even worsen
the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Nonconvex Optimization with Guaranteed Privacy and
  Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Wang, Tamer Basar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy protection and nonconvexity are two challenging problems in
decentralized optimization and learning involving sensitive data. Despite some
recent advances addressing each of the two problems separately, no results have
been reported that have theoretical guarantees on both privacy protection and
saddle/maximum avoidance in decentralized nonconvex optimization. We propose a
new algorithm for decentralized nonconvex optimization that can enable both
rigorous differential privacy and saddle/maximum avoiding performance. The new
algorithm allows the incorporation of persistent additive noise to enable
rigorous differential privacy for data samples, gradients, and intermediate
optimization variables without losing provable convergence, and thus
circumventing the dilemma of trading accuracy for privacy in differential
privacy design. More interestingly, the algorithm is theoretically proven to be
able to efficiently { guarantee accuracy by avoiding} convergence to local
maxima and saddle points, which has not been reported before in the literature
on decentralized nonconvex optimization. The algorithm is efficient in both
communication (it only shares one variable in each iteration) and computation
(it is encryption-free), and hence is promising for large-scale nonconvex
optimization and learning involving high-dimensional optimization parameters.
Numerical experiments for both a decentralized estimation problem and an
Independent Component Analysis (ICA) problem confirm the effectiveness of the
proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causes and Cures for Interference in Multilingual Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, Shruti Bhosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation models can benefit from synergy between
different language pairs, but also suffer from interference. While there is a
growing number of sophisticated methods that aim to eliminate interference, our
understanding of interference as a phenomenon is still limited. This work
identifies the main factors that contribute to interference in multilingual
machine translation. Through systematic experimentation, we find that
interference (or synergy) are primarily determined by model size, data size,
and the proportion of each language pair within the total dataset. We observe
that substantial interference occurs mainly when the model is very small with
respect to the available training data, and that using standard transformer
configurations with less than one billion parameters largely alleviates
interference and promotes synergy. Moreover, we show that tuning the sampling
temperature to control the proportion of each language pair in the data is key
to balancing the amount of interference between low and high resource language
pairs effectively, and can lead to superior performance overall.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Self-supervised</span> Learning with Contextualized Target
  Representations for Vision, Speech and Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexei Baevski, Arun Babu, Wei-Ning Hsu, Michael Auli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current self-supervised learning algorithms are often modality-specific and
require large amounts of computational resources. To address these issues, we
increase the training efficiency of data2vec, a learning objective that
generalizes across several modalities. We do not encode masked tokens, use a
fast convolutional decoder and amortize the effort to build teacher
representations. data2vec 2.0 benefits from the rich contextualized target
representations introduced in data2vec which enable a fast self-supervised
learner. Experiments on ImageNet-1K image classification show that data2vec 2.0
matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,
on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x
less time, and on GLUE natural language understanding it matches a retrained
RoBERTa model in half the time. Trading some speed for accuracy results in
ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invariant Lipschitz Bandits: A Side Observation Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Phuong Tran, The-Anh Ta, Long Tran-Thanh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetry arises in many optimization and decision-making problems, and has
attracted considerable attention from the optimization community: By utilizing
the existence of such symmetries, the process of searching for optimal
solutions can be improved significantly. Despite its success in (offline)
optimization, the utilization of symmetries has not been well examined within
the online optimization settings, especially in the bandit literature. As such,
in this paper we study the invariant Lipschitz bandit setting, a subclass of
the Lipschitz bandits where the reward function and the set of arms are
preserved under a group of transformations. We introduce an algorithm named
\texttt{UniformMesh-N}, which naturally integrates side observations using
group orbits into the \texttt{UniformMesh} algorithm
(\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of
arms. Using the side-observation approach, we prove an improved regret upper
bound, which depends on the cardinality of the group, given that the group is
finite. We also prove a matching regret's lower bound for the invariant
Lipschitz bandit class (up to logarithmic factors). We hope that our work will
ignite further investigation of symmetry in bandit theory and sequential
decision-making theory in general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analytical Engines With Context-Rich Processing: Towards Efficient
  Next-Generation Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Sanca, Anastasia Ailamaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As modern data pipelines continue to collect, produce, and store a variety of
data formats, extracting and combining value from traditional and context-rich
sources such as strings, text, video, audio, and logs becomes a manual process
where such formats are unsuitable for RDBMS. To tap into the dark data, domain
experts analyze and extract insights and integrate them into the data
repositories. This process can involve out-of-DBMS, ad-hoc analysis, and
processing resulting in ETL, engineering effort, and suboptimal performance.
While AI systems based on ML models can automate the analysis process, they
often further generate context-rich answers. Using multiple sources of truth,
for either training the models or in the form of knowledge bases, further
exacerbates the problem of consolidating the data of interest.
  We envision an analytical engine co-optimized with components that enable
context-rich analysis. Firstly, as the data from different sources or resulting
from model answering cannot be cleaned ahead of time, we propose using online
data integration via model-assisted similarity operations. Secondly, we aim for
a holistic pipeline cost- and rule-based optimization across relational and
model-based operators. Thirdly, with increasingly heterogeneous hardware and
equally heterogeneous workloads ranging from traditional relational analytics
to generative model inference, we envision a system that just-in-time adapts to
the complex analytical query requirements. To solve increasingly complex
analytical problems, ML offers attractive solutions that must be combined with
traditional analytical processing and benefit from decades of database
community research to achieve scalability and performance effortless for the
end user.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal
  Imputation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell A. Xu, Alexander Moreno, Supriya Nagesh, V. Burak Aydemir, David W. Wetter, Santosh Kumar, James M. Rehg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The promise of Mobile Health (mHealth) is the ability to use wearable sensors
to monitor participant physiology at high frequencies during daily life to
enable temporally-precise health interventions. However, a major challenge is
frequent missing data. Despite a rich imputation literature, existing
techniques are ineffective for the pulsative signals which comprise many
mHealth applications, and a lack of available datasets has stymied progress. We
address this gap with PulseImpute, the first large-scale pulsative signal
imputation challenge which includes realistic mHealth missingness models, an
extensive set of baselines, and clinically-relevant downstream tasks. Our
baseline models include a novel transformer-based architecture designed to
exploit the structure of pulsative signals. We hope that PulseImpute will
enable the ML community to tackle this significant and challenging task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensions Between the Proxies of Human Values in AI <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Datta, Daniel Nissani, Max Cembalest, Akash Khanna, Haley Massa, John P. Dickerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by mitigating potentially harmful impacts of technologies, the AI
community has formulated and accepted mathematical definitions for certain
pillars of accountability: e.g. privacy, fairness, and model transparency. Yet,
we argue this is fundamentally misguided because these definitions are
imperfect, siloed constructions of the human values they hope to proxy, while
giving the guise that those values are sufficiently embedded in our
technologies. Under popularized methods, tensions arise when practitioners
attempt to achieve each pillar of fairness, privacy, and transparency in
isolation or simultaneously. In this position paper, we push for redirection.
We argue that the AI community needs to consider all the consequences of
choosing certain formulations of these pillars -- not just the technical
incompatibilities, but also the effects within the context of deployment. We
point towards sociotechnical research for frameworks for the latter, but push
for broader efforts into implementing these in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contributed Talk, NeurIPS 2022 Workshop on Algorithmic Fairness
  through the Lens of Causality and Privacy; To be published in 2023 IEEE
  Conference on Secure and Trustworthy Machine Learning (SaTML)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Coarse-Grained Potentials of Protein Thermodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Majewski, Adrià Pérez, Philipp Thölke, Stefan Doerr, Nicholas E. Charron, Toni Giorgino, Brooke E. Husic, Cecilia Clementi, Frank Noé, Gianni De Fabritiis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A generalized understanding of protein dynamics is an unsolved scientific
problem, the solution of which is critical to the interpretation of the
structure-function relationships that govern essential biological processes.
Here, we approach this problem by constructing coarse-grained molecular
potentials based on artificial neural networks and grounded in statistical
mechanics. For training, we build a unique dataset of unbiased all-atom
molecular dynamics simulations of approximately 9 ms for twelve different
proteins with multiple secondary structure arrangements. The coarse-grained
models are capable of accelerating the dynamics by more than three orders of
magnitude while preserving the thermodynamics of the systems. Coarse-grained
simulations identify relevant structural states in the ensemble with comparable
energetics to the all-atom systems. Furthermore, we show that a single
coarse-grained potential can integrate all twelve proteins and can capture
experimental structural features of mutated proteins. These results indicate
that machine learning coarse-grained potentials could provide a feasible
approach to simulate and understand protein dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N. Foerster, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of challenging benchmarks has played a key role in the
recent progress of machine learning. In cooperative multi-agent reinforcement
learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular
testbed for centralised training with decentralised execution. However, after
years of sustained improvement on SMAC, algorithms now achieve near-perfect
performance. In this work, we conduct new analysis demonstrating that SMAC is
not sufficiently stochastic to require complex closed-loop policies. In
particular, we show that an open-loop policy conditioned only on the timestep
can achieve non-trivial win rates for many SMAC scenarios. To address this
limitation, we introduce SMACv2, a new version of the benchmark where scenarios
are procedurally generated and require agents to generalise to previously
unseen settings (from the same distribution) during evaluation. We show that
these changes ensure the benchmark requires the use of closed-loop policies. We
evaluate state-of-the-art algorithms on SMACv2 and show that it presents
significant challenges not present in the original benchmark. Our analysis
illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can
help benchmark the next generation of MARL methods. Videos of training are
available at https://sites.google.com/view/smacv2
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Marginalized Importance Sampling to High-Dimensional
  State-Spaces via State Abstraction <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brahma S. Pavse, Josiah P. Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of off-policy evaluation (OPE) in reinforcement
learning (RL), where the goal is to estimate the performance of an evaluation
policy, $\pi_e$, using a fixed dataset, $\mathcal{D}$, collected by one or more
policies that may be different from $\pi_e$. Current OPE algorithms may produce
poor OPE estimates under policy distribution shift i.e., when the probability
of a particular state-action pair occurring under $\pi_e$ is very different
from the probability of that same pair occurring in $\mathcal{D}$ (Voloshin et
al. 2021, Fu et al. 2021). In this work, we propose to improve the accuracy of
OPE estimators by projecting the high-dimensional state-space into a
low-dimensional state-space using concepts from the state abstraction
literature. Specifically, we consider marginalized importance sampling (MIS)
OPE algorithms which compute state-action distribution correction ratios to
produce their OPE estimate. In the original ground state-space, these ratios
may have high variance which may lead to high variance OPE. However, we prove
that in the lower-dimensional abstract state-space the ratios can have lower
variance resulting in lower variance OPE. We then highlight the challenges that
arise when estimating the abstract ratios from data, identify sufficient
conditions to overcome these issues, and present a minimax optimization problem
whose solution yields these abstract ratios. Finally, our empirical evaluation
on difficult, high-dimensional state-space OPE tasks shows that the abstract
ratios can make MIS OPE estimators achieve lower mean-squared error and more
robust to hyperparameter tuning than the ground ratios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding continuous operator learning through Physics-based boundary
  constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadim Saad, Gaurav Gupta, Shima Alizadeh, Danielle C. Maddix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boundary conditions (BCs) are important groups of physics-enforced
constraints that are necessary for solutions of Partial Differential Equations
(PDEs) to satisfy at specific spatial locations. These constraints carry
important physical meaning, and guarantee the existence and the uniqueness of
the PDE solution. Current neural-network based approaches that aim to solve
PDEs rely only on training data to help the model learn BCs implicitly. There
is no guarantee of BC satisfaction by these models during evaluation. In this
work, we propose Boundary enforcing Operator Network (BOON) that enables the BC
satisfaction of neural operators by making structural changes to the operator
kernel. We provide our refinement procedure, and demonstrate the satisfaction
of physics-based BCs, e.g. Dirichlet, Neumann, and periodic by the solutions
obtained by BOON. Numerical experiments based on multiple PDEs with a wide
variety of applications indicate that the proposed approach ensures
satisfaction of BCs, and leads to more accurate solutions over the entire
domain. The proposed correction method exhibits a (2X-20X) improvement over a
given operator model in relative $L^2$ error (0.000084 relative $L^2$ error for
Burgers' equation).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Nadim and Gaurav contributed equally in this work. 31 pages, 7
  figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MABSplit: Faster Forest Training Using Multi-Armed Bandits <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Tiwari, Ryan Kang, Je-Yong Lee, Sebastian Thrun, Chris Piech, Ilan Shomorony, Martin Jinye Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random forests are some of the most widely used machine learning models
today, especially in domains that necessitate interpretability. We present an
algorithm that accelerates the training of random forests and other popular
tree-based learning methods. At the core of our algorithm is a novel
node-splitting subroutine, dubbed MABSplit, used to efficiently find split
points when constructing decision trees. Our algorithm borrows techniques from
the multi-armed bandit literature to judiciously determine how to allocate
samples and computational power across candidate split points. We provide
theoretical guarantees that MABSplit improves the sample complexity of each
node split from linear to logarithmic in the number of data points. In some
settings, MABSplit leads to 100x faster training (an 99% reduction in training
time) without any decrease in generalization performance. We demonstrate
similar speedups when MABSplit is used across a variety of forest-based
variants, such as Extremely Random Forests and Random Patches. We also show our
algorithm can be used in both classification and regression tasks. Finally, we
show that MABSplit outperforms existing methods in generalization performance
and feature importance calculations under a fixed computational budget. All of
our experimental results are reproducible via a one-line script at
https://github.com/ThrunGroup/FastForest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2022, 30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning threshold neurons via the "edge of stability" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangjun Ahn, Sébastien Bubeck, Sinho Chewi, Yin Tat Lee, Felipe Suarez, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing analyses of neural network training often operate under the
unrealistic assumption of an extremely small learning rate. This lies in stark
contrast to practical wisdom and empirical studies, such as the work of J.
Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the "edge of
stability" or "unstable convergence") and potential benefits for generalization
in the large learning rate regime. Despite a flurry of recent works on this
topic, however, the latter effect is still poorly understood. In this paper, we
take a step towards understanding genuinely non-convex training dynamics with
large learning rates by performing a detailed analysis of gradient descent for
simplified models of two-layer neural networks. For these models, we provably
establish the edge of stability phenomenon and discover a sharp phase
transition for the step size below which the neural network fails to learn
"threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This
elucidates one possible mechanism by which the edge of stability can in fact
lead to better generalization, as threshold neurons are basic building blocks
with useful inductive bias for many tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmonic (Quantum) Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atiyo Ghosh, Antonio A. Gentile, Mario Dagrada, Chul Lee, Seong-hyok Kim, Hyukgeun Cha, Yunjun Choi, Brad Kim, Jeong-il Kye, Vincent E. Elfving
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmonic functions are abundant in nature, appearing in limiting cases of
Maxwell's, Navier-Stokes equations, the heat and the wave equation.
Consequently, there are many applications of harmonic functions, spanning
applications from industrial process optimisation to robotic path planning and
the calculation of first exit times of random walks. Despite their ubiquity and
relevance, there have been few attempts to develop effective means of
representing harmonic functions in the context of machine learning
architectures, either in machine learning on classical computers, or in the
nascent field of quantum machine learning. Architectures which impose or
encourage an inductive bias towards harmonic functions would facilitate
data-driven modelling and the solution of inverse problems in a range of
applications. For classical neural networks, it has already been established
how leveraging inductive biases can in general lead to improved performance of
learning algorithms. The introduction of such inductive biases within a quantum
machine learning setting is instead still in its nascent stages. In this work,
we derive exactly-harmonic (conventional- and quantum-) neural networks in two
dimensions for simply-connected domains by leveraging the characteristics of
holomorphic complex functions. We then demonstrate how these can be
approximately extended to multiply-connected two-dimensional domains using
techniques inspired by domain decomposition in physics-informed neural
networks. We further provide architectures and training protocols to
effectively impose approximately harmonic constraints in three dimensions and
higher, and as a corollary we report divergence-free network architectures in
arbitrary dimensions. Our approaches are demonstrated with applications to heat
transfer, electrostatics and robot navigation, with comparisons to
physics-informed neural networks included.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (main), 6 pages (supplementary), 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergent Data-driven Regularizations for CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Kabri, Alexander Auras, Danilo Riccio, Hartmut Bauermeister, Martin Benning, Michael Moeller, Martin Burger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reconstruction of images from their corresponding noisy Radon transform
is a typical example of an ill-posed linear inverse problem as arising in the
application of computerized tomography (CT). As the (na\"{\i}ve) solution does
not depend on the measured data continuously, regularization is needed to
re-establish a continuous dependence. In this work, we investigate simple, but
yet still provably convergent approaches to learning linear regularization
methods from data. More specifically, we analyze two approaches: One generic
linear regularization that learns how to manipulate the singular values of the
linear operator in an extension of [1], and one tailored approach in the
Fourier domain that is specific to CT-reconstruction. We prove that such
approaches become convergent regularization methods as well as the fact that
the reconstructions they provide are typically much smoother than the training
data they were trained on. Finally, we compare the spectral as well as the
Fourier-based approaches for CT-reconstruction numerically, discuss their
advantages and disadvantages and investigate the effect of discretization
errors at different resolutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The alignment problem from a deep learning perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ngo, Lawrence Chan, Sören Mindermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the coming decades, artificial general intelligence (AGI) may surpass
human capabilities at a wide range of important tasks. We outline a case for
expecting that, without substantial effort to prevent it, AGIs could learn to
pursue goals which are very undesirable (in other words, misaligned) from a
human perspective. We argue that AGIs trained in similar ways as today's most
capable models could learn to act deceptively to receive higher reward; learn
internally-represented goals which generalize beyond their training
distributions; and pursue those goals using power-seeking strategies. We
outline how the deployment of misaligned AGIs might irreversibly undermine
human control over the world, and briefly review research directions aimed at
preventing these problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principal-Agent Hypothesis Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Bates, Michael I. Jordan, Michael Sklar, Jake A. Soloff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the relationship between a regulator (the principal) and a
pharmaceutical company (the agent). The pharmaceutical company wishes to sell a
product to make a profit, and the FDA wishes to ensure that only efficacious
drugs are released to the public. The efficacy of the drug is not known to the
FDA, so the pharmaceutical company must run a costly trial to prove efficacy to
the FDA. Critically, the statistical protocol used to establish efficacy
affects the behavior of a strategic, self-interested pharmaceutical company; a
lower standard of statistical evidence incentivizes the pharmaceutical company
to run more trials for drugs that are less likely to be effective, since the
drug may pass the trial by chance, resulting in large profits. The interaction
between the statistical protocol and the incentives of the pharmaceutical
company is crucial to understanding this system and designing protocols with
high social utility. In this work, we discuss how the principal and agent can
enter into a contract with payoffs based on statistical evidence. When there is
stronger evidence for the quality of the product, the principal allows the
agent to make a larger profit. We show how to design contracts that are robust
to an agent's strategic actions, and derive the optimal contract in the
presence of strategic behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning particle swarming models from data with Gaussian processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.02735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.02735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinchao Feng, Charles Kulick, Yunxiang Ren, Sui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interacting particle or agent systems that display a rich variety of swarming
behaviours are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and swarming. In this paper, we study the data-driven discovery of a
second-order particle swarming model that describes the evolution of $N$
particles in $\mathbb{R}^d$ under radial interactions. We propose a learning
approach that models the latent radial interaction function as Gaussian
processes, which can simultaneously fulfill two inference goals: one is the
nonparametric inference of {the} interaction function with pointwise
uncertainty quantification, and the other one is the inference of unknown
scalar parameters in the non-collective friction forces of the system. We
formulate the learning problem as a statistical inverse problem and provide a
detailed analysis of recoverability conditions, establishing that a coercivity
condition is sufficient for recoverability. Given data collected from $M$ i.i.d
trajectories with independent Gaussian observational noise, we provide a
finite-sample analysis, showing that our posterior mean estimator converges in
a Reproducing kernel Hilbert space norm, at an optimal rate in $M$ equal to the
one in the classical 1-dimensional Kernel Ridge regression. As a byproduct, we
show we can obtain a parametric learning rate in $M$ for the posterior marginal
variance using $L^{\infty}$ norm, and the rate could also involve $N$ and $L$
(the number of observation time instances for each trajectory), depending on
the condition number of the inverse problem. Numerical results on systems that
exhibit different swarming behaviors demonstrate efficient learning of our
approach from scarce noisy trajectory data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages; Appendix 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning soft interventions in complex equilibrium systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.05729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.05729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michel Besserve, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex systems often contain feedback loops that can be described as cyclic
causal models. Intervening in such systems may lead to counterintuitive
effects, which cannot be inferred directly from the graph structure. After
establishing a framework for differentiable soft interventions based on Lie
groups, we take advantage of modern automatic differentiation techniques and
their application to implicit functions in order to optimize interventions in
cyclic causal models. We illustrate the use of this framework by investigating
scenarios of transition to sustainable economies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sparse Training via More Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoyi Huang, Bowen Lei, Dongkuan Xu, Hongwu Peng, Yue Sun, Mimi Xie, Caiwen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-parameterization of deep neural networks (DNNs) has shown high
prediction accuracy for many applications. Although effective, the large number
of parameters hinders its popularity on resource-limited devices and has an
outsize environmental impact. Sparse training (using a fixed number of nonzero
weights in each iteration) could significantly mitigate the training costs by
reducing the model size. However, existing sparse training methods mainly use
either random-based or greedy-based drop-and-grow strategies, resulting in
local minimal and low accuracy. In this work, we consider the dynamic sparse
training as a sparse connectivity search problem and design an exploitation and
exploration acquisition function to escape from local optima and saddle points.
We further design an acquisition function and provide the theoretical
guarantees for the proposed method and clarify its convergence property.
Experimental results show that sparse models (up to 98\% sparsity) obtained by
our proposed method outperform the SOTA sparse training methods on a wide
variety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10,
ResNet-50 / CIFAR-100, our method has even higher accuracy than dense models.
On ResNet-50 / ImageNet, the proposed method has up to 8.2\% accuracy
improvement compared to SOTA sparse training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XRand: Differentially Private Defense against Explanation-Guided Attacks <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Truc Nguyen, Phung Lai, NhatHai Phan, My T. Thai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent development in the field of explainable artificial intelligence (XAI)
has helped improve trust in Machine-Learning-as-a-Service (MLaaS) systems, in
which an explanation is provided together with the model prediction in response
to each query. However, XAI also opens a door for adversaries to gain insights
into the black-box models in MLaaS, thereby making the models more vulnerable
to several attacks. For example, feature-based explanations (e.g., SHAP) could
expose the top important features that a black-box model focuses on. Such
disclosure has been exploited to craft effective backdoor triggers against
malware classifiers. To address this trade-off, we introduce a new concept of
achieving local differential privacy (LDP) in the explanations, and from that
we establish a defense, called XRand, against such attacks. We show that our
mechanism restricts the information that the adversary can learn about the top
important features, while maintaining the faithfulness of the explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REPAIR: REnormalizing Permuted Activations for Interpolation Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, Behnam Neyshabur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we look into the conjecture of Entezari et al. (2021) which
states that if the permutation invariance of neural networks is taken into
account, then there is likely no loss barrier to the linear interpolation
between SGD solutions. First, we observe that neuron alignment methods alone
are insufficient to establish low-barrier linear connectivity between SGD
solutions due to a phenomenon we call variance collapse: interpolated deep
networks suffer a collapse in the variance of their activations, causing poor
performance. Next, we propose REPAIR (REnormalizing Permuted Activations for
Interpolation Repair) which mitigates variance collapse by rescaling the
preactivations of such interpolated networks. We explore the interaction
between our method and the choice of normalization layer, network width, and
depth, and demonstrate that using REPAIR on top of neuron alignment methods
leads to 60%-100% relative barrier reduction across a wide variety of
architecture families and tasks. In particular, we report a 74% barrier
reduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on
CIFAR10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Commercial Cooling Systems Using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Luo, Cosmin Paduraru, Octavian Voicu, Yuri Chervonyi, Scott Munns, Jerry Li, Crystal Qian, Praneet Dutta, Jared Quincy Davis, Ningjia Wu, Xingwei Yang, Chu-Ming Chang, Ted Li, Rob Rose, Mingyan Fan, Hootan Nakhost, Tinglin Liu, Brian Kirkman, Frank Altamura, Lee Cline, Patrick Tonker, Joel Gouker, Dave Uden, Warren Buddy Bryan, Jason Law, Deeni Fatiha, Neil Satra, Juliet Rothenberg, Mandeep Waraich, Molly Carlin, Satish Tallapaka, Sims Witherspoon, David Parish, Peter Dolan, Chenyu Zhao, Daniel J. Mankowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is a technical overview of DeepMind and Google's recent work on
reinforcement learning for controlling commercial cooling systems. Building on
expertise that began with cooling Google's data centers more efficiently, we
recently conducted live experiments on two real-world facilities in partnership
with Trane Technologies, a building management system provider. These live
experiments had a variety of challenges in areas such as evaluation, learning
from offline data, and constraint satisfaction. Our paper describes these
challenges in the hope that awareness of them will benefit future applied RL
work. We also describe the way we adapted our RL system to deal with these
challenges, resulting in energy savings of approximately 9% and 13%
respectively at the two live experiment sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmentation-guided Domain Adaptation for Efficient Depth Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Märkert, Martin Sunkel, Anselm Haselhoff, Stefan Rudolph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complete depth information and efficient estimators have become vital
ingredients in scene understanding for automated driving tasks. A major problem
for LiDAR-based depth completion is the inefficient utilization of convolutions
due to the lack of coherent information as provided by the sparse nature of
uncorrelated LiDAR point clouds, which often leads to complex and
resource-demanding networks. The problem is reinforced by the expensive
aquisition of depth data for supervised training. In this work, we propose an
efficient depth completion model based on a vgg05-like CNN architecture and
propose a semi-supervised domain adaptation approach to transfer knowledge from
synthetic to real world data to improve data-efficiency and reduce the need for
a large database. In order to boost spatial coherence, we guide the learning
process using segmentations as additional source of information. The efficiency
and accuracy of our approach is evaluated on the KITTI dataset. Our approach
improves on previous efficient and low parameter state of the art approaches
while having a noticeably lower computational footprint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Covariance-Generalized Matching Component Analysis for Data Fusion and
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.13194v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.13194v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Lorenzo, Sean O'Rourke, Theresa Scarnati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to encode additional statistical information in data fusion and
transfer learning applications, we introduce a generalized covariance
constraint for the matching component analysis (MCA) transfer learning
technique. We provide a closed-form solution to the resulting
covariance-generalized optimization problem and an algorithm for its
computation. We call the resulting technique -- applicable to both data fusion
and transfer learning -- covariance-generalized MCA (CGMCA). We also
demonstrate via numerical experiments that CGMCA is capable of meaningfully
encoding into its maps more information than MCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: Made major organizational changes; added numerical results;
  eliminated statement and proof of lemma in favor of offering a solution
  achieving a cited bound</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Graph Structure Refinement for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06545v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06545v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, Yanfang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph structure learning (GSL), which aims to learn the adjacency matrix for
graph neural networks (GNNs), has shown great potential in boosting the
performance of GNNs. Most existing GSL works apply a joint learning framework
where the estimated adjacency matrix and GNN parameters are optimized for
downstream tasks. However, as GSL is essentially a link prediction task, whose
goal may largely differ from the goal of the downstream task. The inconsistency
of these two goals limits the GSL methods to learn the potential optimal graph
structure. Moreover, the joint learning framework suffers from scalability
issues in terms of time and space during the process of estimation and
optimization of the adjacency matrix. To mitigate these issues, we propose a
graph structure refinement (GSR) framework with a pretrain-finetune pipeline.
Specifically, The pre-training phase aims to comprehensively estimate the
underlying graph structure by a multi-view contrastive learning framework with
both intra- and inter-view link prediction tasks. Then, the graph structure is
refined by adding and removing edges according to the edge probabilities
estimated by the pre-trained model. Finally, the fine-tuning GNN is initialized
by the pre-trained model and optimized toward downstream tasks. With the
refined graph structure remaining static in the fine-tuning space, GSR avoids
estimating and optimizing graph structure in the fine-tuning phase which enjoys
great scalability and efficiency. Moreover, the fine-tuning GNN is boosted by
both migrating knowledge and refining graphs. Extensive experiments are
conducted to evaluate the effectiveness (best performance on six benchmark
datasets), efficiency, and scalability (13.8x faster using 32.8% GPU memory
compared to the best GSL baseline on Cora) of the proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>there are some issues of the paper, we need to revise the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Amortized Inference for Causal Structure Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars Lorch, Scott Sussex, Jonas Rothfuss, Andreas Krause, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal structure poses a combinatorial search problem that
typically involves evaluating structures with a score or independence test. The
resulting search is costly, and designing suitable scores or tests that capture
prior knowledge is difficult. In this work, we propose to amortize causal
structure learning. Rather than searching over structures, we train a
variational inference model to directly predict the causal structure from
observational or interventional data. This allows our inference model to
acquire domain-specific inductive biases for causal discovery solely from data
generated by a simulator, bypassing both the hand-engineering of suitable score
functions and the search over graphs. The architecture of our inference model
emulates permutation invariances that are crucial for statistical efficiency in
structure learning, which facilitates generalization to significantly larger
problem instances than seen during training. On synthetic data and
semisynthetic gene expression data, our models exhibit robust generalization
capabilities when subject to substantial distribution shifts and significantly
outperform existing algorithms, especially in the challenging genomics domain.
Our code and models are publicly available at:
https://github.com/larslorch/avici.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022, minor formatting changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accounting for Temporal Variability in Functional Magnetic Resonance
  Imaging Improves Prediction of Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Xin Ma, Raj Sunderraman, Shihao Ji, Suprateek Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuroimaging-based prediction methods for intelligence and cognitive
abilities have seen a rapid development in literature. Among different
neuroimaging modalities, prediction based on functional connectivity (FC) has
shown great promise. Most literature has focused on prediction using static FC,
but there are limited investigations on the merits of such analysis compared to
prediction based on dynamic FC or region level functional magnetic resonance
imaging (fMRI) times series that encode temporal variability. To account for
the temporal dynamics in fMRI data, we propose a deep neural network involving
bi-directional long short-term memory (bi-LSTM) approach that also incorporates
feature selection mechanism. The proposed pipeline is implemented via an
efficient GPU computation framework and applied to predict intelligence scores
based on region level fMRI time series as well as dynamic FC. We compare the
prediction performance for different intelligence measures based on static FC,
dynamic FC, and region level time series acquired from the Adolescent Brain
Cognitive Development (ABCD) study involving close to 7000 individuals. Our
detailed analysis illustrates that static FC consistently has inferior
prediction performance compared to region level time series or dynamic FC for
unimodal rest and task fMRI experiments, and in almost all cases using a
combination of task and rest features. In addition, the proposed bi-LSTM
pipeline based on region level time series identifies several shared and
differential important brain regions across task and rest fMRI experiments that
drive intelligence prediction. A test-retest analysis of the selected features
shows strong reliability across cross-validation folds. Given the large sample
size from ABCD study, our results provide strong evidence that superior
prediction of intelligence can be achieved by accounting for temporal
variations in fMRI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">review</span> of Generative Adversarial Networks for Electronic Health
  Records: applications, evaluation measures and data sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghadeer Ghosheh, Jin Li, Tingting Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) are a valuable asset to facilitate clinical
research and point of care applications; however, many challenges such as data
privacy concerns impede its optimal utilization. Deep generative models,
particularly, Generative Adversarial Networks (GANs) show great promise in
generating synthetic EHR data by learning underlying data distributions while
achieving excellent performance and addressing these challenges. This work aims
to review the major developments in various applications of GANs for EHRs and
provides an overview of the proposed methodologies. For this purpose, we
combine perspectives from healthcare applications and machine learning
techniques in terms of source datasets and the fidelity and privacy evaluation
of the generated synthetic datasets. We also compile a list of the metrics and
datasets used by the reviewed works, which can be utilized as benchmarks for
future research in the field. We conclude by discussing challenges in GANs for
EHRs development and proposing recommended practices. We hope that this work
motivates novel research development directions in the intersection of
healthcare and machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Insurance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.00839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.00839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Bertsimas, Agni Orfanoudaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning algorithms start to get integrated into the
decision-making process of companies and organizations, insurance products are
being developed to protect their owners from liability risk. Algorithmic
liability differs from human liability since it is based on a single model
compared to multiple heterogeneous decision-makers and its performance is known
a priori for a given set of data. Traditional actuarial tools for human
liability do not take these properties into consideration, primarily focusing
on the distribution of historical claims. We propose, for the first time, a
quantitative framework to estimate the risk exposure of insurance contracts for
machine-driven liability, introducing the concept of algorithmic insurance.
Specifically, we present an optimization formulation to estimate the risk
exposure of a binary classification model given a pre-defined range of
premiums. We adjust the formulation to account for uncertainty in the resulting
losses using robust optimization. Our approach outlines how properties of the
model, such as accuracy, interpretability, and generalizability, can influence
the insurance contract evaluation. To showcase a practical implementation of
the proposed framework, we present a case study of medical malpractice in the
context of breast cancer detection. Our analysis focuses on measuring the
effect of the model parameters on the expected financial loss and identifying
the aspects of algorithmic performance that predominantly affect the risk of
the contract.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conservative SPDEs as fluctuating mean field limits of stochastic
  gradient descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Gess, Rishabh S. Gvalani, Vitalii Konarovskyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of stochastic interacting particle systems in the mean-field
limit to solutions of conservative stochastic partial differential equations is
established, with optimal rate of convergence. As a second main result, a
quantitative central limit theorem for such SPDEs is derived, again, with
optimal rate of convergence.
  The results apply, in particular, to the convergence in the mean-field
scaling of stochastic gradient descent dynamics in overparametrized, shallow
neural networks to solutions of SPDEs. It is shown that the inclusion of
fluctuations in the limiting SPDE improves the rate of convergence, and retains
information about the fluctuations of stochastic gradient descent in the
continuum limit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Learning of Stochastic Policies with Continuous Actions:
  from Models to Offline Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.11722v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.11722v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houssam Zenati, Alberto Bietti, Matthieu Martin, Eustache Diemert, Pierre Gaillard, Julien Mairal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning from logged data has become increasingly important
for many applications such as web advertising or healthcare. In this paper, we
address the problem of learning stochastic policies with continuous actions
from the viewpoint of counterfactual risk minimization (CRM). While the CRM
framework is appealing and well studied for discrete actions, the continuous
action case raises new challenges about modelization, optimization, and~offline
model selection with real data which turns out to be particularly challenging.
Our paper contributes to these three aspects of the CRM estimation pipeline.
First, we introduce a modelling strategy based on a joint kernel embedding of
contexts and actions, which overcomes the shortcomings of previous
discretization approaches. Second, we empirically show that the optimization
aspect of counterfactual learning is important, and we demonstrate the benefits
of proximal point algorithms and differentiable estimators. Finally, we propose
an evaluation protocol for offline policies in real-world logged systems, which
is challenging since policies cannot be replayed on test data, and we release a
new large-scale dataset along with multiple synthetic, yet realistic,
evaluation setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-Scale Chemical Language Representations Capture Molecular
  Structure and Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.09553v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.09553v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models based on machine learning can enable accurate and fast molecular
property predictions, which is of interest in drug discovery and material
design. Various supervised machine learning models have demonstrated promising
performance, but the vast chemical space and the limited availability of
property labels make supervised learning challenging. Recently, unsupervised
transformer-based language models pretrained on a large unlabelled corpus have
produced state-of-the-art results in many downstream natural language
processing tasks. Inspired by this development, we present molecular embeddings
obtained by training an efficient transformer encoder model, MoLFormer, which
uses rotary positional embeddings. This model employs a linear attention
mechanism, coupled with highly distributed training, on SMILES sequences of 1.1
billion unlabelled molecules from the PubChem and ZINC datasets. We show that
the learned molecular representation outperforms existing baselines, including
supervised and self-supervised graph neural networks and language models, on
several downstream tasks from ten benchmark datasets. They perform
competitively on two others. Further analyses, specifically through the lens of
attention, demonstrate that MoLFormer trained on chemical SMILES indeed learns
the spatial relationships between atoms within a molecule. These results
provide encouraging evidence that large-scale molecular language models can
capture sufficient chemical and structural information to predict various
distinct molecular properties, including quantum-chemical properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NMI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSMTL++: Revisiting <span class="highlight-title">Self-Supervised</span> Multi-Task Learning for Video
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Barbalau, Radu Tudor Ionescu, Mariana-Iuliana Georgescu, Jacob Dueholm, Bharathkumar Ramachandra, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moeslund, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A self-supervised multi-task learning (SSMTL) framework for video anomaly
detection was recently introduced in literature. Due to its highly accurate
results, the method attracted the attention of many researchers. In this work,
we revisit the self-supervised multi-task learning framework, proposing several
updates to the original method. First, we study various detection methods, e.g.
based on detecting high-motion regions using optical flow or background
subtraction, since we believe the currently used pre-trained YOLOv3 is
suboptimal, e.g. objects in motion or objects from unknown classes are never
detected. Second, we modernize the 3D convolutional backbone by introducing
multi-head self-attention modules, inspired by the recent success of vision
transformers. As such, we alternatively introduce both 2D and 3D convolutional
vision transformer (CvT) blocks. Third, in our attempt to further improve the
model, we study additional self-supervised learning tasks, such as predicting
segmentation maps through knowledge distillation, solving jigsaw puzzles,
estimating body pose through knowledge distillation, predicting masked regions
(inpainting), and adversarial learning with pseudo-anomalies. We conduct
experiments to assess the performance impact of the introduced changes. Upon
finding more promising configurations of the framework, dubbed SSMTL++v1 and
SSMTL++v2, we extend our preliminary experiments to more data sets,
demonstrating that our performance gains are consistent across all data sets.
In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the
state-of-the-art performance bar to a new level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under consideration at Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot
  Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.10421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.10421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to be generally useful, they must be able to find arbitrary
objects described by people (i.e., be language-driven) even without expensive
navigation training on in-domain data (i.e., perform zero-shot inference). We
explore these capabilities in a unified setting: language-driven zero-shot
object navigation (L-ZSON). Inspired by the recent success of open-vocabulary
models for image classification, we investigate a straightforward framework,
CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without
fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark,
which considers finding uncommon objects, objects described by spatial and
appearance attributes, and hidden objects described relative to visible
objects. We conduct an in-depth empirical study by directly deploying 21 CoW
baselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k
navigation episodes and find that (1) CoW baselines often struggle to leverage
language descriptions, but are proficient at finding uncommon objects. (2) A
simple CoW, with CLIP-based object localization and classical exploration --
and no additional training -- matches the navigation efficiency of a
state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This
same CoW provides a 15.6 percentage point improvement in success over a
state-of-the-art RoboTHOR ZSON model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Explanations Using Optimization With Constraint Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donato Maragno, Tabea E. Röber, Ilker Birbil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To increase the adoption of counterfactual explanations in practice, several
criteria that these should adhere to have been put forward in the literature.
We propose counterfactual explanations using optimization with constraint
learning (CE-OCL), a generic and flexible approach that addresses all these
criteria and allows room for further extensions. Specifically, we discuss how
we can leverage an optimization with constraint learning framework for the
generation of counterfactual explanations, and how components of this framework
readily map to the criteria. We also propose two novel modeling approaches to
address data manifold closeness and diversity, which are two key criteria for
practical counterfactual explanations. We test CE-OCL on several datasets and
present our results in a case study. Compared against the current
state-of-the-art methods, CE-OCL allows for more flexibility and has an overall
superior performance in terms of several evaluation metrics proposed in related
work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZippyPoint: Fast Interest Point Detection, Description, and Matching
  through Mixed Precision Discretization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03610v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03610v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menelaos Kanakis, Simon Maurer, Matteo Spallanzani, Ajad Chhatkuli, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient detection and description of geometric regions in images is a
prerequisite in visual systems for localization and mapping. Such systems still
rely on traditional hand-crafted methods for efficient generation of
lightweight descriptors, a common limitation of the more powerful neural
network models that come with high compute and specific hardware requirements.
In this paper, we focus on the adaptations required by detection and
description neural networks to enable their use in computationally limited
platforms such as robots, mobile, and augmented reality devices. To that end,
we investigate and adapt network quantization techniques to accelerate
inference and enable its use on compute limited platforms. In addition, we
revisit common practices in descriptor quantization and propose the use of a
binary descriptor normalization layer, enabling the generation of distinctive
binary descriptors with a constant number of ones. ZippyPoint, our efficient
quantized network with binary descriptors, improves the network runtime speed,
the descriptor matching speed, and the 3D model size, by at least an order of
magnitude when compared to full-precision counterparts. These improvements come
at a minor performance degradation as evaluated on the tasks of homography
estimation, visual localization, and map-free visual relocalization. Code and
trained models will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Not Sleep on Traditional Machine Learning: Simple and Interpretable
  Techniques Are Competitive to Deep Learning for Sleep Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.07753v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.07753v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeroen Van Der Donckt, Jonas Van Der Donckt, Emiel Deprost, Nicolas Vandenbussche, Michael Rademaker, Gilles Vandewiele, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last few years, research in automatic sleep scoring has mainly
focused on developing increasingly complex deep learning architectures.
However, recently these approaches achieved only marginal improvements, often
at the expense of requiring more data and more expensive training procedures.
Despite all these efforts and their satisfactory performance, automatic sleep
staging solutions are not widely adopted in a clinical context yet. We argue
that most deep learning solutions for sleep scoring are limited in their
real-world applicability as they are hard to train, deploy, and reproduce.
Moreover, these solutions lack interpretability and transparency, which are
often key to increase adoption rates. In this work, we revisit the problem of
sleep stage classification using classical machine learning. Results show that
competitive performance can be achieved with a conventional machine learning
pipeline consisting of preprocessing, feature extraction, and a simple machine
learning model. In particular, we analyze the performance of a linear model and
a non-linear (gradient boosting) model. Our approach surpasses state-of-the-art
(that uses the same data) on two public datasets: Sleep-EDF SC-20 (MF1 0.810)
and Sleep-EDF ST (MF1 0.795), while achieving competitive results on Sleep-EDF
SC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that, for the sleep stage
scoring task, the expressiveness of an engineered feature vector is on par with
the internally learned representations of deep learning models. This
observation opens the door to clinical adoption, as a representative feature
vector allows to leverage both the interpretability and successful track record
of traditional machine learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Accepted to Biomedical
  Signal Processing and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Measures of Non-Probabilistic Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.05818v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.05818v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Ellsaesser, Guido Fioretti, Gail E. James
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are two reasons why uncertainty about the future yield of investments
may not be adequately described by Probability Theory. The first one is due to
unique or nearly-unique events, that either never realized or occurred too
seldom for probabilities to be reliable. The second one arises when when one
fears that something may happen, that one is not even able to figure out, e.g.,
if one asks: "Climate change, financial crises, pandemic, war, what next?"
  In both cases, simple one-to-one causal mappings between available
alternatives and possible consequences eventually melt down. However, such
destructions reflect into the changing narratives of business executives,
employees and other stakeholders in specific, identifiable and differential
ways. In particular, texts such as consultants' reports or letters to
shareholders can be analysed in order to detect the impact of both sorts of
uncertainty onto the causal relations that normally guide decision-making.
  We propose structural measures of causal mappings as a means to measure
non-probabilistic uncertainty, eventually suggesting that automated text
analysis can greatly augment the possibilities offered by these techniques.
Prospective applications may concern statistical institutes, stock market
traders, as well as businesses wishing to compare their own vision to those
prevailing in their industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Synthetic Mobility Networks with Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.11028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.11028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Mauro, Massimiliano Luca, Antonio Longa, Bruno Lepri, Luca Pappalardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasingly crucial role of human displacements in complex societal
phenomena, such as traffic congestion, segregation, and the diffusion of
epidemics, is attracting the interest of scientists from several disciplines.
In this article, we address mobility network generation, i.e., generating a
city's entire mobility network, a weighted directed graph in which nodes are
geographic locations and weighted edges represent people's movements between
those locations, thus describing the entire mobility set flows within a city.
Our solution is MoGAN, a model based on Generative Adversarial Networks (GANs)
to generate realistic mobility networks. We conduct extensive experiments on
public datasets of bike and taxi rides to show that MoGAN outperforms the
classical Gravity and Radiation models regarding the realism of the generated
networks. Our model can be used for data augmentation and performing
simulations and what-if analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures. Supplementary 9 pages and 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-Representations: <span class="highlight-title">Self-Supervised</span> Representation Learning on Neural
  Network Weights for Model Characteristic Prediction <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15288v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15288v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Schürholt, Dimche Kostadinov, Damian Borth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) has been shown to learn useful and
information-preserving representations. Neural Networks (NNs) are widely
applied, yet their weight space is still not fully understood. Therefore, we
propose to use SSL to learn hyper-representations of the weights of populations
of NNs. To that end, we introduce domain specific data augmentations and an
adapted attention architecture. Our empirical evaluation demonstrates that
self-supervised representation learning in this domain is able to recover
diverse NN model characteristics. Further, we show that the proposed learned
representations outperform prior work for predicting hyper-parameters, test
accuracy, and generalization gap as well as transfer to out-of-distribution
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 35th Conference on Neural Information Processing Systems
  (NeurIPS 2021), Sydney, Australia. 31 Pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grammar Based Speaker Role Identification for Air Traffic Control Speech
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrutha Prasad, Juan Zuluaga-Gomez, Petr Motlicek, Saeed Sarfjoo, Iuliia Nigmatulina, Oliver Ohneiser, Hartmut Helmke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) for air traffic control is generally
trained by pooling Air Traffic Controller (ATCO) and pilot data into one set.
This is motivated by the fact that pilot's voice communications are more scarce
than ATCOs. Due to this data imbalance and other reasons (e.g., varying
acoustic conditions), the speech from ATCOs is usually recognized more
accurately than from pilots. Automatically identifying the speaker roles is a
challenging task, especially in the case of the noisy voice recordings
collected using Very High Frequency (VHF) receivers or due to the
unavailability of the push-to-talk (PTT) signal, i.e., both audio channels are
mixed. In this work, we propose to (1) automatically segment the ATCO and pilot
data based on an intuitive approach exploiting ASR transcripts and (2)
subsequently consider an automatic recognition of ATCOs' and pilots' voice as
two separate tasks. Our work is performed on VHF audio data with high noise
levels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is
recognized to be helpful for various speech-based machine-learning tasks.
Specifically, for the speaker role identification task, the module is
represented by a simple yet efficient knowledge-based system exploiting a
grammar defined by the International Civil Aviation Organization (ICAO). The
system accepts text as the input, either manually verified annotations or
automatically generated transcripts. The developed approach provides an average
accuracy in speaker role identification of about 83%. Finally, we show that
training an acoustic model for ASR tasks separately (i.e., separate models for
ATCOs and pilots) or using a multitask approach is well suited for the noisy
data and outperforms the traditional ASR system where all data is pooled
together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at Sesar Innovation Days - 2022. See
  https://www.sesarju.eu/sesarinnovationdays</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Provably Convergent Algorithms for Gromov-Wasserstein in Graph
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, Jose Blanchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the design and analysis of a class of efficient
algorithms for computing the Gromov-Wasserstein (GW) distance tailored to
large-scale graph learning tasks. Armed with the Luo-Tseng error bound
condition~\citep{luo1992error}, two proposed algorithms, called Bregman
Alternating Projected Gradient (BAPG) and hybrid Bregman Proximal Gradient
(hBPG) enjoy the convergence guarantees. Upon task-specific properties, our
analysis further provides novel theoretical insights to guide how to select the
best-fit method. As a result, we are able to provide comprehensive experiments
to validate the effectiveness of our methods on a host of tasks, including
graph alignment, graph partition, and shape matching. In terms of both
wall-clock time and modeling performance, the proposed methods achieve
state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCP: <span class="highlight-title">Self-supervised</span> <span class="highlight-title">Pre-train</span>ing for Personalized Chatbots with
  Multi-level Contrastive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08753v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08753v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Zhengyi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized chatbots focus on endowing the chatbots with a consistent
personality to behave like real users and further act as personal assistants.
Previous studies have explored generating implicit user profiles from the
user's dialogue history for building personalized chatbots. However, these
studies only use the response generation loss to train the entire model, thus
it is prone to suffer from the problem of data sparsity. Besides, they
overemphasize the final generated response's quality while ignoring the
correlations and fusions between the user's dialogue history, leading to rough
data representations and performance degradation. To tackle these problems, we
propose a self-supervised learning framework MCP for capturing better
representations from users' dialogue history for personalized chatbots.
Specifically, we apply contrastive sampling methods to leverage the supervised
signals hidden in user dialog history, and generate the pre-training samples
for enhancing the model. We design three pre-training tasks based on three
types of contrastive pairs from user dialogue history, namely response pairs,
sequence augmentation pairs, and user pairs. We pre-train the utterance encoder
and the history encoder towards the contrastive objectives and use these
pre-trained encoders for generating user profiles while personalized response
generation. Experimental results on two real-world datasets show a significant
improvement in our proposed model MCP compared with the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Statistical Significance of Neural Network-based Image
  Segmentation by Selective Inference <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.01823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.01823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vo Nguyen Le Duy, Shogo Iwazaki, Ichiro Takeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although a vast body of literature relates to image segmentation methods that
use deep neural networks (DNNs), less attention has been paid to assessing the
statistical reliability of segmentation results. In this study, we interpret
the segmentation results as hypotheses driven by DNN (called DNN-driven
hypotheses) and propose a method by which to quantify the reliability of these
hypotheses within a statistical hypothesis testing framework. Specifically, we
consider a statistical hypothesis test for the difference between the object
and background regions. This problem is challenging, as the difference would be
falsely large because of the adaptation of the DNN to the data. To overcome
this difficulty, we introduce a conditional selective inference (SI) framework
-- a new statistical inference framework for data-driven hypotheses that has
recently received considerable attention -- to compute exact (non-asymptotic)
valid p-values for the segmentation results. To use the conditional SI
framework for DNN-based segmentation, we develop a new SI algorithm based on
the homotopy method, which enables us to derive the exact (non-asymptotic)
sampling distribution of DNN-driven hypothesis. We conduct experiments on both
synthetic and real-world datasets, through which we offer evidence that our
proposed method can successfully control the false positive rate, has good
performance in terms of computational efficiency, and provides good results
when applied to medical image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anti-Spoofing Using Transfer Learning with Variational Information
  Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngsik Eom, Yeonghyeon Lee, Ji Sub Um, Hoirin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in sophisticated synthetic speech generated from
text-to-speech (TTS) or voice conversion (VC) systems cause threats to the
existing automatic speaker verification (ASV) systems. Since such synthetic
speech is generated from diverse algorithms, generalization ability with using
limited training data is indispensable for a robust anti-spoofing system. In
this work, we propose a transfer learning scheme based on the wav2vec 2.0
pretrained model with variational information bottleneck (VIB) for speech
anti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA)
database shows that our method improves the performance of distinguishing
unseen spoofed and genuine speech, outperforming current state-of-the-art
anti-spoofing systems. Furthermore, we show that the proposed system improves
performance in low-resource and cross-dataset settings of anti-spoofing task
significantly, demonstrating that our system is also robust in terms of data
size and data distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalizing Flows for Hierarchical Bayesian Analysis: A Gravitational
  Wave Population Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Ruhe, Kaze Wong, Miles Cranmer, Patrick Forré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose parameterizing the population distribution of the gravitational
wave population modeling framework (Hierarchical Bayesian Analysis) with a
normalizing flow. We first demonstrate the merit of this method on illustrative
experiments and then analyze four parameters of the latest LIGO/Virgo data
release: primary mass, secondary mass, redshift, and effective spin. Our
results show that despite the small and notoriously noisy dataset, the
posterior predictive distributions (assuming a prior over the parameters of the
flow) of the observed gravitational wave population recover structure that
agrees with robust previous phenomenological modeling results while being less
susceptible to biases introduced by less-flexible distribution models.
Therefore, the method forms a promising flexible, reliable replacement for
population inference distributions, even when data is highly noisy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShadowNet: A Secure and Efficient On-device Model Inference System for
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.05905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.05905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichuang Sun, Ruimin Sun, Changming Liu, Amrita Roy Chowdhury, Long Lu, Somesh Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased usage of AI accelerators on mobile and edge devices,
on-device machine learning (ML) is gaining popularity. Thousands of proprietary
ML models are being deployed today on billions of untrusted devices. This
raises serious security concerns about model privacy. However, protecting model
privacy without losing access to the untrusted AI accelerators is a challenging
problem. In this paper, we present a novel on-device model inference system,
ShadowNet. ShadowNet protects the model privacy with Trusted Execution
Environment (TEE) while securely outsourcing the heavy linear layers of the
model to the untrusted hardware accelerators. ShadowNet achieves this by
transforming the weights of the linear layers before outsourcing them and
restoring the results inside the TEE. The non-linear layers are also kept
secure inside the TEE. ShadowNet's design ensures efficient transformation of
the weights and the subsequent restoration of the results. We build a ShadowNet
prototype based on TensorFlow Lite and evaluate it on five popular CNNs,
namely, MobileNet, ResNet-44, MiniVGG, ResNet-404, and YOLOv4-tiny. Our
evaluation shows that ShadowNet achieves strong security guarantees with
reasonable performance, offering a practical solution for secure on-device
model inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, IEEE Security & Privacy, Oakland'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Devil is in the GAN: Backdoor Attacks and Defenses in Deep
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.01644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.01644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ambrish Rawat, Killian Levacher, Mathieu Sinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Generative Models (DGMs) are a popular class of deep learning models
which find widespread use because of their ability to synthesize data from
complex, high-dimensional manifolds. However, even with their increasing
industrial adoption, they haven't been subject to rigorous security and privacy
analysis. In this work we examine one such aspect, namely backdoor attacks on
DGMs which can significantly limit the applicability of pre-trained models
within a model supply chain and at the very least cause massive reputation
damage for companies outsourcing DGMs form third parties.
  While similar attacks scenarios have been studied in the context of classical
prediction models, their manifestation in DGMs hasn't received the same
attention. To this end we propose novel training-time attacks which result in
corrupted DGMs that synthesize regular data under normal operations and
designated target outputs for inputs sampled from a trigger distribution. These
attacks are based on an adversarial loss function that combines the dual
objectives of attack stealth and fidelity. We systematically analyze these
attacks, and show their effectiveness for a variety of approaches like
Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as
well as different data domains including images and audio. Our experiments show
that - even for large-scale industry-grade DGMs (like StyleGAN) - our attacks
can be mounted with only modest computational effort. We also motivate suitable
defenses based on static/dynamic model and output inspections, demonstrate
their usefulness, and prescribe a practical and comprehensive defense strategy
that paves the way for safe usage of DGMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lower Bounds for the Convergence of Tensor Power Iteration on Random
  Overcomplete Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Wu, Kangjie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor decomposition serves as a powerful primitive in statistics and machine
learning. In this paper, we focus on using power iteration to decompose an
overcomplete random tensor. Past work studying the properties of tensor power
iteration either requires a non-trivial data-independent initialization, or is
restricted to the undercomplete regime. Moreover, several papers implicitly
suggest that logarithmically many iterations (in terms of the input dimension)
are sufficient for the power method to recover one of the tensor components. In
this paper, we analyze the dynamics of tensor power iteration from random
initialization in the overcomplete regime. Surprisingly, we show that
polynomially many steps are necessary for convergence of tensor power iteration
to any of the true component, which refutes the previous conjecture. On the
other hand, our numerical experiments suggest that tensor power iteration
successfully recovers tensor components for a broad range of parameters,
despite that it takes at least polynomially many steps to converge. To further
complement our empirical evidence, we prove that a popular objective function
for tensor decomposition is strictly increasing along the power iteration path.
Our proof is based on the Gaussian conditioning technique, which has been
applied to analyze the approximate message passing (AMP) algorithm. The major
ingredient of our argument is a conditioning lemma that allows us to generalize
AMP-type analysis to non-proportional limit and polynomially many iterations of
the power method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning with Functional Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.09590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.09590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barinder Thind, Kevin Multani, Jiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology for integrating functional data into deep densely
connected feed-forward neural networks. The model is defined for scalar
responses with multiple functional and scalar covariates. A by-product of the
method is a set of dynamic functional weights that can be visualized during the
optimization process. This visualization leads to greater interpretability of
the relationship between the covariates and the response relative to
conventional neural networks. The model is shown to perform well in a number of
contexts including prediction of new data and recovery of the true underlying
functional weights; these results were confirmed through real applications and
simulation studies. A forthcoming R package is developed on top of a popular
deep learning library (Keras) allowing for general use of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures, accepted & published by JCGS; please note that
  the version here is outdated -- look for the updated version using the DOI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-aware Random Walk Diffusion to Improve Dynamic Graph Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01214v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01214v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jong-whi Lee, Jinhong Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we augment a dynamic graph for improving the performance of dynamic
graph neural networks? Graph augmentation has been widely utilized to boost the
learning performance of GNN-based models. However, most existing approaches
only enhance spatial structure within an input static graph by transforming the
graph, and do not consider dynamics caused by time such as temporal locality,
i.e., recent edges are more influential than earlier ones, which remains
challenging for dynamic graph augmentation. In this work, we propose TiaRa
(Time-aware Random Walk Diffusion), a novel diffusion-based method for
augmenting a dynamic graph represented as a discrete-time sequence of graph
snapshots. For this purpose, we first design a time-aware random walk proximity
so that a surfer can walk along the time dimension as well as edges, resulting
in spatially and temporally localized scores. We then derive our diffusion
matrices based on the time-aware random walk, and show they become enhanced
adjacency matrices that both spatial and temporal localities are augmented.
Throughout extensive experiments, we demonstrate that TiaRa effectively
augments a given dynamic graph, and leads to significant improvements in
dynamic GNN models for various graph datasets and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision-making at Unsignalized Intersection for Autonomous Vehicles:
  Left-turn Maneuver with Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Dongjie Shi, Teng Liu, Xiaolin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making module enables autonomous vehicles to reach appropriate
maneuvers in the complex urban environments, especially the intersection
situations. This work proposes a deep reinforcement learning (DRL) based
left-turn decision-making framework at unsignalized intersection for autonomous
vehicles. The objective of the studied automated vehicle is to make an
efficient and safe left-turn maneuver at a four-way unsignalized intersection.
The exploited DRL methods include deep Q-learning (DQL) and double DQL.
Simulation results indicate that the presented decision-making strategy could
efficaciously reduce the collision rate and improve transport efficiency. This
work also reveals that the constructed left-turn control structure has a great
potential to be applied in real-time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Evaluation for Temporal and/or Spatial Dependent Experiments in
  Ride-sourcing Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikai Luo, Ying Yang, Chengchun Shi, Fang Yao, Jieping Ye, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to establish causal relationship between
ride-sharing platform's policies and outcomes of interest under complex
temporal and/or spatial dependent experiments. We propose a
temporal/spatio-temporal varying coefficient decision process (VCDP) model to
capture the dynamic treatment effects in temporal/spatio-temporal dependent
experiments. We characterize the average treatment effect by decomposing it as
the sum of direct effect (DE) and indirect effect (IE) and develop estimation
and inference procedures for both DE and IE. We also establish the statistical
properties (e.g., weak convergence and asymptotic power) of our models. We
conduct extensive simulations and real data analyses to verify the usefulness
of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARCADE: Adversarially Regularized Convolutional Autoencoder for Network
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Willian T. Lunardi, Martin Andreoni Lopez, Jean-Pierre Giacalone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the number of heterogenous IP-connected devices and traffic volume
increase, so does the potential for security breaches. The undetected
exploitation of these breaches can bring severe cybersecurity and privacy
risks. Anomaly-based \acp{IDS} play an essential role in network security. In
this paper, we present a practical unsupervised anomaly-based deep learning
detection system called ARCADE (Adversarially Regularized Convolutional
Autoencoder for unsupervised network anomaly DEtection). With a convolutional
\ac{AE}, ARCADE automatically builds a profile of the normal traffic using a
subset of raw bytes of a few initial packets of network flows so that potential
network anomalies and intrusions can be efficiently detected before they cause
more damage to the network. ARCADE is trained exclusively on normal traffic. An
adversarial training strategy is proposed to regularize and decrease the
\ac{AE}'s capabilities to reconstruct network flows that are out-of-the-normal
distribution, thereby improving its anomaly detection capabilities. The
proposed approach is more effective than state-of-the-art deep learning
approaches for network anomaly detection. Even when examining only two initial
packets of a network flow, ARCADE can effectively detect malware infection and
network attacks. ARCADE presents 20 times fewer parameters than baselines,
achieving significantly faster detection speed and reaction time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel
  Recombination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Adachi, Satoshi Hayakawa, Martin Jørgensen, Harald Oberhauser, Michael A. Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Calculation of Bayesian posteriors and model evidences typically requires
numerical integration. Bayesian quadrature (BQ), a surrogate-model-based
approach to numerical integration, is capable of superb sample efficiency, but
its lack of parallelisation has hindered its practical applications. In this
work, we propose a parallelised (batch) BQ method, employing techniques from
kernel quadrature, that possesses an empirically exponential convergence rate.
Additionally, just as with Nested Sampling, our method permits simultaneous
inference of both posteriors and model evidence. Samples from our BQ surrogate
model are re-selected to give a sparse set of samples, via a kernel
recombination algorithm, requiring negligible additional time to increase the
batch size. Empirically, we find that our approach significantly outperforms
the sampling efficiency of both state-of-the-art BQ techniques and Nested
Sampling in various real-world datasets, including lithium-ion battery
analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological Network: How Far Can We Go with Morphological Neurons? <span class="chip">BMVC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1901.00109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1901.00109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjan Mondal, Sanchayan Santra, Soumendu Sundar Mukherjee, Bhabatosh Chanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Morphological neurons, that is morphological operators such as dilation and
erosion with learnable structuring elements, have intrigued researchers for
quite some time because of the power these operators bring to the table despite
their simplicity. These operators are known to be powerful nonlinear tools, but
for a given problem coming up with a sequence of operations and their
structuring element is a non-trivial task. So, the existing works have mainly
focused on this part of the problem without delving deep into their
applicability as generic operators. A few works have tried to utilize
morphological neurons as a part of classification (and regression) networks
when the input is a feature vector. However, these methods mainly focus on a
specific problem, without going into generic theoretical analysis. In this
work, we have theoretically analyzed morphological neurons and have shown that
these are far more powerful than previously anticipated. Our proposed
morphological block, containing dilation and erosion followed by their linear
combination, represents a sum of hinge functions. Existing works show that
hinge functions perform quite well in classification and regression problems.
Two morphological blocks can even approximate any continuous function. However,
to facilitate the theoretical analysis that we have done in this paper, we have
restricted ourselves to the 1D version of the operators, where the structuring
element operates on the whole input. Experimental evaluations also indicate the
effectiveness of networks built with morphological neurons, over similarly
structured neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BMVC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparing Sequential Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.00115v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.00115v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yo Joong Choe, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider two forecasters, each making a single prediction for a sequence of
events over time. We ask a relatively basic question: how might we compare
these forecasters, either online or post-hoc, while avoiding unverifiable
assumptions on how the forecasts and outcomes were generated? In this paper, we
present a rigorous answer to this question by designing novel sequential
inference procedures for estimating the time-varying difference in forecast
scores. To do this, we employ confidence sequences (CS), which are sequences of
confidence intervals that can be continuously monitored and are valid at
arbitrary data-dependent stopping times ("anytime-valid"). The widths of our
CSs are adaptive to the underlying variance of the score differences.
Underlying their construction is a game-theoretic statistical framework, in
which we further identify e-processes and p-processes for sequentially testing
a weak null hypothesis -- whether one forecaster outperforms another on average
(rather than always). Our methods do not make distributional assumptions on the
forecasts or outcomes; our main theorems apply to any bounded scores, and we
later provide alternative methods for unbounded scores. We empirically validate
our approaches by comparing real-world baseball and weather forecasters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under revision. Code and data sources available at
  https://github.com/yjchoe/ComparingForecasters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A deep learning approach to data-driven model-free pricing and to
  martingale optimal transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.11435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.11435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Neufeld, Julian Sester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel and highly tractable supervised learning approach based
on neural networks that can be applied for the computation of model-free price
bounds of, potentially high-dimensional, financial derivatives and for the
determination of optimal hedging strategies attaining these bounds. In
particular, our methodology allows to train a single neural network offline and
then to use it online for the fast determination of model-free price bounds of
a whole class of financial derivatives with current market data. We show the
applicability of this approach and highlight its accuracy in several examples
involving real market data. Further, we show how a neural network can be
trained to solve martingale optimal transport problems involving fixed marginal
distributions instead of financial market data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards improving discriminative reconstruction via simultaneous dense
  and sparse coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.09534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.09534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abiy Tasissa, Emmanouil Theodosis, Bahareh Tolooshams, Demba Ba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminative features extracted from the sparse coding model have been
shown to perform well for classification. Recent deep learning architectures
have further improved reconstruction in inverse problems by considering new
dense priors learned from data. We propose a novel dense and sparse coding
model that integrates both representation capability and discriminative
features. The model studies the problem of recovering a dense vector
$\mathbf{x}$ and a sparse vector $\mathbf{u}$ given measurements of the form
$\mathbf{y} = \mathbf{A}\mathbf{x}+\mathbf{B}\mathbf{u}$. Our first analysis
proposes a geometric condition based on the minimal angle between spanning
subspaces corresponding to the matrices $\mathbf{A}$ and $\mathbf{B}$ that
guarantees unique solution to the model. The second analysis shows that, under
mild assumptions, a convex program recovers the dense and sparse components. We
validate the effectiveness of the model on simulated data and propose a dense
and sparse autoencoder (DenSaE) tailored to learning the dictionaries from the
dense and sparse model. We demonstrate that (i) DenSaE denoises natural images
better than architectures derived from the sparse coding model
($\mathbf{B}\mathbf{u}$), (ii) in the presence of noise, training the biases in
the latter amounts to implicitly learning the $\mathbf{A}\mathbf{x} +
\mathbf{B}\mathbf{u}$ model, (iii) $\mathbf{A}$ and $\mathbf{B}$ capture low-
and high-frequency contents, respectively, and (iv) compared to the sparse
coding model, DenSaE offers a balance between discriminative power and
representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly
  Multimedia Traffic in Graynet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Nato, Yiqiang Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying anomaly multimedia traffic in cyberspace is a big challenge in
distributed service systems, multiple generation networks and future internet
of everything. This letter explores meta-generalization for a multiparty
privacy learning model in graynet to improve the performance of anomaly
multimedia traffic identification. The multiparty privacy learning model in
graynet is a globally shared model that is partitioned, distributed and trained
by exchanging multiparty parameters updates with preserving private data. The
meta-generalization refers to discovering the inherent attributes of a learning
model to reduce its generalization error. In experiments, three
meta-generalization principles are tested as follows. The generalization error
of the multiparty privacy learning model in graynet is reduced by changing the
dimension of byte-level imbedding. Following that, the error is reduced by
adapting the depth for extracting packet-level features. Finally, the error is
reduced by adjusting the size of support set for preprocessing traffic-level
data. Experimental results demonstrate that the proposal outperforms the
state-of-the-art learning models for identifying anomaly multimedia traffic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correct some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairify: Fairness Verification of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumon Biswas, Hridesh Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness of machine learning (ML) software has become a major concern in the
recent past. Although recent research on testing and improving fairness have
demonstrated impact on real-world software, providing fairness guarantee in
practice is still lacking. Certification of ML models is challenging because of
the complex decision-making process of the models. In this paper, we proposed
Fairify, an SMT-based approach to verify individual fairness property in neural
network (NN) models. Individual fairness ensures that any two similar
individuals get similar treatment irrespective of their protected attributes
e.g., race, sex, age. Verifying this fairness property is hard because of the
global checking and non-linear computation nodes in NN. We proposed sound
approach to make individual fairness verification tractable for the developers.
The key idea is that many neurons in the NN always remain inactive when a
smaller part of the input domain is considered. So, Fairify leverages whitebox
access to the models in production and then apply formal analysis based
pruning. Our approach adopts input partitioning and then prunes the NN for each
partition to provide fairness certification or counterexample. We leveraged
interval arithmetic and activation heuristic of the neurons to perform the
pruning as necessary. We evaluated Fairify on 25 real-world neural networks
collected from four different sources, and demonstrated the effectiveness,
scalability and performance over baseline and closely related work. Fairify is
also configurable based on the domain and size of the NN. Our novel formulation
of the problem can answer targeted verification queries with relaxations and
counterexamples, which have practical implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Frequency Space Diffusion Models for Accelerated MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Hairong Zheng, Dong Liang, Yanjie Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models with continuous stochastic differential equations (SDEs)
have shown superior performances in image generation. It can be used as a deep
generative prior to solve the inverse problem in MR reconstruction. However,
the existing VP-SDE can be treated as maximizing the energy of the MR image to
be reconstructed and may lead to SDE sequence divergence. The VE-SDE based MR
reconstruction is not consistent with actual diffusion process. In addition,
both VE- and VP-SDEs-based models suffer from a time-consuming sampling
procedure, resulting long reconstruction time. In this study, a new SDE
focusing on the diffusion process in high-frequency space is designed
specifically for robust MR reconstruction based on diffusion models.
Experiments on the publicly fastMRI dataset show that HFS-SDE based
reconstruction method outperforms the parallel imaging, supervised deep
learning, and existing VE- and VP-SDEs-based methods in terms of reconstruction
accuracy. It also improves the stability of MR reconstruction and accelerates
sampling procedure of reverse diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE TMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Randomly Initialized Networks for Evaluating Generative
  Models <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyuk Lee, Jun-Hyuk Kim, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation of generative models is mostly based on the comparison between the
estimated distribution and the ground truth distribution in a certain feature
space. To embed samples into informative features, previous works often use
convolutional neural networks optimized for classification, which is criticized
by recent studies. Therefore, various feature spaces have been explored to
discover alternatives. Among them, a surprising approach is to use a randomly
initialized neural network for feature embedding. However, the fundamental
basis to employ the random features has not been sufficiently justified. In
this paper, we rigorously investigate the feature space of models with random
weights in comparison to that of trained models. Furthermore, we provide an
empirical evidence to choose networks for random features to obtain consistent
and reliable results. Our results indicate that the features from random
networks can evaluate generative models well similarly to those from trained
networks, and furthermore, the two types of features can be used together in a
complementary way.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-armed Bandit Learning on a Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianpeng Zhang, Kasper Johansson, Na Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-armed bandit(MAB) problem is a simple yet powerful framework that
has been extensively studied in the context of decision-making under
uncertainty. In many real-world applications, such as robotic applications,
selecting an arm corresponds to a physical action that constrains the choices
of the next available arms (actions). Motivated by this, we study an extension
of MAB called the graph bandit, where an agent travels over a graph to maximize
the reward collected from different nodes. The graph defines the agent's
freedom in selecting the next available nodes at each step. We assume the graph
structure is fully available, but the reward distributions are unknown. Built
upon an offline graph-based planning algorithm and the principle of optimism,
we design a learning algorithm, G-UCB, that balances long-term
exploration-exploitation using the principle of optimism. We show that our
proposed algorithm achieves $O(\sqrt{|S|T\log(T)}+D|S|\log T)$ learning regret,
where $|S|$ is the number of nodes and $D$ is the diameter of the graph, which
matches the theoretical lower bound $\Omega(\sqrt{|S|T})$ up to logarithmic
factors. To our knowledge, this result is among the first tight regret bounds
in non-episodic, un-discounted learning problems with known deterministic
transitions. Numerical experiments confirm that our algorithm outperforms
several benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Converting OpenStreetMap Data to Road Networks for Downstream
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaisar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how to convert OpenStreetMap data to road networks for downstream
applications. OpenStreetMap data has different formats. Extensible Markup
Language (XML) is one of them. OSM data consist of nodes, ways, and relations.
We process OSM XML data to extract the information of nodes and ways to obtain
the map of streets of the Memphis area. We can use this map for different
downstream applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BagPipe: Accelerating Deep Recommendation Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12429v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12429v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Agarwal, Chengpo Yan, Ziyi Zhang, Shivaram Venkataraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based recommendation models (DLRM) are widely used in several
business critical applications. Training such recommendation models efficiently
is challenging primarily because they consist of billions of embedding-based
parameters which are often stored remotely leading to significant overheads
from embedding access. By profiling existing DLRM training, we observe that
only 8.5% of the iteration time is spent in forward/backward pass while the
remaining time is spent on embedding and model synchronization. Our key insight
in this paper is that access to embeddings have a specific structure and
pattern which can be used to accelerate training. We observe that embedding
accesses are heavily skewed, with almost 1% of embeddings represent more than
92% of total accesses. Further, we observe that during training we can
lookahead at future batches to determine exactly which embeddings will be
needed at what iteration in the future. Based on these insight, we propose
Bagpipe, a system for training deep recommendation models that uses caching and
prefetching to overlap remote embedding accesses with the computation. We
designed an Oracle Cacher, a new system component which uses our lookahead
algorithm to generate optimal cache update decisions and provide strong
consistency guarantees. Our experiments using three datasets and two models
shows that our approach provides a speed up of up to 6.2x compared to state of
the art baselines, while providing the same convergence and reproducibility
guarantees as synchronous training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Projection-free Adaptive Regret with Membership Oracles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12638v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12638v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhou Lu, Nataly Brukhim, Paula Gradu, Elad Hazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the framework of online convex optimization, most iterative algorithms
require the computation of projections onto convex sets, which can be
computationally expensive. To tackle this problem HK12 proposed the study of
projection-free methods that replace projections with less expensive
computations. The most common approach is based on the Frank-Wolfe method, that
uses linear optimization computation in lieu of projections. Recent work by
GK22 gave sublinear adaptive regret guarantees with projection free algorithms
based on the Frank Wolfe approach.
  In this work we give projection-free algorithms that are based on a different
technique, inspired by Mhammedi22, that replaces projections by set-membership
computations. We propose a simple lazy gradient-based algorithm with a
Minkowski regularization that attains near-optimal adaptive regret bounds. For
general convex loss functions we improve previous adaptive regret bounds from
$O(T^{3/4})$ to $O(\sqrt{T})$, and further to tight interval dependent bound
$\tilde{O}(\sqrt{I})$ where $I$ denotes the interval length. For strongly
convex functions we obtain the first poly-logarithmic adaptive regret bounds
using a projection-free algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Multivariate Load States Using a Conditional Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenguang Wang, Ensieh Sharifnia, Zhi Gao, Simon H. Tindemans, Peter Palensky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For planning of power systems and for the calibration of operational tools,
it is essential to analyse system performance in a large range of
representative scenarios. When the available historical data is limited,
generative models are a promising solution, but modelling high-dimensional
dependencies is challenging. In this paper, a multivariate load state
generating model on the basis of a conditional variational autoencoder (CVAE)
neural network is proposed. Going beyond common CVAE implementations, the model
includes stochastic variation of output samples under given latent vectors and
co-optimizes the parameters for this output variability. It is shown that this
improves statistical properties of the generated data. The quality of generated
multivariate loads is evaluated using univariate and multivariate performance
metrics. A generation adequacy case study on the European network is used to
illustrate model's ability to generate realistic tail distributions. The
experiments demonstrate that the proposed generator outperforms other data
generating mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Grassucci, Gioia Mancini, Christian Brignone, Aurelio Uncini, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial audio methods are gaining a growing interest due to the spread of
immersive audio experiences and applications, such as virtual and augmented
reality. For these purposes, 3D audio signals are often acquired through arrays
of Ambisonics microphones, each comprising four capsules that decompose the
sound field in spherical harmonics. In this paper, we propose a dual quaternion
representation of the spatial sound field acquired through an array of two
First Order Ambisonics (FOA) microphones. The audio signals are encapsulated in
a dual quaternion that leverages quaternion algebra properties to exploit
correlations among them. This augmented representation with 6 degrees of
freedom (6DOF) involves a more accurate coverage of the sound field, resulting
in a more precise sound localization and a more immersive audio experience. We
evaluate our approach on a sound event localization and detection (SELD)
benchmark. We show that our dual quaternion SELD model with temporal
convolution blocks (DualQSELD-TCN) achieves better results with respect to real
and quaternion-valued baselines thanks to our augmented representation of the
sound field. Full code is available at:
https://github.com/ispamm/DualQSELD-TCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for publication in Elsevier Pattern Recognition
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Nadia Figueroa, Shen Li, Ankit Shah, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from demonstration (LfD) has succeeded in tasks featuring a long
time horizon. However, when the problem complexity also includes
human-in-the-loop perturbations, state-of-the-art approaches do not guarantee
the successful reproduction of a task. In this work, we identify the roots of
this challenge as the failure of a learned continuous policy to satisfy the
discrete plan implicit in the demonstration. By utilizing modes (rather than
subgoals) as the discrete abstraction and motion policies with both mode
invariance and goal reachability properties, we prove our learned continuous
policy can simulate any discrete plan specified by a linear temporal logic
(LTL) formula. Consequently, an imitator is robust to both task- and
motion-level perturbations and guaranteed to achieve task success. Project
page: https://yanweiw.github.io/tli/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2022 Oral Talk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FIS-GAN: GAN with Flow-based Importance Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1910.02519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1910.02519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Yi, Donglin Zhan, Wenqing Zhang, Denglin Jiang, Kang An, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GAN) training process, in most cases, apply
Uniform or Gaussian sampling methods in the latent space, which probably spends
most of the computation on examples that can be properly handled and easy to
generate. Theoretically, importance sampling speeds up stochastic optimization
in supervised learning by prioritizing training examples. In this paper, we
explore the possibility of adapting importance sampling into adversarial
learning. We use importance sampling to replace Uniform and Gaussian sampling
methods in the latent space and employ normalizing flow to approximate latent
space posterior distribution by density estimation. Empirically, results on
MNIST and Fashion-MNIST demonstrate that our method significantly accelerates
GAN's optimization while retaining visual fidelity in generated samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking AutoML algorithms on a collection of synthetic
  classification problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique Ribeiro, Patryk Orzechowski, Joost Wagenaar, Jason H. Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated machine learning (AutoML) algorithms have grown in popularity due
to their high performance and flexibility to adapt to different problems and
data sets. With the increasing number of AutoML algorithms, deciding which
would best suit a given problem becomes increasingly more work. Therefore, it
is essential to use complex and challenging benchmarks which would be able to
differentiate the AutoML algorithms from each other. This paper compares the
performance of four different AutoML algorithms: Tree-based Pipeline
Optimization Tool (TPOT), Auto-Sklearn, Auto-Sklearn 2, and H2O AutoML. We use
the Diverse and Generative ML benchmark (DIGEN), a diverse set of synthetic
datasets derived from generative functions designed to highlight the strengths
and weaknesses of the performance of common machine learning algorithms. We
confirm that AutoML can identify pipelines that perform well on all included
datasets. Most AutoML algorithms performed similarly without much room for
improvement; however, some were more consistent than others at finding
high-performing solutions for some datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A machine learning model to identify corruption in México's public
  procurement contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés Aldana, Andrea Falcón-Cortés, Hernán Larralde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The costs and impacts of government corruption range from impairing a
country's economic growth to affecting its citizens' well-being and safety.
Public contracting between government dependencies and private sector
instances, referred to as public procurement, is a fertile land of opportunity
for corrupt practices, generating substantial monetary losses worldwide. Thus,
identifying and deterring corrupt activities between the government and the
private sector is paramount. However, due to several factors, corruption in
public procurement is challenging to identify and track, leading to corrupt
practices going unnoticed. This paper proposes a machine learning model based
on an ensemble of random forest classifiers, which we call hyper-forest, to
identify and predict corrupt contracts in M\'exico's public procurement data.
This method's results correctly detect most of the corrupt and non-corrupt
contracts evaluated in the dataset. Furthermore, we found that the most
critical predictors considered in the model are those related to the
relationship between buyers and suppliers rather than those related to features
of individual contracts. Also, the method proposed here is general enough to be
trained with data from other countries. Overall, our work presents a tool that
can help in the decision-making process to identify, predict and analyze
corruption in public procurement contracts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures. On revision in Government Information Quarterly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESPNN: A novel electronic stopping power neural-network code built on
  the IAEA stopping power database. I. Atomic targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        F. Bivort Haiek, A. M. P. Mendez, C. C. Montanari, D. M. Mitnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The International Atomic Energy Agency (IAEA) stopping power database is a
highly valued public resource compiling most of the experimental measurements
published over nearly a century. The database-accessible to the global
scientific community-is continuously updated and has been extensively employed
in theoretical and experimental research for more than 30 years. This work aims
to employ machine learning algorithms on the 2021 IAEA database to predict
accurate electronic stopping power cross sections for any ion and target
combination in a wide range of incident energies. Unsupervised machine learning
methods are applied to clean the database in an automated manner. These
techniques purge the data by removing suspicious outliers and old isolated
values. A large portion of the remaining data is used to train a deep neural
network, while the rest is set aside, constituting the test set. The present
work considers collisional systems only with atomic targets. The first version
of the ESPNN (electronic stopping power neural-network code), openly available
to users, is shown to yield predicted values in excellent agreement with the
experimental results of the test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEL-Dock: Molecular Docking-Enabled Modeling of DNA-Encoded Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Shmilovich, Benson Chen, Theofanis Karaletsos, Mohammad M. Sultan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNA-Encoded Library (DEL) technology has enabled significant advances in hit
identification by enabling efficient testing of combinatorially-generated
molecular libraries. DEL screens measure protein binding affinity though
sequencing reads of molecules tagged with unique DNA-barcodes that survive a
series of selection experiments. Computational models have been deployed to
learn the latent binding affinities that are correlated to the sequenced count
data; however, this correlation is often obfuscated by various sources of noise
introduced in its complicated data-generation process. In order to denoise DEL
count data and screen for molecules with good binding affinity, computational
models require the correct assumptions in their modeling structure to capture
the correct signals underlying the data. Recent advances in DEL models have
focused on probabilistic formulations of count data, but existing approaches
have thus far been limited to only utilizing 2-D molecule-level
representations. We introduce a new paradigm, DEL-Dock, that combines
ligand-based descriptors with 3-D spatial information from docked
protein-ligand complexes. 3-D spatial information allows our model to learn
over the actual binding modality rather than using only structured-based
information of the ligand. We show that our model is capable of effectively
denoising DEL count data to predict molecule enrichment scores that are better
correlated with experimental binding affinity measurements compared to prior
works. Moreover, by learning over a collection of docked poses we demonstrate
that our model, trained only on DEL data, implicitly learns to perform good
docking pose selection without requiring external supervision from
expensive-to-source protein crystal structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving generalization in reinforcement learning through forked agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Moulin, Vincent Francois-Lavet, Mark Hoogendoorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An eco-system of agents each having their own policy with some, but limited,
generalizability has proven to be a reliable approach to increase
generalization across procedurally generated environments. In such an approach,
new agents are regularly added to the eco-system when encountering a new
environment that is outside of the scope of the eco-system. The speed of
adaptation and general effectiveness of the eco-system approach highly depends
on the initialization of new agents. In this paper we propose different
techniques for such initialization and study their impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly
  Multimedia Traffic in Graynet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.03027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.03027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Nato, Yiqiang Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying anomaly multimedia traffic in cyberspace is a big challenge in
distributed service systems, multiple generation networks and future internet
of everything. This letter explores meta-generalization for a multiparty
privacy learning model in graynet to improve the performance of anomaly
multimedia traffic identification. The multiparty privacy learning model in
graynet is a globally shared model that is partitioned, distributed and trained
by exchanging multiparty parameters updates with preserving private data. The
meta-generalization refers to discovering the inherent attributes of a learning
model to reduce its generalization error. In experiments, three
meta-generalization principles are tested as follows. The generalization error
of the multiparty privacy learning model in graynet is reduced by changing the
dimension of byte-level imbedding. Following that, the error is reduced by
adapting the depth for extracting packet-level features. Finally, the error is
reduced by adjusting the size of support set for preprocessing traffic-level
data. Experimental results demonstrate that the proposal outperforms the
state-of-the-art learning models for identifying anomaly multimedia traffic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correct some typos</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-13T00:00:00Z">2022-12-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrase Identification with Deep Learning: A <span class="highlight-title">Review</span> of <span class="highlight-title">Dataset</span>s and
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Cheng Qiu, Daniel E. Acuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI technology has made text generation tools like
GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can
pose serious threat to the credibility of various forms of media if these
technologies are used for plagiarism, including scientific literature and news
sources. Despite the development of automated methods for paraphrase
identification, detecting this type of plagiarism remains a challenge due to
the disparate nature of the datasets on which these methods are trained. In
this study, we review traditional and current approaches to paraphrase
identification and propose a refined typology of paraphrases. We also
investigate how this typology is represented in popular datasets and how
under-representation of certain types of paraphrases impacts detection
capabilities. Finally, we outline new directions for future research and
datasets in the pursuit of more effective paraphrase detection using AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 2 figures, 6 tables, 173 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Image Style Transfer from Freeform Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Santanam, Mengyang Liu, Jiangyue Yu, Zhaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper creates a novel method of deep neural style transfer by generating
style images from freeform user text input. The language model and style
transfer model form a seamless pipeline that can create output images with
similar losses and improved quality when compared to baseline style transfer
methods. The language model returns a closely matching image given a style text
and description input, which is then passed to the style transfer model with an
input content image to create a final output. A proof-of-concept tool is also
developed to integrate the models and demonstrate the effectiveness of deep
image style transfer from freeform text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-1: Robotics <span class="highlight-title">Transformer</span> for Real-World Control at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website at robotics-transformer.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A fine-grained comparison of pragmatic language understanding in humans
  and language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pragmatics is an essential part of communication, but it remains unclear what
mechanisms underlie human pragmatic communication and whether NLP systems
capture pragmatic language understanding. To investigate both these questions,
we perform a fine-grained comparison of language models and humans on seven
pragmatic phenomena, using zero-shot prompting on an expert-curated set of
English materials. We ask whether models (1) select pragmatic interpretations
of speaker utterances, (2) make similar error patterns as humans, and (3) use
similar linguistic cues as humans to solve the tasks. We find that the largest
models achieve high accuracy and match human error patterns: within incorrect
responses, models favor the literal interpretation of an utterance over
heuristic-based distractors. We also find evidence that models and humans are
sensitive to similar linguistic cues. Our results suggest that even
paradigmatic pragmatic phenomena may be solved without explicit representations
of other agents' mental states, and that artificial models can be used to gain
mechanistic insights into human pragmatic processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Demonstrations Improve In-context Compositional Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itay Levy, Ben Bogin, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning has shown great success in i.i.d semantic parsing splits,
where the training and test sets are drawn from the same distribution. In this
setup, models are typically prompted with demonstrations that are similar to
the input question. However, in the setup of compositional generalization,
where models are tested on outputs with structures that are absent from the
training set, selecting similar demonstrations is insufficient, as often no
example will be similar enough to the input. In this work, we propose a method
to select diverse demonstrations that aims to collectively cover all of the
structures required in the output program, in order to encourage the model to
generalize to new structures from these demonstrations. We empirically show
that combining diverse demonstrations with in-context learning substantially
improves performance across three compositional generalization semantic parsing
datasets in the pure in-context learning setup and when combined with
finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERNIE-Code: Beyond English-Centric Cross-lingual <span class="highlight-title">Pretrain</span>ing for
  Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
will make our code and pre-trained models publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured <span class="highlight-title">Prompt</span>ing: Scaling In-Context Learning to 1,000 Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have exhibited intriguing in-context learning
capability, achieving promising zero- and few-shot performance without updating
the parameters. However, conventional in-context learning is usually restricted
by length constraints, rendering it ineffective to absorb supervision from a
large number of examples. In order to go beyond few shots, we introduce
structured prompting that breaks the length limit and scales in-context
learning to thousands of examples. Specifically, demonstration examples are
separately encoded with well-designed position embeddings, and then they are
jointly attended by the test example using a rescaled attention mechanism. So
we can scale the number of exemplars with linear complexity instead of
quadratic complexity with respect to length. Experimental results on a diverse
set of tasks show that our approach improves end-task performance and reduces
evaluation variance over conventional in-context learning as the number of
demonstration examples increases. Code has been released at
https://aka.ms/structured-prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Text-to-Text Multi-Task Learners Suffer from Task Conflict? <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Mueller, Nicholas Andrews, Mark Dredze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional multi-task learning architectures train a single model across
multiple tasks through a shared encoder followed by task-specific decoders.
Learning these models often requires specialized training algorithms that
address task-conflict in the shared parameter updates, which otherwise can lead
to negative transfer. A new type of multi-task learning within NLP homogenizes
multi-task architectures as a shared encoder and language model decoder, which
does surprisingly well across a range of diverse tasks. Does this new
architecture suffer from task-conflicts that require specialized training
algorithms? We study how certain factors in the shift towards text-to-text
models affects multi-task conflict and negative transfer, finding that both
directional conflict and transfer are surprisingly constant across
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Categorical Tools for Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni de Felice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis develops the translation between category theory and
computational linguistics as a foundation for natural language processing. The
three chapters deal with syntax, semantics and pragmatics. First, string
diagrams provide a unified model of syntactic structures in formal grammars.
Second, functors compute semantics by turning diagrams into logical, tensor,
neural or quantum computation. Third, the resulting functorial models can be
composed to form games where equilibria are the solutions of language
processing tasks. This framework is implemented as part of DisCoPy, the Python
library for computing with string diagrams. We describe the correspondence
between categorical, linguistic and computational structures, and demonstrate
their applications in compositional natural language processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Category Theory for Quantum Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Toumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis introduces quantum natural language processing (QNLP) models
based on a simple yet powerful analogy between computational linguistics and
quantum mechanics: grammar as entanglement. The grammatical structure of text
and sentences connects the meaning of words in the same way that entanglement
structure connects the states of quantum systems. Category theory allows to
make this language-to-qubit analogy formal: it is a monoidal functor from
grammar to vector spaces. We turn this abstract analogy into a concrete
algorithm that translates the grammatical structure onto the architecture of
parameterised quantum circuits. We then use a hybrid classical-quantum
algorithm to train the model so that evaluating the circuits computes the
meaning of sentences in data-driven tasks.
  The implementation of QNLP models motivated the development of DisCoPy
(Distributional Compositional Python), the toolkit for applied category theory
of which the first chapter gives a comprehensive overview. String diagrams are
the core data structure of DisCoPy, they allow to reason about computation at a
high level of abstraction. We show how they can encode both grammatical
structures and quantum circuits, but also logical formulae, neural networks or
arbitrary Python code. Monoidal functors allow to translate these abstract
diagrams into concrete computation, interfacing with optimised task-specific
libraries.
  The second chapter uses DisCopy to implement QNLP models as parameterised
functors from grammar to quantum circuits. It gives a first proof-of-concept
for the more general concept of functorial learning: generalising machine
learning from functions to functors by learning from diagram-like data. In
order to learn optimal functor parameters via gradient descent, we introduce
the notion of diagrammatic differentiation: a graphical calculus for computing
the gradients of parameterised diagrams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis, University of Oxford</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fake News Detection with Heterogeneous Social Media Context
  Graphs <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Donabauer, Udo Kruschwitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection has become a research area that goes way beyond a purely
academic interest as it has direct implications on our society as a whole.
Recent advances have primarily focused on textbased approaches. However, it has
become clear that to be effective one needs to incorporate additional,
contextual information such as spreading behaviour of news articles and user
interaction patterns on social media. We propose to construct heterogeneous
social context graphs around news articles and reformulate the problem as a
graph classification task. Exploring the incorporation of different types of
information (to get an idea as to what level of social context is most
effective) and using different graph neural network architectures indicates
that this approach is highly effective with robust results on a common
benchmark dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted at the 45th European Conference on Information
  Retrieval (ECIR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localized Latent Updates for Fine-Tuning Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Ibing, Isaak Lim, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although massive pre-trained vision-language models like CLIP show impressive
generalization capabilities for many tasks, still it often remains necessary to
fine-tune them for improved performance on specific datasets. When doing so, it
is desirable that updating the model is fast and that the model does not lose
its capabilities on data outside of the dataset, as is often the case with
classical fine-tuning approaches. In this work we suggest a lightweight
adapter, that only updates the models predictions close to seen datapoints. We
demonstrate the effectiveness and speed of this relatively simple approach in
the context of few-shot learning, where our results both on classes seen and
unseen during training are comparable with or improve on the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Stance Detection as Textual Entailment Recognition and
  Leveraging Measurement Knowledge from Social Sciences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection (SD) can be considered a special case of textual entailment
recognition (TER), a generic natural language task. Modelling SD as TER may
offer benefits like more training data and a more general learning scheme. In
this paper, we present an initial empirical analysis of this approach. We apply
it to a difficult but relevant test case where no existing labelled SD dataset
is available, because this is where modelling SD as TER may be especially
helpful. We also leverage measurement knowledge from social sciences to improve
model performance. We discuss our findings and suggest future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distantly-Supervised Named Entity Recognition with Adaptive Teacher
  Learning and Fine-grained Student Ensemble <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoye Qu, Jun Zeng, Daizong Liu, Zhefeng Wang, Baoxing Huai, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates
the data scarcity problem in NER by automatically generating training samples.
Unfortunately, the distant supervision may induce noisy labels, thus
undermining the robustness of the learned models and restricting the practical
application. To relieve this problem, recent works adopt self-training
teacher-student frameworks to gradually refine the training labels and improve
the generalization ability of NER models. However, we argue that the
performance of the current self-training frameworks for DS-NER is severely
underestimated by their plain designs, including both inadequate student
learning and coarse-grained teacher updating. Therefore, in this paper, we make
the first attempt to alleviate these issues by proposing: (1) adaptive teacher
learning comprised of joint training of two teacher-student networks and
considering both consistent and inconsistent predictions between two teachers,
thus promoting comprehensive student learning. (2) fine-grained student
ensemble that updates each fragment of the teacher model with a temporal moving
average of the corresponding fragment of the student, which enhances consistent
predictions on each model fragment against noise. To verify the effectiveness
of our proposed method, we conduct experiments on four DS-NER datasets. The
experimental results demonstrate that our method significantly surpasses
previous SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lisan: Yemenu, Irqi, Libyan, and Sudanese Arabic Dialect Copora with
  Morphological Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Jarrar, Fadi A Zaraket, Tymaa Hammouda, Daanish Masood Alavi, Martin Waahlisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and
Libyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.
We collected the content of the corpora from several social media platforms.
The Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.
The corpora of the other three dialects (~ 50K tokens each) came manually from
Facebook and YouTube posts and comments.
  Thirty five (35) annotators who are native speakers of the target dialects
carried out the annotations. The annotators segemented all words in the four
corpora into prefixes, stems and suffixes and labeled each with different
morphological features such as part of speech, lemma, and a gloss in English.
An Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the
annation. The annotators were trained on a set of guidelines and on how to use
ADAT. We developed ADAT to assist the annotators and to ensure compatibility
with SAMA and Curras tagsets. The tool is open source, and the four corpora are
also available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Style-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and
  Speaker-wise Normalization in Speech Synthesis <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Qiang, Peng Yang, Hao Che, Xiaorui Wang, Zhongyuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-speaker style transfer in speech synthesis aims at transferring a style
from source speaker to synthesised speech of a target speaker's timbre. Most
previous approaches rely on data with style labels, but manually-annotated
labels are expensive and not always reliable. In response to this problem, we
propose Style-Label-Free, a cross-speaker style transfer method, which can
realize the style transfer from source speaker to target speaker without style
labels. Firstly, a reference encoder structure based on quantized variational
autoencoder (Q-VAE) and style bottleneck is designed to extract discrete style
representations. Secondly, a speaker-wise batch normalization layer is proposed
to reduce the source speaker leakage. In order to improve the style extraction
ability of the reference encoder, a style invariant and contrastive data
augmentation method is proposed. Experimental results show that the method
outperforms the baseline. We provide a website with audio samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published to ISCSLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tencent<span class="highlight-title">Pretrain</span>: A Scalable and Flexible Toolkit for <span class="highlight-title">Pre-train</span>ing Models
  of Different Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong Tian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan Liu, Weiquan Mao, Han Guo, Weigang Guo, Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen, Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xiaoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoyong Du, Linlin Shen, Kimmo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the success of pre-training in text domain has been fully extended
to vision, audio, and cross-modal scenarios. The proposed pre-training models
of different modalities are showing a rising trend of homogeneity in their
model structures, which brings the opportunity to implement different
pre-training models within a uniform framework. In this paper, we present
TencentPretrain, a toolkit supporting pre-training models of different
modalities. The core feature of TencentPretrain is the modular design. The
toolkit uniformly divides pre-training models into 5 components: embedding,
encoder, target embedding, decoder, and target. As almost all of common modules
are provided in each component, users can choose the desired modules from
different components to build a complete pre-training model. The modular design
enables users to efficiently reproduce existing pre-training models or build
brand-new one. We test the toolkit on text, vision, and audio benchmarks and
show that it can match the performance of the original implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a general purpose machine translation system for Sranantongo <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Just Zwennicker, David Stap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation for Sranantongo (Sranan, srn), a low-resource Creole
language spoken predominantly in Surinam, is virgin territory. In this study we
create a general purpose machine translation system for srn. In order to
facilitate this research, we introduce the SRNcorpus, a collection of parallel
Dutch (nl) to srn and monolingual srn data. We experiment with a wide range of
proven machine translation methods. Our results demonstrate a strong baseline
machine translation system for srn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WiNLP (EMNLP). 2 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InferEM: Inferring the Speaker's Intention for Empathetic Dialogue
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqing Lv, Xiaoping Wang, Jiang Li, Zhigang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches to empathetic response generation typically encode the
entire dialogue history directly and put the output into a decoder to generate
friendly feedback. These methods focus on modelling contextual information but
neglect capturing the direct intention of the speaker. We argue that the last
utterance in the dialogue empirically conveys the intention of the speaker.
Consequently, we propose a novel model named InferEM for empathetic response
generation. We separately encode the last utterance and fuse it with the entire
dialogue through multi-head attention based intention fusion module to capture
the speaker's intention. Besides, we utilize previous utterances to predict the
last utterance, which simulates human's psychology to guess what the
interlocutor may speak in advance. To balance the optimizing rates of the
utterance prediction and response generation, a multi-task learning strategy is
designed for InferEM. Experimental results demonstrate the plausibility and
validity of InferEM in improving empathetic expression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Massively Multilingual Natural Language Understanding 2022
  (MMNLU-22) Workshop and Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Hench, Charith Peris, Jack FitzGerald, Kay Rottmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in Natural Language Understanding (NLU), the creation
of multilingual NLU systems remains a challenge. It is common to have NLU
systems limited to a subset of languages due to lack of available data. They
also often vary widely in performance. We launch a three-phase approach to
address the limitations in NLU and help propel NLU technology to new heights.
We release a 52 language dataset called the Multilingual Amazon SLU resource
package (SLURP) for Slot-filling, Intent classification, and Virtual assistant
Evaluation, or MASSIVE, in an effort to address parallel data availability for
voice assistants. We organize the Massively Multilingual NLU 2022 Challenge to
provide a competitive environment and push the state-of-the art in the
transferability of models into other languages. Finally, we host the first
Massively Multilingual NLU workshop which brings these components together. The
MMNLU workshop seeks to advance the science behind multilingual NLU by
providing a platform for the presentation of new research in the field and
connecting teams working on this research direction. This paper summarizes the
dataset, workshop and the competition and the findings of each phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Despite "super-human" performance, current LLMs are unsuited for
  decisions about ethics and safety <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exploded in popularity in the past few
years and have achieved undeniably impressive results on benchmarks as varied
as question answering and text summarization. We provide a simple new prompting
strategy that leads to yet another supposedly "super-human" result, this time
outperforming humans at common sense ethical reasoning (as measured by accuracy
on a subset of the ETHICS dataset). Unfortunately, we find that relying on
average performance to judge capabilities can be highly misleading. LLM errors
differ systematically from human errors in ways that make it easy to craft
adversarial examples, or even perturb existing examples to flip the output
label. We also observe signs of inverse scaling with model size on some
examples, and show that prompting models to "explain their reasoning" often
leads to alarming justifications of unethical actions. Our results highlight
how human-like performance does not necessarily imply human-like understanding
or reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ML Safety Workshop, NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CREPE: Can Vision-Language Foundation Models Reason Compositionally? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental characteristic common to both human vision and natural language
is their compositional nature. Yet, despite the performance gains contributed
by large vision and language pretraining, we find that - across 6 architectures
trained with 4 algorithms on massive datasets - they exhibit little
compositionality. To arrive at this conclusion, we introduce a new
compositionality evaluation benchmark CREPE which measures two important
aspects of compositionality identified by cognitive science literature:
systematicity and productivity. To measure systematicity, CREPE consists of
three test datasets. The three test sets are designed to test models trained on
three of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They
contain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard
negative captions. To test productivity, CREPE contains 17K image-text pairs
with nine different complexities plus 246K hard negative captions with atomic,
swapping, and negation foils. The datasets are generated by repurposing the
Visual Genome scene graphs and region descriptions and applying handcrafted
templates and GPT-3. For systematicity, we find that model performance
decreases consistently when novel compositions dominate the retrieval set, with
Recall@1 dropping by up to 8%. For productivity, models' retrieval success
decays as complexity increases, frequently nearing random chance at high
complexity. These results hold regardless of model and training dataset size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foresight -- Deep Generative Modelling of Patient Timelines using
  Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeljko Kraljevic, Dan Bean, Anthony Shek, Rebecca Bendayan, Joshua Au Yeung, Alexander Deng, Alfie Baston, Jack Ross, Esther Idowu, James T Teo, Richard J Dobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) hold detailed longitudinal information about
each patient's health status and general clinical history, a large portion of
which is stored within the unstructured text. Temporal modelling of this
medical history, which considers the sequence of events, can be used to
forecast and simulate future events, estimate risk, suggest alternative
diagnoses or forecast complications. While most prediction approaches use
mainly structured data or a subset of single-domain forecasts and outcomes, we
processed the entire free-text portion of EHRs for longitudinal modelling. We
present Foresight, a novel GPT3-based pipeline that uses NER+L tools (i.e.
MedCAT) to convert document text into structured, coded concepts, followed by
providing probabilistic forecasts for future medical events such as disorders,
medications, symptoms and interventions. Since large portions of EHR data are
in text form, such an approach benefits from a granular and detailed view of a
patient while introducing modest additional noise. On tests in two large UK
hospitals (King's College Hospital, South London and Maudsley) and the US
MIMIC-III dataset precision@10 of 0.80, 0.81 and 0.91 was achieved for
forecasting the next biomedical concept. Foresight was also validated on 34
synthetic patient timelines by 5 clinicians and achieved relevancy of 97% for
the top forecasted candidate disorder. Foresight can be easily trained and
deployed locally as it only requires free-text data (as a minimum). As a
generative model, it can simulate follow-on disorders, medications and
interventions for as many steps as required. Foresight is a general-purpose
model for biomedical concept modelling that can be used for real-world risk
estimation, virtual trials and clinical research to study the progression of
diseases, simulate interventions and counterfactuals, and for educational
purposes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEREO: Scientific Text Reuse in Open Access Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Gienapp, Wolfgang Kircheis, Bjarne Sievers, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Webis-STEREO-21 dataset, a massive collection of Scientific
Text Reuse in Open-access publications. It contains more than 91 million cases
of reused text passages found in 4.2 million unique open-access publications.
Featuring a high coverage of scientific disciplines and varieties of reuse, as
well as comprehensive metadata to contextualize each case, our dataset
addresses the most salient shortcomings of previous ones on scientific writing.
Webis-STEREO-21 allows for tackling a wide range of research questions from
different scientific backgrounds, facilitating both qualitative and
quantitative analysis of the phenomenon as well as a first-time grounding on
the base rate of text reuse in scientific publications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce IndicXTREME, a benchmark consisting of nine
diverse tasks covering 18 languages from the Indic sub-continent belonging to
four different families. Across languages and tasks, IndicXTREME contains a
total of 103 evaluation sets, of which 51 are new contributions to the
literature. To maintain high quality, we only use human annotators to curate or
translate our datasets. To the best of our knowledge, this is the first effort
toward creating a standard benchmark for Indic languages that aims to test the
zero-shot capabilities of pretrained language models. We also release IndicCorp
v2, an updated and much larger version of IndicCorp that contains 20.9 billion
tokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate
it on IndicXTREME to show that it outperforms existing multilingual language
models such as XLM-R and MuRIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technological taxonomies for hypernym and hyponym retrieval in patent
  texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Zuo, Yixuan Li, Alma Parias García, Kim Gerdes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an automatic approach to creating taxonomies of technical
terms based on the Cooperative Patent Classification (CPC). The resulting
taxonomy contains about 170k nodes in 9 separate technological branches and is
freely available. We also show that a Text-to-Text Transfer Transformer (T5)
model can be fine-tuned to generate hypernyms and hyponyms with relatively high
precision, confirming the manually assessed quality of the resource. The T5
model opens the taxonomy to any new technological terms for which a hypernym
can be generated, thus making the resource updateable with new terms, an
essential feature for the constantly evolving field of technological
terminology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ToTh 2022 - Terminology & Ontology: Theories and applications, Jun
  2022, Chamb{\'e}ry, France</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniSumm: Unified Few-shot Summarization with Multi-Task <span class="highlight-title">Pre-Train</span>ing and
  Prefix-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09783v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09783v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diverse demands of different summarization tasks and their high
annotation costs are driving a need for few-shot summarization. However,
despite the emergence of many summarization tasks and datasets, the current
training paradigm for few-shot summarization systems ignores potentially
shareable knowledge in heterogeneous datasets. To this end, we propose
\textsc{UniSumm}, a unified few-shot summarization model pre-trained with
multiple summarization tasks and can be prefix-tuned to excel at any few-shot
summarization datasets. Meanwhile, to better evaluate few-shot summarization
systems, under the principles of diversity and robustness, we assemble and
publicize a new benchmark \textsc{SummZoo}. It consists of $8$ diverse
summarization tasks with multiple sets of few-shot samples for each task,
covering both monologue and dialogue domains. Experimental results and ablation
studies show that \textsc{UniSumm} outperforms strong baseline systems by a
large margin across all tasks in \textsc{SummZoo} under both automatic and
human evaluations. We release our code and benchmark at
\url{https://github.com/microsoft/UniSumm}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pivotal Role of Language Modeling in Recommender Systems: Enriching
  Task-specific and Task-agnostic Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyung-Min Kim, Jung-Woo Ha, Sang-Woo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have proposed unified user modeling frameworks that leverage
user behavior data from various applications. Many of them benefit from
utilizing users' behavior sequences as plain texts, representing rich
information in any domain or system without losing generality. Hence, a
question arises: Can language modeling for user history corpus help improve
recommender systems? While its versatile usability has been widely investigated
in many domains, its applications to recommender systems still remain
underexplored. We show that language modeling applied directly to task-specific
user histories achieves excellent results on diverse recommendation tasks.
Also, leveraging additional task-agnostic user histories delivers significant
performance benefits. We further demonstrate that our approach can provide
promising transfer learning capabilities for a broad spectrum of real-world
recommender systems, even on unseen domains and services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Textless Speech Emotion Conversion using Discrete and Decomposed
  Representations <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.07402v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.07402v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu-Anh Nguyen, Morgane Rivière, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, Yossi Adi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion conversion is the task of modifying the perceived emotion of a
speech utterance while preserving the lexical content and speaker identity. In
this study, we cast the problem of emotion conversion as a spoken language
translation task. We use a decomposition of the speech signal into discrete
learned representations, consisting of phonetic-content units, prosodic
features, speaker, and emotion. First, we modify the speech content by
translating the phonetic-content units to a target emotion, and then predict
the prosodic features based on these units. Finally, the speech waveform is
generated by feeding the predicted representations into a neural vocoder. Such
a paradigm allows us to go beyond spectral and parametric changes of the
signal, and model non-verbal vocalizations, such as laughter insertion, yawning
removal, etc. We demonstrate objectively and subjectively that the proposed
method is vastly superior to current approaches and even beats text-based
systems in terms of perceived emotion and audio quality. We rigorously evaluate
all components of such a complex system and conclude with an extensive model
analysis and ablation study to better emphasize the architectural choices,
strengths and weaknesses of the proposed method. Samples are available under
the following link: https://speechbot.github.io/emotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper was published at EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate
  Relation Selection and Entity Boundary Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuquan Lan, Dongxu Li, Yunqi Zhang, Hui Zhao, Gang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation
triplets from unstructured texts under the zero-shot setting, where the
relation sets at the training and testing stages are disjoint. Previous
state-of-the-art method handles this challenging task by leveraging pretrained
language models to generate data as additional training samples, which
increases the training cost and severely constrains the model performance. To
address the above issues, we propose a novel method named PCRED for ZeroRTE
with Potential Candidate Relation Selection and Entity Boundary Detection. The
remarkable characteristic of PCRED is that it does not rely on additional data
and still achieves promising performance. The model adopts a relation-first
paradigm, recognizing unseen relations through candidate relation selection.
With this approach, the semantics of relations are naturally infused in the
context. Entities are extracted based on the context and the semantics of
relations subsequently. We evaluate our model on two ZeroRTE datasets. The
experiment results show that our method consistently outperforms previous
works. Our code will be available at https://anonymous.4open.science/r/PCRED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Math Word Problem via Cooperative Reasoning induced Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.16257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.16257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained language models (PLMs) bring new opportunities to
challenge problems, especially those that need high-level intelligence, such as
the math word problem (MWPs). However, directly applying existing PLMs to MWPs
can fail as the generation process lacks sufficient supervision and thus lacks
fast adaptivity as humans. We notice that human reasoning has a dual reasoning
framework that consists of an immediate reaction system (system 1) and a
delicate reasoning system (system 2), where the entire reasoning is determined
by their interaction. This inspires us to develop a cooperative
reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),
resulting in a human-like reasoning architecture with system 1 as the generator
and system 2 as the verifier. In our approach, the generator is responsible for
generating reasoning paths, and the verifiers are used to supervise the
evaluation in order to obtain reliable feedback for the generator. We evaluate
our CoRe framework on several mathematical reasoning datasets and achieve
decent improvement over state-of-the-art methods, up to 9.8% increase over best
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experimental results are not sufficient</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORAL: Contextual Response Retrievability Loss Function for Training
  Dialog Generation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishal Santra, Ravi Ghadia, Arpit Dwivedi, Manish Gupta, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation (NLG) represents a large collection of tasks in
the field of NLP. While many of these tasks have been tackled well by the
cross-entropy (CE) loss, the task of dialog generation poses a few unique
challenges for this loss function. First, CE loss assumes that for any given
input, the only possible output is the one available as the ground truth in the
training dataset. In general, this is not true for any task, as there can be
multiple semantically equivalent sentences, each with a different surface form.
This problem gets exaggerated further for the dialog generation task, as there
can be multiple valid responses (for a given context) that not only have
different surface forms but are also not semantically equivalent. Second, CE
loss does not take the context into consideration while processing the response
and, hence, it treats all ground truths with equal importance irrespective of
the context. But, we may want our final agent to avoid certain classes of
responses (e.g. bland, non-informative or biased responses) and give relatively
higher weightage for more context-specific responses. To circumvent these
shortcomings of the CE loss, in this paper, we propose a novel loss function,
CORAL, that directly optimizes recently proposed estimates of human preference
for generated responses. Using CORAL, we can train dialog generation models
without assuming non-existence of response other than the ground-truth. Also,
the CORAL loss is computed based on both the context and the response.
Extensive comparisons on two benchmark datasets show that the proposed methods
outperform strong state-of-the-art baseline models of different sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over Different Types of Knowledge Graphs: Static, Temporal and
  Multi-Modal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Reuse Distractors to support Multiple Choice Question
  Generation in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semere Kiros Bitew, Amir Hadifar, Lucas Sterckx, Johannes Deleu, Chris Develder, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple choice questions (MCQs) are widely used in digital learning systems,
as they allow for automating the assessment process. However, due to the
increased digital literacy of students and the advent of social media
platforms, MCQ tests are widely shared online, and teachers are continuously
challenged to create new questions, which is an expensive and time-consuming
task. A particularly sensitive aspect of MCQ creation is to devise relevant
distractors, i.e., wrong answers that are not easily identifiable as being
wrong. This paper studies how a large existing set of manually created answers
and distractors for questions over a variety of domains, subjects, and
languages can be leveraged to help teachers in creating new MCQs, by the smart
reuse of existing distractors. We built several data-driven models based on
context-aware question and distractor representations, and compared them with
static feature-based models. The proposed models are evaluated with automated
metrics and in a realistic user test with teachers. Both automatic and human
evaluations indicate that context-aware models consistently outperform a static
feature-based approach. For our best-performing context-aware model, on average
3 distractors out of the 10 shown to teachers were rated as high-quality
distractors. We create a performance benchmark, and make it public, to enable
comparison between different approaches and to introduce a more standardized
evaluation of the task. The benchmark contains a test of 298 educational
questions covering multiple subjects & languages and a 77k multilingual pool of
distractor vocabulary for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages and 4 figures Accepted for publication in IEEE Transactions
  on Learning technologies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Round-Trip Translation for Machine Translation Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Qiongkai Xu, Xuanli He, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation on low-resource language translation suffers from a
deficiency of parallel corpora. Round-trip translation could be served as a
clever and straightforward technique to alleviate the requirement of the
parallel evaluation corpus. However, there was an observation of obscure
correlations between the evaluation scores by forward and round-trip
translations in the era of statistical machine translation (SMT). In this
paper, we report the surprising finding that round-trip translation can be used
for automatic evaluation without the references. Firstly, our revisit on the
round-trip translation in SMT evaluation unveils that its long-standing
misunderstanding is essentially caused by copying mechanism. After removing
copying mechanism in SMT, round-trip translation scores can appropriately
reflect the forward translation performance. Then, we demonstrate the
rectification is overdue as round-trip translation could benefit multiple
machine translation evaluation tasks. To be more specific, round-trip
translation could be used i) to predict corresponding forward translation
scores; ii) to improve the performance of the recently advanced quality
estimation model; and iii) to identify adversarial competitors in shared tasks
via cross-system verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Multiple Replies for Natural Language Generation Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eason Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, by responding to users' utterances with multiple replies to
create a group chat atmosphere, we alleviate the problem that Natural Language
Generation chatbots might reply with inappropriate content, thus causing a bad
user experience. Because according to our findings, users tend to pay attention
to appropriate replies and ignore inappropriate replies. We conducted a 2
(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)
repeated measures experiment to compare the chatting experience in different
conditions. The result shows that users will have a better chatting experience
when receiving multiple replies at once from the NLG model compared to the
single reply. Furthermore, according to the effect size of our result, to
improve the chatting experience for NLG chatbots which is single reply and
anonymous avatar, providing five replies will have more benefits than setting
an anime avatar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning for On-Device Speech Recognition using Disentangled
  Conformers <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Diwan, Ching-Feng Yeh, Wei-Ning Hsu, Paden Tomasello, Eunsol Choi, David Harwath, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition research focuses on training and evaluating on
static datasets. Yet, as speech models are increasingly deployed on personal
devices, such models encounter user-specific distributional shifts. To simulate
this real-world scenario, we introduce LibriContinual, a continual learning
benchmark for speaker-specific domain adaptation derived from LibriVox
audiobooks, with data corresponding to 118 individual speakers and 6 train
splits per speaker of different sizes. Additionally, current speech recognition
models and continual learning algorithms are not optimized to be
compute-efficient. We adapt a general-purpose training algorithm NetAug for ASR
and create a novel Conformer variant called the DisConformer (Disentangled
Conformer). This algorithm produces ASR models consisting of a frozen 'core'
network for general-purpose use and several tunable 'augment' networks for
speaker-specific tuning. Using such models, we propose a novel
compute-efficient continual learning algorithm called DisentangledCL. Our
experiments show that the DisConformer models significantly outperform
baselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On
speaker-specific LibriContinual they significantly outperform
trainable-parameter-matched baselines (by 20.65% rel. WER on test) and even
match fully finetuned baselines in some settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures. Submitted to ICASSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keywords Reinforcement LM: Improving End-to-End Response Generation in
  Task Oriented Dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Qingyang Wu, Kun Qian, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In task-oriented dialogs such as MultiWoZ (Budzianowski et al., 2018), an
informative and successful system response needs to include key information
such as the phone number of a hotel. Therefore, we hypothesize that by asking
the model to focus on generating more key quantities correctly, it can achieve
better overall performance. In this paper, we propose a new training algorithm,
Keywords Reinforcement Language Modeling (KRLM), that aims to use a
fine-grained reward function for each token and a new per-token Reinforcement
Learning procedure to help the model learn keywords generation more robustly
during inference. Empirical results show that our proposed KRLM training
algorithm can achieve state-of-the-art performance on the inform rate, success
rate, and combined score in the MultiWoZ benchmark dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>added algorithm analysis, and supplemental information such as error
  examples and more training/validation curves</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual
  Understanding With Multilingual Language Models <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifu Tu, Caiming Xiong, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained multilingual language models show significant performance gains
for zero-shot cross-lingual model transfer on a wide range of natural language
understanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,
pre-trained models are only fine-tuned on English data and tested on a variety
of target languages. In this paper, we do cross-lingual evaluation on various
NLU tasks (sentence classification, sequence labeling, question answering)
using prompt-tuning and compare it with fine-tuning. The results show that
prompt tuning achieves much better cross-lingual transfer than fine-tuning
across datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we
demonstrate through the analysis that prompt tuning can have better
cross-lingual transferability of representations on downstream tasks with
better aligned decision boundaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022. Code link is added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HighMMT: Quantifying Modality & Interaction Heterogeneity for
  High-Modality Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world problems are inherently multimodal, from the communicative
modalities humans use to express social and emotional states to the force,
proprioception, and visual sensors ubiquitous on robots. While there has been
an explosion of interest in multimodal representation learning, these methods
are still largely focused on a small set of modalities, primarily in the
language, vision, and audio space. In order to accelerate generalization
towards diverse and understudied modalities, this paper studies efficient
representation learning for high-modality scenarios. Since adding new models
for every new modality or task becomes prohibitively expensive, a critical
technical challenge is heterogeneity quantification: how can we measure which
modalities encode similar information and interactions in order to permit
parameter sharing with previous modalities? We propose two new
information-theoretic metrics for heterogeneity quantification: (1) modality
heterogeneity studies how similar 2 modalities $\{X_1,X_2\}$ are by measuring
how much information can be transferred from $X_1$ to $X_2$, while (2)
interaction heterogeneity studies how similarly pairs of modalities
$\{X_1,X_2\}, \{X_3,X_4\}$ interact by measuring how much interaction
information can be transferred from $\{X_1,X_2\}$ to $\{X_3,X_4\}$. We show the
importance of these proposed metrics in high-modality scenarios as a way to
automatically prioritize the fusion of modalities that contain unique
information or interactions. The result is a single model, HighMMT, that scales
up to $10$ modalities and $15$ tasks from $5$ different research areas. Not
only does HighMMT outperform prior methods on the tradeoff between performance
and efficiency, it also demonstrates a crucial scaling behavior: performance
continues to improve with each modality added, and transfers to entirely new
modalities and tasks during fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/pliang279/HighMMT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated ICD Coding using Extreme Multi-label Long Text
  <span class="highlight-title">Transformer</span>-based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, Louisa Jorm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Encouraged by the success of pretrained Transformer models in
many natural language processing tasks, their use for International
Classification of Diseases (ICD) coding tasks is now actively being explored.
In this study, we investigate three types of Transformer-based models, aiming
to address the extreme label set and long text classification challenges that
are posed by automated ICD coding tasks. Methods: The Transformer-based model
PLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD
coding benchmark dataset MIMIC-III. It was chosen as our baseline model to be
further optimised. XR-Transformer, the new SOTA model in the general extreme
multi-label text classification domain, and XR-LAT, a novel adaptation of the
XR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a
recursively trained model chain on a predefined hierarchical code tree with
label-wise attention, knowledge transferring and dynamic negative sampling
mechanisms. Results: Our optimised PLM-ICD model, which was trained with longer
total and chunk sequence lengths, significantly outperformed the current SOTA
PLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The
XR-Transformer model, although SOTA in the general domain, did not perform well
across all metrics. The best XR-LAT based model obtained results that were
competitive with the current SOTA PLM-ICD model, including improving the
macro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA
model for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT
model performs competitively with the previous SOTA PLM-ICD model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question
  Answering <span class="chip">NAACL 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.06378v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.06378v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of answering questions using knowledge from pre-trained language
models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA
context (question and answer choice), methods need to (i) identify relevant
knowledge from large KGs, and (ii) perform joint reasoning over the QA context
and KG. In this work, we propose a new model, QA-GNN, which addresses the above
challenges through two key innovations: (i) relevance scoring, where we use LMs
to estimate the importance of KG nodes relative to the given QA context, and
(ii) joint reasoning, where we connect the QA context and KG to form a joint
graph, and mutually update their representations through graph neural networks.
We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,
OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing
LM and LM+KG models, and exhibits capabilities to perform interpretable and
structured reasoning, e.g., correctly handling negation in questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2021. Code & data available at
  https://github.com/michiyasunaga/qagnn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned
  Receipt Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkuan Zhang, Edward Whittaker, Ikuo Kitagishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digitization of scanned receipts aims to extract text from receipt images and
save it into structured documents. This is usually split into two sub-tasks:
text localization and optical character recognition (OCR). Most existing OCR
models only focus on the cropped text instance images, which require the
bounding box information provided by a text region detection model. Introducing
an additional detector to identify the text instance images in advance is
inefficient, however instance-level OCR models have very low accuracy when
processing the whole image for the document-level OCR, such as receipt images
containing multiple text lines arranged in various layouts. To this end, we
propose a localization-free document-level OCR model for transcribing all the
characters in a receipt image into an ordered sequence end-to-end.
Specifically, we finetune the pretrained Transformer-based instance-level model
TrOCR with randomly cropped image chunks, and gradually increase the image
chunk size to generalize the recognition ability from instance images to
full-page images. In our experiments on the SROIE receipt OCR dataset, the
model finetuned with our strategy achieved 64.4 F1-score and a 22.8% character
error rates (CER) on the word-level and character-level metrics, respectively,
which outperforms the baseline results with 48.5 F1-score and 50.6% CER. The
best model, which splits the full image into 15 equally sized chunks, gives
87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of
the output. Moreover, the characters in the generated document-level sequences
are arranged in the reading order, which is practical for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image
  Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, William Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided image editing can have a transformative impact in supporting
creative applications. A key challenge is to generate edits that are faithful
to input text prompts, while consistent with input images. We present Imagen
Editor, a cascaded diffusion model built, by fine-tuning Imagen on text-guided
image inpainting. Imagen Editor's edits are faithful to the text prompts, which
is accomplished by using object detectors to propose inpainting masks during
training. In addition, Imagen Editor captures fine details in the input image
by conditioning the cascaded pipeline on the original high resolution image. To
improve qualitative and quantitative evaluation, we introduce EditBench, a
systematic benchmark for text-guided image inpainting. EditBench evaluates
inpainting edits on natural and generated images exploring objects, attributes,
and scenes. Through extensive human evaluation on EditBench, we find that
object-masking during training leads to across-the-board improvements in
text-image alignment -- such that Imagen Editor is preferred over DALL-E 2 and
Stable Diffusion -- and, as a cohort, these models are better at
object-rendering than text-rendering, and handle material/color/size attributes
better than count/shape attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Season Crop Progress in Un<span class="highlight-title">survey</span>ed Regions using Networks Trained on
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Worrall, Jasmeet Judge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many commodity crops have growth stages during which they are particularly
vulnerable to stress-induced yield loss. In-season crop progress information is
useful for quantifying crop risk, and satellite remote sensing (RS) can be used
to track progress at regional scales. At present, all existing RS-based crop
progress estimation (CPE) methods which target crop-specific stages rely on
ground truth data for training/calibration. This reliance on ground survey data
confines CPE methods to surveyed regions, limiting their utility. In this
study, a new method is developed for conducting RS-based in-season CPE in
unsurveyed regions by combining data from surveyed regions with synthetic crop
progress data generated for an unsurveyed region. Corn-growing zones in
Argentina were used as surrogate 'unsurveyed' regions. Existing weather
generation, crop growth, and optical radiative transfer models were linked to
produce synthetic weather, crop progress, and canopy reflectance data. A neural
network (NN) method based upon bi-directional Long Short-Term Memory was
trained separately on surveyed data, synthetic data, and two different
combinations of surveyed and synthetic data. A stopping criterion was developed
which uses the weighted divergence of surveyed and synthetic data validation
loss. Net F1 scores across all crop progress stages increased by 8.7% when
trained on a combination of surveyed region and synthetic data, and overall
performance was only 21% lower than when the NN was trained on surveyed data
and applied in the US Midwest. Performance gain from synthetic data was
greatest in zones with dual planting windows, while the inclusion of surveyed
region data from the US Midwest helped mitigate NN sensitivity to noise in NDVI
data. Overall results suggest in-season CPE in other unsurveyed regions may be
possible with increased quantity and variety of synthetic crop progress data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Difference Among <span class="highlight-title">Transformer</span>s and CNNs with Explanation
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Jiang, Saeed Khorram, Li Fuxin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a methodology that systematically applies deep explanation
algorithms on a dataset-wide basis, to compare different types of visual
recognition backbones, such as convolutional networks (CNNs), global attention
networks, and local attention networks. Examination of both qualitative
visualizations and quantitative statistics across the dataset helps us to gain
intuitions that are not just anecdotal, but are supported by the statistics
computed on the entire dataset. Specifically, we propose two methods. The first
one, sub-explanation counting, systematically searches for minimally-sufficient
explanations of all images and count the amount of sub-explanations for each
network. The second one, called cross-testing, computes salient regions using
one network and then evaluates the performance by only showing these regions as
an image to other networks. Through a combination of qualitative insights and
quantitative statistics, we illustrate that 1) there are significant
differences between the salient features of CNNs and attention models; 2) the
occlusion-robustness in local attention models and global attention models may
come from different decision-making mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages with 39 figures, uses cvpr.sty</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Labbé, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MegaPose, a method to estimate the 6D pose of novel objects,
that is, objects unseen during training. At inference time, the method only
assumes knowledge of (i) a region of interest displaying the object in the
image and (ii) a CAD model of the observed object. The contributions of this
work are threefold. First, we present a 6D pose refiner based on a
render&compare strategy which can be applied to novel objects. The shape and
coordinate system of the novel object are provided as inputs to the network by
rendering multiple synthetic views of the object's CAD model. Second, we
introduce a novel approach for coarse pose estimation which leverages a network
trained to classify whether the pose error between a synthetic rendering and an
observed image of the same object can be corrected by the refiner. Third, we
introduce a large-scale synthetic dataset of photorealistic images of thousands
of objects with diverse visual and shape properties and show that this
diversity is crucial to obtain good generalization performance on novel
objects. We train our approach on this large synthetic dataset and apply it
without retraining to hundreds of novel objects in real images from several
pose estimation benchmarks. Our approach achieves state-of-the-art performance
on the ModelNet and YCB-Video datasets. An extensive evaluation on the 7 core
datasets of the BOP challenge demonstrates that our approach achieves
performance competitive with existing approaches that require access to the
target objects during training. Code, dataset and trained models are available
on the project page: https://megapose6d.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Image Style Transfer from Freeform Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Santanam, Mengyang Liu, Jiangyue Yu, Zhaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper creates a novel method of deep neural style transfer by generating
style images from freeform user text input. The language model and style
transfer model form a seamless pipeline that can create output images with
similar losses and improved quality when compared to baseline style transfer
methods. The language model returns a closely matching image given a style text
and description input, which is then passed to the style transfer model with an
input content image to create a final output. A proof-of-concept tool is also
developed to integrate the models and demonstrate the effectiveness of deep
image style transfer from freeform text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LidarCLIP or: How I Learned to Talk to Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, Kalle Åström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research connecting text and images has recently seen several breakthroughs,
with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection
between text and other visual modalities, such as lidar data, has received less
attention, prohibited by the lack of text-lidar datasets. In this work, we
propose LidarCLIP, a mapping from automotive point clouds to a pre-existing
CLIP embedding space. Using image-lidar pairs, we supervise a point cloud
encoder with the image CLIP embeddings, effectively relating text and lidar
data with the image domain as an intermediary. We show the effectiveness of
LidarCLIP by demonstrating that lidar-based retrieval is generally on par with
image-based retrieval, but with complementary strengths and weaknesses. By
combining image and lidar features, we improve upon both single-modality
methods and enable a targeted search for challenging detection scenarios under
adverse sensor conditions. We also use LidarCLIP as a tool to investigate
fundamental lidar capabilities through natural language. Finally, we leverage
our compatibility with CLIP to explore a range of applications, such as point
cloud captioning and lidar-to-image generation, without any additional
training. We hope LidarCLIP can inspire future work to dive deeper into
connections between text and point cloud understanding. Code and trained models
available at https://github.com/atonderski/lidarclip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Before You Match: Instance Understanding Matters in Video Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Chuanxin Tang, Xiyang Dai, Yucheng Zhao, Yujia Xie, Lu Yuan, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring dense matching between the current frame and past frames for
long-range context modeling, memory-based methods have demonstrated impressive
results in video object segmentation (VOS) recently. Nevertheless, due to the
lack of instance understanding ability, the above approaches are oftentimes
brittle to large appearance variations or viewpoint changes resulted from the
movement of objects and cameras. In this paper, we argue that instance
understanding matters in VOS, and integrating it with memory-based matching can
enjoy the synergy, which is intuitively sensible from the definition of VOS
task, \ie, identifying and segmenting object instances within the video.
Towards this goal, we present a two-branch network for VOS, where the
query-based instance segmentation (IS) branch delves into the instance details
of the current frame and the VOS branch performs spatial-temporal matching with
the memory bank. We employ the well-learned object queries from IS branch to
inject instance-specific information into the query key, with which the
instance-augmented matching is further performed. In addition, we introduce a
multi-path fusion block to effectively combine the memory readout with
multi-scale features from the instance segmentation decoder, which incorporates
high-resolution instance-aware features to produce final segmentation results.
Our method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6%
and 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3%
and 86.3%), outperforming alternative methods by clear margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks and Defences for Skin Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinay Jogani, Joy Purohit, Ishaan Shivhare, Samina Attari, Shraddha Surtkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a concurrent significant improvement in the medical images
used to facilitate diagnosis and the performance of machine learning techniques
to perform tasks such as classification, detection, and segmentation in recent
years. As a result, a rapid increase in the usage of such systems can be
observed in the healthcare industry, for instance in the form of medical image
classification systems, where these models have achieved diagnostic parity with
human physicians. One such application where this can be observed is in
computer vision tasks such as the classification of skin lesions in
dermatoscopic images. However, as stakeholders in the healthcare industry, such
as insurance companies, continue to invest extensively in machine learning
infrastructure, it becomes increasingly important to understand the
vulnerabilities in such systems. Due to the highly critical nature of the tasks
being carried out by these machine learning models, it is necessary to analyze
techniques that could be used to take advantage of these vulnerabilities and
methods to defend against them. This paper explores common adversarial attack
techniques. The Fast Sign Gradient Method and Projected Descent Gradient are
used against a Convolutional Neural Network trained to classify dermatoscopic
images of skin lesions. Following that, it also discusses one of the most
popular adversarial defense techniques, adversarial training. The performance
of the model that has been trained on adversarial examples is then tested
against the previously mentioned attacks, and recommendations to improve neural
networks robustness are thus provided based on the results of the experiment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 2 tables, 2nd International Conference for
  Advancement in Technology (ICONAT 2023), Goa, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured 3D Features for Reconstructing Relightable and Animatable
  Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, Cristian Sminchisescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Structured 3D Features, a model based on a novel implicit 3D
representation that pools pixel-aligned image features onto dense 3D points
sampled from a parametric, statistical human mesh surface. The 3D points have
associated semantics and can move freely in 3D space. This allows for optimal
coverage of the person of interest, beyond just the body shape, which in turn,
additionally helps modeling accessories, hair, and loose clothing. Owing to
this, we present a complete 3D transformer-based attention framework which,
given a single image of a person in an unconstrained pose, generates an
animatable 3D reconstruction with albedo and illumination decomposition, as a
result of a single end-to-end model, trained semi-supervised, and with no
additional postprocessing. We show that our S3F model surpasses the previous
state-of-the-art on various tasks, including monocular 3D reconstruction, as
well as albedo and shading estimation. Moreover, we show that the proposed
methodology allows novel view synthesis, relighting, and re-posing the
reconstruction, and can naturally be extended to handle multiple input images
(e.g. different views of a person, or the same view, in different poses, in
video). Finally, we demonstrate the editing capabilities of our model for 3D
virtual try-on applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://enriccorona.github.io/s3f/ , Video:
  https://www.youtube.com/watch?v=mcZGcQ6L-2s</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-1: Robotics <span class="highlight-title">Transformer</span> for Real-World Control at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website at robotics-transformer.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Artificial Intelligence Assistance for Safe Laparoscopic
  Cholecystectomy: Early-Stage Clinical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Mascagni, Deepak Alapatt, Alfonso Lapergola, Armine Vardazaryan, Jean-Paul Mazellier, Bernard Dallemagne, Didier Mutter, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is set to be deployed in operating rooms to improve
surgical care. This early-stage clinical evaluation shows the feasibility of
concurrently attaining real-time, high-quality predictions from several deep
neural networks for endoscopic video analysis deployed for assistance during
three laparoscopic cholecystectomies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a face tell us anything about an NBA prospect? -- A Deep Learning
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Gavros, Foteini Gavrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical analysis and modeling is becoming increasingly popular for the
world's leading organizations, especially for professional NBA teams.
Sophisticated methods and models of sport talent evaluation have been created
for this purpose. In this research, we present a different perspective from the
dominant tactic of statistical data analysis. Based on a strategy that NBA
teams have followed in the past, hiring human professionals, we deploy image
analysis and Convolutional Neural Networks in an attempt to predict the career
trajectory of newly drafted players from each draft class. We created a
database consisting of about 1500 image data from players from every draft
since 1990. We then divided the players into five different quality classes
based on their expected NBA career. Next, we trained popular pre-trained image
classification models in our data and conducted a series of tests in an attempt
to create models that give reliable predictions of the rookie players' careers.
The results of this study suggest that there is a potential correlation between
facial characteristics and athletic talent, worth of further investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPViT: A High Resolution Non-Hierarchical Vision <span class="highlight-title">Transformer</span> with Group
  Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Jiarui Xu, Shalini De Mello, Elliot J. Crowley, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Group Propagation Vision Transformer (GPViT): a novel
nonhierarchical (i.e. non-pyramidal) transformer model designed for general
visual recognition with high-resolution features. High-resolution features (or
tokens) are a natural fit for tasks that involve perceiving fine-grained
details such as detection and segmentation, but exchanging global information
between these features is expensive in memory and computation because of the
way self-attention scales. We provide a highly efficient alternative Group
Propagation Block (GP Block) to exchange global information. In each GP Block,
features are first grouped together by a fixed number of learnable group
tokens; we then perform Group Propagation where global information is exchanged
between the grouped features; finally, global information in the updated
grouped features is returned back to the image features through a transformer
decoder. We evaluate GPViT on a variety of visual recognition tasks including
image classification, semantic segmentation, object detection, and instance
segmentation. Our method achieves significant performance gains over previous
works across all tasks, especially on tasks that require high-resolution
outputs, for example, our GPViT-L3 outperforms Swin Transformer-B by 2.0 mIoU
on ADE20K semantic segmentation with only half as many parameters. Code and
pre-trained models are available at https://github.com/ChenhongyiYang/GPViT .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ChenhongyiYang/GPViT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning 3D Representations from 2D <span class="highlight-title">Pre-train</span>ed Models via
  Image-to-Point Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training by numerous image data has become de-facto for robust 2D
representations. In contrast, due to the expensive data acquisition and
annotation, a paucity of large-scale 3D datasets severely hinders the learning
for high-quality 3D features. In this paper, we propose an alternative to
obtain superior 3D representations from 2D pre-trained models via
Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised
pre-training, we leverage the well learned 2D knowledge to guide 3D masked
autoencoding, which reconstructs the masked point tokens with an
encoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D
models to extract the multi-view visual features of the input point cloud, and
then conduct two types of image-to-point learning schemes on top. For one, we
introduce a 2D-guided masking strategy that maintains semantically important
point tokens to be visible for the encoder. Compared to random masking, the
network can better concentrate on significant 3D structures and recover the
masked tokens from key spatial cues. For another, we enforce these visible
tokens to reconstruct the corresponding multi-view 2D features after the
decoder. This enables the network to effectively inherit high-level 2D
semantics learned from rich image data for discriminative 3D modeling. Aided by
our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning,
achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to the fully
trained results of existing methods. By further fine-tuning on on
ScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11%
accuracy, +3.68% to the second-best, demonstrating superior transferable
capacity. Code will be available at https://github.com/ZrrSkywalker/I2P-MAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Lorenz, Margret Keuper, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNN) define the state-of-the-art solution on
many perceptual tasks. However, current CNN approaches largely remain
vulnerable against adversarial perturbations of the input that have been
crafted specifically to fool the system while being quasi-imperceptible to the
human eye. In recent years, various approaches have been proposed to defend
CNNs against such attacks, for example by model hardening or by adding explicit
defence mechanisms. Thereby, a small "detector" is included in the network and
trained on the binary classification task of distinguishing genuine data from
data containing adversarial perturbations. In this work, we propose a simple
and light-weight detector, which leverages recent findings on the relation
between networks' local intrinsic dimensionality (LID) and adversarial attacks.
Based on a re-interpretation of the LID measure and several simple adaptations,
we surpass the state-of-the-art on adversarial detection by a significant
margin and reach almost perfect results in terms of F1-score for several
networks and datasets. Sources available at:
https://github.com/adverML/multiLID
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at VISAPP23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connectivity-constrained Interactive Panoptic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruobing Shen, Bo Tang, Andrea Lodi, Ismail Ben Ayed, Thomas Guthier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address interactive panoptic annotation, where one segment all object and
stuff regions in an image. We investigate two graph-based segmentation
algorithms that both enforce connectivity of each region, with a notable
class-aware Integer Linear Programming (ILP) formulation that ensures global
optimum. Both algorithms can take RGB, or utilize the feature maps from any
DCNN, whether trained on the target dataset or not, as input. We then propose
an interactive, scribble-based annotation framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image
  and Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Falanti, Eugenio Lomurno, Danilo Ardagna, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated machine learning (AutoML) field has become increasingly
relevant in recent years. These algorithms can develop models without the need
for expert knowledge, facilitating the application of machine learning
techniques in the industry. Neural Architecture Search (NAS) exploits deep
learning techniques to autonomously produce neural network architectures whose
results rival the state-of-the-art models hand-crafted by AI experts. However,
this approach requires significant computational resources and hardware
investments, making it less appealing for real-usage applications. This article
presents the third version of Pareto-Optimal Progressive Neural Architecture
Search (POPNASv3), a new sequential model-based optimization NAS algorithm
targeting different hardware environments and multiple classification tasks.
Our method is able to find competitive architectures within large search
spaces, while keeping a flexible structure and data processing pipeline to
adapt to different tasks. The algorithm employs Pareto optimality to reduce the
number of architectures sampled during the search, drastically improving the
time efficiency without loss in accuracy. The experiments performed on images
and time series classification datasets provide evidence that POPNASv3 can
explore a large set of assorted operators and converge to optimal architectures
suited for the type of data provided under different scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What do Vision <span class="highlight-title">Transformer</span>s Learn? A Visual Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Ghiasi, Hamid Kazemi, Eitan Borgnia, Steven Reich, Manli Shu, Micah Goldblum, Andrew Gordon Wilson, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) are quickly becoming the de-facto architecture for
computer vision, yet we understand very little about why they work and what
they learn. While existing studies visually analyze the mechanisms of
convolutional neural networks, an analogous exploration of ViTs remains
challenging. In this paper, we first address the obstacles to performing
visualizations on ViTs. Assisted by these solutions, we observe that neurons in
ViTs trained with language model supervision (e.g., CLIP) are activated by
semantic concepts rather than visual features. We also explore the underlying
differences between ViTs and CNNs, and we find that transformers detect image
background features, just like their convolutional counterparts, but their
predictions depend far less on high-frequency information. On the other hand,
both architecture types behave similarly in the way features progress from
abstract patterns in early layers to concrete objects in late layers. In
addition, we show that ViTs maintain spatial information in all layers except
the final layer. In contrast to previous works, we show that the last layer
most likely discards the spatial information and behaves as a learned global
pooling operation. Finally, we conduct large-scale visualizations on a wide
range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to
validate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Brain Decoding: from fMRI to conceptually similar image
  reconstruction of visual stimuli 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Ferrante, Tommaso Boccato, Nicola Toschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain decoding is a field of computational neuroscience that uses measurable
brain activity to infer mental states or internal representations of perceptual
inputs. Therefore, we propose a novel approach to brain decoding that also
relies on semantic and contextual similarity. We employ an fMRI dataset of
natural image vision and create a deep learning decoding pipeline inspired by
the existence of both bottom-up and top-down processes in human vision. We
train a linear brain-to-feature model to map fMRI activity features to visual
stimuli features, assuming that the brain projects visual information onto a
space that is homeomorphic to the latent space represented by the last
convolutional layer of a pretrained convolutional neural network, which
typically collects a variety of semantic features that summarize and highlight
similarities and differences between concepts. These features are then
categorized in the latent space using a nearest-neighbor strategy, and the
results are used to condition a generative latent diffusion model to create
novel images. From fMRI data only, we produce reconstructions of visual stimuli
that match the original content very well on a semantic level, surpassing the
state of the art in previous literature. We evaluate our work and obtain good
results using a quantitative semantic metric (the Wu-Palmer similarity metric
over the WordNet lexicon, which had an average value of 0.57) and perform a
human evaluation experiment that resulted in correct evaluation, according to
the multiplicity of human criteria in evaluating image similarity, in over 80%
of the test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine Learning Enhanced Approach for Automated Sunquake Detection in
  Acoustic Emission Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Mercea, Alin Razvan Paraschiv, Daniela Adriana Lacatus, Anca Marginean, Diana Besliu-Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sunquakes are seismic emissions visible on the solar surface, associated with
some solar flares. Although discovered in 1998, they have only recently become
a more commonly detected phenomenon. Despite the availability of several manual
detection guidelines, to our knowledge, the astrophysical data produced for
sunquakes is new to the field of Machine Learning. Detecting sunquakes is a
daunting task for human operators and this work aims to ease and, if possible,
to improve their detection. Thus, we introduce a dataset constructed from
acoustic egression-power maps of solar active regions obtained for Solar Cycles
23 and 24 using the holography method. We then present a pedagogical approach
to the application of machine learning representation methods for sunquake
detection using AutoEncoders, Contrastive Learning, Object Detection and
recurrent techniques, which we enhance by introducing several custom
domain-specific data augmentation transformations. We address the main
challenges of the automated sunquake detection task, namely the very high noise
patterns in and outside the active region shadow and the extreme class
imbalance given by the limited number of frames that present sunquake
signatures. With our trained models, we find temporal and spatial locations of
peculiar acoustic emission and qualitatively associate them to eruptive and
high energy emission. While noting that these models are still in a prototype
stage and there is much room for improvement in metrics and bias levels, we
hypothesize that their agreement on example use cases has the potential to
enable detection of weak solar acoustic manifestations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Solar Physics accepted for publication, 44 total pages, 9 appendix
  pages, 21 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CNN-<span class="highlight-title">transformer</span> mixed model for object detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, one of the three main tasks of computer vision, has been
used in various applications. The main process is to use deep neural networks
to extract the features of an image and then use the features to identify the
class and location of an object. Therefore, the main direction to improve the
accuracy of object detection tasks is to improve the neural network to extract
features better. In this paper, I propose a convolutional module with a
transformer[1], which aims to improve the recognition accuracy of the model by
fusing the detailed features extracted by CNN[2] with the global features
extracted by a transformer and significantly reduce the computational effort of
the transformer module by deflating the feature mAP. The main execution steps
are convolutional downsampling to reduce the feature map size, then
self-attention calculation and upsampling, and finally concatenation with the
initial input. In the experimental part, after splicing the block to the end of
YOLOv5n[3] and training 300 epochs on the coco dataset, the mAP improved by
1.7% compared with the previous YOLOv5n, and the mAP curve did not show any
saturation phenomenon, so there is still potential for improvement. After 100
rounds of training on the Pascal VOC dataset, the accuracy of the results
reached 81%, which is 4.6 better than the faster RCNN[4] using resnet101[5] as
the backbone, but the number of parameters is less than one-twentieth of it.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIER: Text-Image Entropy Regularization for CLIP-style models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Palepu, Andrew L. Beam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the effect of a novel regularization scheme on
contrastive language-image pre-trained (CLIP) models. Our approach is based on
the observation that, in many domains, text tokens should only describe a small
number of image regions and, likewise, each image region should correspond to
only a few text tokens. In CLIP-style models, this implies that text-token
embeddings should have high similarity to only a small number of image-patch
embeddings for a given image-text pair. We formalize this observation using a
novel regularization scheme that penalizes the entropy of the text-token to
image-patch similarity scores. We qualitatively and quantitatively demonstrate
that the proposed regularization scheme shrinks the text-token and image-patch
similarity scores towards zero, thus achieving the desired effect. We
demonstrate the promise of our approach in an important medical context where
this underlying hypothesis naturally arises. Using our proposed approach, we
achieve state of the art (SOTA) zero-shot performance on all tasks from the
CheXpert chest x-ray dataset, outperforming an unregularized version of the
model and several recently published self-supervised models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Approach For Generating Customizable Light Field <span class="highlight-title">Dataset</span>s for
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Huang, Toure Smith, Aloukika Patro, Vidhi Chhabra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To train deep learning models, which often outperform traditional approaches,
large datasets of a specified medium, e.g., images, are used in numerous areas.
However, for light field-specific machine learning tasks, there is a lack of
such available datasets. Therefore, we create our own light field datasets,
which have great potential for a variety of applications due to the abundance
of information in light fields compared to singular images. Using the Unity and
C# frameworks, we develop a novel approach for generating large, scalable, and
reproducible light field datasets based on customizable hardware configurations
to accelerate light field deep learning research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted to and presented at MIT URTC Conference,
  and will be published in IEEE proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Neural Networks integrating genomics and histopathological images
  for predicting stages and survival time-to-event in colon cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olalekan Ogundipe, Zeyneb Kurt, Wai Lok Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There exists unexplained diverse variation within the predefined colon cancer
stages using only features either from genomics or histopathological whole
slide images as prognostic factors. Unraveling this variation will bring about
improved in staging and treatment outcome, hence motivated by the advancement
of Deep Neural Network libraries and different structures and factors within
some genomic dataset, we aggregate atypical patterns in histopathological
images with diverse carcinogenic expression from mRNA, miRNA and DNA
Methylation as an integrative input source into an ensemble deep neural network
for colon cancer stages classification and samples stratification into low or
high risk survival groups. The results of our Ensemble Deep Convolutional
Neural Network model show an improved performance in stages classification on
the integrated dataset. The fused input features return Area under curve
Receiver Operating Characteristic curve (AUC ROC) of 0.95 compared with AUC ROC
of 0.71 and 0.68 obtained when only genomics and images features are used for
the stage's classification, respectively. Also, the extracted features were
used to split the patients into low or high risk survival groups. Among the
2548 fused features, 1695 features showed a statistically significant survival
probability differences between the two risk groups defined by the extracted
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Deeper and Better Multi-view Feature Fusion for 3D Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaolong Yang, Yuyao Yan, Weiguang Zhao, Jianan Ye, Xi Yang, Amir Hussain, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point clouds are rich in geometric structure information, while 2D images
contain important and continuous texture information. Combining 2D information
to achieve better 3D semantic segmentation has become mainstream in 3D scene
understanding. Albeit the success, it still remains elusive how to fuse and
process the cross-dimensional features from these two distinct spaces. Existing
state-of-the-art usually exploit bidirectional projection methods to align the
cross-dimensional features and realize both 2D & 3D semantic segmentation
tasks. However, to enable bidirectional mapping, this framework often requires
a symmetrical 2D-3D network structure, thus limiting the network's flexibility.
Meanwhile, such dual-task settings may distract the network easily and lead to
over-fitting in the 3D segmentation task. As limited by the network's
inflexibility, fused features can only pass through a decoder network, which
affects model performance due to insufficient depth. To alleviate these
drawbacks, in this paper, we argue that despite its simplicity, projecting
unidirectionally multi-view 2D deep semantic features into the 3D space aligned
with 3D deep semantic features could lead to better feature fusion. On the one
hand, the unidirectional projection enforces our model focused more on the core
task, i.e., 3D segmentation; on the other hand, unlocking the bidirectional to
unidirectional projection enables a deeper cross-domain semantic alignment and
enjoys the flexibility to fuse better and complicated features from very
different spaces. In joint 2D-3D approaches, our proposed method achieves
superior performance on the ScanNetv2 benchmark for 3D semantic segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Hateful Memes Challenge Next Move 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijun Jin, Lance Wilhelm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art image and text classification models, such as Convectional
Neural Networks and Transformers, have long been able to classify their
respective unimodal reasoning satisfactorily with accuracy close to or
exceeding human accuracy. However, images embedded with text, such as hateful
memes, are hard to classify using unimodal reasoning when difficult examples,
such as benign confounders, are incorporated into the data set. We attempt to
generate more labeled memes in addition to the Hateful Memes data set from
Facebook AI, based on the framework of a winning team from the Hateful Meme
Challenge. To increase the number of labeled memes, we explore semi-supervised
learning using pseudo-labels for newly introduced, unlabeled memes gathered
from the Memotion Dataset 7K. We find that the semi-supervised learning task on
unlabeled data required human intervention and filtering and that adding a
limited amount of new data yields no extra classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Semi-Supervised Learning with Contrastive Complementary
  Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyi Deng, Yong Guo, Zhibang Yang, Haolin Pan, Jian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has achieved great success in leveraging a
large amount of unlabeled data to learn a promising classifier. A popular
approach is pseudo-labeling that generates pseudo labels only for those
unlabeled data with high-confidence predictions. As for the low-confidence
ones, existing methods often simply discard them because these unreliable
pseudo labels may mislead the model. Nevertheless, we highlight that these data
with low-confidence pseudo labels can be still beneficial to the training
process. Specifically, although the class with the highest probability in the
prediction is unreliable, we can assume that this sample is very unlikely to
belong to the classes with the lowest probabilities. In this way, these data
can be also very informative if we can effectively exploit these complementary
labels, i.e., the classes that a sample does not belong to. Inspired by this,
we propose a novel Contrastive Complementary Labeling (CCL) method that
constructs a large number of reliable negative pairs based on the complementary
labels and adopts contrastive learning to make use of all the unlabeled data.
Extensive experiments demonstrate that CCL significantly improves the
performance on top of existing methods. More critically, our CCL is
particularly effective under the label-scarce settings. For example, we yield
an improvement of 2.43% over FixMatch on CIFAR-10 only with 40 labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Visual and Lexical Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fausto Giunchiglia, Mayukh Bagchi, Xiaolei Diao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss two kinds of semantics relevant to Computer Vision (CV) systems -
Visual Semantics and Lexical Semantics. While visual semantics focus on how
humans build concepts when using vision to perceive a target reality, lexical
semantics focus on how humans build concepts of the same target reality through
the use of language. The lack of coincidence between visual and lexical
semantics, in turn, has a major impact on CV systems in the form of the
Semantic Gap Problem (SGP). The paper, while extensively exemplifying the lack
of coincidence as above, introduces a general, domain-agnostic methodology to
enforce alignment between visual and lexical semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>iConference 2023, Barcelona, March 27 - 29, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo <span class="chip">WACV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Sormann, Emanuele Santellani, Mattia Rossi, Andreas Kuhn, Friedrich Fraundorfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for deep learning-based Multi-View Stereo (MVS).
For each pixel in the reference image, our method leverages a deep architecture
to search for the corresponding point in the source image directly along the
corresponding epipolar line. We denote our method DELS-MVS: Deep Epipolar Line
Search Multi-View Stereo. Previous works in deep MVS select a range of interest
within the depth space, discretize it, and sample the epipolar line according
to the resulting depth values: this can result in an uneven scanning of the
epipolar line, hence of the image space. Instead, our method works directly on
the epipolar line: this guarantees an even scanning of the image space and
avoids both the need to select a depth range of interest, which is often not
known a priori and can vary dramatically from scene to scene, and the need for
a suitable discretization of the depth space. In fact, our search is iterative,
which avoids the building of a cost volume, costly both to store and to
process. Finally, our method performs a robust geometry-aware fusion of the
estimated depth maps, leveraging a confidence predicted alongside each depth.
We test DELS-MVS on the ETH3D, Tanks and Temples and DTU benchmarks and achieve
competitive results with respect to state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at WACV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OAMixer: Object-aware Mixing Layer for Vision <span class="highlight-title">Transformer</span>s <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Kang, Sangwoo Mo, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patch-based models, e.g., Vision Transformers (ViTs) and Mixers, have shown
impressive results on various visual recognition tasks, alternating classic
convolutional networks. While the initial patch-based models (ViTs) treated all
patches equally, recent studies reveal that incorporating inductive bias like
spatiality benefits the representations. However, most prior works solely
focused on the location of patches, overlooking the scene structure of images.
Thus, we aim to further guide the interaction of patches using the object
information. Specifically, we propose OAMixer (object-aware mixing layer),
which calibrates the patch mixing layers of patch-based models based on the
object labels. Here, we obtain the object labels in unsupervised or
weakly-supervised manners, i.e., no additional human-annotating cost is
necessary. Using the object labels, OAMixer computes a reweighting mask with a
learnable scale parameter that intensifies the interaction of patches
containing similar objects and applies the mask to the patch mixing layers. By
learning an object-centric representation, we demonstrate that OAMixer improves
the classification accuracy and background robustness of various patch-based
models, including ViTs, MLP-Mixers, and ConvMixers. Moreover, we show that
OAMixer enhances various downstream tasks, including large-scale
classification, self-supervised learning, and multi-object recognition,
verifying the generic applicability of OAMixer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR Transformers for Vision Workshop 2022. First two authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastMIM: Expediting Masked Image Modeling <span class="highlight-title">Pre-train</span>ing for Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Yunhe Wang, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of transformers and masked image modeling (MIM) pre-training
framework has shown great potential in various vision tasks. However, the
pre-training computational budget is too heavy and withholds the MIM from
becoming a practical training paradigm. This paper presents FastMIM, a simple
and generic framework for expediting masked image modeling with the following
two steps: (i) pre-training vision backbones with low-resolution input images;
and (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead
of original RGB values of the input images. In addition, we propose FastMIM-P
to progressively enlarge the input resolution during pre-training stage to
further enhance the transfer results of models with high capacity. We point out
that: (i) a wide range of input resolutions in pre-training phase can lead to
similar performances in fine-tuning phase and downstream tasks such as
detection and segmentation; (ii) the shallow layers of encoder are more
important during pre-training and discarding last several layers can speed up
the training stage with no harm to fine-tuning performance; (iii) the decoder
should match the size of selected network; and (iv) HOG is more stable than RGB
values when resolution transfers;. Equipped with FastMIM, all kinds of vision
backbones can be pre-trained in an efficient way. For example, we can achieve
83.8%/84.1% top-1 accuracy on ImageNet-1K with ViT-B/Swin-B as backbones.
Compared to previous relevant approaches, we can achieve comparable or better
top-1 accuracy while accelerate the training procedure by $\sim$5$\times$. Code
can be found in https://github.com/ggjy/FastMIM.pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localized Latent Updates for Fine-Tuning Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Ibing, Isaak Lim, Leif Kobbelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although massive pre-trained vision-language models like CLIP show impressive
generalization capabilities for many tasks, still it often remains necessary to
fine-tune them for improved performance on specific datasets. When doing so, it
is desirable that updating the model is fast and that the model does not lose
its capabilities on data outside of the dataset, as is often the case with
classical fine-tuning approaches. In this work we suggest a lightweight
adapter, that only updates the models predictions close to seen datapoints. We
demonstrate the effectiveness and speed of this relatively simple approach in
the context of few-shot learning, where our results both on classes seen and
unseen during training are comparable with or improve on the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Body Segmentation Using Multi-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julijan Jug, Ajda Lampe, Vitomir Štruc, Peter Peer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Body segmentation is an important step in many computer vision problems
involving human images and one of the key components that affects the
performance of all downstream tasks. Several prior works have approached this
problem using a multi-task model that exploits correlations between different
tasks to improve segmentation performance. Based on the success of such
solutions, we present in this paper a novel multi-task model for human
segmentation/parsing that involves three tasks, i.e., (i) keypoint-based
skeleton estimation, (ii) dense pose prediction, and (iii) human-body
segmentation. The main idea behind the proposed Segmentation--Pose--DensePose
model (or SPD for short) is to learn a better segmentation model by sharing
knowledge across different, yet related tasks. SPD is based on a shared deep
neural network backbone that branches off into three task-specific model heads
and is learned using a multi-task optimization objective. The performance of
the model is analysed through rigorous experiments on the LIP and ATR datasets
and in comparison to a recent (state-of-the-art) multi-task body-segmentation
model. Comprehensive ablation studies are also presented. Our experimental
results show that the proposed multi-task (segmentation) model is highly
competitive and that the introduction of additional tasks contributes towards a
higher overall segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Mini-Batch Training with Varying Length Time Series <span class="chip">ICASSP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Kenji Iwana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world time series recognition applications, it is possible to have
data with varying length patterns. However, when using artificial neural
networks (ANN), it is standard practice to use fixed-sized mini-batches. To do
this, time series data with varying lengths are typically normalized so that
all the patterns are the same length. Normally, this is done using zero padding
or truncation without much consideration. We propose a novel method of
normalizing the lengths of the time series in a dataset by exploiting the
dynamic matching ability of Dynamic Time Warping (DTW). In this way, the time
series lengths in a dataset can be set to a fixed size while maintaining
features typical to the dataset. In the experiments, all 11 datasets with
varying length time series from the 2018 UCR Time Series Archive are used. We
evaluate the proposed method by comparing it with 18 other length normalization
methods on a Convolutional Neural Network (CNN), a Long-Short Term Memory
network (LSTM), and a Bidirectional LSTM (BLSTM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse
  Spatial-Temporal Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyangguang Zhang, Zhiqiang Lou, Yan Di, Federico Tombari, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time monocular 3D reconstruction is a challenging problem that remains
unsolved. Although recent end-to-end methods have demonstrated promising
results, tiny structures and geometric boundaries are hardly captured due to
their insufficient supervision neglecting spatial details and oversimplified
feature fusion ignoring temporal cues. To address the problems, we propose an
end-to-end 3D reconstruction network SST, which utilizes Sparse estimated
points from visual SLAM system as additional Spatial guidance and fuses
Temporal features via a novel cross-modal attention mechanism, achieving more
detailed reconstruction results. We propose a Local Spatial-Temporal Fusion
module to exploit more informative spatial-temporal cues from multi-view color
information and sparse priors, as well a Global Spatial-Temporal Fusion module
to refine the local TSDF volumes with the world-frame model from coarse to
fine. Extensive experiments on ScanNet and 7-Scenes demonstrate that SST
outperforms all state-of-the-art competitors, whilst keeping a high inference
speed at 59 FPS, enabling real-world applications with real-time requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of The MediaEval 2022 Predicting Video Memorability Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorin Sweeney, Mihai Gabriel Constantin, Claire-Hélène Demarty, Camilo Fosco, Alba G. Seco de Herrera, Sebastian Halder, Graham Healy, Bogdan Ionescu, Ana Matran-Fernandez, Alan F. Smeaton, Mushfika Sultana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the 5th edition of the Predicting Video Memorability
Task as part of MediaEval2022. This year we have reorganised and simplified the
task in order to lubricate a greater depth of inquiry. Similar to last year,
two datasets are provided in order to facilitate generalisation, however, this
year we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem
dataset in order to remedy underlying data quality issues, and to prioritise
short-term memorability prediction by elevating the Memento10k dataset as the
primary dataset. Additionally, a fully fledged electroencephalography
(EEG)-based prediction sub-task is introduced. In this paper, we outline the
core facets of the task and its constituent sub-tasks; describing the datasets,
evaluation metrics, and requirements for participant submissions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages. In: MediaEval Multimedia Benchmark Workshop Working Notes,
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvMIL: Adversarial Multiple Instance Learning for the Survival Analysis
  on Whole-Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Liu, Luping Ji, Feng Ye, Bo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The survival analysis on histological whole-slide images (WSIs) is one of the
most important means to estimate patient prognosis. Although many
weakly-supervised deep learning models have been developed for gigapixel WSIs,
their potential is generally restricted by classical survival analysis rules
and fully-supervision requirements. As a result, these models provide patients
only with a completely-certain point estimation of time-to-event, and they
could only learn from the well-annotated WSI data currently at a small scale.
To tackle these problems, we propose a novel adversarial multiple instance
learning (AdvMIL) framework. This framework is based on adversarial
time-to-event modeling, and it integrates the multiple instance learning (MIL)
that is much necessary for WSI representation learning. It is a plug-and-play
one, so that most existing WSI-based models with embedding-level MIL networks
can be easily upgraded by applying this framework, gaining the improved ability
of survival distribution estimation and semi-supervised learning. Our extensive
experiments show that AdvMIL could not only bring performance improvement to
mainstream WSI models at a relatively low computational cost, but also enable
these models to learn from unlabeled data with semi-supervised learning. Our
AdvMIL framework could promote the research of time-to-event modeling in
computational pathology with its novel paradigm of adversarial MIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DifFace: Blind Face Restoration with Diffused Error Contraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongsheng Yue, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning-based methods for blind face restoration have achieved
unprecedented success, they still suffer from two major limitations. First,
most of them deteriorate when facing complex degradations out of their training
data. Second, these methods require multiple constraints, e.g., fidelity,
perceptual, and adversarial losses, which require laborious hyper-parameter
tuning to stabilize and balance their influences. In this work, we propose a
novel method named DifFace that is capable of coping with unseen and complex
degradations more gracefully without complicated loss designs. The key of our
method is to establish a posterior distribution from the observed low-quality
(LQ) image to its high-quality (HQ) counterpart. In particular, we design a
transition distribution from the LQ image to the intermediate state of a
pre-trained diffusion model and then gradually transmit from this intermediate
state to the HQ target by recursively applying a pre-trained diffusion model.
The transition distribution only relies on a restoration backbone that is
trained with $L_2$ loss on some synthetic data, which favorably avoids the
cumbersome training process in existing methods. Moreover, the transition
distribution can contract the error of the restoration backbone and thus makes
our method more robust to unknown degradations. Comprehensive experiments show
that DifFace is superior to current state-of-the-art methods, especially in
cases with severe degradations. Our code and model are available at
https://github.com/zsyOAOA/DifFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3rd Continual Learning Workshop Challenge on Egocentric Category and
  Instance Level Object Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Pellegrini, Chenchen Zhu, Fanyi Xiao, Zhicheng Yan, Antonio Carta, Matthias De Lange, Vincenzo Lomonaco, Roshan Sumbaly, Pau Rodriguez, David Vazquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning, also known as Lifelong or Incremental Learning, has
recently gained renewed interest among the Artificial Intelligence research
community. Recent research efforts have quickly led to the design of novel
algorithms able to reduce the impact of the catastrophic forgetting phenomenon
in deep neural networks. Due to this surge of interest in the field, many
competitions have been held in recent years, as they are an excellent
opportunity to stimulate research in promising directions. This paper
summarizes the ideas, design choices, rules, and results of the challenge held
at the 3rd Continual Learning in Computer Vision (CLVision) Workshop at CVPR
2022. The focus of this competition is the complex continual object detection
task, which is still underexplored in literature compared to classification
tasks. The challenge is based on the challenge version of the novel EgoObjects
dataset, a large-scale egocentric object dataset explicitly designed to
benchmark continual learning algorithms for egocentric category-/instance-level
object understanding, which covers more than 1k unique main objects and 250+
categories in around 100k video frames.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Sample-Level Out-of-Distribution Detection on 3D Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Frolova, Anton Vasiliuk, Mikhail Belyaev, Boris Shirokikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) models tend to perform poorly when the data comes from a
distribution different from the training one. In critical applications such as
medical imaging, out-of-distribution (OOD) detection helps to identify such
data samples, increasing the model's reliability. Recent works have developed
DL-based OOD detection that achieves promising results on 2D medical images.
However, scaling most of these approaches on 3D images is computationally
intractable. Furthermore, the current 3D solutions struggle to achieve
acceptable results in detecting even synthetic OOD samples. Such limited
performance might indicate that DL often inefficiently embeds large volumetric
images. We argue that using the intensity histogram of the original CT or MRI
scan as embedding is descriptive enough to run OOD detection. Therefore, we
propose a histogram-based method that requires no DL and achieves almost
perfect results in this domain. Our proposal is supported two-fold. We evaluate
the performance on the publicly available datasets, where our method scores 1.0
AUROC in most setups. And we score second in the Medical Out-of-Distribution
challenge without fine-tuning and exploiting task-specific knowledge. Carefully
discussing the limitations, we conclude that our method solves the sample-level
OOD detection on 3D medical images in the current setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures, submitted to Computerized Medical Imaging and
  Graphics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning
  for Salient Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Lin Wang, Wei Wang, Qing Xia, Chenglizhao Chen, Aimin Hao, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although weakly-supervised techniques can reduce the labeling effort, it is
unclear whether a saliency model trained with weakly-supervised data (e.g.,
point annotation) can achieve the equivalent performance of its
fully-supervised version. This paper attempts to answer this unexplored
question by proving a hypothesis: there is a point-labeled dataset where
saliency models trained on it can achieve equivalent performance when trained
on the densely annotated dataset. To prove this conjecture, we proposed a novel
yet effective adversarial trajectory-ensemble active learning (ATAL). Our
contributions are three-fold: 1) Our proposed adversarial attack triggering
uncertainty can conquer the overconfidence of existing active learning methods
and accurately locate these uncertain pixels. {2)} Our proposed
trajectory-ensemble uncertainty estimation method maintains the advantages of
the ensemble networks while significantly reducing the computational cost. {3)}
Our proposed relationship-aware diversity sampling algorithm can conquer
oversampling while boosting performance. Experimental results show that our
ATAL can find such a point-labeled dataset, where a saliency model trained on
it obtained $97\%$ -- $99\%$ performance of its fully-supervised version with
only ten annotated points per image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantics-Consistent Feature Search for <span class="highlight-title">Self-Supervised</span> Visual
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyou Song, Shan Zhang, Zihao An, Zimeng Luo, Tong Wang, Jin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrastive self-supervised learning, the common way to learn
discriminative representation is to pull different augmented "views" of the
same image closer while pushing all other images further apart, which has been
proven to be effective. However, it is unavoidable to construct undesirable
views containing different semantic concepts during the augmentation procedure.
It would damage the semantic consistency of representation to pull these
augmentations closer in the feature space indiscriminately. In this study, we
introduce feature-level augmentation and propose a novel semantics-consistent
feature search (SCFS) method to mitigate this negative effect. The main idea of
SCFS is to adaptively search semantics-consistent features to enhance the
contrast between semantics-consistent regions in different augmentations. Thus,
the trained model can learn to focus on meaningful object regions, improving
the semantic representation ability. Extensive experiments conducted on
different datasets and tasks demonstrate that SCFS effectively improves the
performance of self-supervised learning and achieves state-of-the-art
performance on different downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Statistical Model for Predicting Generalization in Few-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassir Bendou, Vincent Gripon, Bastien Pasdeloup, Lukas Mauch, Stefan Uhlich, Fabien Cardinaux, Ghouthi Boukli Hacene, Javier Alonso Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The estimation of the generalization error of classifiers often relies on a
validation set. Such a set is hardly available in few-shot learning scenarios,
a highly disregarded shortcoming in the field. In these scenarios, it is common
to rely on features extracted from pre-trained neural networks combined with
distance-based classifiers such as nearest class mean. In this work, we
introduce a Gaussian model of the feature distribution. By estimating the
parameters of this model, we are able to predict the generalization error on
new classification tasks with few samples. We observe that accurate distance
estimates between class-conditional densities are the key to accurate estimates
of the generalization performance. Therefore, we propose an unbiased estimator
for these distances and integrate it in our numerical analysis. We show that
our approach outperforms alternatives such as the leave-one-out
cross-validation strategy in few-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HS-Diffusion: Learning a Semantic-Guided Diffusion Model for Head
  Swapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghe Wang, Lijie Liu, Miao Hua, Qian He, Pengfei Zhu, Bing Cao, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based head swapping task aims to stitch a source head to another source
body flawlessly. This seldom-studied task faces two major challenges: 1)
Preserving the head and body from various sources while generating a seamless
transition region. 2) No paired head swapping dataset and benchmark so far. In
this paper, we propose an image-based head swapping framework (HS-Diffusion)
which consists of a semantic-guided latent diffusion model (SG-LDM) and a
semantic layout generator. We blend the semantic layouts of source head and
source body, and then inpaint the transition region by the semantic layout
generator, achieving a coarse-grained head swapping. SG-LDM can further
implement fine-grained head swapping with the blended layout as condition by a
progressive fusion process, while preserving source head and source body with
high-quality reconstruction. To this end, we design a head-cover augmentation
strategy for training and a neck alignment trick for geometric realism.
Importantly, we construct a new image-based head swapping benchmark and propose
two tailor-designed metrics (Mask-FID and Focal-FID). Extensive experiments
demonstrate the superiority of our framework. The code will be available:
https://github.com/qinghew/HS-Diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypercomplex Neural Architectures for Multi-View Breast Cancer
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.05798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.05798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Eleonora Grassucci, Martina Valleriani, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, deep learning methods for breast cancer classification perform
a single-view analysis. However, radiologists simultaneously analyze all four
views that compose a mammography exam, owing to the correlations contained in
mammography views, which present crucial information for identifying tumors. In
light of this, some studies have started to propose multi-view methods.
Nevertheless, in such existing architectures, mammogram views are processed as
independent images by separate convolutional branches, thus losing correlations
among them. To overcome such limitations, in this paper we propose a novel
approach for multi-view breast cancer classification based on parameterized
hypercomplex neural networks. Thanks to hypercomplex algebra properties, our
networks are able to model, and thus leverage, existing correlations between
the different views that comprise a mammogram, thus mimicking the reading
process performed by clinicians. The proposed methods are able to handle the
information of a patient altogether without breaking the multi-view nature of
the exam. We define architectures designed to process two-view exams, namely
PHResNets, and four-view exams, i.e., PHYSEnet and PHYBOnet. Through an
extensive experimental evaluation conducted with publicly available datasets,
we demonstrate that our proposed models clearly outperform real-valued
counterparts and also state-of-the-art methods, proving that breast cancer
classification benefits from the proposed multi-view architectures. We also
assess the method's robustness beyond mammogram analysis by considering
different benchmarks, as well as a finer-scaled task such as segmentation. Full
code and pretrained models for complete reproducibility of our experiments are
freely available at: https://github.com/ispamm/PHBreast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Transactions on Neural Networks
  and Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructRL: Instruction-Following Agents with Jointly <span class="highlight-title">Pre-Train</span>ed
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Lisa Lee, Kimin Lee, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are excellent at understanding language and vision to accomplish a
wide range of tasks. In contrast, creating general instruction-following
embodied agents remains a difficult challenge. Prior work that uses pure
language-only models lack visual grounding, making it difficult to connect
language instructions with visual observations. On the other hand, methods that
use pre-trained vision-language models typically come with divided language and
visual representations, requiring designing specialized network architecture to
fuse them together. We propose a simple yet effective model for robots to solve
instruction-following tasks in vision-based environments. Our \ours method
consists of a multimodal transformer that encodes visual observations and
language instructions, and a policy transformer that predicts actions based on
encoded representations. The multimodal transformer is pre-trained on millions
of image-text pairs and natural language text, thereby producing generic
cross-modal representations of observations and instructions. The policy
transformer keeps track of the full history of observations and actions, and
predicts actions autoregressively. We show that this unified transformer model
outperforms all state-of-the-art pre-trained or trained-from-scratch methods in
both single-task and multi-task settings. Our model also shows better model
scalability and generalization ability than prior work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keaton Hamm, Nick Henscheid, Shujie Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a
nonlinear dimensionality reduction technique that provides solutions to some
drawbacks in existing global nonlinear dimensionality reduction algorithms in
imaging applications. Wassmap represents images via probability measures in
Wasserstein space, then uses pairwise Wasserstein distances between the
associated measures to produce a low-dimensional, approximately isometric
embedding. We show that the algorithm is able to exactly recover parameters of
some image manifolds including those generated by translations or dilations of
a fixed generating measure. Additionally, we show that a discrete version of
the algorithm retrieves parameters from manifolds generated from discrete
measures by providing a theoretical bridge to transfer recovery results from
functional data to discrete data. Testing of the proposed algorithms on various
image data manifolds show that Wassmap yields good embeddings compared with
other global and local techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Importance of Image Interpretation: Patterns of Semantic
  Misclassification in Real-World Adversarial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Zhao, Nga Dang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial images are created with the intention of causing an image
classifier to produce a misclassification. In this paper, we propose that
adversarial images should be evaluated based on semantic mismatch, rather than
label mismatch, as used in current work. In other words, we propose that an
image of a "mug" would be considered adversarial if classified as "turnip", but
not as "cup", as current systems would assume. Our novel idea of taking
semantic misclassification into account in the evaluation of adversarial images
offers two benefits. First, it is a more realistic conceptualization of what
makes an image adversarial, which is important in order to fully understand the
implications of adversarial images for security and privacy. Second, it makes
it possible to evaluate the transferability of adversarial images to a
real-world classifier, without requiring the classifier's label set to have
been available during the creation of the images. The paper carries out an
evaluation of a transfer attack on a real-world image classifier that is made
possible by our semantic misclassification approach. The attack reveals
patterns in the semantics of adversarial misclassifications that could not be
investigated using conventional label mismatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Multimedia Modeling (MMM) 2023. Resources
  are publicly available at
  https://github.com/ZhengyuZhao/Targeted-Transfer/tree/main/human_eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computer Vision for Transit Travel Time Prediction: An End-to-End
  Framework Using Roadside Urban Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awad Abdelhalim, Jinhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate travel time estimation is paramount for providing transit users with
reliable schedules and dependable real-time information. This paper is the
first to utilize roadside urban imagery for direct transit travel time
prediction. We propose and evaluate an end-to-end framework integrating
traditional transit data sources with a roadside camera for automated roadside
image data acquisition, labeling, and model training to predict transit travel
times across a segment of interest. First, we show how the GTFS real-time data
can be utilized as an efficient activation mechanism for a roadside camera unit
monitoring a segment of interest. Second, AVL data is utilized to generate
ground truth labels for the acquired images based on the observed transit
travel time percentiles across the camera-monitored segment during the time of
image acquisition. Finally, the generated labeled image dataset is used to
train and thoroughly evaluate a Vision Transformer (ViT) model to predict a
discrete transit travel time range (band). The results illustrate that the ViT
model is able to learn image features and contents that best help it deduce the
expected travel time range with an average validation accuracy ranging between
80%-85%. We assess the interpretability of the ViT model's predictions and
showcase how this discrete travel time band prediction can subsequently improve
continuous transit travel time estimation. The workflow and results presented
in this study provide an end-to-end, scalable, automated, and highly efficient
approach for integrating traditional transit data sources and roadside imagery
to improve the estimation of transit travel duration. This work also
demonstrates the value of incorporating real-time information from
computer-vision sources, which are becoming increasingly accessible and can
have major implications for improving operations and passenger real-time
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final revised preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Generalization: The Impact of Annotation Style on Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brennan Nichyporuk, Jillian Cardinell, Justin Szeto, Raghav Mehta, Jean-Pierre R. Falet, Douglas L. Arnold, Sotirios A. Tsaftaris, Tal Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization is an important attribute of machine learning models,
particularly for those that are to be deployed in a medical context, where
unreliable predictions can have real world consequences. While the failure of
models to generalize across datasets is typically attributed to a mismatch in
the data distributions, performance gaps are often a consequence of biases in
the 'ground-truth' label annotations. This is particularly important in the
context of medical image segmentation of pathological structures (e.g.
lesions), where the annotation process is much more subjective, and affected by
a number underlying factors, including the annotation protocol, rater
education/experience, and clinical aims, among others. In this paper, we show
that modeling annotation biases, rather than ignoring them, poses a promising
way of accounting for differences in annotation style across datasets. To this
end, we propose a generalized conditioning framework to (1) learn and account
for different annotation styles across multiple datasets using a single model,
(2) identify similar annotation styles across different datasets in order to
permit their effective aggregation, and (3) fine-tune a fully trained model to
a new annotation style with just a few samples. Next, we present an
image-conditioning approach to model annotation styles that correlate with
specific image features, potentially enabling detection biases to be more
easily identified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:029.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-train</span>ing of 3D Point Cloud Networks with Image Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrej Janda, Brandon Wagstaff, Edwin G. Ng, Jonathan Kelly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the quantity of annotations required for supervised training is
vital when labels are scarce and costly. This reduction is especially important
for semantic segmentation tasks involving 3D datasets that are often
significantly smaller and more challenging to annotate than their image-based
counterparts. Self-supervised pre-training on large unlabelled datasets is one
way to reduce the amount of manual annotations needed. Previous work has
focused on pre-training with point cloud data exclusively; this approach often
requires two or more registered views. In the present work, we combine image
and point cloud modalities, by first learning self-supervised image features
and then using these features to train a 3D model. By incorporating image data,
which is often included in many 3D datasets, our pre-training method only
requires a single scan of a scene. We demonstrate that our pre-training
approach, despite using single scans, achieves comparable performance to other
multi-scan, point cloud-only methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Conference on Robot Learning (CoRL'22) Workshop on
  Pre-training Robot Learning, Auckland, New Zealand, December 14-18, 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Detection by Approximation of Ensemble Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Windeatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A spectral approximation of a Boolean function is proposed for approximating
the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving
two-class pattern recognition problems. The Walsh combination of relatively
weak DNN classifiers is shown experimentally to be capable of detecting
adversarial attacks. By observing the difference in Walsh coefficient
approximation between clean and adversarial images, it appears that
transferability of attack may be used for detection. Approximating the decision
boundary may also aid in understanding the learning and transferability
properties of DNNs. While the experiments here use images, the proposed
approach of modelling two-class ensemble decision boundaries could in principle
be applied to any application area. Code for this paper implementing Walsh
Coefficient Examples of approximating artificial Boolean functions can be found
at https://doi.org/10.24433/CO.3695905.v1
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Human Pose Regression using Graph Convolutional Network <span class="chip">ICIP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.10379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.10379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soubarna Banik, Alejandro Mendoza Gracia, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation is a difficult task, due to challenges such as
occluded body parts and ambiguous poses. Graph convolutional networks encode
the structural information of the human skeleton in the form of an adjacency
matrix, which is beneficial for better pose prediction. We propose one such
graph convolutional network named PoseGraphNet for 3D human pose regression
from 2D poses. Our network uses an adaptive adjacency matrix and kernels
specific to neighbor groups. We evaluate our model on the Human3.6M dataset
which is a standard dataset for 3D pose estimation. Our model's performance is
close to the state-of-the-art, but with much fewer parameters. The model learns
interesting adjacency relations between joints that have no physical
connections, but are behaviorally similar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in IEEE ICIP 2021, DOI will be updated once published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransZero++: Cross Attribute-Guided <span class="highlight-title">Transformer</span> for Zero-Shot Learning <span class="chip">AAAI'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiming Chen, Ziming Hong, Wenjin Hou, Guo-Sen Xie, Yibing Song, Jian Zhao, Xinge You, Shuicheng Yan, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot learning (ZSL) tackles the novel class recognition problem by
transferring semantic knowledge from seen classes to unseen ones. Existing
attention-based models have struggled to learn inferior region features in a
single image by solely using unidirectional attention, which ignore the
transferability and discriminative attribute localization of visual features.
In this paper, we propose a cross attribute-guided Transformer network, termed
TransZero++, to refine visual features and learn accurate attribute
localization for semantic-augmented visual embedding representations in ZSL.
TransZero++ consists of an attribute$\rightarrow$visual Transformer sub-net
(AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT).
Specifically, AVT first takes a feature augmentation encoder to alleviate the
cross-dataset problem, and improves the transferability of visual features by
reducing the entangled relative geometry relationships among region features.
Then, an attribute$\rightarrow$visual decoder is employed to localize the image
regions most relevant to each attribute in a given image for attribute-based
visual feature representations. Analogously, VAT uses the similar feature
augmentation encoder to refine the visual features, which are further applied
in visual$\rightarrow$attribute decoder to learn visual-based attribute
features. By further introducing semantical collaborative losses, the two
attribute-guided transformers teach each other to learn semantic-augmented
visual embeddings via semantical collaborative learning. Extensive experiments
show that TransZero++ achieves the new state-of-the-art results on three
challenging ZSL benchmarks. The codes are available at:
\url{https://github.com/shiming-chen/TransZero_pp}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an extention of AAAI'22 paper (TransZero). Accepted to TPAMI.
  arXiv admin note: substantial text overlap with arXiv:2112.01683</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLogist: Fast Observation Strategy on Whole-slide Images with Deep
  Reinforcement Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxuan Zhao, Jun Zhang, Deheng Ye, Jian Cao, Xiao Han, Qiang Fu, Wei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-slide images (WSI) in computational pathology have high resolution with
gigapixel size, but are generally with sparse regions of interest, which leads
to weak diagnostic relevance and data inefficiency for each area in the slide.
Most of the existing methods rely on a multiple instance learning framework
that requires densely sampling local patches at high magnification. The
limitation is evident in the application stage as the heavy computation for
extracting patch-level features is inevitable. In this paper, we develop
RLogist, a benchmarking deep reinforcement learning (DRL) method for fast
observation strategy on WSIs. Imitating the diagnostic logic of human
pathologists, our RL agent learns how to find regions of observation value and
obtain representative features across multiple resolution levels, without
having to analyze each part of the WSI at the high magnification. We benchmark
our method on two whole-slide level classification tasks, including detection
of metastases in WSIs of lymph node sections, and subtyping of lung cancer.
Experimental results demonstrate that RLogist achieves competitive
classification performance compared to typical multiple instance learning
algorithms, while having a significantly short observation path. In addition,
the observation path given by RLogist provides good decision-making
interpretability, and its ability of reading path navigation can potentially be
used by pathologists for educational/assistive purposes. Our code is available
at: \url{https://github.com/tencent-ailab/RLogist}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALSO: Automotive Lidar Self-supervision by Occupancy estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Boulch, Corentin Sautier, Björn Michele, Gilles Puy, Renaud Marlet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new self-supervised method for pre-training the backbone of deep
perception models operating on point clouds. The core idea is to train the
model on a pretext task which is the reconstruction of the surface on which the
3D points are sampled, and to use the underlying latent vectors as input to the
perception head. The intuition is that if the network is able to reconstruct
the scene surface, given only sparse input points, then it probably also
captures some fragments of semantic information, that can be used to boost an
actual perception task. This principle has a very simple formulation, which
makes it both easy to implement and widely applicable to a large range of 3D
sensors and deep networks performing semantic segmentation or object detection.
In fact, it supports a single-stream pipeline, as opposed to most contrastive
learning approaches, allowing training on limited resources. We conducted
extensive experiments on various autonomous driving datasets, involving very
different kinds of lidars, for both semantic segmentation and object detection.
The results show the effectiveness of our method to learn useful
representations without any annotation, compared to existing approaches. Code
is available at https://github.com/valeoai/ALSO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised visualization of image <span class="highlight-title">dataset</span>s using contrastive learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Niklas Böhm, Philipp Berens, Dmitry Kobak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visualization methods based on the nearest neighbor graph, such as t-SNE or
UMAP, are widely used for visualizing high-dimensional data. Yet, these
approaches only produce meaningful results if the nearest neighbors themselves
are meaningful. For images represented in pixel space this is not the case, as
distances in pixel space are often not capturing our sense of similarity and
therefore neighbors are not semantically close. This problem can be
circumvented by self-supervised approaches based on contrastive learning, such
as SimCLR, relying on data augmentation to generate implicit neighbors, but
these methods do not produce two-dimensional embeddings suitable for
visualization. Here, we present a new method, called t-SimCNE, for unsupervised
visualization of image data. T-SimCNE combines ideas from contrastive learning
and neighbor embeddings, and trains a parametric mapping from the
high-dimensional pixel space into two dimensions. We show that the resulting 2D
embeddings achieve classification accuracy comparable to the state-of-the-art
high-dimensional SimCLR representations, thus faithfully capturing semantic
relationships. Using t-SimCNE, we obtain informative visualizations of the
CIFAR-10 and CIFAR-100 datasets, showing rich cluster structure and
highlighting artifacts and outliers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Factorization via Transformed Tensor-Tensor Product for Image
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Xia, Duo Qiu, Xiongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of a batch of linearly correlated image
alignment, where the observed images are deformed by some unknown domain
transformations, and corrupted by additive Gaussian noise and sparse noise
simultaneously. By stacking these images as the frontal slices of a third-order
tensor, we propose to utilize the tensor factorization method via transformed
tensor-tensor product to explore the low-rankness of the underlying tensor,
which is factorized into the product of two smaller tensors via transformed
tensor-tensor product under any unitary transformation. The main advantage of
transformed tensor-tensor product is that its computational complexity is lower
compared with the existing literature based on transformed tensor nuclear norm.
Moreover, the tensor $\ell_p$ $(0<p<1)$ norm is employed to characterize the
sparsity of sparse noise and the tensor Frobenius norm is adopted to model
additive Gaussian noise. A generalized Gauss-Newton algorithm is designed to
solve the resulting model by linearizing the domain transformations and a
proximal Gauss-Seidel algorithm is developed to solve the corresponding
subproblem. Furthermore, the convergence of the proximal Gauss-Seidel algorithm
is established, whose convergence rate is also analyzed based on the
Kurdyka-$\L$ojasiewicz property. Extensive numerical experiments on real-world
image datasets are carried out to demonstrate the superior performance of the
proposed method as compared to several state-of-the-art methods in both
accuracy and computational time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Factorizer: A Scalable Interpretable Approach to Context Modeling for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12295v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12295v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pooya Ashtari, Diana M. Sima, Lieven De Lathauwer, Dominique Sappey-Marinier, Frederik Maes, Sabine Van Huffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) with U-shaped architectures have
dominated medical image segmentation, which is crucial for various clinical
purposes. However, the inherent locality of convolution makes CNNs fail to
fully exploit global context, essential for better recognition of some
structures, e.g., brain lesions. Transformers have recently proven promising
performance on vision tasks, including semantic segmentation, mainly due to
their capability of modeling long-range dependencies. Nevertheless, the
quadratic complexity of attention makes existing Transformer-based models use
self-attention layers only after somehow reducing the image resolution, which
limits the ability to capture global contexts present at higher resolutions.
Therefore, this work introduces a family of models, dubbed Factorizer, which
leverages the power of low-rank matrix factorization for constructing an
end-to-end segmentation model. Specifically, we propose a linearly scalable
approach to context modeling, formulating Nonnegative Matrix Factorization
(NMF) as a differentiable layer integrated into a U-shaped architecture. The
shifted window technique is also utilized in combination with NMF to
effectively aggregate local information. Factorizers compete favorably with
CNNs and Transformers in terms of accuracy, scalability, and interpretability,
achieving state-of-the-art results on the BraTS dataset for brain tumor
segmentation and ISLES'22 dataset for stroke lesion segmentation. Highly
meaningful NMF components give an additional interpretability advantage to
Factorizers over CNNs and Transformers. Moreover, our ablation studies reveal a
distinctive feature of Factorizers that enables a significant speed-up in
inference for a trained Factorizer without any extra steps and without
sacrificing much accuracy. The code and models are publicly available at
https://github.com/pashtari/factorizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGBD1K: A Large-scale <span class="highlight-title">Dataset</span> and Benchmark for RGB-D Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, Zucheng Wu, Haodong Liu, Xiao Yang, Xiao-Jun Wu, Josef Kittler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-D object tracking has attracted considerable attention recently,
achieving promising performance thanks to the symbiosis between visual and
depth channels. However, given a limited amount of annotated RGB-D tracking
data, most state-of-the-art RGB-D trackers are simple extensions of
high-performance RGB-only trackers, without fully exploiting the underlying
potential of the depth channel in the offline training stage. To address the
dataset deficiency issue, a new RGB-D dataset named RGBD1K is released in this
paper. The RGBD1K contains 1,050 sequences with about 2.5M frames in total. To
demonstrate the benefits of training on a larger RGB-D data set in general, and
RGBD1K in particular, we develop a transformer-based RGB-D tracker, named SPT,
as a baseline for future visual object tracking studies using the new dataset.
The results, of extensive experiments using the SPT tracker emonstrate the
potential of the RGBD1K dataset to improve the performance of RGB-D tracking,
inspiring future developments of effective tracker designs. The dataset and
codes will be available on the project homepage:
https://github.com/xuefeng-zhu5/RGBD1K.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medical Diffusion -- Denoising Diffusion Probabilistic Models for 3D
  Medical Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Firas Khader, Gustav Mueller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baessler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computer vision have shown promising results in image
generation. Diffusion probabilistic models in particular have generated
realistic images from textual input, as demonstrated by DALL-E 2, Imagen and
Stable Diffusion. However, their use in medicine, where image data typically
comprises three-dimensional volumes, has not been systematically evaluated.
Synthetic images may play a crucial role in privacy preserving artificial
intelligence and can also be used to augment small datasets. Here we show that
diffusion probabilistic models can synthesize high quality medical imaging
data, which we show for Magnetic Resonance Images (MRI) and Computed Tomography
(CT) images. We provide quantitative measurements of their performance through
a reader study with two medical experts who rated the quality of the
synthesized images in three categories: Realistic image appearance, anatomical
correctness and consistency between slices. Furthermore, we demonstrate that
synthetic images can be used in a self-supervised pre-training and improve the
performance of breast segmentation models when data is scarce (dice score 0.91
vs. 0.95 without vs. with synthetic data).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAN: a Segmentation-free Document Attention Network for Handwritten
  Document Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12273v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12273v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Coquenet, Clément Chatelain, Thierry Paquet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unconstrained handwritten text recognition is a challenging computer vision
task. It is traditionally handled by a two-step approach, combining line
segmentation followed by text line recognition. For the first time, we propose
an end-to-end segmentation-free architecture for the task of handwritten
document recognition: the Document Attention Network. In addition to text
recognition, the model is trained to label text parts using begin and end tags
in an XML-like fashion. This model is made up of an FCN encoder for feature
extraction and a stack of transformer decoder layers for a recurrent
token-by-token prediction process. It takes whole text documents as input and
sequentially outputs characters, as well as logical layout tokens. Contrary to
the existing segmentation-based approaches, the model is trained without using
any segmentation label. We achieve competitive results on the READ 2016 dataset
at page level, as well as double-page level with a CER of 3.43% and 3.70%,
respectively. We also provide results for the RIMES 2009 dataset at page level,
reaching 4.54% of CER.
  We provide all source code and pre-trained model weights at
https://github.com/FactoDeepLearning/DAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Join the High Accuracy Club on ImageNet with A Binary Neural Network
  Ticket 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianhui Guo, Joseph Bethge, Christoph Meinel, Haojin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary neural networks are the extreme case of network quantization, which
has long been thought of as a potential edge machine learning solution.
However, the significant accuracy gap to the full-precision counterparts
restricts their creative potential for mobile applications. In this work, we
revisit the potential of binary neural networks and focus on a compelling but
unanswered problem: how can a binary neural network achieve the crucial
accuracy level (e.g., 80%) on ILSVRC-2012 ImageNet? We achieve this goal by
enhancing the optimization process from three complementary perspectives: (1)
We design a novel binary architecture BNext based on a comprehensive study of
binary architectures and their optimization process. (2) We propose a novel
knowledge-distillation technique to alleviate the counter-intuitive overfitting
problem observed when attempting to train extremely accurate binary models. (3)
We analyze the data augmentation pipeline for binary networks and modernize it
with up-to-date techniques from full-precision models. The evaluation results
on ImageNet show that BNext, for the first time, pushes the binary model
accuracy boundary to 80.57% and significantly outperforms all the existing
binary networks. Code and trained models are available at:
https://github.com/hpi-xnor/BNext.git.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrase Identification with Deep Learning: A <span class="highlight-title">Review</span> of <span class="highlight-title">Dataset</span>s and
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhou, Cheng Qiu, Daniel E. Acuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI technology has made text generation tools like
GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can
pose serious threat to the credibility of various forms of media if these
technologies are used for plagiarism, including scientific literature and news
sources. Despite the development of automated methods for paraphrase
identification, detecting this type of plagiarism remains a challenge due to
the disparate nature of the datasets on which these methods are trained. In
this study, we review traditional and current approaches to paraphrase
identification and propose a refined typology of paraphrases. We also
investigate how this typology is represented in popular datasets and how
under-representation of certain types of paraphrases impacts detection
capabilities. Finally, we outline new directions for future research and
datasets in the pursuit of more effective paraphrase detection using AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 2 figures, 6 tables, 173 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairRoad: Achieving Fairness for Recommender Systems with Optimized
  Antidote Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Fang, Jia Liu, Michinari Momma, Yi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, recommender systems have played an increasingly important role in
shaping our experiences of digital environments and social interactions.
However, as recommender systems become ubiquitous in our society, recent years
have also witnessed significant fairness concerns for recommender systems.
Specifically, studies have shown that recommender systems may inherit or even
amplify biases from historical data, and as a result, provide unfair
recommendations. To address fairness risks in recommender systems, most of the
previous approaches to date are focused on modifying either the existing
training data samples or the deployed recommender algorithms, but unfortunately
with limited degrees of success. In this paper, we propose a new approach
called fair recommendation with optimized antidote data (FairRoad), which aims
to improve the fairness performances of recommender systems through the
construction of a small and carefully crafted antidote dataset. Toward this
end, we formulate our antidote data generation task as a mathematical
optimization problem, which minimizes the unfairness of the targeted
recommender systems while not disrupting the deployed recommendation
algorithms. Extensive experiments show that our proposed antidote data
generation algorithm significantly improve the fairness of recommender systems
with a small amounts of antidote data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SACMAT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Knowledge Gain for MOOC Video Consumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Otto, Markos Stamatakis, Anett Hoppe, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Informal learning on the Web using search engines as well as more structured
learning on MOOC platforms have become very popular in recent years. As a
result of the vast amount of available learning resources, intelligent
retrieval and recommendation methods are indispensable -- this is true also for
MOOC videos. However, the automatic assessment of this content with regard to
predicting (potential) knowledge gain has not been addressed by previous work
yet. In this paper, we investigate whether we can predict learning success
after MOOC video consumption using 1) multimodal features covering slide and
speech content, and 2) a wide range of text-based features describing the
content of the video. In a comprehensive experimental setting, we test four
different classifiers and various feature subset combinations. We conduct a
detailed feature importance analysis to gain insights in which modality
benefits knowledge gain prediction the most.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Fake News Detection with Heterogeneous Social Media Context
  Graphs <span class="chip">ECIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Donabauer, Udo Kruschwitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection has become a research area that goes way beyond a purely
academic interest as it has direct implications on our society as a whole.
Recent advances have primarily focused on textbased approaches. However, it has
become clear that to be effective one needs to incorporate additional,
contextual information such as spreading behaviour of news articles and user
interaction patterns on social media. We propose to construct heterogeneous
social context graphs around news articles and reformulate the problem as a
graph classification task. Exploring the incorporation of different types of
information (to get an idea as to what level of social context is most
effective) and using different graph neural network architectures indicates
that this approach is highly effective with robust results on a common
benchmark dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted at the 45th European Conference on Information
  Retrieval (ECIR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation for Dense Retrieval through Self-Supervision by
  Pseudo-Relevance Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Eric Gaussier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although neural information retrieval has witnessed great improvements,
recent works showed that the generalization ability of dense retrieval models
on target domains with different distributions is limited, which contrasts with
the results obtained with interaction-based models. To address this issue,
researchers have resorted to adversarial learning and query generation
approaches; both approaches nevertheless resulted in limited improvements. In
this paper, we propose to use a self-supervision approach in which
pseudo-relevance labels are automatically generated on the target domain. To do
so, we first use the standard BM25 model on the target domain to obtain a first
ranking of documents, and then use the interaction-based model T53B to re-rank
top documents. We further combine this approach with knowledge distillation
relying on an interaction-based teacher model trained on the source domain. Our
experiments reveal that pseudo-relevance labeling using T53B and the MiniLM
teacher performs on average better than other approaches and helps improve the
state-of-the-art query generation approach GPL when it is fine-tuned on the
pseudo-relevance labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Stance Detection as Textual Entailment Recognition and
  Leveraging Measurement Knowledge from Social Sciences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection (SD) can be considered a special case of textual entailment
recognition (TER), a generic natural language task. Modelling SD as TER may
offer benefits like more training data and a more general learning scheme. In
this paper, we present an initial empirical analysis of this approach. We apply
it to a difficult but relevant test case where no existing labelled SD dataset
is available, because this is where modelling SD as TER may be especially
helpful. We also leverage measurement knowledge from social sciences to improve
model performance. We discuss our findings and suggest future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic ESG Assessment of Companies by Mining and Evaluating Media
  Coverage Data: NLP Approach and Tool 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Fischbach, Max Adam, Victor Dzhagatspanyan, Daniel Mendez, Julian Frattini, Oleksandr Kosenkov, Parisa Elahidoost
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Sustainable corporate behavior is increasingly valued by society and
impacts corporate reputation and customer trust. Hence, companies regularly
publish sustainability reports to shed light on their impact on environmental,
social, and governance (ESG) factors. Problem: Sustainability reports are
written by companies themselves and are therefore considered a
company-controlled source. Contrary, studies reveal that non-corporate channels
(e.g., media coverage) represent the main driver for ESG transparency. However,
analysing media coverage regarding ESG factors is challenging since (1) the
amount of published news articles grows daily, (2) media coverage data does not
necessarily deal with an ESG-relevant topic, meaning that it must be carefully
filtered, and (3) the majority of media coverage data is unstructured. Research
Goal: We aim to extract ESG-relevant information from textual media reactions
automatically to calculate an ESG score for a given company. Our goal is to
reduce the cost of ESG data collection and make ESG information available to
the general public. Contribution: Our contributions are three-fold: First, we
publish a corpus of 432,411 news headlines annotated as being environmental-,
governance-, social-related, or ESG-irrelevant. Second, we present our
tool-supported approach called ESG-Miner capable of analyzing and evaluating
headlines on corporate ESG-performance automatically. Third, we demonstrate the
feasibility of our approach in an experiment and apply the ESG-Miner on 3000
manually labeled headlines. Our approach processes 96.7 % of the headlines
correctly and shows a great performance in detecting environmental-related
headlines along with their correct sentiment. We encourage fellow researchers
and practitioners to use the ESG-Miner at https://www.esg-miner.com.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEREO: Scientific Text Reuse in Open Access Publications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Gienapp, Wolfgang Kircheis, Bjarne Sievers, Benno Stein, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Webis-STEREO-21 dataset, a massive collection of Scientific
Text Reuse in Open-access publications. It contains more than 91 million cases
of reused text passages found in 4.2 million unique open-access publications.
Featuring a high coverage of scientific disciplines and varieties of reuse, as
well as comprehensive metadata to contextualize each case, our dataset
addresses the most salient shortcomings of previous ones on scientific writing.
Webis-STEREO-21 allows for tackling a wide range of research questions from
different scientific backgrounds, facilitating both qualitative and
quantitative analysis of the phenomenon as well as a first-time grounding on
the base rate of text reuse in scientific publications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pivotal Role of Language Modeling in Recommender Systems: Enriching
  Task-specific and Task-agnostic Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyung-Min Kim, Jung-Woo Ha, Sang-Woo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have proposed unified user modeling frameworks that leverage
user behavior data from various applications. Many of them benefit from
utilizing users' behavior sequences as plain texts, representing rich
information in any domain or system without losing generality. Hence, a
question arises: Can language modeling for user history corpus help improve
recommender systems? While its versatile usability has been widely investigated
in many domains, its applications to recommender systems still remain
underexplored. We show that language modeling applied directly to task-specific
user histories achieves excellent results on diverse recommendation tasks.
Also, leveraging additional task-agnostic user histories delivers significant
performance benefits. We further demonstrate that our approach can provide
promising transfer learning capabilities for a broad spectrum of real-world
recommender systems, even on unseen domains and services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over Different Types of Knowledge Graphs: Static, Temporal and
  Multi-Modal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing
facts based on mined logic rules underlying knowledge graphs (KGs), has become
a fast-growing research direction. It has been proven to significantly benefit
the usage of KGs in many AI applications, such as question answering and
recommendation systems, etc. According to the graph types, the existing KGR
models can be roughly divided into three categories, i.e., static models,
temporal models, and multi-modal models. The early works in this domain mainly
focus on static KGR and tend to directly apply general knowledge graph
embedding models to the reasoning task. However, these models are not suitable
for more complex but practical tasks, such as inductive static KGR, temporal
KGR, and multi-modal KGR. To this end, multiple works have been developed
recently, but no survey papers and open-source repositories comprehensively
summarize and discuss models in this important direction. To fill the gap, we
conduct a survey for knowledge graph reasoning tracing from static to temporal
and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR
models, and typical datasets are introduced and discussed consequently.
Moreover, we discuss the challenges and potential opportunities. The
corresponding open-source repository is shared on GitHub:
https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multivariate Powered Dirichlet Hawkes Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Poux-Médard, Julien Velcin, Sabine Loudcher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The publication time of a document carries a relevant information about its
semantic content. The Dirichlet-Hawkes process has been proposed to jointly
model textual information and publication dynamics. This approach has been used
with success in several recent works, and extended to tackle specific
challenging problems --typically for short texts or entangled publication
dynamics. However, the prior in its current form does not allow for complex
publication dynamics. In particular, inferred topics are independent from each
other --a publication about finance is assumed to have no influence on
publications about politics, for instance.
  In this work, we develop the Multivariate Powered Dirichlet-Hawkes Process
(MPDHP), that alleviates this assumption. Publications about various topics can
now influence each other. We detail and overcome the technical challenges that
arise from considering interacting topics. We conduct a systematic evaluation
of MPDHP on a range of synthetic datasets to define its application domain and
limitations. Finally, we develop a use case of the MPDHP on Reddit data. At the
end of this article, the interested reader will know how and when to use MPDHP,
and when not to.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential Recommendation with Causal Behavior Discovery <span class="chip">ICDE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.00216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.00216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenlei Wang, Xu Chen, Rui Zhou, Quanyu Dai, Zhenhua Dong, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key of sequential recommendation lies in the accurate item correlation
modeling. Previous models infer such information based on item co-occurrences,
which may fail to capture the real causal relations, and impact the
recommendation performance and explainability. In this paper, we equip
sequential recommendation with a novel causal discovery module to capture
causalities among user behaviors. Our general idea is firstly assuming a causal
graph underlying item correlations, and then we learn the causal graph jointly
with the sequential recommender model by fitting the real user behavior data.
More specifically, in order to satisfy the causality requirement, the causal
graph is regularized by a differentiable directed acyclic constraint.
Considering that the number of items in recommender systems can be very large,
we represent different items with a unified set of latent clusters, and the
causal graph is defined on the cluster level, which enhances the model
scalability and robustness. In addition, we provide theoretical analysis on the
identifiability of the learned causal graph. To the best of our knowledge, this
paper makes a first step towards combining sequential recommendation with
causal discovery. For evaluating the recommendation performance, we implement
our framework with different neural sequential architectures, and compare them
with many state-of-the-art methods based on real-world datasets. Empirical
studies manifest that our model can on average improve the performance by about
7% and 11% on f1 and NDCG, respectively. To evaluate the model explainability,
we build a new dataset with human labeled explanations for both quantitative
and qualitative analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Bidding Algorithms for Return-on-Spend Constrained Advertisers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Feng, Swati Padmanabhan, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online advertising has recently grown into a highly competitive and complex
multi-billion-dollar industry, with advertisers bidding for ad slots at large
scales and high frequencies. This has resulted in a growing need for efficient
"auto-bidding" algorithms that determine the bids for incoming queries to
maximize advertisers' targets subject to their specified constraints. This work
explores efficient online algorithms for a single value-maximizing advertiser
under an increasingly popular constraint: Return-on-Spend (RoS). We quantify
efficiency in terms of regret relative to the optimal algorithm, which knows
all queries a priori.
  We contribute a simple online algorithm that achieves near-optimal regret in
expectation while always respecting the specified RoS constraint when the input
sequence of queries are i.i.d. samples from some distribution. We also
integrate our results with the previous work of Balseiro, Lu, and Mirrokni
[BLM20] to achieve near-optimal regret while respecting both RoS and fixed
budget constraints.
  Our algorithm follows the primal-dual framework and uses online mirror
descent (OMD) for the dual updates. However, we need to use a non-canonical
setup of OMD, and therefore the classic low-regret guarantee of OMD, which is
for the adversarial setting in online learning, no longer holds. Nonetheless,
in our case and more generally where low-regret dynamics are applied in
algorithm design, the gradients encountered by OMD can be far from adversarial
but influenced by our algorithmic choices. We exploit this key insight to show
our OMD setup achieves low regret in the realm of our algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating extreme quantum scattering in graphene with machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-Di Han, Ying-Cheng Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphene quantum dots provide a platform for manipulating electron behaviors
in two-dimensional (2D) Dirac materials. Most previous works were of the
"forward" type in that the objective was to solve various confinement,
transport and scattering problems with given structures that can be generated
by, e.g., applying an external electrical field. There are applications such as
cloaking or superscattering where the challenging problem of inverse design
needs to be solved: finding a quantum-dot structure according to certain
desired functional characteristics. A brute-force search of the system
configuration based directly on the solutions of the Dirac equation is
computational infeasible. We articulate a machine-learning approach to
addressing the inverse-design problem where artificial neural networks subject
to physical constraints are exploited to replace the rigorous Dirac equation
solver. In particular, we focus on the problem of designing a quantum dot
structure to generate both cloaking and superscattering in terms of the
scattering efficiency as a function of the energy. We construct a physical loss
function that enables accurate prediction of the scattering characteristics. We
demonstrate that, in the regime of Klein tunneling, the scattering efficiency
can be designed to vary over two orders of magnitudes, allowing any scattering
curve to be generated from a proper combination of the gate potentials. Our
physics-based machine-learning approach can be a powerful design tool for 2D
Dirac material-based electronics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Relationship Between Explanation and Prediction: A Causal View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir-Hossein Karimi, Krikamol Muandet, Simon Kornblith, Bernhard Schölkopf, Been Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability has become a central requirement for the development,
deployment, and adoption of machine learning (ML) models and we are yet to
understand what explanation methods can and cannot do. Several factors such as
data, model prediction, hyperparameters used in training the model, and random
initialization can all influence downstream explanations. While previous work
empirically hinted that explanations (E) may have little relationship with the
prediction (Y), there is a lack of conclusive study to quantify this
relationship. Our work borrows tools from causal inference to systematically
assay this relationship. More specifically, we measure the relationship between
E and Y by measuring the treatment effect when intervening on their causal
ancestors (hyperparameters) (inputs to generate saliency-based Es or Ys). We
discover that Y's relative direct influence on E follows an odd pattern; the
influence is higher in the lowest-performing models than in mid-performing
models, and it then decreases in the top-performing models. We believe our work
is a promising first step towards providing better guidance for practitioners
who can make more informed decisions in utilizing these explanations by knowing
what factors are at play and how they relate to their end task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Losses over Labels: Weakly Supervised Learning via Direct Loss
  Construction <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the prohibitive costs of generating large amounts of labeled data,
programmatic weak supervision is a growing paradigm within machine learning. In
this setting, users design heuristics that provide noisy labels for subsets of
the data. These weak labels are combined (typically via a graphical model) to
form pseudolabels, which are then used to train a downstream model. In this
work, we question a foundational premise of the typical weakly supervised
learning pipeline: given that the heuristic provides all ``label" information,
why do we need to generate pseudolabels at all? Instead, we propose to directly
transform the heuristics themselves into corresponding loss functions that
penalize differences between our model and the heuristic. By constructing
losses directly from the heuristics, we can incorporate more information than
is used in the standard weakly supervised pipeline, such as how the heuristics
make their decisions, which explicitly informs feature selection during
training. We call our method Losses over Labels (LoL) as it creates losses
directly from heuristics without going through the intermediate step of a
label. We show that LoL improves upon existing weak supervision methods on
several benchmark text and image classification tasks and further demonstrate
that incorporating gradient information leads to better performance on almost
every task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures, To be published in AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling the Wireless Metaverse via Semantic Multiverse Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihong Park, Jinho Choi, Seong-Lyun Kim, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse over wireless networks is an emerging use case of the sixth
generation (6G) wireless systems, posing unprecedented challenges in terms of
its multi-modal data transmissions with stringent latency and reliability
requirements. Towards enabling this wireless metaverse, in this article we
propose a novel semantic communication (SC) framework by decomposing the
metaverse into human/machine agent-specific semantic multiverses (SMs). An SM
stored at each agent comprises a semantic encoder and a generator, leveraging
recent advances in generative artificial intelligence (AI). To improve
communication efficiency, the encoder learns the semantic representations (SRs)
of multi-modal data, while the generator learns how to manipulate them for
locally rendering scenes and interactions in the metaverse. Since these learned
SMs are biased towards local environments, their success hinges on
synchronizing heterogeneous SMs in the background while communicating SRs in
the foreground, turning the wireless metaverse problem into the problem of
semantic multiverse communication (SMC). Based on this SMC architecture, we
propose several promising algorithmic and analytic tools for modeling and
designing SMC, ranging from distributed learning and multi-agent reinforcement
learning (MARL) to signaling games and symbolic AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Graph Position Encodings for <span class="highlight-title">Transformer</span>s with Weighted
  Graph-Walking Automata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Soga, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A current goal in the graph neural network literature is to enable
transformers to operate on graph-structured data, given their success on
language and vision tasks. Since the transformer's original sinusoidal
positional encodings (PEs) are not applicable to graphs, recent work has
focused on developing graph PEs, rooted in spectral graph theory or various
spatial features of a graph. In this work, we introduce a new graph PE, Graph
Automaton PE (GAPE), based on weighted graph-walking automata (a novel
extension of graph-walking automata). We compare the performance of GAPE with
other PE schemes on both machine translation and graph-structured tasks, and we
show that it generalizes several other PEs. An additional contribution of this
study is a theoretical and controlled experimental comparison of many recent
PEs in graph transformers, independent of the use of edge features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Season Crop Progress in Un<span class="highlight-title">survey</span>ed Regions using Networks Trained on
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Worrall, Jasmeet Judge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many commodity crops have growth stages during which they are particularly
vulnerable to stress-induced yield loss. In-season crop progress information is
useful for quantifying crop risk, and satellite remote sensing (RS) can be used
to track progress at regional scales. At present, all existing RS-based crop
progress estimation (CPE) methods which target crop-specific stages rely on
ground truth data for training/calibration. This reliance on ground survey data
confines CPE methods to surveyed regions, limiting their utility. In this
study, a new method is developed for conducting RS-based in-season CPE in
unsurveyed regions by combining data from surveyed regions with synthetic crop
progress data generated for an unsurveyed region. Corn-growing zones in
Argentina were used as surrogate 'unsurveyed' regions. Existing weather
generation, crop growth, and optical radiative transfer models were linked to
produce synthetic weather, crop progress, and canopy reflectance data. A neural
network (NN) method based upon bi-directional Long Short-Term Memory was
trained separately on surveyed data, synthetic data, and two different
combinations of surveyed and synthetic data. A stopping criterion was developed
which uses the weighted divergence of surveyed and synthetic data validation
loss. Net F1 scores across all crop progress stages increased by 8.7% when
trained on a combination of surveyed region and synthetic data, and overall
performance was only 21% lower than when the NN was trained on surveyed data
and applied in the US Midwest. Performance gain from synthetic data was
greatest in zones with dual planting windows, while the inclusion of surveyed
region data from the US Midwest helped mitigate NN sensitivity to noise in NDVI
data. Overall results suggest in-season CPE in other unsurveyed regions may be
possible with increased quantity and variety of synthetic crop progress data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Learning with Pricing for Optimal and Stable Allocations in
  Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yigit Efe Erginbas, Soham Phade, Kannan Ramchandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale online recommendation systems must facilitate the allocation of a
limited number of items among competing users while learning their preferences
from user feedback. As a principled way of incorporating market constraints and
user incentives in the design, we consider our objectives to be two-fold:
maximal social welfare with minimal instability. To maximize social welfare,
our proposed framework enhances the quality of recommendations by exploring
allocations that optimistically maximize the rewards. To minimize instability,
a measure of users' incentives to deviate from recommended allocations, the
algorithm prices the items based on a scheme derived from the Walrasian
equilibria. Though it is known that these equilibria yield stable prices for
markets with known user preferences, our approach accounts for the inherent
uncertainty in the preferences and further ensures that the users accept their
recommendations under offered prices. To the best of our knowledge, our
approach is the first to integrate techniques from combinatorial bandits,
optimal resource allocation, and collaborative filtering to obtain an algorithm
that achieves sub-linear social welfare regret as well as sub-linear
instability. Empirical studies on synthetic and real-world data also
demonstrate the efficacy of our strategy compared to approaches that do not
fully incorporate all these aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2207.04143</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Safety and Robustness Guarantees for Feedback Motion
  Planning of Unknown Underactuated Stochastic Systems <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Craig Knuth, Glen Chou, Jamie Reese, Joe Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for providing statistical guarantees on runtime safety
and goal reachability for integrated planning and control of a class of systems
with unknown nonlinear stochastic underactuated dynamics. Specifically, given a
dynamics dataset, our method jointly learns a mean dynamics model, a
spatially-varying disturbance bound that captures the effect of noise and model
mismatch, and a feedback controller based on contraction theory that stabilizes
the learned dynamics. We propose a sampling-based planner that uses the mean
dynamics model and simultaneously bounds the closed-loop tracking error via a
learned disturbance bound. We employ techniques from Extreme Value Theory (EVT)
to estimate, to a specified level of confidence, several constants which
characterize the learned components and govern the size of the tracking error
bound. This ensures plans are guaranteed to be safely tracked at runtime. We
validate that our guarantees translate to empirical safety in simulation on a
10D quadrotor, and in the real world on a physical CrazyFlie quadrotor and
Clearpath Jackal robot, whereas baselines that ignore the model error and
stochasticity are unsafe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Image Style Transfer from Freeform Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Santanam, Mengyang Liu, Jiangyue Yu, Zhaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper creates a novel method of deep neural style transfer by generating
style images from freeform user text input. The language model and style
transfer model form a seamless pipeline that can create output images with
similar losses and improved quality when compared to baseline style transfer
methods. The language model returns a closely matching image given a style text
and description input, which is then passed to the style transfer model with an
input content image to create a final output. A proof-of-concept tool is also
developed to integrate the models and demonstrate the effectiveness of deep
image style transfer from freeform text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LidarCLIP or: How I Learned to Talk to Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, Kalle Åström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research connecting text and images has recently seen several breakthroughs,
with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection
between text and other visual modalities, such as lidar data, has received less
attention, prohibited by the lack of text-lidar datasets. In this work, we
propose LidarCLIP, a mapping from automotive point clouds to a pre-existing
CLIP embedding space. Using image-lidar pairs, we supervise a point cloud
encoder with the image CLIP embeddings, effectively relating text and lidar
data with the image domain as an intermediary. We show the effectiveness of
LidarCLIP by demonstrating that lidar-based retrieval is generally on par with
image-based retrieval, but with complementary strengths and weaknesses. By
combining image and lidar features, we improve upon both single-modality
methods and enable a targeted search for challenging detection scenarios under
adverse sensor conditions. We also use LidarCLIP as a tool to investigate
fundamental lidar capabilities through natural language. Finally, we leverage
our compatibility with CLIP to explore a range of applications, such as point
cloud captioning and lidar-to-image generation, without any additional
training. We hope LidarCLIP can inspire future work to dive deeper into
connections between text and point cloud understanding. Code and trained models
available at https://github.com/atonderski/lidarclip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks and Defences for Skin Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinay Jogani, Joy Purohit, Ishaan Shivhare, Samina Attari, Shraddha Surtkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a concurrent significant improvement in the medical images
used to facilitate diagnosis and the performance of machine learning techniques
to perform tasks such as classification, detection, and segmentation in recent
years. As a result, a rapid increase in the usage of such systems can be
observed in the healthcare industry, for instance in the form of medical image
classification systems, where these models have achieved diagnostic parity with
human physicians. One such application where this can be observed is in
computer vision tasks such as the classification of skin lesions in
dermatoscopic images. However, as stakeholders in the healthcare industry, such
as insurance companies, continue to invest extensively in machine learning
infrastructure, it becomes increasingly important to understand the
vulnerabilities in such systems. Due to the highly critical nature of the tasks
being carried out by these machine learning models, it is necessary to analyze
techniques that could be used to take advantage of these vulnerabilities and
methods to defend against them. This paper explores common adversarial attack
techniques. The Fast Sign Gradient Method and Projected Descent Gradient are
used against a Convolutional Neural Network trained to classify dermatoscopic
images of skin lesions. Following that, it also discusses one of the most
popular adversarial defense techniques, adversarial training. The performance
of the model that has been trained on adversarial examples is then tested
against the previously mentioned attacks, and recommendations to improve neural
networks robustness are thus provided based on the results of the experiment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, 2 tables, 2nd International Conference for
  Advancement in Technology (ICONAT 2023), Goa, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-1: Robotics <span class="highlight-title">Transformer</span> for Real-World Control at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See website at robotics-transformer.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient and Domain-Agnostic Evasion Attack with
  High-dimensional Categorical Inputs <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyan Bao, Yufei Han, Yujun Zhou, Xin Gao, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our work targets at searching feasible adversarial perturbation to attack a
classifier with high-dimensional categorical inputs in a domain-agnostic
setting. This is intrinsically an NP-hard knapsack problem where the
exploration space becomes explosively larger as the feature dimension
increases. Without the help of domain knowledge, solving this problem via
heuristic method, such as Branch-and-Bound, suffers from exponential
complexity, yet can bring arbitrarily bad attack results. We address the
challenge via the lens of multi-armed bandit based combinatorial search. Our
proposed method, namely FEAT, treats modifying each categorical feature as
pulling an arm in multi-armed bandit programming. Our objective is to achieve
highly efficient and effective attack using an Orthogonal Matching Pursuit
(OMP)-enhanced Upper Confidence Bound (UCB) exploration strategy. Our
theoretical analysis bounding the regret gap of FEAT guarantees its practical
attack performance. In empirical analysis, we compare FEAT with other
state-of-the-art domain-agnostic attack methods over various real-world
categorical data sets of different applications. Substantial experimental
observations confirm the expected efficiency and attack effectiveness of FEAT
applied in different application scenarios. Our work further hints the
applicability of FEAT for assessing the adversarial vulnerability of
classification systems with high-dimensional categorical inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Infinitesimal Jackknife: Mitigating the Influence of Biased
  Training Data Points Without Refitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prasanna Sattigeri, Soumya Ghosh, Inkit Padhi, Pierre Dognin, Kush R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In consequential decision-making applications, mitigating unwanted biases in
machine learning models that yield systematic disadvantage to members of groups
delineated by sensitive attributes such as race and gender is one key
intervention to strive for equity. Focusing on demographic parity and equality
of opportunity, in this paper we propose an algorithm that improves the
fairness of a pre-trained classifier by simply dropping carefully selected
training data points. We select instances based on their influence on the
fairness metric of interest, computed using an infinitesimal jackknife-based
approach. The dropping of training points is done in principle, but in practice
does not require the model to be refit. Crucially, we find that such an
intervention does not substantially reduce the predictive performance of the
model but drastically improves the fairness metric. Through careful
experiments, we evaluate the effectiveness of the proposed approach on diverse
tasks and find that it consistently improves upon existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Neurips 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPV: Automated photovoltaic forecasts with limited information using
  an ensemble of <span class="highlight-title">pre-train</span>ed models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Meisenbacher, Benedikt Heidrich, Tim Martin, Ralf Mikut, Veit Hagenmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate PhotoVoltaic (PV) power generation forecasting is vital for the
efficient operation of Smart Grids. The automated design of such accurate
forecasting models for individual PV plants includes two challenges: First,
information about the PV mounting configuration (i.e. inclination and azimuth
angles) is often missing. Second, for new PV plants, the amount of historical
data available to train a forecasting model is limited (cold-start problem). We
address these two challenges by proposing a new method for day-ahead PV power
generation forecasts called AutoPV. AutoPV is a weighted ensemble of
forecasting models that represent different PV mounting configurations. This
representation is achieved by pre-training each forecasting model on a separate
PV plant and by scaling the model's output with the peak power rating of the
corresponding PV plant. To tackle the cold-start problem, we initially weight
each forecasting model in the ensemble equally. To tackle the problem of
missing information about the PV mounting configuration, we use new data that
become available during operation to adapt the ensemble weights to minimize the
forecasting error. AutoPV is advantageous as the unknown PV mounting
configuration is implicitly reflected in the ensemble weights, and only the PV
plant's peak power rating is required to re-scale the ensemble's output. AutoPV
also allows to represent PV plants with panels distributed on different roofs
with varying alignments, as these mounting configurations can be reflected
proportionally in the weighting. Additionally, the required computing memory is
decoupled when scaling AutoPV to hundreds of PV plants, which is beneficial in
Smart Grids with limited computing capabilities. For a real-world data set with
11 PV plants, the accuracy of AutoPV is comparable to a model trained on two
years of data and outperforms an incrementally trained model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPViT: A High Resolution Non-Hierarchical Vision <span class="highlight-title">Transformer</span> with Group
  Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Jiarui Xu, Shalini De Mello, Elliot J. Crowley, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Group Propagation Vision Transformer (GPViT): a novel
nonhierarchical (i.e. non-pyramidal) transformer model designed for general
visual recognition with high-resolution features. High-resolution features (or
tokens) are a natural fit for tasks that involve perceiving fine-grained
details such as detection and segmentation, but exchanging global information
between these features is expensive in memory and computation because of the
way self-attention scales. We provide a highly efficient alternative Group
Propagation Block (GP Block) to exchange global information. In each GP Block,
features are first grouped together by a fixed number of learnable group
tokens; we then perform Group Propagation where global information is exchanged
between the grouped features; finally, global information in the updated
grouped features is returned back to the image features through a transformer
decoder. We evaluate GPViT on a variety of visual recognition tasks including
image classification, semantic segmentation, object detection, and instance
segmentation. Our method achieves significant performance gains over previous
works across all tasks, especially on tasks that require high-resolution
outputs, for example, our GPViT-L3 outperforms Swin Transformer-B by 2.0 mIoU
on ADE20K semantic segmentation with only half as many parameters. Code and
pre-trained models are available at https://github.com/ChenhongyiYang/GPViT .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/ChenhongyiYang/GPViT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Robotic Navigation from Experience: Principles, Methods, and
  Recent Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Levine, Dhruv Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation is one of the most heavily studied problems in robotics, and is
conventionally approached as a geometric mapping and planning problem. However,
real-world navigation presents a complex set of physical challenges that defies
simple geometric abstractions. Machine learning offers a promising way to go
beyond geometry and conventional planning, allowing for navigational systems
that make decisions based on actual prior experience. Such systems can reason
about traversability in ways that go beyond geometry, accounting for the
physical outcomes of their actions and exploiting patterns in real-world
environments. They can also improve as more data is collected, potentially
providing a powerful network effect. In this article, we present a general
toolkit for experiential learning of robotic navigation skills that unifies
several recent approaches, describe the underlying design principles, summarize
experimental results from several of our recent papers, and discuss open
problems and directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final print version is here:
  https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0447</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient flow in the gaussian covariate model: exact solution of
  learning curves and multiple descent structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antione Bodin, Nicolas Macris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent line of work has shown remarkable behaviors of the generalization
error curves in simple learning models. Even the least-squares regression has
shown atypical features such as the model-wise double descent, and further
works have observed triple or multiple descents. Another important
characteristic are the epoch-wise descent structures which emerge during
training. The observations of model-wise and epoch-wise descents have been
analytically derived in limited theoretical settings (such as the random
feature model) and are otherwise experimental. In this work, we provide a full
and unified analysis of the whole time-evolution of the generalization curve,
in the asymptotic large-dimensional regime and under gradient-flow, within a
wider theoretical setting stemming from a gaussian covariate model. In
particular, we cover most cases already disparately observed in the literature,
and also provide examples of the existence of multiple descent structures as a
function of a model parameter or time. Furthermore, we show that our
theoretical predictions adequately match the learning curves obtained by
gradient descent over realistic datasets. Technically we compute averages of
rational expressions involving random matrices using recent developments in
random matrix theory based on "linear pencils". Another contribution, which is
also of independent interest in random matrix theory, is a new derivation of
related fixed point equations (and an extension there-off) using Dyson brownian
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-objective Tree-structured Parzen Estimator Meets Meta-learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhei Watanabe, Noow Awad, Masaki Onishi, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperparameter optimization (HPO) is essential for the better performance of
deep learning, and practitioners often need to consider the trade-off between
multiple metrics, such as error rate, latency, memory requirements, robustness,
and algorithmic fairness. Due to this demand and the heavy computation of deep
learning, the acceleration of multi-objective (MO) optimization becomes ever
more important. Although meta-learning has been extensively studied to speedup
HPO, existing methods are not applicable to the MO tree-structured parzen
estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we
extend TPE's acquisition function to the meta-learning setting, using a task
similarity defined by the overlap in promising domains of each task. In a
comprehensive set of experiments, we demonstrate that our method accelerates
MO-TPE on tabular HPO benchmarks and yields state-of-the-art performance. Our
method was also validated externally by winning the AutoML 2022 competition on
"Multiobjective Hyperparameter Optimization for Transformers".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Meta-learning workshop on NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairRoad: Achieving Fairness for Recommender Systems with Optimized
  Antidote Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Fang, Jia Liu, Michinari Momma, Yi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, recommender systems have played an increasingly important role in
shaping our experiences of digital environments and social interactions.
However, as recommender systems become ubiquitous in our society, recent years
have also witnessed significant fairness concerns for recommender systems.
Specifically, studies have shown that recommender systems may inherit or even
amplify biases from historical data, and as a result, provide unfair
recommendations. To address fairness risks in recommender systems, most of the
previous approaches to date are focused on modifying either the existing
training data samples or the deployed recommender algorithms, but unfortunately
with limited degrees of success. In this paper, we propose a new approach
called fair recommendation with optimized antidote data (FairRoad), which aims
to improve the fairness performances of recommender systems through the
construction of a small and carefully crafted antidote dataset. Toward this
end, we formulate our antidote data generation task as a mathematical
optimization problem, which minimizes the unfairness of the targeted
recommender systems while not disrupting the deployed recommendation
algorithms. Extensive experiments show that our proposed antidote data
generation algorithm significantly improve the fairness of recommender systems
with a small amounts of antidote data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SACMAT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERNIE-Code: Beyond English-Centric Cross-lingual <span class="highlight-title">Pretrain</span>ing for
  Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers working with the same programming language (PL) may speak
different natural languages (NLs) and vice versa, erecting huge barriers to
communication and working efficiency. Recent studies have demonstrated the
effectiveness of generative pre-training in computer programs, yet they are
always English-centric. In this work, we step towards bridging the gap between
multilingual NLs and multilingual PLs for large language models (LLMs). We
release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.
We employ two methods for universal cross-lingual pre-training: span-corruption
language modeling that learns patterns from monolingual NL or PL; and
pivot-based translation language modeling that relies on parallel data of many
NLs and PLs. Extensive results show that ERNIE-Code outperforms previous
multilingual LLMs for PL or NL across a wide range of end tasks of code
intelligence, including multilingual code-to-text, text-to-code, code-to-code,
and text-to-text generation. We further show its advantage of zero-shot
prompting on multilingual code summarization and text-to-text translation. We
will make our code and pre-trained models publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image
  and Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Falanti, Eugenio Lomurno, Danilo Ardagna, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated machine learning (AutoML) field has become increasingly
relevant in recent years. These algorithms can develop models without the need
for expert knowledge, facilitating the application of machine learning
techniques in the industry. Neural Architecture Search (NAS) exploits deep
learning techniques to autonomously produce neural network architectures whose
results rival the state-of-the-art models hand-crafted by AI experts. However,
this approach requires significant computational resources and hardware
investments, making it less appealing for real-usage applications. This article
presents the third version of Pareto-Optimal Progressive Neural Architecture
Search (POPNASv3), a new sequential model-based optimization NAS algorithm
targeting different hardware environments and multiple classification tasks.
Our method is able to find competitive architectures within large search
spaces, while keeping a flexible structure and data processing pipeline to
adapt to different tasks. The algorithm employs Pareto optimality to reduce the
number of architectures sampled during the search, drastically improving the
time efficiency without loss in accuracy. The experiments performed on images
and time series classification datasets provide evidence that POPNASv3 can
explore a large set of assorted operators and converge to optimal architectures
suited for the type of data provided under different scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine Learning Enhanced Approach for Automated Sunquake Detection in
  Acoustic Emission Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Mercea, Alin Razvan Paraschiv, Daniela Adriana Lacatus, Anca Marginean, Diana Besliu-Ionescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sunquakes are seismic emissions visible on the solar surface, associated with
some solar flares. Although discovered in 1998, they have only recently become
a more commonly detected phenomenon. Despite the availability of several manual
detection guidelines, to our knowledge, the astrophysical data produced for
sunquakes is new to the field of Machine Learning. Detecting sunquakes is a
daunting task for human operators and this work aims to ease and, if possible,
to improve their detection. Thus, we introduce a dataset constructed from
acoustic egression-power maps of solar active regions obtained for Solar Cycles
23 and 24 using the holography method. We then present a pedagogical approach
to the application of machine learning representation methods for sunquake
detection using AutoEncoders, Contrastive Learning, Object Detection and
recurrent techniques, which we enhance by introducing several custom
domain-specific data augmentation transformations. We address the main
challenges of the automated sunquake detection task, namely the very high noise
patterns in and outside the active region shadow and the extreme class
imbalance given by the limited number of frames that present sunquake
signatures. With our trained models, we find temporal and spatial locations of
peculiar acoustic emission and qualitatively associate them to eruptive and
high energy emission. While noting that these models are still in a prototype
stage and there is much room for improvement in metrics and bias levels, we
hypothesize that their agreement on example use cases has the potential to
enable detection of weak solar acoustic manifestations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Solar Physics accepted for publication, 44 total pages, 9 appendix
  pages, 21 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIER: Text-Image Entropy Regularization for CLIP-style models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Palepu, Andrew L. Beam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the effect of a novel regularization scheme on
contrastive language-image pre-trained (CLIP) models. Our approach is based on
the observation that, in many domains, text tokens should only describe a small
number of image regions and, likewise, each image region should correspond to
only a few text tokens. In CLIP-style models, this implies that text-token
embeddings should have high similarity to only a small number of image-patch
embeddings for a given image-text pair. We formalize this observation using a
novel regularization scheme that penalizes the entropy of the text-token to
image-patch similarity scores. We qualitatively and quantitatively demonstrate
that the proposed regularization scheme shrinks the text-token and image-patch
similarity scores towards zero, thus achieving the desired effect. We
demonstrate the promise of our approach in an important medical context where
this underlying hypothesis naturally arises. Using our proposed approach, we
achieve state of the art (SOTA) zero-shot performance on all tasks from the
CheXpert chest x-ray dataset, outperforming an unregularized version of the
model and several recently published self-supervised models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Neural Networks integrating genomics and histopathological images
  for predicting stages and survival time-to-event in colon cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olalekan Ogundipe, Zeyneb Kurt, Wai Lok Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There exists unexplained diverse variation within the predefined colon cancer
stages using only features either from genomics or histopathological whole
slide images as prognostic factors. Unraveling this variation will bring about
improved in staging and treatment outcome, hence motivated by the advancement
of Deep Neural Network libraries and different structures and factors within
some genomic dataset, we aggregate atypical patterns in histopathological
images with diverse carcinogenic expression from mRNA, miRNA and DNA
Methylation as an integrative input source into an ensemble deep neural network
for colon cancer stages classification and samples stratification into low or
high risk survival groups. The results of our Ensemble Deep Convolutional
Neural Network model show an improved performance in stages classification on
the integrated dataset. The fused input features return Area under curve
Receiver Operating Characteristic curve (AUC ROC) of 0.95 compared with AUC ROC
of 0.71 and 0.68 obtained when only genomics and images features are used for
the stage's classification, respectively. Also, the extracted features were
used to split the patients into low or high risk survival groups. Among the
2548 fused features, 1695 features showed a statistically significant survival
probability differences between the two risk groups defined by the extracted
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Clustering with k-Means: a Hybrid Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Poggiali, Alessandro Berti, Anna Bernasconi, Gianna Del Corso, Riccardo Guidotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing is a promising paradigm based on quantum theory for
performing fast computations. Quantum algorithms are expected to surpass their
classical counterparts in terms of computational complexity for certain tasks,
including machine learning. In this paper, we design, implement, and evaluate
three hybrid quantum k-Means algorithms, exploiting different degree of
parallelism. Indeed, each algorithm incrementally leverages quantum parallelism
to reduce the complexity of the cluster assignment step up to a constant cost.
In particular, we exploit quantum phenomena to speed up the computation of
distances. The core idea is that the computation of distances between records
and centroids can be executed simultaneously, thus saving time, especially for
big datasets. We show that our hybrid quantum k-Means algorithms can be more
efficient than the classical version, still obtaining comparable clustering
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Hateful Memes Challenge Next Move 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijun Jin, Lance Wilhelm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art image and text classification models, such as Convectional
Neural Networks and Transformers, have long been able to classify their
respective unimodal reasoning satisfactorily with accuracy close to or
exceeding human accuracy. However, images embedded with text, such as hateful
memes, are hard to classify using unimodal reasoning when difficult examples,
such as benign confounders, are incorporated into the data set. We attempt to
generate more labeled memes in addition to the Hateful Memes data set from
Facebook AI, based on the framework of a winning team from the Hateful Meme
Challenge. To increase the number of labeled memes, we explore semi-supervised
learning using pseudo-labels for newly introduced, unlabeled memes gathered
from the Memotion Dataset 7K. We find that the semi-supervised learning task on
unlabeled data required human intervention and filtering and that adding a
limited amount of new data yields no extra classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypercomplex Neural Architectures for Multi-View Breast Cancer
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.05798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.05798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Eleonora Grassucci, Martina Valleriani, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, deep learning methods for breast cancer classification perform
a single-view analysis. However, radiologists simultaneously analyze all four
views that compose a mammography exam, owing to the correlations contained in
mammography views, which present crucial information for identifying tumors. In
light of this, some studies have started to propose multi-view methods.
Nevertheless, in such existing architectures, mammogram views are processed as
independent images by separate convolutional branches, thus losing correlations
among them. To overcome such limitations, in this paper we propose a novel
approach for multi-view breast cancer classification based on parameterized
hypercomplex neural networks. Thanks to hypercomplex algebra properties, our
networks are able to model, and thus leverage, existing correlations between
the different views that comprise a mammogram, thus mimicking the reading
process performed by clinicians. The proposed methods are able to handle the
information of a patient altogether without breaking the multi-view nature of
the exam. We define architectures designed to process two-view exams, namely
PHResNets, and four-view exams, i.e., PHYSEnet and PHYBOnet. Through an
extensive experimental evaluation conducted with publicly available datasets,
we demonstrate that our proposed models clearly outperform real-valued
counterparts and also state-of-the-art methods, proving that breast cancer
classification benefits from the proposed multi-view architectures. We also
assess the method's robustness beyond mammogram analysis by considering
different benchmarks, as well as a finer-scaled task such as segmentation. Full
code and pretrained models for complete reproducibility of our experiments are
freely available at: https://github.com/ispamm/PHBreast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Transactions on Neural Networks
  and Learning Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable amortized variational inference with physics-based latent
  distribution correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Siahkoohi, Gabrio Rizzuti, Rafael Orozco, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference for high-dimensional inverse problems is computationally
costly and requires selecting a suitable prior distribution. Amortized
variational inference addresses these challenges via a neural network that acts
as a surrogate conditional distribution, matching the posterior distribution
not only for one instance of data, but a distribution of data pertaining to a
specific inverse problem. During inference, the neural network -- in our case a
conditional normalizing flow -- provides posterior samples with virtually no
cost. However, the accuracy of Amortized variational inference relies on the
availability of high-fidelity training data, which seldom exists in geophysical
inverse problems due to the Earth's heterogeneity. In addition, the network is
prone to errors if evaluated over out-of-distribution data. As such, we propose
to increases the resilience of amortized variational inference in presence of
moderate data distribution shifts. We achieve this via a correction to the
latent distribution that improves the posterior distribution approximation for
the data at hand. The correction involves relaxing the standard Gaussian
assumption on the latent distribution and parameterizing it via a Gaussian
distribution with an unknown mean and (diagonal) covariance. These unknowns are
then estimated by minimizing the Kullback-Leibler divergence between the
corrected and (physics-based) true posterior distributions. While generic and
applicable to other inverse problems, by means of a linearized seismic imaging
example, we show that our correction step improves the robustness of amortized
variational inference with respect to changes in number of seismic sources,
noise variance, and shifts in the prior distribution. This approach provides a
seismic image with limited artifacts and an assessment of its uncertainty with
approximately the same cost as five reverse-time migrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Lookahead and Approximate Policy Evaluation in Reinforcement
  Learning with Linear Value Function Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.13419v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.13419v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Winnicki, Joseph Lubars, Michael Livesay, R. Srikant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Function approximation is widely used in reinforcement learning to handle the
computational difficulties associated with very large state spaces. However,
function approximation introduces errors which may lead to instabilities when
using approximate dynamic programming techniques to obtain the optimal policy.
Therefore, techniques such as lookahead for policy improvement and m-step
rollout for policy evaluation are used in practice to improve the performance
of approximate dynamic programming with function approximation. We
quantitatively characterize, for the first time, the impact of lookahead and
m-step rollout on the performance of approximate dynamic programming (DP) with
function approximation: (i) without a sufficient combination of lookahead and
m-step rollout, approximate DP may not converge, (ii) both lookahead and m-step
rollout improve the convergence rate of approximate DP, and (iii) lookahead
helps mitigate the effect of function approximation and the discount factor on
the asymptotic performance of the algorithm. Our results are presented for two
approximate DP methods: one which uses least-squares regression to perform
function approximation and another which performs several steps of gradient
descent of the least-squares objective in each iteration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Complexity of Offline Reinforcement Learning with Deep ReLU
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.06671v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.06671v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh Nguyen-Tang, Sunil Gupta, Hung Tran-The, Svetha Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) leverages previously collected data for
policy optimization without any further active exploration. Despite the recent
interest in this problem, its theoretical results in neural network function
approximation settings remain elusive. In this paper, we study the statistical
theory of offline RL with deep ReLU network function approximation. In
particular, we establish the sample complexity of $n = \tilde{\mathcal{O}}(
H^{4 + 4 \frac{d}{\alpha}} \kappa_{\mu}^{1 + \frac{d}{\alpha}} \epsilon^{-2 -
2\frac{d}{\alpha}} )$ for offline RL with deep ReLU networks, where
$\kappa_{\mu}$ is a measure of distributional shift, {$H = (1-\gamma)^{-1}$ is
the effective horizon length}, $d$ is the dimension of the state-action space,
$\alpha$ is a (possibly fractional) smoothness parameter of the underlying
Markov decision process (MDP), and $\epsilon$ is a user-specified error.
Notably, our sample complexity holds under two novel considerations: the Besov
dynamic closure and the correlated structure. While the Besov dynamic closure
subsumes the dynamic conditions for offline RL in the prior works, the
correlated structure renders the prior works of offline RL with general/neural
network function approximation improper or inefficient {in long (effective)
horizon problems}. To the best of our knowledge, this is the first theoretical
characterization of the sample complexity of offline RL with deep neural
network function approximation under the general Besov regularity condition
that goes beyond {the linearity regime} in the traditional Reproducing Hilbert
kernel spaces and Neural Tangent Kernels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://openreview.net/forum?id=LdEm0umNcv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Collaborative Learning via Representation Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frédéric Berdoz, Abhishek Singh, Martin Jaggi, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving machine learning has become a key conundrum for
multi-party artificial intelligence. Federated learning (FL) and Split Learning
(SL) are two frameworks that enable collaborative learning while keeping the
data private (on device). In FL, each data holder trains a model locally and
releases it to a central server for aggregation. In SL, the clients must
release individual cut-layer activations (smashed data) to the server and wait
for its response (during both inference and back propagation). While relevant
in several settings, both of these schemes have a high communication cost, rely
on server-level computation algorithms and do not allow for tunable levels of
collaboration. In this work, we present a novel approach for privacy-preserving
machine learning, where the clients collaborate via online knowledge
distillation using a contrastive loss (contrastive w.r.t. the labels). The goal
is to ensure that the participants learn similar features on similar classes
without sharing their input data. To do so, each client releases averaged last
hidden layer activations of similar labels to a central server that only acts
as a relay (i.e., is not involved in the training or aggregation of the
models). Then, the clients download these last layer activations (feature
representations) of the ensemble of users and distill their knowledge in their
personal model using a contrastive objective. For cross-device applications
(i.e., small local datasets and limited computational capacity), this approach
increases the utility of the models compared to independent learning and other
federated knowledge distillation (FD) schemes, is communication efficient and
is scalable with the number of clients. We prove theoretically that our
framework is well-posed, and we benchmark its performance against standard FD
and FL on various datasets using different model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FeDXL: Provable Federated Learning for Deep X-Risk Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishuai Guo, Rong Jin, Jiebo Luo, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a novel federated learning (FL) problem for
optimizing a family of X-risks, to which no existing FL algorithms are
applicable. In particular, the objective has the form of $\mathbb E_{z\sim S_1}
f(\mathbb E_{z'\sim S_2} \ell(w; z, z'))$, where two sets of data $S_1, S_2$
are distributed over multiple machines, $\ell(\cdot)$ is a pairwise loss that
only depends on the prediction outputs of the input data pairs $(z, z')$, and
$f(\cdot)$ is possibly a non-linear non-convex function. This problem has
important applications in machine learning, e.g., AUROC maximization with a
pairwise loss, and partial AUROC maximization with a compositional loss. The
challenges for designing an FL algorithm lie in the non-decomposability of the
objective over multiple machines and the interdependency between different
machines. To address the challenges, we propose an active-passive decomposition
framework that decouples the gradient's components with two types, namely
active parts and passive parts, where the active parts depend on local data
that are computed with the local model and the passive parts depend on other
machines that are communicated/computed based on historical models and samples.
Under this framework, we develop two provable FL algorithms (FeDXL) for
handling linear and nonlinear $f$, respectively, based on federated averaging
and merging. We develop a novel theoretical analysis to combat the latency of
the passive parts and the interdependency between the local model parameters
and the involved data for computing local gradient estimators. We establish
both iteration and communication complexities and show that using the
historical samples and models for computing the passive parts do not degrade
the complexities. We conduct empirical studies of FeDXL for deep AUROC and
partial AUROC maximization, and demonstrate their performance compared with
several baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Regression by Inverse Distance Weighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07177v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07177v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Bemporad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an active learning (AL) algorithm to solve regression
problems based on inverse-distance weighting functions for selecting the
feature vectors to query. The algorithm has the following features: (i)
supports both pool-based and population-based sampling; (ii) is not tailored to
a particular class of predictors; (iii) can handle known and unknown
constraints on the queryable feature vectors; and (iv) can run either
sequentially, or in batch mode, depending on how often the predictor is
retrained. The potentials of the method are shown in numerical tests on
illustrative synthetic problems and real-world datasets. An implementation of
the algorithm, which we call IDEAL (Inverse-Distance based Exploration for
Active Learning), is available at http://cse.lab.imtlucca.it/~bemporad/ideal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures. Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-Supervised</span> Learning for Anomalous Channel Detection in EEG Graphs:
  Application to Seizure Analysis <span class="chip">AAAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi Kieu Khanh Ho, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals are effective tools towards seizure
analysis where one of the most important challenges is accurate detection of
seizure events and brain regions in which seizure happens or initiates.
However, all existing machine learning-based algorithms for seizure analysis
require access to the labeled seizure data while acquiring labeled data is very
labor intensive, expensive, as well as clinicians dependent given the
subjective nature of the visual qualitative interpretation of EEG signals. In
this paper, we propose to detect seizure channels and clips in a
self-supervised manner where no access to the seizure data is needed. The
proposed method considers local structural and contextual information embedded
in EEG graphs by employing positive and negative sub-graphs. We train our
method through minimizing contrastive and generative losses. The employ of
local EEG sub-graphs makes the algorithm an appropriate choice when accessing
to the all EEG channels is impossible due to complications such as skull
fractures. We conduct an extensive set of experiments on the largest seizure
dataset and demonstrate that our proposed framework outperforms the
state-of-the-art methods in the EEG-based seizure study. The proposed method is
the only study that requires no access to the seizure data in its training
phase, yet establishes a new state-of-the-art to the field, and outperforms all
related supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shining light on data: Geometric data analysis through quantum dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Kumar, Mohan Sarovar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experimental sciences have come to depend heavily on our ability to organize
and interpret high-dimensional datasets. Natural laws, conservation principles,
and inter-dependencies among observed variables yield geometric structure, with
fewer degrees of freedom, on the dataset. We introduce the frameworks of
semiclassical and microlocal analysis to data analysis and develop a novel, yet
natural uncertainty principle for extracting fine-scale features of this
geometric structure in data, crucially dependent on data-driven approximations
to quantum mechanical processes underlying geometric optics. This leads to the
first tractable algorithm for approximation of wave dynamics and geodesics on
data manifolds with rigorous probabilistic convergence rates under the manifold
hypothesis. We demonstrate our algorithm on real-world datasets, including an
analysis of population mobility information during the COVID-19 pandemic to
achieve four-fold improvement in dimensionality reduction over existing
state-of-the-art and reveal anomalous behavior exhibited by less than 1.2% of
the entire dataset. Our work initiates the study of data-driven quantum
dynamics for analyzing datasets, and we outline several future directions for
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary Material has high overlap with arXiv:2112.11161 by the
  same authors. v2 reorganizes presentation of results in paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keaton Hamm, Nick Henscheid, Shujie Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a
nonlinear dimensionality reduction technique that provides solutions to some
drawbacks in existing global nonlinear dimensionality reduction algorithms in
imaging applications. Wassmap represents images via probability measures in
Wasserstein space, then uses pairwise Wasserstein distances between the
associated measures to produce a low-dimensional, approximately isometric
embedding. We show that the algorithm is able to exactly recover parameters of
some image manifolds including those generated by translations or dilations of
a fixed generating measure. Additionally, we show that a discrete version of
the algorithm retrieves parameters from manifolds generated from discrete
measures by providing a theoretical bridge to transfer recovery results from
functional data to discrete data. Testing of the proposed algorithms on various
image data manifolds show that Wassmap yields good embeddings compared with
other global and local techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Importance of Image Interpretation: Patterns of Semantic
  Misclassification in Real-World Adversarial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyu Zhao, Nga Dang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial images are created with the intention of causing an image
classifier to produce a misclassification. In this paper, we propose that
adversarial images should be evaluated based on semantic mismatch, rather than
label mismatch, as used in current work. In other words, we propose that an
image of a "mug" would be considered adversarial if classified as "turnip", but
not as "cup", as current systems would assume. Our novel idea of taking
semantic misclassification into account in the evaluation of adversarial images
offers two benefits. First, it is a more realistic conceptualization of what
makes an image adversarial, which is important in order to fully understand the
implications of adversarial images for security and privacy. Second, it makes
it possible to evaluate the transferability of adversarial images to a
real-world classifier, without requiring the classifier's label set to have
been available during the creation of the images. The paper carries out an
evaluation of a transfer attack on a real-world image classifier that is made
possible by our semantic misclassification approach. The attack reveals
patterns in the semantics of adversarial misclassifications that could not be
investigated using conventional label mismatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Multimedia Modeling (MMM) 2023. Resources
  are publicly available at
  https://github.com/ZhengyuZhao/Targeted-Transfer/tree/main/human_eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ripple: Concept-Based Interpretation for Raw Time Series Models in
  Education <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01133v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01133v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Asadi, Vinitra Swamy, Jibril Frej, Julien Vignoud, Mirko Marras, Tanja Käser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series is the most prevalent form of input data for educational
prediction tasks. The vast majority of research using time series data focuses
on hand-crafted features, designed by experts for predictive performance and
interpretability. However, extracting these features is labor-intensive for
humans and computers. In this paper, we propose an approach that utilizes
irregular multivariate time series modeling with graph neural networks to
achieve comparable or better accuracy with raw time series clickstreams in
comparison to hand-crafted features. Furthermore, we extend concept activation
vectors for interpretability in raw time series models. We analyze these
advances in the education domain, addressing the task of early student
performance prediction for downstream targeted interventions and instructional
support. Our experimental analysis on 23 MOOCs with millions of combined
interactions over six behavioral dimensions show that models designed with our
approach can (i) beat state-of-the-art educational time series baselines with
no feature extraction and (ii) provide interpretable insights for personalized
interventions. Source code: https://github.com/epfl-ml4ed/ripple/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper at AAAI 2023: 37th AAAI Conference on
  Artificial Intelligence (EAAI: AI for Education Special Track), 7-14 of
  February 2023, Washington DC, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Sandbox Tool to Bias(Stress)-Test Fairness Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nil-Jana Akpinar, Manish Nagireddy, Logan Stapleton, Hao-Fei Cheng, Haiyi Zhu, Steven Wu, Hoda Heidari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the growing importance of reducing unfairness in ML predictions,
Fair-ML researchers have presented an extensive suite of algorithmic
'fairness-enhancing' remedies. Most existing algorithms, however, are agnostic
to the sources of the observed unfairness. As a result, the literature
currently lacks guiding frameworks to specify conditions under which each
algorithmic intervention can potentially alleviate the underpinning cause of
unfairness. To close this gap, we scrutinize the underlying biases (e.g., in
the training data or design choices) that cause observational unfairness. We
present the conceptual idea and a first implementation of a bias-injection
sandbox tool to investigate fairness consequences of various biases and assess
the effectiveness of algorithmic remedies in the presence of specific types of
bias. We call this process the bias(stress)-testing of algorithmic
interventions. Unlike existing toolkits, ours provides a controlled environment
to counterfactually inject biases in the ML pipeline. This stylized setup
offers the distinct capability of testing fairness interventions beyond
observational data and against an unbiased benchmark. In particular, we can
test whether a given remedy can alleviate the injected bias by comparing the
predictions resulting after the intervention in the biased setting with true
labels in the unbiased regime-that is, before any bias injection. We illustrate
the utility of our toolkit via a proof-of-concept case study on synthetic data.
Our empirical analysis showcases the type of insights that can be obtained
through our simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared as a poster at the second ACM conference on Equity and
  Access in Algorithms, Mechanisms, and Optimization (EAAMO'22)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Path Finding via Tree LSTM <span class="chip">AAAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Jiang, Kunjie Zhang, Qimai Li, Jiaxin Chen, Xiaolong Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Multi-Agent Path Finding (MAPF) has attracted attention from
the fields of both Operations Research (OR) and Reinforcement Learning (RL).
However, in the 2021 Flatland3 Challenge, a competition on MAPF, the best RL
method scored only 27.9, far less than the best OR method. This paper proposes
a new RL solution to Flatland3 Challenge, which scores 125.3, several times
higher than the best RL solution before. We creatively apply a novel network
architecture, TreeLSTM, to MAPF in our solution. Together with several other RL
techniques, including reward shaping, multiple-phase training, and centralized
control, our solution is comparable to the top 2-3 OR methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appear in AAAI23-MAPF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Representations for New Sound Classes With Continual
  <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhepei Wang, Cem Subakan, Xilin Jiang, Junkai Wu, Efthymios Tzinis, Mirco Ravanelli, Paris Smaragdis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we work on a sound recognition system that continually
incorporates new sound classes. Our main goal is to develop a framework where
the model can be updated without relying on labeled data. For this purpose, we
propose adopting representation learning, where an encoder is trained using
unlabeled data. This learning framework enables the study and implementation of
a practically relevant use case where only a small amount of the labels is
available in a continual learning context. We also make the empirical
observation that a similarity-based representation learning method within this
framework is robust to forgetting even if no explicit mechanism against
forgetting is employed. We show that this approach obtains similar performance
compared to several distillation-based continual learning methods when employed
on self-supervised representation learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Signal Processing Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Second-order optimization with lazy Hessians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Doikov, El Mahdi Chayti, Martin Jaggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze Newton's method with lazy Hessian updates for solving general
possibly non-convex optimization problems. We propose to reuse a previously
seen Hessian for several iterations while computing new gradients at each step
of the method. This significantly reduces the overall arithmetical complexity
of second-order optimization schemes. By using the cubic regularization
technique, we establish fast global convergence of our method to a second-order
stationary point, while the Hessian does not need to be updated each iteration.
For convex problems, we justify global and local superlinear rates for lazy
Newton steps with quadratic regularization, which is easier to compute. The
optimal frequency for updating the Hessian is once every $d$ iterations, where
$d$ is the dimension of the problem. This provably improves the total
arithmetical complexity of second-order algorithms by a factor $\sqrt{d}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics Guided Deep Learning for Generative Design of Crystal Materials
  with Symmetry Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14352v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14352v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Zhao, Edirisuriya M. Dilanga Siriwardane, Zhenyao Wu, Nihang Fu, Mohammed Al-Fahdi, Ming Hu, Jianjun Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering new materials is a challenging task in materials science crucial
to the progress of human society. Conventional approaches based on experiments
and simulations are labor-intensive or costly with success heavily depending on
experts' heuristic knowledge. Here, we propose a deep learning based Physics
Guided Crystal Generative Model (PGCGM) for efficient crystal material design
with high structural diversity and symmetry. Our model increases the generation
validity by more than 700\% compared to FTCP, one of the latest structure
generators and by more than 45\% compared to our previous CubicGAN model.
Density Functional Theory (DFT) calculations are used to validate the generated
structures with 1,869 materials out of 2,000 are successfully optimized and
deposited into the Carolina Materials Database \url{www.carolinamatdb.org}, of
which 39.6\% have negative formation energy and 5.3\% have energy-above-hull
less than 0.25 eV/atom, indicating their thermodynamic stability and potential
synthesizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normative Modeling using Multimodal Variational Autoencoders to Identify
  Abnormal Brain Structural Patterns in Alzheimer Disease <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.04903v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.04903v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayantan Kumar, Philip Payne, Aristeidis Sotiras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normative modelling is an emerging method for understanding the underlying
heterogeneity within brain disorders like Alzheimer Disease (AD) by quantifying
how each patient deviates from the expected normative pattern that has been
learned from a healthy control distribution. Since AD is a multifactorial
disease with more than one biological pathways, multimodal magnetic resonance
imaging (MRI) neuroimaging data can provide complementary information about the
disease heterogeneity. However, existing deep learning based normative models
on multimodal MRI data use unimodal autoencoders with a single encoder and
decoder that may fail to capture the relationship between brain measurements
extracted from different MRI modalities. In this work, we propose multi-modal
variational autoencoder (mmVAE) based normative modelling framework that can
capture the joint distribution between different modalities to identify
abnormal brain structural patterns in AD. Our multi-modal framework takes as
input Freesurfer processed brain region volumes from T1-weighted (cortical and
subcortical) and T2-weighed (hippocampal) scans of cognitively normal
participants to learn the morphological characteristics of the healthy brain.
The estimated normative model is then applied on Alzheimer Disease (AD)
patients to quantify the deviation in brain volumes and identify the abnormal
brain structural patterns due to the effect of the different AD stages. Our
experimental results show that modeling joint distribution between the multiple
MRI modalities generates deviation maps that are more sensitive to disease
staging within AD, have a better correlation with patient cognition and result
in higher number of brain regions with statistically significant deviations
compared to a unimodal baseline model with all modalities concatenated as a
single input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Medical Imaging Meets NeurIPS workshop in NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Through the Lens of Sequential Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08056v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08056v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Xiaotian Dou, Alvin Qingkai Pan, Runxue Bao, Haiyi Harry Mao, Lei Luo, Zhi-Hong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling is ubiquitous in machine learning methodologies. Due to the growth
of large datasets and model complexity, we want to learn and adapt the sampling
process while training a representation. Towards achieving this grand goal, a
variety of sampling techniques have been proposed. However, most of them either
use a fixed sampling scheme or adjust the sampling scheme based on simple
heuristics. They cannot choose the best sample for model training in different
stages. Inspired by "Think, Fast and Slow" (System 1 and System 2) in cognitive
science, we propose a reward-guided sampling strategy called Adaptive Sample
with Reward (ASR) to tackle this challenge. To the best of our knowledge, this
is the first work utilizing reinforcement learning (RL) to address the sampling
problem in representation learning. Our approach optimally adjusts the sampling
process to achieve optimal performance. We explore geographical relationships
among samples by distance-based sampling to maximize overall cumulative reward.
We apply ASR to the long-standing sampling problems in similarity-based loss
functions. Empirical results in information retrieval and clustering
demonstrate ASR's superb performance across different datasets. We also discuss
an engrossing phenomenon which we name as "ASR gravity well" in experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Detection by Approximation of Ensemble Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Windeatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A spectral approximation of a Boolean function is proposed for approximating
the decision boundary of an ensemble of Deep Neural Networks (DNNs) solving
two-class pattern recognition problems. The Walsh combination of relatively
weak DNN classifiers is shown experimentally to be capable of detecting
adversarial attacks. By observing the difference in Walsh coefficient
approximation between clean and adversarial images, it appears that
transferability of attack may be used for detection. Approximating the decision
boundary may also aid in understanding the learning and transferability
properties of DNNs. While the experiments here use images, the proposed
approach of modelling two-class ensemble decision boundaries could in principle
be applied to any application area. Code for this paper implementing Walsh
Coefficient Examples of approximating artificial Boolean functions can be found
at https://doi.org/10.24433/CO.3695905.v1
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavioral Experiments for Understanding Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Bell, Neil D. Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we explore whether the fundamental tool of experimental
psychology, the behavioral experiment, has the power to generate insight not
only into humans and animals, but artificial systems too. We apply the
techniques of experimental psychology to investigating catastrophic forgetting
in neural networks. We present a series of controlled experiments with
two-layer ReLU networks, and exploratory results revealing a new understanding
of the behavior of catastrophic forgetting. Alongside our empirical findings,
we demonstrate an alternative, behavior-first approach to investigating neural
network phenomena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Unreasonable Effectiveness of Deep Evidential Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nis Meinert, Jakob Gawlikowski, Alexander Lavin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a significant need for principled uncertainty reasoning in machine
learning systems as they are increasingly deployed in safety-critical domains.
A new approach with uncertainty-aware regression-based neural networks (NNs),
based on learning evidential distributions for aleatoric and epistemic
uncertainties, shows promise over traditional deterministic methods and typical
Bayesian NNs, notably with the capabilities to disentangle aleatoric and
epistemic uncertainties. Despite some empirical success of Deep Evidential
Regression (DER), there are important gaps in the mathematical foundation that
raise the question of why the proposed technique seemingly works. We detail the
theoretical shortcomings and analyze the performance on synthetic and
real-world data sets, showing that Deep Evidential Regression is a heuristic
rather than an exact uncertainty quantification. We go on to propose
corrections and redefinitions of how aleatoric and epistemic uncertainties
should be extracted from NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 25 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of The MediaEval 2022 Predicting Video Memorability Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorin Sweeney, Mihai Gabriel Constantin, Claire-Hélène Demarty, Camilo Fosco, Alba G. Seco de Herrera, Sebastian Halder, Graham Healy, Bogdan Ionescu, Ana Matran-Fernandez, Alan F. Smeaton, Mushfika Sultana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the 5th edition of the Predicting Video Memorability
Task as part of MediaEval2022. This year we have reorganised and simplified the
task in order to lubricate a greater depth of inquiry. Similar to last year,
two datasets are provided in order to facilitate generalisation, however, this
year we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem
dataset in order to remedy underlying data quality issues, and to prioritise
short-term memorability prediction by elevating the Memento10k dataset as the
primary dataset. Additionally, a fully fledged electroencephalography
(EEG)-based prediction sub-task is introduced. In this paper, we outline the
core facets of the task and its constituent sub-tasks; describing the datasets,
evaluation metrics, and requirements for participant submissions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages. In: MediaEval Multimedia Benchmark Workshop Working Notes,
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02339v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02339v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Jie Zhang, Han Fang, Zehua Ma, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio watermarking is widely used for leaking source tracing. The robustness
of the watermark determines the traceability of the algorithm. With the
development of digital technology, audio re-recording (AR) has become an
efficient and covert means to steal secrets. AR process could drastically
destroy the watermark signal while preserving the original information. This
puts forward a new requirement for audio watermarking at this stage, that is,
to be robust to AR distortions. Unfortunately, none of the existing algorithms
can effectively resist AR attacks due to the complexity of the AR process. To
address this limitation, this paper proposes DeAR, a deep-learning-based audio
re-recording resistant watermarking. Inspired by DNN-based image watermarking,
we pioneer a deep learning framework for audio carriers, based on which the
watermark signal can be effectively embedded and extracted. Meanwhile, in order
to resist the AR attack, we delicately analyze the distortions that occurred in
the AR process and design the corresponding distortion layer to cooperate with
the proposed watermarking framework. Extensive experiments show that the
proposed algorithm can resist not only common electronic channel distortions
but also AR distortions. Under the premise of high-quality embedding
(SNR=25.86dB), in the case of a common re-recording distance (20cm), the
algorithm can effectively achieve an average bit recovery accuracy of 98.55%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HighMMT: Quantifying Modality & Interaction Heterogeneity for
  High-Modality Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world problems are inherently multimodal, from the communicative
modalities humans use to express social and emotional states to the force,
proprioception, and visual sensors ubiquitous on robots. While there has been
an explosion of interest in multimodal representation learning, these methods
are still largely focused on a small set of modalities, primarily in the
language, vision, and audio space. In order to accelerate generalization
towards diverse and understudied modalities, this paper studies efficient
representation learning for high-modality scenarios. Since adding new models
for every new modality or task becomes prohibitively expensive, a critical
technical challenge is heterogeneity quantification: how can we measure which
modalities encode similar information and interactions in order to permit
parameter sharing with previous modalities? We propose two new
information-theoretic metrics for heterogeneity quantification: (1) modality
heterogeneity studies how similar 2 modalities $\{X_1,X_2\}$ are by measuring
how much information can be transferred from $X_1$ to $X_2$, while (2)
interaction heterogeneity studies how similarly pairs of modalities
$\{X_1,X_2\}, \{X_3,X_4\}$ interact by measuring how much interaction
information can be transferred from $\{X_1,X_2\}$ to $\{X_3,X_4\}$. We show the
importance of these proposed metrics in high-modality scenarios as a way to
automatically prioritize the fusion of modalities that contain unique
information or interactions. The result is a single model, HighMMT, that scales
up to $10$ modalities and $15$ tasks from $5$ different research areas. Not
only does HighMMT outperform prior methods on the tradeoff between performance
and efficiency, it also demonstrates a crucial scaling behavior: performance
continues to improve with each modality added, and transfers to entirely new
modalities and tasks during fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/pliang279/HighMMT</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-12T00:00:00Z">2022-12-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mortality Prediction Models with Clinical Notes Using Sparse Attention
  at the Word and Sentence Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Rios, Ameen Abu-Hanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intensive Care in-hospital mortality prediction has various clinical
applications. Neural prediction models, especially when capitalising on
clinical notes, have been put forward as improvement on currently existing
models. However, to be acceptable these models should be performant and
transparent. This work studies different attention mechanisms for clinical
neural prediction models in terms of their discrimination and calibration.
Specifically, we investigate sparse attention as an alternative to dense
attention weights in the task of in-hospital mortality prediction from clinical
notes. We evaluate the attention mechanisms based on: i) local self-attention
over words in a sentence, and ii) global self-attention with a transformer
architecture across sentences. We demonstrate that the sparse mechanism
approach outperforms the dense one for the local self-attention in terms of
predictive performance with a publicly available dataset, and puts higher
attention to prespecified relevant directive words. The performance at the
sentence level, however, deteriorates as sentences including the influential
directive words tend to be dropped all together.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Reports at the Department of Medical Informatics, Amsterdam
  UMC, 2021. https://kik.amc.nl/KIK/reports/TR2021-01.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Defense of Cross-Encoders for Zero-Shot Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bi-encoders and cross-encoders are widely used in many state-of-the-art
retrieval pipelines. In this work we study the generalization ability of these
two types of architectures on a wide range of parameter count on both in-domain
and out-of-domain scenarios. We find that the number of parameters and early
query-document interactions of cross-encoders play a significant role in the
generalization ability of retrieval models. Our experiments show that
increasing model size results in marginal gains on in-domain test sets, but
much larger gains in new domains never seen during fine-tuning. Furthermore, we
show that cross-encoders largely outperform bi-encoders of similar size in
several tasks. In the BEIR benchmark, our largest cross-encoder surpasses a
state-of-the-art bi-encoder by more than 4 average points. Finally, we show
that using bi-encoders as first-stage retrievers provides no gains in
comparison to a simpler retriever such as BM25 on out-of-domain tasks. The code
is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2206.02873</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>ing Is Programming: A Query Language For Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Beurer-Kellner, Marc Fischer, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated outstanding performance on a wide
range of tasks such as question answering and code generation. On a high level,
given an input, a language model can be used to automatically complete the
sequence in a statistically-likely way. Based on this, users prompt these
models with language instructions or examples, to implement a variety of
downstream tasks. Advanced prompting methods can even imply interaction between
the language model, a user, and external tools such as calculators. However, to
obtain state-of-the-art performance or adapt language models for specific
tasks, complex task- and model-specific programs have to be implemented, which
may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP).
LMP generalizes language model prompting from pure text prompts to an intuitive
combination of text prompting and scripting. Additionally, LMP allows
constraints to be specified over the language model output. This enables easy
adaption to many tasks, while abstracting language model internals and
providing high-level semantics. To enable LMP, we implement LMQL (short for
Language Model Query Language), which leverages the constraints and control
flow from an LMP prompt to generate an efficient inference procedure that
minimizes the number of expensive calls to the underlying language model. We
show that LMQL can capture a wide range of state-of-the-art prompting methods
in an intuitive way, especially facilitating interactive flows that are
challenging to implement with existing high-level APIs. Our evaluation shows
that we retain or increase the accuracy on several downstream tasks, while also
significantly reducing the required amount of computation or cost in the case
of pay-to-use APIs (13-85% cost savings).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages + Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Seed-Guided Topic Discovery by Integrating Multiple Types of
  Contexts <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yunyi Zhang, Martin Michalski, Yucheng Jiang, Yu Meng, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instead of mining coherent topics from a given text corpus in a completely
unsupervised manner, seed-guided topic discovery methods leverage user-provided
seed words to extract distinctive and coherent topics so that the mined topics
can better cater to the user's interest. To model the semantic correlation
between words and seeds for discovering topic-indicative terms, existing
seed-guided approaches utilize different types of context signals, such as
document-level word co-occurrences, sliding window-based local contexts, and
generic linguistic knowledge brought by pre-trained language models. In this
work, we analyze and show empirically that each type of context information has
its value and limitation in modeling word semantics under seed guidance, but
combining three types of contexts (i.e., word embeddings learned from local
contexts, pre-trained language model representations obtained from
general-domain training, and topic-indicative sentences retrieved based on seed
information) allows them to complement each other for discovering quality
topics. We propose an iterative framework, SeedTopicMine, which jointly learns
from the three types of contexts and gradually fuses their context signals via
an ensemble ranking process. Under various sets of seeds and on multiple
datasets, SeedTopicMine consistently yields more coherent and accurate topics
than existing seed-guided topic discovery approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; Accepted to WSDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuation KD: Improved Knowledge Distillation through the Lens of
  Continuation Optimization <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Jafari, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart, Ali Ghodsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) has been extensively used for natural language
understanding (NLU) tasks to improve a small model's (a student) generalization
by transferring the knowledge from a larger model (a teacher). Although KD
methods achieve state-of-the-art performance in numerous settings, they suffer
from several problems limiting their performance. It is shown in the literature
that the capacity gap between the teacher and the student networks can make KD
ineffective. Additionally, existing KD techniques do not mitigate the noise in
the teacher's output: modeling the noisy behaviour of the teacher can distract
the student from learning more useful features. We propose a new KD method that
addresses these problems and facilitates the training compared to previous
techniques. Inspired by continuation optimization, we design a training
procedure that optimizes the highly non-convex KD objective by starting with
the smoothed version of this objective and making it more complex as the
training proceeds. Our method (Continuation-KD) achieves state-of-the-art
performance across various compact architectures on NLU (GLUE benchmark) and
computer vision tasks (CIFAR-10 and CIFAR-100).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2022 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-World Compositional Generalization with Disentangled
  Sequence-to-Sequence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zheng, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional generalization is a basic mechanism in human language learning,
which current neural networks struggle with. A recently proposed Disentangled
sequence-to-sequence model (Dangle) shows promising generalization capability
by learning specialized encodings for each decoding step. We introduce two key
modifications to this model which encourage more disentangled representations
and improve its compute and memory efficiency, allowing us to tackle
compositional generalization in a more realistic setting. Specifically, instead
of adaptively re-encoding source keys and values at each time step, we
disentangle their representations and only re-encode keys periodically, at some
interval. Our new architecture leads to better generalization performance
across existing tasks and datasets, and a new machine translation benchmark
which we create by detecting naturally occurring compositional patterns in
relation to a training set. We show this methodology better emulates real-world
requirements than artificial challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated NLP in Few-shot Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Cai, Shangguang Wang, Yaozong Wu, Felix Xiaozhu Lin, Mengwei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) sees rich mobile applications. To support
various language understanding tasks, a foundation NLP model is often
fine-tuned in a federated, privacy-preserving setting (FL). This process
currently relies on at least hundreds of thousands of labeled training samples
from mobile clients; yet mobile users often lack willingness or knowledge to
label their data. Such an inadequacy of data labels is known as a few-shot
scenario; it becomes the key blocker for mobile NLP applications.
  For the first time, this work investigates federated NLP in the few-shot
scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and
prompt learning, we first establish a training pipeline that delivers
competitive accuracy when only 0.05% (fewer than 100) of the training data is
labeled and the remaining is unlabeled. To instantiate the workflow, we further
present a system FFNLP, addressing the high execution cost with novel designs.
(1) Curriculum pacing, which injects pseudo labels to the training workflow at
a rate commensurate to the learning progress; (2) Representational diversity, a
mechanism for selecting the most learnable data, only for which pseudo labels
will be generated; (3) Co-planning of a model's training depth and layer
capacity. Together, these designs reduce the training delay, client energy, and
network traffic by up to 46.0$\times$, 41.2$\times$ and 3000.0$\times$,
respectively. Through algorithm/system co-design, FFNLP demonstrates that FL
can apply to challenging settings where most training samples are unlabeled.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning
  for Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Zhuanzhe Zhao, Yongming Liu, Xiaolong Zhang, Xuecong Hou, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new data augmentation algorithm for natural
understanding tasks, called RPN:Random Position Noise algorithm.Due to the
relative paucity of current text augmentation methods. Few of the extant
methods apply to natural language understanding tasks for all sentence-level
tasks.RPN applies the traditional augmentation on the original text to the word
vector level. The RPN algorithm makes a substitution in one or several
dimensions of some word vectors. As a result, the RPN can introduce a certain
degree of perturbation to the sample and can adjust the range of perturbation
on different tasks. The augmented samples are then used to give the model
training.This makes the model more robust. In subsequent experiments, we found
that adding RPN to the training or fine-tuning model resulted in a stable boost
on all 8 natural language processing tasks, including TweetEval, CoLA, and
SST-2 datasets, and more significant improvements than other data augmentation
algorithms.The RPN algorithm applies to all sentence-level tasks for language
understanding and is used in any deep learning model with a word embedding
layer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generalization of <span class="highlight-title">Pre-train</span>ed Language Models via Stochastic
  Weight Averaging <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Ahmad Rashid, Ali Ghodsi, Philippe Langlais
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is a commonly used technique for improving the
generalization of compact Pre-trained Language Models (PLMs) on downstream
tasks. However, such methods impose the additional burden of training a
separate teacher model for every new dataset. Alternatively, one may directly
work on the improvement of the optimization procedure of the compact model
toward better generalization. Recent works observe that the flatness of the
local minimum correlates well with better generalization. In this work, we
adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a
flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on
various NLP tasks (text classification, question answering, and generation) and
different model architectures and demonstrate that our adaptation improves the
generalization without extra computation cost. Moreover, we observe that this
simple optimization technique is able to outperform the state-of-the-art KD
methods for compact models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP 2022 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parameter-Efficient Finetuning of <span class="highlight-title">Transformer</span>s for Source Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamil Ayupov, Nadezhda Chirkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained Transformers achieve state-of-the-art performance in various
code-processing tasks but may be too large to be deployed. As software
development tools often incorporate modules for various purposes which may
potentially use a single instance of the pretrained model, it appears relevant
to utilize parameter-efficient fine-tuning for the pretrained models of code.
In this work, we test two widely used approaches, adapters and LoRA, which were
initially tested on NLP tasks, on four code-processing tasks. We find that
though the efficient fine-tuning approaches may achieve comparable or higher
performance than the standard, full, fine-tuning in code understanding tasks,
they underperform full fine-tuning in code-generative tasks. These results
underline the importance of testing efficient fine-tuning approaches on other
domains than NLP and motivate future research in efficient fine-tuning for
source code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Earthquake Impact Analysis Based on Text Mining and Social Media
  Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zheng, Hong-Zheng Shi, Yu-Cheng Zhou, Xin-Zheng Lu, Jia-Rui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earthquakes have a deep impact on wide areas, and emergency rescue operations
may benefit from social media information about the scope and extent of the
disaster. Therefore, this work presents a text miningbased approach to collect
and analyze social media data for early earthquake impact analysis. First,
disasterrelated microblogs are collected from the Sina microblog based on
crawler technology. Then, after data cleaning a series of analyses are
conducted including (1) the hot words analysis, (2) the trend of the number of
microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and
rule-based text classification for earthquake impact analysis. Finally, two
recent earthquakes with the same magnitude and focal depth in China are
analyzed to compare their impacts. The results show that the public opinion
trend analysis and the trend of public opinion sentiment can estimate the
earthquake's social impact at an early stage, which will be helpful to
decision-making and rescue management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Mining-Based Patent Analysis for Automated Rule Checking in AEC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zheng, Bo-Rui Kang, Qi-Tian Yuan, Yu-Cheng Zhou, Xin-Zheng Lu, Jia-Rui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated rule checking (ARC), which is expected to promote the efficiency of
the compliance checking process in the architecture, engineering, and
construction (AEC) industry, is gaining increasing attention. Throwing light on
the ARC application hotspots and forecasting its trends are useful to the
related research and drive innovations. Therefore, this study takes the patents
from the database of the Derwent Innovations Index database (DII) and China
national knowledge infrastructure (CNKI) as data sources and then carried out a
three-step analysis including (1) quantitative characteristics (i.e., annual
distribution analysis) of patents, (2) identification of ARC topics using a
latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of
ARC topics. The results show that the research hotspots and trends of Chinese
and English patents are different. The contributions of this study have three
aspects: (1) an approach to a comprehensive analysis of patents by integrating
multiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the
application hotspots and development trends of ARC are reviewed based on patent
analysis; and (3) a signpost for technological development and innovation of
ARC is provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "I think this is the most disruptive technology": Exploring Sentiments
  of Chat<span class="highlight-title">GPT</span> Early Adopters using Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubin Ul Haque, Isuru Dharmadasa, Zarrin Tasnim Sworna, Roshan Namal Rajapakse, Hussain Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have recently attracted significant attention due to
their impressive performance on a variety of tasks. ChatGPT developed by OpenAI
is one such implementation of a large, pre-trained language model that has
gained immense popularity among early adopters, where certain users go to the
extent of characterizing it as a disruptive technology in many domains.
Understanding such early adopters' sentiments is important because it can
provide insights into the potential success or failure of the technology, as
well as its strengths and weaknesses. In this paper, we conduct a mixed-method
study using 10,732 tweets from early ChatGPT users. We first use topic
modelling to identify the main topics and then perform an in-depth qualitative
sentiment analysis of each topic. Our results show that the majority of the
early adopters have expressed overwhelmingly positive sentiments related to
topics such as Disruptions to software development, Entertainment and
exercising creativity. Only a limited percentage of users expressed concerns
about issues such as the potential for misuse of ChatGPT, especially regarding
topics such as Impact on educational aspects. We discuss these findings by
providing specific examples for each topic and then detail implications related
to addressing these concerns for both researchers and users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an early version of this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P-<span class="highlight-title">Transformer</span>: Towards Better Document-to-Document Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly training a document-to-document (Doc2Doc) neural machine translation
(NMT) via Transformer from scratch, especially on small datasets usually fails
to converge. Our dedicated probing tasks show that 1) both the absolute
position and relative position information gets gradually weakened or even
vanished once it reaches the upper encoder layers, and 2) the vanishing of
absolute position information in encoder output causes the training failure of
Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer
(P-Transformer) to enhance both the absolute and relative position information
in both self-attention and cross-attention. Specifically, we integrate absolute
positional information, i.e., position embeddings, into the query-key pairs
both in self-attention and cross-attention through a simple yet effective
addition operation. Moreover, we also integrate relative position encoding in
self-attention. The proposed P-Transformer utilizes sinusoidal position
encoding and does not require any task-specified position embedding, segment
embedding, or attention mechanism. Through the above methods, we build a
Doc2Doc NMT model with P-Transformer, which ingests the source document and
completely generates the target document in a sequence-to-sequence (seq2seq)
way. In addition, P-Transformer can be applied to seq2seq-based
document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)
translation. Extensive experimental results of Doc2Doc NMT show that
P-Transformer significantly outperforms strong baselines on widely-used 9
document-level datasets in 7 language pairs, covering small-, middle-, and
large-scales, and achieves a new state-of-the-art. Experimentation on discourse
phenomena shows that our Doc2Doc NMT models improve the translation quality in
both BLEU and discourse coherence. We make our code available on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TASLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Speech-to-speech Translation without Textual Annotation using
  Bottleneck Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhui Zhang, Junjie Pan, Xiang Yin, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-to-speech translation directly translates a speech utterance to
another between different languages, and has great potential in tasks such as
simultaneous interpretation. State-of-art models usually contains an auxiliary
module for phoneme sequences prediction, and this requires textual annotation
of the training dataset. We propose a direct speech-to-speech translation model
which can be trained without any textual annotation or content information.
Instead of introducing an auxiliary phoneme prediction task in the model, we
propose to use bottleneck features as intermediate training objectives for our
model to ensure the translation performance of the system. Experiments on
Mandarin-Cantonese speech translation demonstrate the feasibility of the
proposed approach and the performance can match a cascaded system with respect
of translation and synthesis qualities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Xu, Maria Biryukov, Martin Theobald, Vinu Ellampallil Venugopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex questions over textual resources remains a challenging
problem$\unicode{x2013}$especially when interpreting the fine-grained
relationships among multiple entities that occur within a natural-language
question or clue. Curated knowledge bases (KBs), such as YAGO, DBpedia,
Freebase and Wikidata, have been widely used in this context and gained great
acceptance for question-answering (QA) applications in the past decade. While
current KBs offer a concise representation of structured knowledge, they lack
the variety of formulations and semantic nuances as well as the context of
information provided by the natural-language sources. With BigText-QA, we aim
to develop an integrated QA system which is able to answer questions based on a
more redundant form of a knowledge graph (KG) that organizes both structured
and unstructured (i.e., "hybrid") knowledge in a unified graphical
representation. BigText-QA thereby is able to combine the best of both
worlds$\unicode{x2013}$a canonical set of named entities, mapped to a
structured background KB (such as YAGO or Wikidata), as well as an open set of
textual clauses providing highly diversified relational paraphrases with rich
context information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborating Heterogeneous Natural Language Processing Tasks via
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhe Dong, Yuexiang Xie, Bolin Ding, Ying Shen, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing privacy concerns on personal private text data promote the
development of federated learning (FL) in recent years. However, the existing
studies on applying FL in NLP are not suitable to coordinate participants with
heterogeneous or private learning objectives. In this study, we further broaden
the application scope of FL in NLP by proposing an Assign-Then-Contrast
(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks
to construct an FL course and learn useful knowledge from each other.
Specifically, the clients are suggested to first perform local training with
the unified tasks assigned by the server rather than using their own learning
objectives, which is called the Assign training stage. After that, in the
Contrast training stage, clients train with different local learning objectives
and exchange knowledge with other clients who contribute consistent and useful
model updates. We conduct extensive experiments on six widely-used datasets
covering both Natural Language Understanding (NLU) and Natural Language
Generation (NLG) tasks, and the proposed ATC framework achieves significant
improvements compared with various baseline methods. The source code is
available at
\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Natural Language Processing for Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingfu Zhu, Xianzhen Luo, Fang Liu, Cuiyun Gao, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing for programming, which aims to use NLP techniques
to assist programming, has experienced an explosion in recent years. However,
there is no literature that systematically reviews related work from the full
spectrum. In this paper, we comprehensively investigate existing work, ranging
from early deductive models to the latest competition-level models. Another
advantage of this paper is the completeness of the technique category, which
provides easy access to locating and comparing future works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Text Hallucination Reduction for Video-grounded
  Dialogue <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjae Yoon, Eunseop Yoon, Hee Suk Yoon, Junyeong Kim, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question
regarding a given video and dialogue context. Despite the recent success of
multi-modal reasoning to generate answer sentences, existing dialogue systems
still suffer from a text hallucination problem, which denotes indiscriminate
text-copying from input texts without an understanding of the question. This is
due to learning spurious correlations from the fact that answer sentences in
the dataset usually include the words of input texts, thus the VGD system
excessively relies on copying words from input texts by hoping those words to
overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating
(THAM) framework, which incorporates Text Hallucination Regularization (THR)
loss derived from the proposed information-theoretic text hallucination
measurement approach. Applying THAM with current dialogue systems validates the
effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows
enhanced interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, Accepted in EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation of <span class="highlight-title">Transformer</span>-Based Models using Unlabeled Data for
  Relevance and Polarity Classification of German Customer Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Idrissi-Yaghir, Henning Schäfer, Nadja Bauer, Christoph M. Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding customer feedback is becoming a necessity for companies to
identify problems and improve their products and services. Text classification
and sentiment analysis can play a major role in analyzing this data by using a
variety of machine and deep learning approaches. In this work, different
transformer-based models are utilized to explore how efficient these models are
when working with a German customer feedback dataset. In addition, these
pre-trained models are further analyzed to determine if adapting them to a
specific domain using unlabeled data can yield better results than
off-the-shelf pre-trained models. To evaluate the models, two downstream tasks
from the GermEval 2017 are considered. The experimental results show that
transformer-based models can reach significant improvements compared to a
fastText baseline and outperform the published scores and previous models. For
the subtask Relevance Classification, the best models achieve a micro-averaged
$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a
score of 85.1 % and 85.3 % for the subtask Polarity Classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to SN Computer Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Momentum Contrastive <span class="highlight-title">Pre-train</span>ing for Question Answering <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minda Hu, Muzhi Li, Yasheng Wang, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing pre-training methods for extractive Question Answering (QA) generate
cloze-like queries different from natural questions in syntax structure, which
could overfit pre-trained models to simple keyword matching. In order to
address this problem, we propose a novel Momentum Contrastive pRe-training fOr
queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS
introduces a momentum contrastive learning framework to align the answer
probability between cloze-like and natural query-passage sample pairs. Hence,
the pre-trained models can better transfer the knowledge learned in cloze-like
samples to answering natural questions. Experimental results on three
benchmarking QA datasets show that our method achieves noticeable improvement
compared with all baselines in both supervised and zero-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This has been accepted by EMNLP 2022. The reference to ACL Anthology
  will soon be added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching for Effective Multilingual Fine-Tuning Methods: A Case Study
  in Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Qin, Graham Neubig, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of tuning strategies have been proposed to adapt
pre-trained language models to downstream tasks. In this paper, we perform an
extensive empirical evaluation of various tuning strategies for multilingual
learning, particularly in the context of text summarization. Specifically, we
explore the relative advantages of three families of multilingual tuning
strategies (a total of five models) and empirically evaluate them for
summarization over 45 languages. Experimentally, we not only established a new
state-of-the-art on the XL-Sum dataset but also derive a series of observations
that hopefully can provide hints for future research on the design of
multilingual tuning strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Qin, Weizhe Yuan, Graham Neubig, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern embedding-based metrics for evaluation of generated text generally
fall into one of two paradigms: discriminative metrics that are trained to
directly predict which outputs are of higher quality according to supervised
human annotations, and generative metrics that are trained to evaluate text
based on the probabilities of a generative model. Both have their advantages;
discriminative metrics are able to directly optimize for the problem of
distinguishing between good and bad outputs, while generative metrics can be
trained using abundant raw text. In this paper, we present a framework that
combines the best of both worlds, using both supervised and unsupervised
signals from whatever data we have available. We operationalize this idea by
training T5Score, a metric that uses these training signals with mT5 as the
backbone. We perform an extensive empirical comparison with other existing
metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility
of our method. Experimental results show that: T5Score achieves the best
performance on all datasets against existing top-scoring metrics at the segment
level. We release our code and models at https://github.com/qinyiwei/T5Score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementing Deep Learning-Based Approaches for Article Summarization in
  Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Tangsali, Aabha Pingle, Aditya Vyawahare, Isha Joshi, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on text summarization for low-resource Indian languages has been
limited due to the availability of relevant datasets. This paper presents a
summary of various deep-learning approaches used for the ILSUM 2022 Indic
language summarization datasets. The ISUM 2022 dataset consists of news
articles written in Indian English, Hindi, and Gujarati respectively, and their
ground-truth summarizations. In our work, we explore different pre-trained
seq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,
the fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned
IndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS
model along with a translation mapping-based approach for Gujarati. Our scores
on the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4
as the evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ILSUM at FIRE 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensembling <span class="highlight-title">Transformer</span>s for Cross-domain Automatic Term Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanh Thi Hong Tran, Matej Martinc, Andraz Pelicon, Antoine Doucet, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic term extraction plays an essential role in domain language
understanding and several natural language processing downstream tasks. In this
paper, we propose a comparative study on the predictive power of
Transformers-based pretrained language models toward term extraction in a
multi-language cross-domain setting. Besides evaluating the ability of
monolingual models to extract single- and multi-word terms, we also experiment
with ensembles of mono- and multilingual models by conducting the intersection
or union on the term output sets of different language models. Our experiments
have been conducted on the ACTER corpus covering four specialized domains
(Corruption, Wind energy, Equitation, and Heart failure) and three languages
(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four
additional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The
results show that the strategy of employing monolingual models outperforms the
state-of-the-art approaches from the related work leveraging multilingual
models, regarding all the languages except Dutch and French if the term
extraction task excludes the extraction of named entity terms. Furthermore, by
combining the outputs of the two best performing models, we achieve significant
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages including references, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Explainable Identification of Logical Fallacies in Natural
  Language Arguments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhivar Sourati, Vishnu Priya Prasanna Venkatesh, Darshan Deshpande, Himanshu Rawlani, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spread of misinformation, propaganda, and flawed argumentation has been
amplified in the Internet era. Given the volume of data and the subtlety of
identifying violations of argumentation norms, supporting information analytics
tasks, like content moderation, with trustworthy methods that can identify
logical fallacies is essential. In this paper, we formalize prior theoretical
work on logical fallacies into a comprehensive three-stage evaluation framework
of detection, coarse-grained, and fine-grained classification. We adapt
existing evaluation datasets for each stage of the evaluation. We devise three
families of robust and explainable methods based on prototype reasoning,
instance-based reasoning, and knowledge injection. The methods are designed to
combine language models with background knowledge and explainable mechanisms.
Moreover, we address data sparsity with strategies for data augmentation and
curriculum learning. Our three-stage framework natively consolidates prior
datasets and methods from existing tasks, like propaganda detection, serving as
an overarching evaluation testbed. We extensively evaluate these methods on our
datasets, focusing on their robustness and explainability. Our results provide
insight into the strengths and weaknesses of the methods on different
components and fallacy classes, indicating that fallacy identification is a
challenging task that may require specialized forms of reasoning to capture
various classes. We share our open-source code and data on GitHub to support
further work on logical fallacy identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Building Text-To-Speech Systems for the Next Billion Users <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Karthik Kumar, Praveen S V, Pratyush Kumar, Mitesh M. Khapra, Karthik Nandakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based text-to-speech (TTS) systems have been evolving rapidly
with advances in model architectures, training methodologies, and
generalization across speakers and languages. However, these advances have not
been thoroughly investigated for Indian language speech synthesis. Such
investigation is computationally expensive given the number and diversity of
Indian languages, relatively lower resource availability, and the diverse set
of advances in neural TTS that remain untested. In this paper, we evaluate the
choice of acoustic models, vocoders, supplementary loss functions, training
schedules, and speaker and language diversity for Dravidian and Indo-Aryan
languages. Based on this, we identify monolingual models with FastPitch and
HiFi-GAN V1, trained jointly on male and female speakers to perform the best.
With this setup, we train and evaluate TTS models for 13 languages and find our
models to significantly improve upon existing models in all languages as
measured by mean opinion scores. We open-source all models on the Bhashini
platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICASSP 2023. Gokul and Praveen contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning <span class="highlight-title">Transformer</span>s: Vocabulary Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Mosin, Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, Ivan P. Yamshchikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are responsible for the vast majority of recent advances in
natural language processing. The majority of practical natural language
processing applications of these models are typically enabled through transfer
learning. This paper studies if corpus-specific tokenization used for
fine-tuning improves the resulting performance of the model. Through a series
of experiments, we demonstrate that such tokenization combined with the
initialization and fine-tuning strategy for the vocabulary tokens speeds up the
transfer and boosts the performance of the fine-tuned model. We call this
aspect of transfer facilitation vocabulary transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Parameter Left Behind: How Distillation and Model Size Affect
  Zero-Shot Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02873v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02873v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that small distilled language models are strong
competitors to models that are orders of magnitude larger and slower in a wide
range of information retrieval tasks. This has made distilled and dense models,
due to latency constraints, the go-to choice for deployment in real-world
retrieval applications. In this work, we question this practice by showing that
the number of parameters and early query-document interaction play a
significant role in the generalization ability of retrieval models. Our
experiments show that increasing model size results in marginal gains on
in-domain test sets, but much larger gains in new domains never seen during
fine-tuning. Furthermore, we show that rerankers largely outperform dense ones
of similar size in several tasks. Our largest reranker reaches the state of the
art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the
previous state of the art by 3 average points. Finally, we confirm that
in-domain effectiveness is not a good indicator of zero-shot effectiveness.
Code is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploration of A <span class="highlight-title">Self-Supervised</span> Speech Model: A Study on Emotional
  Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Yumnah Mohamied, Peter Bell, Catherine Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech models have grown fast during the past few years and
have proven feasible for use in various downstream tasks. Some recent work has
started to look at the characteristics of these models, yet many concerns have
not been fully addressed. In this work, we conduct a study on emotional corpora
to explore a popular self-supervised model -- wav2vec 2.0. Via a set of
quantitative analysis, we mainly demonstrate that: 1) wav2vec 2.0 appears to
discard paralinguistic information that is less useful for word recognition
purposes; 2) for emotion recognition, representations from the middle layer
alone perform as well as those derived from layer averaging, while the final
layer results in the worst performance in some cases; 3) current
self-supervised models may not be the optimal solution for downstream tasks
that make use of non-lexical features. Our work provides novel findings that
will aid future research in this area and theoretical basis for the use of
existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SLT 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNN-SL: Sequence Labeling Based on Nearest Examples via GNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02017v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02017v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Yuxian Meng, Rongbin Ouyang, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Guoyin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To better handle long-tail cases in the sequence labeling (SL) task, in this
work, we introduce graph neural networks sequence labeling (GNN-SL), which
augments the vanilla SL model output with similar tagging examples retrieved
from the whole training set. Since not all the retrieved tagging examples
benefit the model prediction, we construct a heterogeneous graph, and leverage
graph neural networks (GNNs) to transfer information between the retrieved
tagging examples and the input word sequence. The augmented node which
aggregates information from neighbors is used to do prediction. This strategy
enables the model to directly acquire similar tagging examples and improves the
general quality of predictions. We conduct a variety of experiments on three
typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech
Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant
performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2)
on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the
CWS task, and results comparable to SOTA performances on NER datasets, and POS
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural
  Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhui Zhang, Wudi Bao, Junjie Pan, Xiang Yin, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese dialects are different variations of Chinese and can be considered as
different languages in the same language family with Mandarin. Though they all
use Chinese characters, the pronunciations, grammar and idioms can vary
significantly, and even local speakers may find it hard to input correct
written forms of dialect. Besides, using Mandarin text as text-to-speech inputs
would generate speech with poor naturalness. In this paper, we propose a novel
Chinese dialect TTS frontend with a translation module, which converts Mandarin
text into dialectic expressions to improve the intelligibility and naturalness
of synthesized speech. A non-autoregressive neural machine translation model
with various tricks is proposed for the translation task. It is the first known
work to incorporate translation with TTS frontend. Experiments on Cantonese
show the proposed model improves 2.56 BLEU and TTS improves 0.27 MOS with
Mandarin inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dziri<span class="highlight-title">BERT</span>: a <span class="highlight-title">Pre-train</span>ed Language Model for the Algerian Dialect 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.12346v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.12346v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amine Abdaoui, Mohamed Berrimi, Mourad Oussalah, Abdelouahab Moussaoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained transformers are now the de facto models in Natural Language
Processing given their state-of-the-art results in many tasks and languages.
However, most of the current models have been trained on languages for which
large text resources are already available (such as English, French, Arabic,
etc.). Therefore, there are still a number of low-resource languages that need
more attention from the community. In this paper, we study the Algerian dialect
which has several specificities that make the use of Arabic or multilingual
models inappropriate. To address this issue, we collected more than one million
Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
When compared with existing models, DziriBERT achieves better results,
especially when dealing with the Roman script. The obtained results show that
pre-training a dedicated model on a small dataset (150 MB) can outperform
existing models that have been trained on much more data (hundreds of GB).
Finally, our model is publicly available to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05705v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05705v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bobo Li, Hao Fei, Fei Li, Yuhan Wu, Jinsong Zhang, Shengqiong Wu, Jingye Li, Yijiang Liu, Lizi Liao, Tat-Seng Chua, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of aspect-based sentiment analysis (ABSA) within recent
decades shows great potential for real-world society. The current ABSA works,
however, are mostly limited to the scenario of a single text piece, leaving the
study in dialogue contexts unexplored. In this work, we introduce a novel task
of conversational aspect-based sentiment quadruple analysis, namely DiaASQ,
aiming to detect the sentiment quadruple of target-aspect-opinion-sentiment in
a dialogue. DiaASQ bridges the gap between fine-grained sentiment analysis and
conversational opinion mining. We manually construct a large-scale high-quality
DiaASQ dataset in both Chinese and English languages. We deliberately develop a
neural model to benchmark the task, which advances in effectively performing
end-to-end quadruple prediction, and manages to incorporate rich
dialogue-specific and discourse feature representations for better
cross-utterance quadruple extraction. We finally point out several potential
future works to facilitate the follow-up research of this new task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Fallacy Detection <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13758v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13758v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning is central to human intelligence. However, fallacious arguments are
common, and some exacerbate problems such as spreading misinformation about
climate change. In this paper, we propose the task of logical fallacy
detection, and provide a new dataset (Logic) of logical fallacies generally
found in text, together with an additional challenge set for detecting logical
fallacies in climate change claims (LogicClimate). Detecting logical fallacies
is a hard problem as the model must understand the underlying logical structure
of the argument. We find that existing pretrained large language models perform
poorly on this task. In contrast, we show that a simple structure-aware
classifier outperforms the best language model by 5.46% on Logic and 4.51% on
LogicClimate. We encourage future work to explore this task as (a) it can serve
as a new reasoning challenge for language models, and (b) it can have potential
applications in tackling the spread of misinformation. Our dataset and code are
available at https://github.com/causalNLP/logical-fallacy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2021 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Benchmark for Understanding and Generating Dialogue between Characters
  in Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhu Yao, Ziqi Liu, Jian Guan, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many classical fairy tales, fiction, and screenplays leverage dialogue to
advance story plots and establish characters. We present the first study to
explore whether machines can understand and generate dialogue in stories, which
requires capturing traits of different characters and the relationships between
them. To this end, we propose two new tasks including Masked Dialogue
Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue
turns and predicting speakers for specified dialogue turns, respectively. We
build a new dataset DialStory, which consists of 105k Chinese stories with a
large amount of dialogue weaved into the plots to support the evaluation. We
show the difficulty of the proposed tasks by testing existing models with
automatic and manual evaluation on DialStory. Furthermore, we propose to learn
explicit character representations to improve performance on these tasks.
Extensive experiments and case studies show that our approach can generate more
coherent and informative dialogue, and achieve higher speaker recognition
accuracy than strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In Defense of Cross-Encoders for Zero-Shot Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bi-encoders and cross-encoders are widely used in many state-of-the-art
retrieval pipelines. In this work we study the generalization ability of these
two types of architectures on a wide range of parameter count on both in-domain
and out-of-domain scenarios. We find that the number of parameters and early
query-document interactions of cross-encoders play a significant role in the
generalization ability of retrieval models. Our experiments show that
increasing model size results in marginal gains on in-domain test sets, but
much larger gains in new domains never seen during fine-tuning. Furthermore, we
show that cross-encoders largely outperform bi-encoders of similar size in
several tasks. In the BEIR benchmark, our largest cross-encoder surpasses a
state-of-the-art bi-encoder by more than 4 average points. Finally, we show
that using bi-encoders as first-stage retrievers provides no gains in
comparison to a simpler retriever such as BM25 on out-of-domain tasks. The code
is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2206.02873</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Seed-Guided Topic Discovery by Integrating Multiple Types of
  Contexts <span class="chip">WSDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Yunyi Zhang, Martin Michalski, Yucheng Jiang, Yu Meng, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instead of mining coherent topics from a given text corpus in a completely
unsupervised manner, seed-guided topic discovery methods leverage user-provided
seed words to extract distinctive and coherent topics so that the mined topics
can better cater to the user's interest. To model the semantic correlation
between words and seeds for discovering topic-indicative terms, existing
seed-guided approaches utilize different types of context signals, such as
document-level word co-occurrences, sliding window-based local contexts, and
generic linguistic knowledge brought by pre-trained language models. In this
work, we analyze and show empirically that each type of context information has
its value and limitation in modeling word semantics under seed guidance, but
combining three types of contexts (i.e., word embeddings learned from local
contexts, pre-trained language model representations obtained from
general-domain training, and topic-indicative sentences retrieved based on seed
information) allows them to complement each other for discovering quality
topics. We propose an iterative framework, SeedTopicMine, which jointly learns
from the three types of contexts and gradually fuses their context signals via
an ensemble ranking process. Under various sets of seeds and on multiple
datasets, SeedTopicMine consistently yields more coherent and accurate topics
than existing seed-guided topic discovery approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages; Accepted to WSDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dirichlet-Survival Process: Scalable Inference of Topic-Dependent
  Diffusion Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Poux-Médard, Julien Velcin, Sabine Loudcher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information spread on networks can be efficiently modeled by considering
three features: documents' content, time of publication relative to other
publications, and position of the spreader in the network. Most previous works
model up to two of those jointly, or rely on heavily parametric approaches.
Building on recent Dirichlet-Point processes literature, we introduce the
Houston (Hidden Online User-Topic Network) model, that jointly considers all
those features in a non-parametric unsupervised framework. It infers dynamic
topic-dependent underlying diffusion networks in a continuous-time setting
along with said topics. It is unsupervised; it considers an unlabeled stream of
triplets shaped as \textit{(time of publication, information's content,
spreading entity)} as input data. Online inference is conducted using a
sequential Monte-Carlo algorithm that scales linearly with the size of the
dataset. Our approach yields consequent improvements over existing baselines on
both cluster recovery and subnetworks inference tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Mining-Based Patent Analysis for Automated Rule Checking in AEC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zheng, Bo-Rui Kang, Qi-Tian Yuan, Yu-Cheng Zhou, Xin-Zheng Lu, Jia-Rui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated rule checking (ARC), which is expected to promote the efficiency of
the compliance checking process in the architecture, engineering, and
construction (AEC) industry, is gaining increasing attention. Throwing light on
the ARC application hotspots and forecasting its trends are useful to the
related research and drive innovations. Therefore, this study takes the patents
from the database of the Derwent Innovations Index database (DII) and China
national knowledge infrastructure (CNKI) as data sources and then carried out a
three-step analysis including (1) quantitative characteristics (i.e., annual
distribution analysis) of patents, (2) identification of ARC topics using a
latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of
ARC topics. The results show that the research hotspots and trends of Chinese
and English patents are different. The contributions of this study have three
aspects: (1) an approach to a comprehensive analysis of patents by integrating
multiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the
application hotspots and development trends of ARC are reviewed based on patent
analysis; and (3) a signpost for technological development and innovation of
ARC is provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Low-Precision Training for Embeddings in Click-Through Rate
  Prediction <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Li, Huifeng Guo, Lu Hou, Wei Zhang, Xing Tang, Ruiming Tang, Rui Zhang, Ruixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding tables are usually huge in click-through rate (CTR) prediction
models. To train and deploy the CTR models efficiently and economically, it is
necessary to compress their embedding tables at the training stage. To this
end, we formulate a novel quantization training paradigm to compress the
embeddings from the training stage, termed low-precision training (LPT). Also,
we provide theoretical analysis on its convergence. The results show that
stochastic weight quantization has a faster convergence rate and a smaller
convergence error than deterministic weight quantization in LPT. Further, to
reduce the accuracy degradation, we propose adaptive low-precision training
(ALPT) that learns the step size (i.e., the quantization resolution) through
gradient descent. Experiments on two real-world datasets confirm our analysis
and show that ALPT can significantly improve the prediction accuracy,
especially at extremely low bit widths. For the first time in CTR models, we
successfully train 8-bit embeddings without sacrificing prediction accuracy.
The code of ALPT is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Sequential Learning via Hankel Matrix Representation for
  Next Item Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny Frolov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-attentive transformer models have recently been shown to solve the next
item recommendation task very efficiently. The learned attention weights
capture sequential dynamics in user behavior and generalize well. Motivated by
the special structure of learned parameter space, we question if it is possible
to mimic it with an alternative and more lightweight approach. We develop a new
tensor factorization-based model that ingrains the structural knowledge about
sequential data within the learning process. We demonstrate how certain
properties of a self-attention network can be reproduced with our approach
based on special Hankel matrix representation. The resulting model has a
shallow linear architecture and compares competitively to its neural
counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Roadmap to Domain Knowledge Integration in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himel Das Gupta, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many machine learning algorithms have been developed in recent years to
enhance the performance of a model in different aspects of artificial
intelligence. But the problem persists due to inadequate data and resources.
Integrating knowledge in a machine learning model can help to overcome these
obstacles up to a certain degree. Incorporating knowledge is a complex task
though because of various forms of knowledge representation. In this paper, we
will give a brief overview of these different forms of knowledge integration
and their performance in certain machine learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMNet: Multi-modal Fusion with Mutual Learning Network for Fake News
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linmei Hu, Ziwang Zhao, Xinkai Ge, Xuemeng Song, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of social media provides a hotbed for the dissemination
of fake news, which misleads readers and causes negative effects on society.
News usually involves texts and images to be more vivid. Consequently,
multi-modal fake news detection has received wide attention. Prior efforts
primarily conduct multi-modal fusion by simple concatenation or co-attention
mechanism, leading to sub-optimal performance. In this paper, we propose a
novel mutual learning network based model MMNet, which enhances the multi-modal
fusion for fake news detection via mutual learning between text- and
vision-centered views towards the same classification objective. Specifically,
we design two detection modules respectively based on text- and vision-centered
multi-modal fusion features, and enable the mutual learning of the two modules
to facilitate the multi-modal fusion, considering the latent consistency
between the two modules towards the same training objective. Moreover, we also
consider the influence of the image-text matching degree on news authenticity
judgement by designing an image-text matching aware co-attention mechanism for
multi-modal fusion. Extensive experiments are conducted on three benchmark
datasets and the results demonstrate that our proposed MMNet achieves superior
performance in fake news detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensembling <span class="highlight-title">Transformer</span>s for Cross-domain Automatic Term Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanh Thi Hong Tran, Matej Martinc, Andraz Pelicon, Antoine Doucet, Senja Pollak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic term extraction plays an essential role in domain language
understanding and several natural language processing downstream tasks. In this
paper, we propose a comparative study on the predictive power of
Transformers-based pretrained language models toward term extraction in a
multi-language cross-domain setting. Besides evaluating the ability of
monolingual models to extract single- and multi-word terms, we also experiment
with ensembles of mono- and multilingual models by conducting the intersection
or union on the term output sets of different language models. Our experiments
have been conducted on the ACTER corpus covering four specialized domains
(Corruption, Wind energy, Equitation, and Heart failure) and three languages
(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four
additional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The
results show that the strategy of employing monolingual models outperforms the
state-of-the-art approaches from the related work leveraging multilingual
models, regarding all the languages except Dutch and French if the term
extraction task excludes the extraction of named entity terms. Furthermore, by
combining the outputs of the two best performing models, we achieve significant
improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages including references, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Synthetic <span class="highlight-title">Dataset</span>s for Conversational Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Lara, Manoj Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For researchers leveraging Large-Language Models (LLMs) in the generation of
training datasets, especially for conversational recommender systems - the
absence of robust evaluation frameworks has been a long-standing problem. The
efficiency brought about by LLMs in the data generation phase is impeded during
the process of evaluation of the generated data, since it generally requires
human-raters to ensure that the data generated is of high quality and has
sufficient diversity. Since the quality of training data is critical for
downstream applications, it is important to develop metrics that evaluate the
quality holistically and identify biases. In this paper, we present a framework
that takes a multi-faceted approach towards evaluating datasets produced by
generative models and discuss the advantages and limitations of various
evaluation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Graph Neural Networks for Social Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Sharma, Yeon-Chang Lee, Sivagami Nambi, Aditya Salian, Shlok Shah, Sang-Wook Kim, Srijan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social recommender systems (SocialRS) simultaneously leverage user-to-item
interactions as well as user-to-user social relations for the task of
generating item recommendations to users. Additionally exploiting social
relations is clearly effective in understanding users' tastes due to the
effects of homophily and social influence. For this reason, SocialRS has
increasingly attracted attention. In particular, with the advance of Graph
Neural Networks (GNN), many GNN-based SocialRS methods have been developed
recently. Therefore, we conduct a comprehensive and systematic review of the
literature on GNN-based SocialRS. In this survey, we first identify 80 papers
on GNN-based SocialRS after annotating 2151 papers by following the PRISMA
framework (Preferred Reporting Items for Systematic Reviews and Meta-Analysis).
Then, we comprehensively review them in terms of their inputs and architectures
to propose a novel taxonomy: (1) input taxonomy includes 5 groups of input type
notations and 7 groups of input representation notations; (2) architecture
taxonomy includes 8 groups of GNN encoder, 2 groups of decoder, and 12 groups
of loss function notations. We classify the GNN-based SocialRS methods into
several categories as per the taxonomy and describe their details. Furthermore,
we summarize the benchmark datasets and metrics widely used to evaluate the
GNN-based SocialRS methods. Finally, we conclude this survey by presenting some
future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository with the curated list of papers:
  https://github.com/claws-lab/awesome-GNN-social-recsys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Parameter Left Behind: How Distillation and Model Size Affect
  Zero-Shot Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02873v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02873v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Moraes Rosa, Luiz Bonifacio, Vitor Jeronymo, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown that small distilled language models are strong
competitors to models that are orders of magnitude larger and slower in a wide
range of information retrieval tasks. This has made distilled and dense models,
due to latency constraints, the go-to choice for deployment in real-world
retrieval applications. In this work, we question this practice by showing that
the number of parameters and early query-document interaction play a
significant role in the generalization ability of retrieval models. Our
experiments show that increasing model size results in marginal gains on
in-domain test sets, but much larger gains in new domains never seen during
fine-tuning. Furthermore, we show that rerankers largely outperform dense ones
of similar size in several tasks. Our largest reranker reaches the state of the
art in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the
previous state of the art by 3 average points. Finally, we confirm that
in-domain effectiveness is not a good indicator of zero-shot effectiveness.
Code is available at
https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMNet: Multi-modal Fusion with Mutual Learning Network for Fake News
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linmei Hu, Ziwang Zhao, Xinkai Ge, Xuemeng Song, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of social media provides a hotbed for the dissemination
of fake news, which misleads readers and causes negative effects on society.
News usually involves texts and images to be more vivid. Consequently,
multi-modal fake news detection has received wide attention. Prior efforts
primarily conduct multi-modal fusion by simple concatenation or co-attention
mechanism, leading to sub-optimal performance. In this paper, we propose a
novel mutual learning network based model MMNet, which enhances the multi-modal
fusion for fake news detection via mutual learning between text- and
vision-centered views towards the same classification objective. Specifically,
we design two detection modules respectively based on text- and vision-centered
multi-modal fusion features, and enable the mutual learning of the two modules
to facilitate the multi-modal fusion, considering the latent consistency
between the two modules towards the same training objective. Moreover, we also
consider the influence of the image-text matching degree on news authenticity
judgement by designing an image-text matching aware co-attention mechanism for
multi-modal fusion. Extensive experiments are conducted on three benchmark
datasets and the results demonstrate that our proposed MMNet achieves superior
performance in fake news detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memories are One-to-Many Mapping Alleviators in Talking Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anni Tang, Tianyu He, Xu Tan, Jun Ling, Runnan Li, Sheng Zhao, Li Song, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation aims at generating photo-realistic video portraits of
a target person driven by input audio. Due to its nature of one-to-many mapping
from the input audio to the output video (e.g., one speech content may have
multiple feasible visual appearances), learning a deterministic mapping like
previous works brings ambiguity during training, and thus causes inferior
visual results. Although this one-to-many mapping could be alleviated in part
by a two-stage framework (i.e., an audio-to-expression model followed by a
neural-rendering model), it is still insufficient since the prediction is
produced without enough information (e.g., emotions, wrinkles, etc.). In this
paper, we propose MemFace to complement the missing information with an
implicit memory and an explicit memory that follow the sense of the two stages
respectively. More specifically, the implicit memory is employed in the
audio-to-expression model to capture high-level semantics in the
audio-expression shared space, while the explicit memory is employed in the
neural-rendering model to help synthesize pixel-level details. Our experimental
results show that our proposed MemFace surpasses all the state-of-the-art
results across multiple scenarios consistently and significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: see https://memoryface.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2022-12-11T00:00:00Z">2022-12-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Slang Representation Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravinda Kolla, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this paper contains content that may be offensive or upsetting.
Considering the large amount of content created online by the minute,
slang-aware automatic tools are critically needed to promote social good, and
assist policymakers and moderators in restricting the spread of offensive
language, abuse, and hate speech. Despite the success of large language models
and the spontaneous emergence of slang dictionaries, it is unclear how far
their combination goes in terms of slang understanding for downstream social
good tasks. In this paper, we provide a framework to study different
combinations of representation learning models and knowledge resources for a
variety of downstream tasks that rely on slang understanding. Our experiments
show the superiority of models that have been pre-trained on social media data,
while the impact of dictionaries is positive only for static word embeddings.
Our error analysis identifies core challenges for slang representation
learning, including out-of-vocabulary words, polysemy, variance, and annotation
disagreements, which can be traced to characteristics of slang as a quickly
evolving and highly subjective language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal and Explainable Internet Meme Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Kumar Thakur, Filip Ilievski, Hông-Ân Sandlin, Alain Mermoud, Zhivar Sourati, Luca Luceri, Riccardo Tommasini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this paper contains content that may be offensive or upsetting. In
the current context where online platforms have been effectively weaponized in
a variety of geo-political events and social issues, Internet memes make fair
content moderation at scale even more difficult. Existing work on meme
classification and tracking has focused on black-box methods that do not
explicitly consider the semantics of the memes or the context of their
creation. In this paper, we pursue a modular and explainable architecture for
Internet meme understanding. We design and implement multimodal classification
methods that perform example- and prototype-based reasoning over training
cases, while leveraging both textual and visual SOTA models to represent the
individual cases. We study the relevance of our modular and explainable models
in detecting harmful memes on two existing tasks: Hate Speech Detection and
Misogyny Classification. We compare the performance between example- and
prototype-based methods, and between text, vision, and multimodal models,
across different categories of harmfulness (e.g., stereotype and
objectification). We devise a user-friendly interface that facilitates the
comparative analysis of examples retrieved by all of our models for any given
meme, informing the community about the strengths and limitations of these
explainable methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastClass: A Time-Efficient Approach to Weakly-Supervised Text
  Classification <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Xia, Yue Wang, Yuan Tian, Yi Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised text classification aims to train a classifier using only
class descriptions and unlabeled data. Recent research shows that
keyword-driven methods can achieve state-of-the-art performance on various
tasks. However, these methods not only rely on carefully-crafted class
descriptions to obtain class-specific keywords but also require substantial
amount of unlabeled data and takes a long time to train. This paper proposes
FastClass, an efficient weakly-supervised classification approach. It uses
dense text representation to retrieve class-relevant documents from external
unlabeled corpus and selects an optimal subset to train a classifier. Compared
to keyword-driven methods, our approach is less reliant on initial class
descriptions as it no longer needs to expand each class description into a set
of class-specific keywords. Experiments on a wide range of classification tasks
show that the proposed approach frequently outperforms keyword-driven models in
terms of classification accuracy and often enjoys orders-of-magnitude faster
training speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Speech Translation of Arabic to English Broadcast News 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fethi Bougares, Salim Jouili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech translation (ST) is the task of directly translating acoustic speech
signals in a source language into text in a foreign language. ST task has been
addressed, for a long time, using a pipeline approach with two modules : first
an Automatic Speech Recognition (ASR) in the source language followed by a
text-to-text Machine translation (MT). In the past few years, we have seen a
paradigm shift towards the end-to-end approaches using sequence-to-sequence
deep neural network models. This paper presents our efforts towards the
development of the first Broadcast News end-to-end Arabic to English speech
translation system. Starting from independent ASR and MT LDC releases, we were
able to identify about 92 hours of Arabic audio recordings for which the manual
transcription was also translated into English at the segment level. These data
was used to train and compare pipeline and end-to-end speech translation
systems under multiple scenarios including transfer learning and data
augmentation techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arabic Natural Language Processing Workshop 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MORTY: Structured Summarization for Targeted Information Extraction from
  Scholarly Articles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Yaser Jaradeh, Markus Stocker, Sören Auer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction from scholarly articles is a challenging task due to
the sizable document length and implicit information hidden in text, figures,
and citations. Scholarly information extraction has various applications in
exploration, archival, and curation services for digital libraries and
knowledge management systems. We present MORTY, an information extraction
technique that creates structured summaries of text from scholarly articles.
Our approach condenses the article's full-text to property-value pairs as a
segmented text snippet called structured summary. We also present a sizable
scholarly dataset combining structured summaries retrieved from a scholarly
knowledge graph and corresponding publicly available scientific articles, which
we openly publish as a resource for the research community. Our results show
that structured summarization is a suitable approach for targeted information
extraction that complements other commonly used methods such as question
answering and named entity recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a short paper in ICADL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-Level Debiased Natural Language Understanding <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Piji Li, Yechang Yang, Maarten de Rijke, Pengjie Ren, Yukun Zhao, Dawei Yin, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing natural language understanding (NLU) models often rely on dataset
biases rather than intended task-relevant features to achieve high performance
on specific datasets. As a result, these models perform poorly on datasets
outside the training distribution. Some recent studies address the above issue
by reducing the weights of biased samples during the training process. However,
these methods still encode biased latent features in representations and
neglect the dynamic nature of bias, which hinders model prediction. We propose
an NLU debiasing method, named debiasing contrastive learning (DCT), to
simultaneously alleviate the above problems based on contrastive learning. We
devise a debiasing positive sampling strategy to mitigate biased latent
features by selecting the least similar biased positive samples. We also
propose a dynamic negative sampling strategy to capture the dynamic influence
of biases by employing a bias-only model to dynamically select the most similar
biased negative samples. We conduct experiments on three NLU benchmark
datasets. Experimental results show that DCT outperforms state-of-the-art
baselines on out-of-distribution datasets while maintaining in-distribution
performance. We also verify that DCT can reduce biased latent features from the
model's representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking About Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murray Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thanks to rapid progress in artificial intelligence, we have entered an era
when technology and philosophy intersect in interesting ways. Sitting squarely
at the centre of this intersection are large language models (LLMs). The more
adept LLMs become at mimicking human language, the more vulnerable we become to
anthropomorphism, to seeing the systems in which they are embedded as more
human-like than they really are. This trend is amplified by the natural
tendency to use philosophically loaded terms, such as "knows", "believes", and
"thinks", when describing these systems. To mitigate this trend, this paper
advocates the practice of repeatedly stepping back to remind ourselves of how
LLMs, and the systems of which they form a part, actually work. The hope is
that increased scientific precision will encourage more philosophical nuance in
the discourse around artificial intelligence, both within the field and in the
public sphere.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spread Love Not Hate: Undermining the Importance of Hateful <span class="highlight-title">Pre-train</span>ing
  for Hate Speech Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omkar Gokhale, Aditya Kane, Shantanu Patankar, Tanmay Chavan, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large neural language models, such as BERT, has led to
impressive gains on many natural language processing (NLP) tasks. Although this
method has proven to be effective for many domains, it might not always provide
desirable benefits. In this paper, we study the effects of hateful pre-training
on low-resource hate speech classification tasks. While previous studies on the
English language have emphasized its importance, we aim to augment their
observations with some non-obvious insights. We evaluate different variations
of tweet-based BERT models pre-trained on hateful, non-hateful, and mixed
subsets of a 40M tweet dataset. This evaluation is carried out for the Indian
languages Hindi and Marathi. This paper is empirical evidence that hateful
pre-training is not the best pre-training option for hate speech detection. We
show that pre-training on non-hateful text from the target domain provides
similar or better results. Further, we introduce HindTweetBERT and
MahaTweetBERT, the first publicly available BERT models pre-trained on Hindi
and Marathi tweets, respectively. We show that they provide state-of-the-art
performance on hate speech classification tasks. We also release hateful BERT
for the two languages and a gold hate speech evaluation benchmark HateEval-Hi
and HateEval-Mr consisting of manually labeled 2000 tweets each. The models and
data are available at https://github.com/l3cube-pune/MarathiNLP .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Knowledge Augmentation to Multi-tasking: Towards Human-like
  Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of building dialogue agents that can converse with humans naturally
has been a long-standing dream of researchers since the early days of
artificial intelligence. The well-known Turing Test proposed to judge the
ultimate validity of an artificial intelligence agent on the
indistinguishability of its dialogues from humans'. It should come as no
surprise that human-level dialogue systems are very challenging to build. But,
while early effort on rule-based systems found limited success, the emergence
of deep learning enabled great advance on this topic.
  In this thesis, we focus on methods that address the numerous issues that
have been imposing the gap between artificial conversational agents and
human-level interlocutors. These methods were proposed and experimented with in
ways that were inspired by general state-of-the-art AI methodologies. But they
also targeted the characteristics that dialogue systems possess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Learning with <span class="highlight-title">Prompt</span>-derived Virtual Semantic Prototypes for
  Unsupervised Sentence Embedding <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Zeng, Yongjing Yin, Yufan Jiang, Shuangzhi Wu, Yunbo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a new paradigm for unsupervised sentence
embeddings. Previous studies focus on instance-wise contrastive learning,
attempting to construct positive pairs with textual data augmentation. In this
paper, we propose a novel Contrastive learning method with Prompt-derived
Virtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,
we construct virtual semantic prototypes to each instance, and derive negative
prototypes by using the negative form of the prompts. Using a prototypical
contrastive loss, we enforce the anchor sentence embedding to be close to its
corresponding semantic prototypes, and far apart from the negative prototypes
as well as the prototypes of other sentences. Extensive experimental results on
semantic textual similarity, transfer, and clustering tasks demonstrate the
effectiveness of our proposed model compared to strong baselines. Code is
available at https://github.com/lemon0830/promptCSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLOOM: A 176B-Parameter Open-Access Multilingual Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        BigScience Workshop,  :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MORTY: Structured Summarization for Targeted Information Extraction from
  Scholarly Articles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Yaser Jaradeh, Markus Stocker, Sören Auer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information extraction from scholarly articles is a challenging task due to
the sizable document length and implicit information hidden in text, figures,
and citations. Scholarly information extraction has various applications in
exploration, archival, and curation services for digital libraries and
knowledge management systems. We present MORTY, an information extraction
technique that creates structured summaries of text from scholarly articles.
Our approach condenses the article's full-text to property-value pairs as a
segmented text snippet called structured summary. We also present a sizable
scholarly dataset combining structured summaries retrieved from a scholarly
knowledge graph and corresponding publicly available scientific articles, which
we openly publish as a resource for the research community. Our results show
that structured summarization is a suitable approach for targeted information
extraction that complements other commonly used methods such as question
answering and named entity recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a short paper in ICADL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Untargeted Attack against Federated Recommendation Systems via Poisonous
  Item Embeddings and the Defense <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Qi Liu, Likang Wu, Runlong Yu, Sanshi Lei Yu, Zaixi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendation (FedRec) can train personalized recommenders without
collecting user data, but the decentralized nature makes it susceptible to
poisoning attacks. Most previous studies focus on the targeted attack to
promote certain items, while the untargeted attack that aims to degrade the
overall performance of the FedRec system remains less explored. In fact,
untargeted attacks can disrupt the user experience and bring severe financial
loss to the service provider. However, existing untargeted attack methods are
either inapplicable or ineffective against FedRec systems. In this paper, we
delve into the untargeted attack and its defense for FedRec systems. (i) We
propose ClusterAttack, a novel untargeted attack method. It uploads poisonous
gradients that converge the item embeddings into several dense clusters, which
make the recommender generate similar scores for these items in the same
cluster and perturb the ranking order. (ii) We propose a uniformity-based
defense mechanism (UNION) to protect FedRec systems from such attacks. We
design a contrastive learning task that regularizes the item embeddings toward
a uniform distribution. Then the server filters out these malicious gradients
by estimating the uniformity of updated item embeddings. Experiments on two
public datasets show that ClusterAttack can effectively degrade the performance
of FedRec systems while circumventing many defense methods, and UNION can
improve the resistance of the system against various untargeted attacks,
including our ClusterAttack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can REF output quality scores be assigned by AI? Experimental evidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Thelwall, Kayvan Kousha, Mahshid Abdoli, Emma Stuart, Meiko Makita, Paul Wilson, Jonathan Levitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document describes strategies for using Artificial Intelligence (AI) to
predict some journal article scores in future research assessment exercises.
Five strategies have been assessed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relational Symmetry based Knowledge Graph Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Yue Liu, Sihang Zhou, Xinwang Liu, Wenxuan Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph embedding (KGE) aims to learn powerful representations to
benefit various artificial intelligence applications, such as question
answering and recommendations. Meanwhile, contrastive learning (CL), as an
effective mechanism to enhance the discriminative capacity of the learned
representations, has been leveraged in different fields, especially graph-based
models. However, since the structures of knowledge graphs (KGs) are usually
more complicated compared to homogeneous graphs, it is hard to construct
appropriate contrastive sample pairs. In this paper, we find that the entities
within a symmetrical structure are usually more similar and correlated. This
key property can be utilized to construct contrastive positive pairs for
contrastive learning. Following the ideas above, we propose a relational
symmetrical structure based knowledge graph contrastive learning framework,
termed KGE-SymCL, which leverages the symmetrical structure information in KGs
to enhance the discriminative ability of KGE models. Concretely, a
plug-and-play approach is designed by taking the entities in the relational
symmetrical positions as the positive samples. Besides, a self-supervised
alignment loss is used to pull together the constructed positive sample pairs
for contrastive learning. Extensive experimental results on benchmark datasets
have verified the good generalization and superiority of the proposed
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-modal Variational Auto-encoder for Content-based Micro-video
  Background Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yi, Yaochen Zhu, Jiayi Xie, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedding Knowledge Graph of Patent Metadata to Measure Knowledge
  Proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01768v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01768v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangtong Li, L Siddharth, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge proximity refers to the strength of association between any two
entities in a structural form that embodies certain aspects of a knowledge
base. In this work, we operationalize knowledge proximity within the context of
the US Patent Database (knowledge base) using a knowledge graph (structural
form) named PatNet built using patent metadata, including citations, inventors,
assignees, and domain classifications. We train various graph embedding models
using PatNet to obtain the embeddings of entities and relations. The cosine
similarity between the corresponding (or transformed) embeddings of entities
denotes the knowledge proximity between these. We compare the embedding models
in terms of their performances in predicting target entities and explaining
domain expansion profiles of inventors and assignees. We then apply the
embeddings of the best-preferred model to associate homogeneous (e.g.,
patent-patent) and heterogeneous (e.g., inventor-assignee) pairs of entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Fangzhao Wu, Chuhan Wu, Jingwei Yi, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News recommendation is a widely adopted technique to provide personalized
news feeds for the user. Recently, pre-trained language models (PLMs) have
demonstrated the great capability of natural language understanding and
benefited news recommendation via improving news modeling. However, most
existing works simply finetune the PLM with the news recommendation task, which
may suffer from the known domain shift problem between the pre-training corpus
and downstream news texts. Moreover, PLMs usually contain a large volume of
parameters and have high computational overhead, which imposes a great burden
on low-latency online services. In this paper, we propose Tiny-NewsRec, which
can improve both the effectiveness and the efficiency of PLM-based news
recommendation. We first design a self-supervised domain-specific post-training
method to better adapt the general PLM to the news domain with a contrastive
matching task between news titles and news bodies. We further propose a
two-stage knowledge distillation method to improve the efficiency of the large
PLM-based news recommendation model while maintaining its performance. Multiple
teacher models originated from different time steps of our post-training
procedure are used to transfer comprehensive knowledge to the student in both
its post-training and finetuning stage. Extensive experiments on two real-world
datasets validate the effectiveness and efficiency of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applicability limitations of differentiable full-reference image-quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siniukov Maksim, Dmitriy Kulikov, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subjective image-quality measurement plays a critical role in the development
of image-processing applications. The purpose of a visual-quality metric is to
approximate the results of subjective assessment. In this regard, more and more
metrics are under development, but little research has considered their
limitations. This paper addresses that deficiency: we show how image
preprocessing before compression can artificially increase the quality scores
provided by the popular metrics DISTS, LPIPS, HaarPSI, and VIF as well as how
these scores are inconsistent with subjective-quality scores. We propose a
series of neural-network preprocessing models that increase DISTS by up to
34.5%, LPIPS by up to 36.8%, VIF by up to 98.0%, and HaarPSI by up to 22.6% in
the case of JPEG-compressed images. A subjective comparison of preprocessed
images showed that for most of the metrics we examined, visual quality drops or
stays unchanged, limiting the applicability of these metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Complexity Assessment of Emerging Learned Image
  Compression on CPU and GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Pakdaman, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned Compression (LC) is the emerging technology for compressing image and
video content, using deep neural networks. Despite being new, LC methods have
already gained a compression efficiency comparable to state-of-the-art image
compression, such as HEVC or even VVC. However, the existing solutions often
require a huge computational complexity, which discourages their adoption in
international standards or products. This paper provides a comprehensive
complexity assessment of several notable methods, that shed light on the
matter, and guide the future development of this field by presenting key
findings. To do so, six existing methods have been evaluated for both encoding
and decoding, on CPU and GPU platforms. Various aspects of complexity such as
the overall complexity, share of each coding module, number of operations,
number of parameters, most demanding GPU kernels, and memory requirements have
been measured and compared on Kodak dataset. The reported results (1) quantify
the complexity of LC methods, (2) fairly compare different methods, and (3) a
major contribution of the work is identifying and quantifying the key factors
affecting the complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-modal Variational Auto-encoder for Content-based Micro-video
  Background Music Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yi, Yaochen Zhu, Jiayi Xie, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for
content-based micro-video background music recommendation. CMVAE is a
hierarchical Bayesian generative model that matches relevant background music
to a micro-video by projecting these two multimodal inputs into a shared
low-dimensional latent space, where the alignment of two corresponding
embeddings of a matched video-music pair is achieved by cross-generation.
Moreover, the multimodal information is fused by the product-of-experts (PoE)
principle, where the semantic information in visual and textual modalities of
the micro-video are weighted according to their variance estimations such that
the modality with a lower noise level is given more weights. Therefore, the
micro-video latent variables contain less irrelevant information that results
in a more robust model generalization. Furthermore, we establish a large-scale
content-based micro-video background music recommendation dataset, TT-150k,
composed of approximately 3,000 different background music clips associated to
150,000 micro-videos from different users. Extensive experiments on the
established TT-150k dataset demonstrate the effectiveness of the proposed
method. A qualitative assessment of CMVAE by visualizing some recommendation
results is also included.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2022-12-19T05:21:23.550059164Z">
            2022-12-19 05:21:23 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
