{"2022-12-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2204.07150v2","updated":"2022-12-14T17:23:11Z","published":"2022-04-14T17:57:53Z","title":"FREDA: Flexible Relation Extraction Data Annotation","summary":"  To effectively train accurate Relation Extraction models, sufficient and\nproperly labeled data is required. Adequately labeled data is difficult to\nobtain and annotating such data is a tricky undertaking. Previous works have\nshown that either accuracy has to be sacrificed or the task is extremely\ntime-consuming, if done accurately. We are proposing an approach in order to\nproduce high-quality datasets for the task of Relation Extraction quickly.\nNeural models, trained to do Relation Extraction on the created datasets,\nachieve very good results and generalize well to other datasets. In our study,\nwe were able to annotate 10,022 sentences for 19 relations in a reasonable\namount of time, and trained a commonly used baseline model for each relation.\n","authors":["Michael Strobl","Amine Trabelsi","Osmar Zaiane"],"pdf_url":"https://arxiv.org/pdf/2204.07150v2.pdf","comment":"Accepted at ACM SAC 2023 Knowledge and Natural Language Processing\n  track"},{"id":"http://arxiv.org/abs/2212.07284v1","updated":"2022-12-14T15:33:44Z","published":"2022-12-14T15:33:44Z","title":"MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End\n  Language Modeling","summary":"  Static subword tokenization algorithms have been an essential component of\nrecent works on language modeling. However, their static nature results in\nimportant flaws that degrade the models' downstream performance and robustness.\nIn this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.\nMANTa is a differentiable tokenizer trained end-to-end with the language model.\nThe resulting system offers a trade-off between the expressiveness of\nbyte-level models and the speed of models trained using subword tokenization.\nIn addition, our tokenizer is highly explainable since it produces an explicit\nsegmentation of sequences into blocks. We evaluate our pre-trained model on\nseveral English datasets from different domains as well as on synthetic noise.\nWe find that MANTa improves robustness to character perturbations and\nout-of-domain data. We then show that MANTa performs comparably to other models\non the general-domain GLUE benchmark. Finally, we show that it is considerably\nfaster than strictly byte-level models.\n","authors":["Nathan Godey","Roman Castagné","Éric de la Clergerie","Benoît Sagot"],"pdf_url":"https://arxiv.org/pdf/2212.07284v1.pdf","comment":"EMNLP 2022 Findings\n  (https://aclanthology.org/2022.findings-emnlp.207/)"},{"id":"http://arxiv.org/abs/2106.09553v3","updated":"2022-12-14T14:52:16Z","published":"2021-06-17T14:33:55Z","title":"Large-Scale Chemical Language Representations Capture Molecular\n  Structure and Properties","summary":"  Models based on machine learning can enable accurate and fast molecular\nproperty predictions, which is of interest in drug discovery and material\ndesign. Various supervised machine learning models have demonstrated promising\nperformance, but the vast chemical space and the limited availability of\nproperty labels make supervised learning challenging. Recently, unsupervised\ntransformer-based language models pretrained on a large unlabelled corpus have\nproduced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, we present molecular embeddings\nobtained by training an efficient transformer encoder model, MoLFormer, which\nuses rotary positional embeddings. This model employs a linear attention\nmechanism, coupled with highly distributed training, on SMILES sequences of 1.1\nbillion unlabelled molecules from the PubChem and ZINC datasets. We show that\nthe learned molecular representation outperforms existing baselines, including\nsupervised and self-supervised graph neural networks and language models, on\nseveral downstream tasks from ten benchmark datasets. They perform\ncompetitively on two others. Further analyses, specifically through the lens of\nattention, demonstrate that MoLFormer trained on chemical SMILES indeed learns\nthe spatial relationships between atoms within a molecule. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient chemical and structural information to predict various\ndistinct molecular properties, including quantum-chemical properties.\n","authors":["Jerret Ross","Brian Belgodere","Vijil Chenthamarakshan","Inkit Padhi","Youssef Mroueh","Payel Das"],"pdf_url":"https://arxiv.org/pdf/2106.09553v3.pdf","comment":"NMI 2022"},{"id":"http://arxiv.org/abs/2212.07249v1","updated":"2022-12-14T14:34:15Z","published":"2022-12-14T14:34:15Z","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","summary":"  Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n","authors":["Jiashuo Sun","Hang Zhang","Chen Lin","Yeyun Gong","Jian Guo","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2212.07249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06711v2","updated":"2022-12-14T13:54:19Z","published":"2022-12-13T16:29:51Z","title":"On Text-based Personality Computing: Challenges and Future Directions","summary":"  Text-based personality computing (TPC) has gained many research interests in\nNLP. In this paper, we describe 15 challenges that we consider deserving the\nattention of the research community. These challenges are organized by the\nfollowing topics: personality taxonomies, measurement quality, datasets,\nperformance evaluation, modelling choices, as well as ethics and fairness. When\naddressing each challenge, not only do we combine perspectives from both NLP\nand social sciences, but also offer concrete suggestions towards more valid and\nreliable TPC research.\n","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri","Laura Boeschoten","Erik-Jan van Kesteren","Mahdi Shafiee Kamalabad","Daniel L Oberski"],"pdf_url":"https://arxiv.org/pdf/2212.06711v2.pdf","comment":"Added acknowledgements"},{"id":"http://arxiv.org/abs/2212.07223v1","updated":"2022-12-14T13:48:32Z","published":"2022-12-14T13:48:32Z","title":"Evaluating Byte and Wordpiece Level Models for Massively Multilingual\n  Semantic Parsing","summary":"  Token free approaches have been successfully applied to a series of word and\nspan level tasks. In this work, we compare a byte-level (ByT5) and a wordpiece\nbased (mT5) sequence to sequence model on the 51 languages of the MASSIVE\nmultilingual semantic parsing dataset. We examine multiple experimental\nsettings: (i) zero-shot, (ii) full gold data and (iii) zero-shot with synthetic\ndata. By leveraging a state-of-the-art label projection method for machine\ntranslated examples, we are able to reduce the gap in exact match accuracy to\nonly 5 points with respect to a model trained on gold data from all the\nlanguages. We additionally provide insights on the cross-lingual transfer of\nByT5 and show how the model compares with respect to mT5 across all parameter\nsizes.\n","authors":["Massimo Nicosia","Francesco Piccinno"],"pdf_url":"https://arxiv.org/pdf/2212.07223v1.pdf","comment":"Massively Multilingual NLU 2022 Workshop Paper @ EMNLP 2022 - Winning\n  approach of the MMNLU-22 Zero-Shot Challenge"},{"id":"http://arxiv.org/abs/2212.07220v1","updated":"2022-12-14T13:41:49Z","published":"2022-12-14T13:41:49Z","title":"Understanding Translationese in Cross-Lingual Summarization","summary":"  Given a document in a source language, cross-lingual summarization (CLS) aims\nat generating a concise summary in a different target language. Unlike\nmonolingual summarization (MS), naturally occurring source-language documents\npaired with target-language summaries are rare. To collect large-scale CLS\nsamples, existing datasets typically involve translation in their creation.\nHowever, the translated text is distinguished from the text originally written\nin that language, i.e., translationese. Though many efforts have been devoted\nto CLS, none of them notice the phenomenon of translationese. In this paper, we\nfirst confirm that the different approaches to constructing CLS datasets will\nlead to different degrees of translationese. Then we design systematic\nexperiments to investigate how translationese affects CLS model evaluation and\nperformance when it appears in source documents or target summaries. In detail,\nwe find that (1) the translationese in documents or summaries of test sets\nmight lead to the discrepancy between human judgment and automatic evaluation;\n(2) the translationese in training sets would harm model performance in the\nreal scene; (3) though machine-translated documents involve translationese,\nthey are very useful for building CLS systems on low-resource languages under\nspecific training strategies. Furthermore, we give suggestions for future CLS\nresearch including dataset and model developments. We hope that our work could\nlet researchers notice the phenomenon of translationese in CLS and take it into\naccount in the future.\n","authors":["Jiaan Wang","Fandong Meng","Tingyi Zhang","Yunlong Liang","Jiarong Xu","Zhixu Li","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07220v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2212.07219v1","updated":"2022-12-14T13:41:36Z","published":"2022-12-14T13:41:36Z","title":"VTCC-NLP at NL4Opt competition subtask 1: An Ensemble Pre-trained\n  language models for Named Entity Recognition","summary":"  We propose a combined three pre-trained language models (XLM-R, BART, and\nDeBERTa-V3) as an empower of contextualized embedding for named entity\nrecognition. Our model achieves a 92.9% F1 score on the test set and ranks 5th\non the leaderboard at NL4Opt competition subtask 1.\n","authors":["Xuan-Dung Doan"],"pdf_url":"https://arxiv.org/pdf/2212.07219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07183v1","updated":"2022-12-14T12:13:34Z","published":"2022-12-14T12:13:34Z","title":"Mitigating Negative Style Transfer in Hybrid Dialogue System","summary":"  As the functionality of dialogue systems evolves, hybrid dialogue systems\nthat accomplish user-specific goals and participate in open-topic chitchat with\nusers are attracting growing attention. Existing research learns both tasks\nconcurrently utilizing a multi-task fusion technique but ignores the negative\ntransfer phenomenon induced by the unique textual style differences. Therefore,\ncontrastive learning based on the latent variable model is used to decouple the\nvarious textual genres in the latent space. We devise supervised and\nself-supervised positive and negative sample constructions for diverse\ndatasets. In addition, to capitalize on the style information contained in the\ndecoupled latent variables, we employ a style prefix that incorporates latent\nvariables further to control the generation of responses with varying styles.\nWe performed extensive experiments on three dialogue datasets, including a\nhybrid dialogue dataset and two task-oriented dialogue datasets. The\nexperimental results demonstrate that our method can mitigate the negative\nstyle transfer issue and achieves state-of-the-art performance on multiple\ndialogue datasets.\n","authors":["Shimin Li","Qinyuan Cheng","Linyang Li","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2212.07183v1.pdf","comment":"Accepted by AAAI-2023"},{"id":"http://arxiv.org/abs/2212.07172v1","updated":"2022-12-14T11:54:12Z","published":"2022-12-14T11:54:12Z","title":"Quotations, Coreference Resolution, and Sentiment Annotations in\n  Croatian News Articles: An Exploratory Study","summary":"  This paper presents a corpus annotated for the task of direct-speech\nextraction in Croatian. The paper focuses on the annotation of the quotation,\nco-reference resolution, and sentiment annotation in SETimes news corpus in\nCroatian and on the analysis of its language-specific differences compared to\nEnglish. From this, a list of the phenomena that require special attention when\nperforming these annotations is derived. The generated corpus with quotation\nfeatures annotations can be used for multiple tasks in the field of Natural\nLanguage Processing.\n","authors":["Jelena Sarajlić","Gaurish Thakkar","Diego Alves","Nives Mikelic Preradović"],"pdf_url":"https://arxiv.org/pdf/2212.07172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.12175v2","updated":"2022-12-14T11:42:14Z","published":"2021-08-27T08:40:08Z","title":"Grammar Based Speaker Role Identification for Air Traffic Control Speech\n  Recognition","summary":"  Automatic Speech Recognition (ASR) for air traffic control is generally\ntrained by pooling Air Traffic Controller (ATCO) and pilot data into one set.\nThis is motivated by the fact that pilot's voice communications are more scarce\nthan ATCOs. Due to this data imbalance and other reasons (e.g., varying\nacoustic conditions), the speech from ATCOs is usually recognized more\naccurately than from pilots. Automatically identifying the speaker roles is a\nchallenging task, especially in the case of the noisy voice recordings\ncollected using Very High Frequency (VHF) receivers or due to the\nunavailability of the push-to-talk (PTT) signal, i.e., both audio channels are\nmixed. In this work, we propose to (1) automatically segment the ATCO and pilot\ndata based on an intuitive approach exploiting ASR transcripts and (2)\nsubsequently consider an automatic recognition of ATCOs' and pilots' voice as\ntwo separate tasks. Our work is performed on VHF audio data with high noise\nlevels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is\nrecognized to be helpful for various speech-based machine-learning tasks.\nSpecifically, for the speaker role identification task, the module is\nrepresented by a simple yet efficient knowledge-based system exploiting a\ngrammar defined by the International Civil Aviation Organization (ICAO). The\nsystem accepts text as the input, either manually verified annotations or\nautomatically generated transcripts. The developed approach provides an average\naccuracy in speaker role identification of about 83%. Finally, we show that\ntraining an acoustic model for ASR tasks separately (i.e., separate models for\nATCOs and pilots) or using a multitask approach is well suited for the noisy\ndata and outperforms the traditional ASR system where all data is pooled\ntogether.\n","authors":["Amrutha Prasad","Juan Zuluaga-Gomez","Petr Motlicek","Saeed Sarfjoo","Iuliia Nigmatulina","Oliver Ohneiser","Hartmut Helmke"],"pdf_url":"https://arxiv.org/pdf/2108.12175v2.pdf","comment":"Presented at Sesar Innovation Days - 2022. See\n  https://www.sesarju.eu/sesarinnovationdays"},{"id":"http://arxiv.org/abs/2212.07164v1","updated":"2022-12-14T11:34:59Z","published":"2022-12-14T11:34:59Z","title":"Speech and Natural Language Processing Technologies for Pseudo-Pilot\n  Simulator","summary":"  This paper describes a simple yet efficient repetition-based modular system\nfor speeding up air-traffic controllers (ATCos) training. E.g., a human pilot\nis still required in EUROCONTROL's ESCAPE lite simulator (see\nhttps://www.eurocontrol.int/simulator/escape) during ATCo training. However,\nthis need can be substituted by an automatic system that could act as a pilot.\nIn this paper, we aim to develop and integrate a pseudo-pilot agent into the\nATCo training pipeline by merging diverse artificial intelligence (AI) powered\nmodules. The system understands the voice communications issued by the ATCo,\nand, in turn, it generates a spoken prompt that follows the pilot's phraseology\nto the initial communication. Our system mainly relies on open-source AI tools\nand air traffic control (ATC) databases, thus, proving its simplicity and ease\nof replicability. The overall pipeline is composed of the following: (1) a\nsubmodule that receives and pre-processes the input stream of raw audio, (2) an\nautomatic speech recognition (ASR) system that transforms audio into a sequence\nof words; (3) a high-level ATC-related entity parser, which extracts relevant\ninformation from the communication, i.e., callsigns and commands, and finally,\n(4) a speech synthesizer submodule that generates responses based on the\nhigh-level ATC entities previously extracted. Overall, we show that this system\ncould pave the way toward developing a real proof-of-concept pseudo-pilot\nsystem. Hence, speeding up the training of ATCos while drastically reducing its\noverall cost.\n","authors":["Amrutha Prasad","Juan Zuluaga-Gomez","Petr Motlicek","Saeed Sarfjoo","Iuliia Nigmatulina","Karel Vesely"],"pdf_url":"https://arxiv.org/pdf/2212.07164v1.pdf","comment":"Presented at Sesar Innovation Days 2022.\n  https://www.sesarju.eu/sesarinnovationdays"},{"id":"http://arxiv.org/abs/2212.07162v1","updated":"2022-12-14T11:32:24Z","published":"2022-12-14T11:32:24Z","title":"Building and Evaluating Universal Named-Entity Recognition English\n  corpus","summary":"  This article presents the application of the Universal Named Entity framework\nto generate automatically annotated corpora. By using a workflow that extracts\nWikipedia data and meta-data and DBpedia information, we generated an English\ndataset which is described and evaluated. Furthermore, we conducted a set of\nexperiments to improve the annotations in terms of precision, recall, and\nF1-measure. The final dataset is available and the established workflow can be\napplied to any language with existing Wikipedia and DBpedia. As part of future\nresearch, we intend to continue improving the annotation process and extend it\nto other languages.\n","authors":["Diego Alves","Gaurish Thakkar","Marko Tadić"],"pdf_url":"https://arxiv.org/pdf/2212.07162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07160v1","updated":"2022-12-14T11:29:03Z","published":"2022-12-14T11:29:03Z","title":"Multi-task Learning for Cross-Lingual Sentiment Analysis","summary":"  This paper presents a cross-lingual sentiment analysis of news articles using\nzero-shot and few-shot learning. The study aims to classify the Croatian news\narticles with positive, negative, and neutral sentiments using the Slovene\ndataset. The system is based on a trilingual BERT-based model trained in three\nlanguages: English, Slovene, Croatian. The paper analyses different setups\nusing datasets in two languages and proposes a simple multi-task model to\nperform sentiment classification. The evaluation is performed using the\nfew-shot and zero-shot scenarios in single-task and multi-task experiments for\nCroatian and Slovene.\n","authors":["Gaurish Thakkar","Nives Mikelic Preradovic","Marko Tadic"],"pdf_url":"https://arxiv.org/pdf/2212.07160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07156v1","updated":"2022-12-14T11:10:03Z","published":"2022-12-14T11:10:03Z","title":"MIST: a Large-Scale Annotated Resource and Neural Models for Functions\n  of Modal Verbs in English Scientific Text","summary":"  Modal verbs (e.g., \"can\", \"should\", or \"must\") occur highly frequently in\nscientific articles. Decoding their function is not straightforward: they are\noften used for hedging, but they may also denote abilities and restrictions.\nUnderstanding their meaning is important for various NLP tasks such as writing\nassistance or accurate information extraction from scientific text.\n  To foster research on the usage of modals in this genre, we introduce the\nMIST (Modals In Scientific Text) dataset, which contains 3737 modal instances\nin five scientific domains annotated for their semantic, pragmatic, or\nrhetorical function. We systematically evaluate a set of competitive neural\narchitectures on MIST. Transfer experiments reveal that leveraging\nnon-scientific data is of limited benefit for modeling the distinctions in\nMIST. Our corpus analysis provides evidence that scientific communities differ\nin their usage of modal verbs, yet, classifiers trained on scientific data\ngeneralize to some extent to unseen scientific domains.\n","authors":["Sophie Henning","Nicole Macher","Stefan Grünewald","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2212.07156v1.pdf","comment":"20 pages, 7 figures. Accepted to EMNLP Findings 2022; typesetting of\n  this version slightly differs from conference version"},{"id":"http://arxiv.org/abs/2210.08753v4","updated":"2022-12-14T10:20:53Z","published":"2022-10-17T05:16:23Z","title":"MCP: Self-supervised Pre-training for Personalized Chatbots with\n  Multi-level Contrastive Sampling","summary":"  Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.\n","authors":["Zhaoheng Huang","Zhicheng Dou","Yutao Zhu","Zhengyi Ma"],"pdf_url":"https://arxiv.org/pdf/2210.08753v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07127v1","updated":"2022-12-14T09:26:07Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07126v1","updated":"2022-12-14T09:25:49Z","published":"2022-12-14T09:25:49Z","title":"Explainability of Text Processing and Retrieval Methods: A Critical\n  Survey","summary":"  Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.\n","authors":["Sourav Saha","Debapriyo Majumdar","Mandar Mitra"],"pdf_url":"https://arxiv.org/pdf/2212.07126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07112v1","updated":"2022-12-14T09:05:14Z","published":"2022-12-14T09:05:14Z","title":"DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service\n  Chatlog","summary":"  Harvesting question-answer (QA) pairs from customer service chatlog in the\nwild is an efficient way to enrich the knowledge base for customer service\nchatbots in the cold start or continuous integration scenarios. Prior work\nattempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which\nfails to integrate the incomplete utterances from the dialog context for\ncomposite QA retrieval. In this paper, we propose N-to-N QA extraction task in\nwhich the derived questions and corresponding answers might be separated across\ndifferent utterances. We introduce a suite of generative/discriminative tagging\nbased methods with end-to-end and two-stage variants that perform well on 5\ncustomer service datasets and for the first time setup a benchmark for N-to-N\nDialogQAE with utterance and session level evaluation metrics. With a deep dive\ninto extracted QA pairs, we find that the relations between and inside the QA\npairs can be indicators to analyze the dialogue structure, e.g. information\nseeking, clarification, barge-in and elaboration. We also show that the\nproposed models can adapt to different domains and languages, and reduce the\nlabor cost of knowledge accumulation in the real-world product dialogue\nplatform.\n","authors":["Xin Zheng","Tianyu Liu","Haoran Meng","Xu Wang","Yufan Jiang","Mengliang Rao","Binghuai Lin","Zhifang Sui","Yunbo Cao"],"pdf_url":"https://arxiv.org/pdf/2212.07112v1.pdf","comment":"Preprint version; The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2210.12958v2","updated":"2022-12-14T08:04:07Z","published":"2022-10-24T05:30:02Z","title":"Composition, Attention, or Both?","summary":"  In this paper, we propose a novel architecture called Composition Attention\nGrammars (CAGs) that recursively compose subtrees into a single vector\nrepresentation with a composition function, and selectively attend to previous\nstructural information with a self-attention mechanism. We investigate whether\nthese components -- the composition function and the self-attention mechanism\n-- can both induce human-like syntactic generalization. Specifically, we train\nlanguage models (LMs) with and without these two components with the model\nsizes carefully controlled, and evaluate their syntactic generalization\nperformance against six test circuits on the SyntaxGym benchmark. The results\ndemonstrated that the composition function and the self-attention mechanism\nboth play an important role to make LMs more human-like, and closer inspection\nof linguistic phenomenon implied that the composition function allowed\nsyntactic features, but not semantic features, to percolate into subtree\nrepresentations.\n","authors":["Ryo Yoshida","Yohei Oseki"],"pdf_url":"https://arxiv.org/pdf/2210.12958v2.pdf","comment":"Accepted by Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.06369v2","updated":"2022-12-14T07:58:10Z","published":"2022-12-13T04:57:04Z","title":"Technical Report -- Competition Solution for Prompt Tuning using\n  Pretrained Language Model","summary":"  Prompt tuning recently becomes a hot-spot in the applications of large\npretrained language models on specific downstream tasks. Regarding the Language\nModel as a Service (LMaaS), black-box tuning using derivative-free optimization\n(DFO) provides a novel approach to expand the practical scenarios of pretrained\nmodels and enrich the researches of few-shot learning. In this report, we\npresent our solution in this competition that is based on the LMaaS scenario.\nOur solution consists of several modifications to BBTv2, including multiple\nlabel words, selection of P0, rolling update strategy, multi-task loss from MLP\nclassifier, and finally using the ensemble method to further improve\ngeneralization ability. We also shared some strategies that we tried but didn't\nuse in the final submission for further discussion. In the end we raised a\nquestion about the SNLI dataset and the impact on the results, as well as our\nconcerns about the competition.\n","authors":["Jiang-Long Song","Wu-He Zou","Feng Li","Xiao-Lei Qin"],"pdf_url":"https://arxiv.org/pdf/2212.06369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07075v1","updated":"2022-12-14T07:52:36Z","published":"2022-12-14T07:52:36Z","title":"Cross-Modal Similarity-Based Curriculum Learning for Image Captioning","summary":"  Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.\n","authors":["Hongkuan Zhang","Saku Sugawara","Akiko Aizawa","Lei Zhou","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2212.07075v1.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.07072v1","updated":"2022-12-14T07:48:42Z","published":"2022-12-14T07:48:42Z","title":"SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation","summary":"  Word Sense Disambiguation (WSD) is an NLP task aimed at determining the\ncorrect sense of a word in a sentence from discrete sense choices. Although\ncurrent systems have attained unprecedented performances for such tasks, the\nnonuniform distribution of word senses during training generally results in\nsystems performing poorly on rare senses. To this end, we consider data\naugmentation to increase the frequency of these least frequent senses (LFS) to\nreduce the distributional bias of senses during training. We propose\nSense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that\nmaintains the sense of a target word. SMSMix smoothly blends two sentences\nusing mask prediction while preserving the relevant span determined by saliency\nscores to maintain a specific word's sense. To the best of our knowledge, this\nis the first attempt to apply mixup in NLP while preserving the meaning of a\nspecific word. With extensive experiments, we validate that our augmentation\nmethod can effectively give more information about rare senses during training\nwith maintained target sense label.\n","authors":["Hee Suk Yoon","Eunseop Yoon","John Harvill","Sunjae Yoon","Mark Hasegawa-Johnson","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2212.07072v1.pdf","comment":"Accepted to Findings of EMNLP2022"},{"id":"http://arxiv.org/abs/2002.10710v4","updated":"2022-12-14T06:40:45Z","published":"2020-02-25T07:49:12Z","title":"End-to-end Emotion-Cause Pair Extraction via Learning to Link","summary":"  Emotion-cause pair extraction (ECPE), as an emergent natural language\nprocessing task, aims at jointly investigating emotions and their underlying\ncauses in documents. It extends the previous emotion cause extraction (ECE)\ntask, yet without requiring a set of pre-given emotion clauses as in ECE.\nExisting approaches to ECPE generally adopt a two-stage method, i.e., (1)\nemotion and cause detection, and then (2) pairing the detected emotions and\ncauses. Such pipeline method, while intuitive, suffers from two critical\nissues, including error propagation across stages that may hinder the\neffectiveness, and high computational cost that would limit the practical\napplication of the method. To tackle these issues, we propose a multi-task\nlearning model that can extract emotions, causes and emotion-cause pairs\nsimultaneously in an end-to-end manner. Specifically, our model regards pair\nextraction as a link prediction task, and learns to link from emotion clauses\nto cause clauses, i.e., the links are directional. Emotion extraction and cause\nextraction are incorporated into the model as auxiliary tasks, which further\nboost the pair extraction. Experiments are conducted on an ECPE benchmarking\ndataset. The results show that our proposed model outperforms a range of\nstate-of-the-art approaches.\n","authors":["Haolin Song","Chen Zhang","Qiuchi Li","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2002.10710v4.pdf","comment":"7 pages, 3 figures, 5 tables, code is available at\n  https://github.com/shl5133/E2EECPE"},{"id":"http://arxiv.org/abs/2212.07043v1","updated":"2022-12-14T05:36:18Z","published":"2022-12-14T05:36:18Z","title":"AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach","summary":"  Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).\nIt is a well-studied topic in several resource-rich languages. However, the\ndevelopment of computational linguistic resources is still in its infancy\ndespite the existence of numerous languages that are historically and literary\nrich. Assamese, an Indian scheduled language, spoken by more than 25 million\npeople, falls under this category. In this paper, we present a Deep Learning\n(DL)-based POS tagger for Assamese. The development process is divided into two\nstages. In the first phase, several pre-trained word embeddings are employed to\ntrain several tagging models. This allows us to evaluate the performance of the\nword embeddings in the POS tagging task. The top-performing model from the\nfirst phase is employed to annotate another set of new sentences. In the second\nphase, the model is trained further using the fresh dataset. Finally, we attain\na tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for\nfurther study on DL-based Assamese POS tagging.\n","authors":["Dhrubajyoti Pathak","Sukumar Nandi","Priyankoo Sarmah"],"pdf_url":"https://arxiv.org/pdf/2212.07043v1.pdf","comment":"Accepted in AICCSA 2022"},{"id":"http://arxiv.org/abs/2201.12502v3","updated":"2022-12-14T03:17:16Z","published":"2022-01-29T05:56:35Z","title":"Unsupervised Multi-Granularity Summarization","summary":"  Text summarization is a user-preference based task, i.e., for one document,\nusers often have different priorities for summary. As a key aspect of\ncustomization in summarization, granularity is used to measure the semantic\ncoverage between the summary and source document. However, developing systems\nthat can generate summaries with customizable semantic coverage is still an\nunder-explored topic. In this paper, we propose the first unsupervised\nmulti-granularity summarization framework, GranuSum. We take events as the\nbasic semantic units of the source documents and propose to rank these events\nby their salience. We also develop a model to summarize input documents with\ngiven events as anchors and hints. By inputting different numbers of events,\nGranuSum is capable of producing multi-granular summaries in an unsupervised\nmanner. Meanwhile, we annotate a new benchmark GranuDUC that contains multiple\nsummaries at different granularities for each document cluster. Experimental\nresults confirm the substantial superiority of GranuSum on multi-granularity\nsummarization over strong baselines. Further, by exploiting the event\ninformation, GranuSum also exhibits state-of-the-art performance under the\nconventional unsupervised abstractive setting. Dataset for this paper can be\nfound at: https://github.com/maszhongming/GranuDUC\n","authors":["Ming Zhong","Yang Liu","Suyu Ge","Yuning Mao","Yizhu Jiao","Xingxing Zhang","Yichong Xu","Chenguang Zhu","Michael Zeng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2201.12502v3.pdf","comment":"EMNLP 2022 Findings"},{"id":"http://arxiv.org/abs/2209.14901v2","updated":"2022-12-14T02:51:59Z","published":"2022-09-29T16:05:53Z","title":"DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language\n  Processing","summary":"  The meaningful use of electronic health records (EHR) continues to progress\nin the digital era with clinical decision support systems augmented by\nartificial intelligence. A priority in improving provider experience is to\novercome information overload and reduce the cognitive burden so fewer medical\nerrors and cognitive biases are introduced during patient care. One major type\nof medical error is diagnostic error due to systematic or predictable errors in\njudgment that rely on heuristics. The potential for clinical natural language\nprocessing (cNLP) to model diagnostic reasoning in humans with forward\nreasoning from data to diagnosis and potentially reduce the cognitive burden\nand medical error has not been investigated. Existing tasks to advance the\nscience in cNLP have largely focused on information extraction and named entity\nrecognition through classification tasks. We introduce a novel suite of tasks\ncoined as Diagnostic Reasoning Benchmarks, DR.BENCH, as a new benchmark for\ndeveloping and evaluating cNLP models with clinical diagnostic reasoning\nability. The suite includes six tasks from ten publicly available datasets\naddressing clinical text understanding, medical knowledge reasoning, and\ndiagnosis generation. DR.BENCH is the first clinical suite of tasks designed to\nbe a natural language generation framework to evaluate pre-trained language\nmodels. Experiments with state-of-the-art pre-trained generative language\nmodels using large general domain models and models that were continually\ntrained on a medical corpus demonstrate opportunities for improvement when\nevaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab\nrepository with a systematic approach to load and evaluate models for the cNLP\ncommunity.\n","authors":["Yanjun Gao","Dmitriy Dligach","Timothy Miller","John Caskey","Brihat Sharma","Matthew M Churpek","Majid Afshar"],"pdf_url":"https://arxiv.org/pdf/2209.14901v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2210.12902v2","updated":"2022-12-14T02:03:39Z","published":"2022-10-24T01:15:06Z","title":"Event-Centric Question Answering via Contrastive Learning and Invertible\n  Event Transformation","summary":"  Human reading comprehension often requires reasoning of event semantic\nrelations in narratives, represented by Event-centric Question-Answering (QA).\nTo address event-centric QA, we propose a novel QA model with contrastive\nlearning and invertible event transformation, call TranCLR. Our proposed model\nutilizes an invertible transformation matrix to project semantic vectors of\nevents into a common event embedding space, trained with contrastive learning,\nand thus naturally inject event semantic knowledge into mainstream QA\npipelines. The transformation matrix is fine-tuned with the annotated event\nrelation types between events that occurred in questions and those in answers,\nusing event-aware question vectors. Experimental results on the Event Semantic\nRelation Reasoning (ESTER) dataset show significant improvements in both\ngenerative and extractive settings compared to the existing strong baselines,\nachieving over 8.4% gain in the token-level F1 score and 3.0% gain in Exact\nMatch (EM) score under the multi-answer setting. Qualitative analysis reveals\nthe high quality of the generated answers by TranCLR, demonstrating the\nfeasibility of injecting event knowledge into QA model learning. Our code and\nmodels can be found at https://github.com/LuJunru/TranCLR.\n","authors":["Junru Lu","Xingwei Tan","Gabriele Pergola","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2210.12902v2.pdf","comment":"Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.06972v1","updated":"2022-12-14T01:37:35Z","published":"2022-12-14T01:37:35Z","title":"Disentangling Prosody Representations with Unsupervised Speech\n  Reconstruction","summary":"  Human speech can be characterized by different components, including semantic\ncontent, speaker identity and prosodic information. Significant progress has\nbeen made in disentangling representations for semantic content and speaker\nidentity in Automatic Speech Recognition (ASR) and speaker verification tasks\nrespectively. However, it is still an open challenging research question to\nextract prosodic information because of the intrinsic association of different\nattributes, such as timbre and rhythm, and because of the need for unsupervised\ntraining schemes to achieve robust large-scale and speaker-independent ASR. The\naim of this paper is to address the disentanglement of emotional prosody from\nspeech based on unsupervised reconstruction. Specifically, we identify, design,\nimplement and integrate three crucial components in our proposed speech\nreconstruction model Prosody2Vec: (1) a unit encoder that transforms speech\nsignals into discrete units for semantic content, (2) a pretrained speaker\nverification model to generate speaker identity embeddings, and (3) a trainable\nprosody encoder to learn prosody representations. We first pretrain the\nProsody2Vec representations on unlabelled emotional speech corpora, then\nfine-tune the model on specific datasets to perform Speech Emotion Recognition\n(SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective\nevaluations on the EVC task suggest that Prosody2Vec effectively captures\ngeneral prosodic features that can be smoothly transferred to other emotional\nspeech. In addition, our SER experiments on the IEMOCAP dataset reveal that the\nprosody features learned by Prosody2Vec are complementary and beneficial for\nthe performance of widely used speech pretraining models and surpass the\nstate-of-the-art methods when combining Prosody2Vec with HuBERT\nrepresentations. Some audio samples can be found on our demo website.\n","authors":["Leyuan Qu","Taihao Li","Cornelius Weber","Theresa Pekarek-Rosin","Fuji Ren","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2212.06972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06971v1","updated":"2022-12-14T01:37:16Z","published":"2022-12-14T01:37:16Z","title":"Find Someone Who: Visual Commonsense Understanding in Human-Centric\n  Grounding","summary":"  From a visual scene containing multiple people, human is able to distinguish\neach individual given the context descriptions about what happened before,\ntheir mental/physical states or intentions, etc. Above ability heavily relies\non human-centric commonsense knowledge and reasoning. For example, if asked to\nidentify the \"person who needs healing\" in an image, we need to first know that\nthey usually have injuries or suffering expressions, then find the\ncorresponding visual clues before finally grounding the person. We present a\nnew commonsense task, Human-centric Commonsense Grounding, that tests the\nmodels' ability to ground individuals given the context descriptions about what\nhappened before, and their mental/physical states or intentions. We further\ncreate a benchmark, HumanCog, a dataset with 130k grounded commonsensical\ndescriptions annotated on 67k images, covering diverse types of commonsense and\nvisual scenes. We set up a context-object-aware method as a strong baseline\nthat outperforms previous pre-trained and non-pretrained models. Further\nanalysis demonstrates that rich visual commonsense and powerful integration of\nmulti-modal commonsense are essential, which sheds light on future works. Data\nand code will be available https://github.com/Hxyou/HumanCog.\n","authors":["Haoxuan You","Rui Sun","Zhecan Wang","Kai-Wei Chang","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2212.06971v1.pdf","comment":"11 pages, 7 figures. EMNLP 2022-findings"},{"id":"http://arxiv.org/abs/2212.05546v2","updated":"2022-12-14T00:35:10Z","published":"2022-12-11T17:15:02Z","title":"Associations Between Natural Language Processing (NLP) Enriched Social\n  Determinants of Health and Suicide Death among US Veterans","summary":"  Importance: Social determinants of health (SDOH) are known to be associated\nwith increased risk of suicidal behaviors, but few studies utilized SDOH from\nunstructured electronic health record (EHR) notes.\n  Objective: To investigate associations between suicide and recent SDOH,\nidentified using structured and unstructured data.\n  Design: Nested case-control study.\n  Setting: EHR data from the US Veterans Health Administration (VHA).\n  Participants: 6,122,785 Veterans who received care in the US VHA between\nOctober 1, 2010, and September 30, 2015.\n  Exposures: Occurrence of SDOH over a maximum span of two years compared with\nno occurrence of SDOH.\n  Main Outcomes and Measures: Cases of suicide deaths were matched with 4\ncontrols on birth year, cohort entry date, sex, and duration of follow-up. We\ndeveloped an NLP system to extract SDOH from unstructured notes. Structured\ndata, NLP on unstructured data, and combining them yielded seven, eight and\nnine SDOH respectively. Adjusted odds ratios (aORs) and 95% confidence\nintervals (CIs) were estimated using conditional logistic regression.\n  Results: In our cohort, 8,821 Veterans committed suicide during 23,725,382\nperson-years of follow-up (incidence rate 37.18 /100,000 person-years). Our\ncohort was mostly male (92.23%) and white (76.99%). Across the six common SDOH\nas covariates, NLP-extracted SDOH, on average, covered 84.38% of all SDOH\noccurrences. All SDOH, measured by structured data and NLP, were significantly\nassociated with increased risk of suicide. The SDOH with the largest effects\nwas legal problems (aOR=2.67, 95% CI=2.46-2.89), followed by violence\n(aOR=2.26, 95% CI=2.11-2.43). NLP-extracted and structured SDOH were also\nassociated with suicide.\n  Conclusions and Relevance: NLP-extracted SDOH were always significantly\nassociated with increased risk of suicide among Veterans, suggesting the\npotential of NLP in public health studies.\n","authors":["Avijit Mitra","Richeek Pradhan","Rachel D Melamed","Kun Chen","David C Hoaglin","Katherine L Tucker","Joel I Reisman","Zhichao Yang","Weisong Liu","Jack Tsai","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2212.05546v2.pdf","comment":"Submitted at JAMA Network Open"},{"id":"http://arxiv.org/abs/2212.06950v1","updated":"2022-12-14T00:03:52Z","published":"2022-12-14T00:03:52Z","title":"Pre-trained Language Models can be Fully Zero-Shot Learners","summary":"  How can we extend a pre-trained model to many language understanding tasks,\nwithout labeled or additional unlabeled data? Pre-trained language models\n(PLMs) have been effective for a wide range of NLP tasks. However, existing\napproaches either require fine-tuning on downstream labeled datasets or\nmanually constructing proper prompts. In this paper, we propose nonparametric\nprompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike\nprevious methods, NPPrompt uses only pre-trained language models and does not\nrequire any labeled data or additional raw corpus for further fine-tuning, nor\ndoes it rely on humans to construct a comprehensive set of prompt label words.\nWe evaluate NPPrompt against previous major few-shot and zero-shot learning\nmethods on diverse NLP tasks: including text classification, text entailment,\nsimilar text retrieval, and paraphrasing. Experimental results demonstrate that\nour NPPrompt outperforms the previous best fully zero-shot method by big\nmargins, with absolute gains of 12.8% in accuracy on text classification and\n18.9% on the GLUE benchmark.\n","authors":["Xuandong Zhao","Siqi Ouyang","Zhiguo Yu","Ming Wu","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2212.06950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01328v2","updated":"2022-12-14T23:42:24Z","published":"2022-06-02T22:55:51Z","title":"Augmenting Scientific Creativity with Retrieval across Knowledge Domains","summary":"  Exposure to ideas in domains outside a scientist's own may benefit her in\nreformulating existing research problems in novel ways and discovering new\napplication domains for existing solution ideas. While improved performance in\nscholarly search engines can help scientists efficiently identify relevant\nadvances in domains they may already be familiar with, it may fall short of\nhelping them explore diverse ideas \\textit{outside} such domains. In this paper\nwe explore the design of systems aimed at augmenting the end-user ability in\ncross-domain exploration with flexible query specification. To this end, we\ndevelop an exploratory search system in which end-users can select a portion of\ntext core to their interest from a paper abstract and retrieve papers that have\na high similarity to the user-selected core aspect but differ in terms of\ndomains. Furthermore, end-users can `zoom in' to specific domain clusters to\nretrieve more papers from them and understand nuanced differences within the\nclusters. Our case studies with scientists uncover opportunities and design\nimplications for systems aimed at facilitating cross-domain exploration and\ninspiration.\n","authors":["Hyeonsu B. Kang","Sheshera Mysore","Kevin Huang","Haw-Shiuan Chang","Thorben Prein","Andrew McCallum","Aniket Kittur","Elsa Olivetti"],"pdf_url":"https://arxiv.org/pdf/2206.01328v2.pdf","comment":"NLP+HCI Workshop at NAACL 2022"},{"id":"http://arxiv.org/abs/2212.07549v1","updated":"2022-12-14T23:41:57Z","published":"2022-12-14T23:41:57Z","title":"ReDDIT: Regret Detection and Domain Identification from Text","summary":"  In this paper, we present a study of regret and its expression on social\nmedia platforms. Specifically, we present a novel dataset of Reddit texts that\nhave been classified into three classes: Regret by Action, Regret by Inaction,\nand No Regret. We then use this dataset to investigate the language used to\nexpress regret on Reddit and to identify the domains of text that are most\ncommonly associated with regret. Our findings show that Reddit users are most\nlikely to express regret for past actions, particularly in the domain of\nrelationships. We also found that deep learning models using GloVe embedding\noutperformed other models in all experiments, indicating the effectiveness of\nGloVe for representing the meaning and context of words in the domain of\nregret. Overall, our study provides valuable insights into the nature and\nprevalence of regret on social media, as well as the potential of deep learning\nand word embeddings for analyzing and understanding emotional language in\nonline text. These findings have implications for the development of natural\nlanguage processing algorithms and the design of social media platforms that\nsupport emotional expression and communication.\n","authors":["Fazlourrahman Balouchzahi","Sabur Butt","Grigori Sidorov","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2212.07549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07547v1","updated":"2022-12-14T23:31:14Z","published":"2022-12-14T23:31:14Z","title":"Unsupervised Detection of Contextualized Embedding Bias with Application\n  to Ideology","summary":"  We propose a fully unsupervised method to detect bias in contextualized\nembeddings. The method leverages the assortative information latently encoded\nby social networks and combines orthogonality regularization, structured\nsparsity learning, and graph neural networks to find the embedding subspace\ncapturing this information. As a concrete example, we focus on the phenomenon\nof ideological bias: we introduce the concept of an ideological subspace, show\nhow it can be found by applying our method to online discussion forums, and\npresent techniques to probe it. Our experiments suggest that the ideological\nsubspace encodes abstract evaluative semantics and reflects changes in the\npolitical left-right spectrum during the presidency of Donald Trump.\n","authors":["Valentin Hofmann","Janet B. Pierrehumbert","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2212.07547v1.pdf","comment":"ICML 2022"},{"id":"http://arxiv.org/abs/2212.07542v1","updated":"2022-12-14T22:57:44Z","published":"2022-12-14T22:57:44Z","title":"Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent\n  Recognition and Question Answering Architecture","summary":"  As artificial intelligence (AI) becomes a prominent part of modern life, AI\nliteracy is becoming important for all citizens, not just those in technology\ncareers. Previous research in AI education materials has largely focused on the\nintroduction of terminology as well as AI use cases and ethics, but few allow\nstudents to learn by creating their own machine learning models. Therefore,\nthere is a need for enriching AI educational tools with more adaptable and\nflexible platforms for interested educators with any level of technical\nexperience to utilize within their teaching material. As such, we propose the\ndevelopment of an open-source tool (Build-a-Bot) for students and teachers to\nnot only create their own transformer-based chatbots based on their own course\nmaterial, but also learn the fundamentals of AI through the model creation\nprocess. The primary concern of this paper is the creation of an interface for\nstudents to learn the principles of artificial intelligence by using a natural\nlanguage pipeline to train a customized model to answer questions based on\ntheir own school curriculums. The model uses contexts given by their\ninstructor, such as chapters of a textbook, to answer questions and is deployed\non an interactive chatbot/voice agent. The pipeline teaches students data\ncollection, data augmentation, intent recognition, and question answering by\nhaving them work through each of these processes while creating their AI agent,\ndiverging from previous chatbot work where students and teachers use the bots\nas black-boxes with no abilities for customization or the bots lack AI\ncapabilities, with the majority of dialogue scripts being rule-based. In\naddition, our tool is designed to make each step of this pipeline intuitive for\nstudents at a middle-school level. Further work primarily lies in providing our\ntool to schools and seeking student and teacher evaluations.\n","authors":["Kate Pearce","Sharifa Alghowinem","Cynthia Breazeal"],"pdf_url":"https://arxiv.org/pdf/2212.07542v1.pdf","comment":"Accepted for presentation at EAAI-23"},{"id":"http://arxiv.org/abs/2212.07538v1","updated":"2022-12-14T22:51:49Z","published":"2022-12-14T22:51:49Z","title":"Leveraging Natural Language Processing to Augment Structured Social\n  Determinants of Health Data in the Electronic Health Record","summary":"  Objective: Social Determinants of Health (SDOH) influence personal health\noutcomes and health systems interactions. Health systems capture SDOH\ninformation through structured data and unstructured clinical notes; however,\nclinical notes often contain a more comprehensive representation of several key\nSDOH. The objective of this work is to assess the SDOH information gain\nachievable by extracting structured semantic representations of SDOH from the\nclinical narrative and combining these extracted representations with available\nstructured data.\n  Materials and Methods: We developed a natural language processing (NLP)\ninformation extraction model for SDOH that utilizes a deep learning entity and\nrelation extraction architecture. In an electronic health record (EHR) case\nstudy, we applied the SDOH extractor to a large existing clinical data set with\nover 200,000 patients and 400,000 notes and compared the extracted information\nwith available structured data.\n  Results: The SDOH extractor achieved 0.86 F1 on a withheld test set. In the\nEHR case study, we found 19\\% of current tobacco users, 10\\% of drug users, and\n32\\% of homeless patients only include documentation of these risk factors in\nthe clinical narrative.\n  Conclusions: Patients who are at-risk for negative health outcomes due to\nSDOH may be better served if health systems are able to identify SDOH risk\nfactors and associated social needs. Structured semantic representations of\ntext-encoded SDOH information can augment existing structured, and this more\ncomprehensive SDOH representation can assist health systems in identifying and\naddressing social needs.\n","authors":["Kevin Lybarger","Nicholas J Dobbins","Ritche Long","Angad Singh","Patrick Wedgeworth","Ozlem Ozuner","Meliha Yetisgen"],"pdf_url":"https://arxiv.org/pdf/2212.07538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07530v1","updated":"2022-12-14T22:30:55Z","published":"2022-12-14T22:30:55Z","title":"Causes and Cures for Interference in Multilingual Translation","summary":"  Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.\n","authors":["Uri Shaham","Maha Elbayad","Vedanuj Goswami","Omer Levy","Shruti Bhosale"],"pdf_url":"https://arxiv.org/pdf/2212.07530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07526v1","updated":"2022-12-14T22:13:55Z","published":"2022-12-14T22:13:55Z","title":"Relationship Between Online Harmful Behaviors and Social Network Message\n  Writing Style","summary":"  In this paper, we explore the relationship between an individual's writing\nstyle and the risk that they will engage in online harmful behaviors (such as\ncyberbullying). In particular, we consider whether measurable differences in\nwriting style relate to different personality types, as modeled by the Big-Five\npersonality traits and the Dark Triad traits, and can differentiate between\nusers who do or do not engage in harmful behaviors. We study messages from\nnearly 2,500 users from two online communities (Twitter and Reddit) and find\nthat we can measure significant personality differences between regular and\nharmful users from the writing style of as few as 100 tweets or 40 Reddit\nposts, aggregate these values to distinguish between healthy and harmful\ncommunities, and also use style attributes to predict which users will engage\nin harmful behaviors.\n","authors":["Talia Sanchez Viera","Richard Khoury"],"pdf_url":"https://arxiv.org/pdf/2212.07526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07525v1","updated":"2022-12-14T22:13:11Z","published":"2022-12-14T22:13:11Z","title":"Efficient Self-supervised Learning with Contextualized Target\n  Representations for Vision, Speech and Language","summary":"  Current self-supervised learning algorithms are often modality-specific and\nrequire large amounts of computational resources. To address these issues, we\nincrease the training efficiency of data2vec, a learning objective that\ngeneralizes across several modalities. We do not encode masked tokens, use a\nfast convolutional decoder and amortize the effort to build teacher\nrepresentations. data2vec 2.0 benefits from the rich contextualized target\nrepresentations introduced in data2vec which enable a fast self-supervised\nlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0\nmatches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,\non Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x\nless time, and on GLUE natural language understanding it matches a retrained\nRoBERTa model in half the time. Trading some speed for accuracy results in\nImageNet-1K top-1 accuracy of 86.8\\% with a ViT-L model trained for 150 epochs.\n","authors":["Alexei Baevski","Arun Babu","Wei-Ning Hsu","Michael Auli"],"pdf_url":"https://arxiv.org/pdf/2212.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.08829v3","updated":"2022-12-14T22:04:14Z","published":"2021-04-18T11:48:25Z","title":"Modeling Ideological Salience and Framing in Polarized Online Groups\n  with Graph Neural Networks and Structured Sparsity","summary":"  The increasing polarization of online political discourse calls for\ncomputational tools that automatically detect and monitor ideological divides\nin social media. We introduce a minimally supervised method that leverages the\nnetwork structure of online discussion forums, specifically Reddit, to detect\npolarized concepts. We model polarization along the dimensions of salience and\nframing, drawing upon insights from moral psychology. Our architecture combines\ngraph neural networks with structured sparsity learning and results in\nrepresentations for concepts and subreddits that capture temporal ideological\ndynamics such as right-wing and left-wing radicalization.\n","authors":["Valentin Hofmann","Xiaowen Dong","Janet B. Pierrehumbert","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2104.08829v3.pdf","comment":"NAACL 2022 (Findings)"},{"id":"http://arxiv.org/abs/2011.09410v5","updated":"2022-12-14T21:25:54Z","published":"2020-11-18T17:10:02Z","title":"A Definition and a Test for Human-Level Artificial Intelligence","summary":"  Despite recent advances of AI research in many application-specific domains,\nwe do not know how to build a human-level artificial intelligence (HLAI). We\nconjecture that learning from others' experience with the language is the\nessential characteristic that distinguishes human intelligence from the rest.\nHumans can update the action-value function with the verbal description as if\nthey experience states, actions, and corresponding rewards sequences firsthand.\nIn this paper, we present a classification of intelligence according to how\nindividual agents learn and propose a definition and a test for HLAI. The main\nidea is that language acquisition without explicit rewards can be a sufficient\ntest for HLAI.\n","authors":["Deokgun Park","Md Ashaduzzaman Rubel Mondol","Aishwarya Pothula","Mazharul Islam"],"pdf_url":"https://arxiv.org/pdf/2011.09410v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07507v1","updated":"2022-12-14T21:13:08Z","published":"2022-12-14T21:13:08Z","title":"Artificial Intelligence for Health Message Generation: Theory, Method,\n  and an Empirical Study Using Prompt Engineering","summary":"  This study introduces and examines the potential of an AI system to generate\nhealth awareness messages. The topic of folic acid, a vitamin that is critical\nduring pregnancy, served as a test case. Using prompt engineering, we generated\nmessages that could be used to raise awareness and compared them to retweeted\nhuman-generated messages via computational and human evaluation methods. The\nsystem was easy to use and prolific, and computational analyses revealed that\nthe AI-generated messages were on par with human-generated ones in terms of\nsentiment, reading ease, and semantic content. Also, the human evaluation study\nshowed that AI-generated messages ranked higher in message quality and clarity.\nWe discuss the theoretical, practical, and ethical implications of these\nresults.\n","authors":["Sue Lim","Ralf Schmälzle"],"pdf_url":"https://arxiv.org/pdf/2212.07507v1.pdf","comment":"26 pages including references, 3 figures"},{"id":"http://arxiv.org/abs/2212.07476v1","updated":"2022-12-14T19:50:35Z","published":"2022-12-14T19:50:35Z","title":"The Infinite Index: Information Retrieval on Generative Text-To-Image\n  Models","summary":"  The text-to-image model Stable Diffusion has recently become very popular.\nOnly weeks after its open source release, millions are experimenting with image\ngeneration. This is due to its ease of use, since all it takes is a brief\ndescription of the desired image to \"prompt\" the generative model. Rarely do\nthe images generated for a new prompt immediately meet the user's expectations.\nUsually, an iterative refinement of the prompt (\"prompt engineering\") is\nnecessary for satisfying images. As a new perspective, we recast image prompt\nengineering as interactive image retrieval - on an \"infinite index\". Thereby, a\nprompt corresponds to a query and prompt engineering to query refinement.\nSelected image-prompt pairs allow direct relevance feedback, as the model can\nmodify an image for the refined prompt. This is a form of one-sided interactive\nretrieval, where the initiative is on the user side, whereas the server side\nremains stateless. In light of an extensive literature review, we develop these\nparallels in detail and apply the findings to a case study of a creative search\ntask on such a model. We note that the uncertainty in searching an infinite\nindex is virtually never-ending. We also discuss future research opportunities\nrelated to retrieval models specialized for generative models and interactive\ngenerative image retrieval. The application of IR technology, such as query\nreformulation and relevance feedback, will contribute to improved workflows\nwhen using generative models, while the notion of an infinite index raises new\nchallenges in IR research.\n","authors":["Niklas Deckers","Maik Fröbe","Johannes Kiesel","Gianluca Pandolfo","Christopher Schröder","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2212.07476v1.pdf","comment":"Accepted at CHIIR 2023"},{"id":"http://arxiv.org/abs/2212.07429v1","updated":"2022-12-14T11:38:48Z","published":"2022-12-14T11:38:48Z","title":"Building Multilingual Corpora for a Complex Named Entity Recognition and\n  Classification Hierarchy using Wikipedia and DBpedia","summary":"  With the ever-growing popularity of the field of NLP, the demand for datasets\nin low resourced-languages follows suit. Following a previously established\nframework, in this paper, we present the UNER dataset, a multilingual and\nhierarchical parallel corpus annotated for named-entities. We describe in\ndetail the developed procedure necessary to create this type of dataset in any\nlanguage available on Wikipedia with DBpedia information. The three-step\nprocedure extracts entities from Wikipedia articles, links them to DBpedia, and\nmaps the DBpedia sets of classes to the UNER labels. This is followed by a\npost-processing procedure that significantly increases the number of identified\nentities in the final results. The paper concludes with a statistical and\nqualitative analysis of the resulting dataset.\n","authors":["Diego Alves","Gaurish Thakkar","Gabriel Amaral","Tin Kuculo","Marko Tadić"],"pdf_url":"https://arxiv.org/pdf/2212.07429v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2212.07162"},{"id":"http://arxiv.org/abs/2212.07428v1","updated":"2022-12-14T10:50:13Z","published":"2022-12-14T10:50:13Z","title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural\n  Language Inference","summary":"  We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n","authors":["Maren Pielka","Svetlana Schmidt","Lisa Pucknat","Rafet Sifa"],"pdf_url":"https://arxiv.org/pdf/2212.07428v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2212.07422v1","updated":"2022-12-14T18:59:19Z","published":"2022-12-14T18:59:19Z","title":"ECON: Explicit Clothed humans Obtained from Normals","summary":"  The combination of artist-curated scans, and deep implicit functions (IF), is\nenabling the creation of detailed, clothed, 3D humans from images. However,\nexisting methods are far from perfect. IF-based methods recover free-form\ngeometry but produce disembodied limbs or degenerate shapes for unseen poses or\nclothes. To increase robustness for these cases, existing work uses an explicit\nparametric body model to constrain surface reconstruction, but this limits the\nrecovery of free-form surfaces such as loose clothing that deviates from the\nbody. What we want is a method that combines the best properties of implicit\nand explicit methods. To this end, we make two key observations: (1) current\nnetworks are better at inferring detailed 2D maps than full-3D surfaces, and\n(2) a parametric model can be seen as a \"canvas\" for stitching together\ndetailed surface patches. ECON infers high-fidelity 3D humans even in loose\nclothes and challenging poses, while having realistic faces and fingers. This\ngoes beyond previous methods. Quantitative, evaluation of the CAPE and\nRenderpeople datasets shows that ECON is more accurate than the state of the\nart. Perceptual studies also show that ECON's perceived realism is better by a\nlarge margin. Code and models are available for research purposes at\nhttps://xiuyuliang.cn/econ\n","authors":["Yuliang Xiu","Jinlong Yang","Xu Cao","Dimitrios Tzionas","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2212.07422v1.pdf","comment":"Homepage: https://xiuyuliang.cn/econ Code:\n  https://github.com/YuliangXiu/ECON"},{"id":"http://arxiv.org/abs/2212.07413v1","updated":"2022-12-14T18:54:13Z","published":"2022-12-14T18:54:13Z","title":"Towards Smooth Video Composition","summary":"  Video generation requires synthesizing consistent and persistent frames with\ndynamic content over time. This work investigates modeling the temporal\nrelations for composing video with arbitrary length, from a few frames to even\ninfinite, using generative adversarial networks (GANs). First, towards\ncomposing adjacent frames, we show that the alias-free operation for single\nimage generation, together with adequately pre-learned knowledge, brings a\nsmooth frame transition without compromising the per-frame quality. Second, by\nincorporating the temporal shift module (TSM), originally designed for video\nunderstanding, into the discriminator, we manage to advance the generator in\nsynthesizing more consistent dynamics. Third, we develop a novel B-Spline based\nmotion representation to ensure temporal smoothness to achieve infinite-length\nvideo generation. It can go beyond the frame number used in training. A\nlow-rank temporal modulation is also proposed to alleviate repeating contents\nfor long video generation. We evaluate our approach on various datasets and\nshow substantial improvements over video generation baselines. Code and models\nwill be publicly available at https://genforce.github.io/StyleSV.\n","authors":["Qihang Zhang","Ceyuan Yang","Yujun Shen","Yinghao Xu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07409v1","updated":"2022-12-14T18:49:50Z","published":"2022-12-14T18:49:50Z","title":"Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion","summary":"  StyleGAN has achieved great progress in 2D face reconstruction and semantic\nediting via image inversion and latent editing. While studies over extending 2D\nStyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion\nframework is still missing, limiting the applications of 3D face reconstruction\nand semantic editing. In this paper, we study the challenging problem of 3D GAN\ninversion where a latent code is predicted given a single face image to\nfaithfully recover its 3D shapes and detailed textures. The problem is\nill-posed: innumerable compositions of shape and texture could be rendered to\nthe current image. Furthermore, with the limited capacity of a global latent\ncode, 2D inversion methods cannot preserve faithful shape and texture at the\nsame time when applied to 3D models. To solve this problem, we devise an\neffective self-training scheme to constrain the learning of inversion. The\nlearning is done efficiently without any real-world 2D-3D training pairs but\nproxy samples generated from a 3D GAN. In addition, apart from a global latent\ncode that captures the coarse shape and texture information, we augment the\ngeneration network with a local branch, where pixel-aligned features are added\nto faithfully reconstruct face details. We further consider a new pipeline to\nperform 3D view-consistent editing. Extensive experiments show that our method\noutperforms state-of-the-art inversion methods in both shape and texture\nreconstruction quality. Code and data will be released.\n","authors":["Yushi Lan","Xuyi Meng","Shuai Yang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2212.07409v1.pdf","comment":"An encoder-based 3D GAN inversion method. Project page:\n  https://nirvanalan.github.io/projects/E3DGE/index.html"},{"id":"http://arxiv.org/abs/2212.07401v1","updated":"2022-12-14T18:34:29Z","published":"2022-12-14T18:34:29Z","title":"BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos","summary":"  Quantifying motion in 3D is important for studying the behavior of humans and\nother animals, but manual pose annotations are expensive and time-consuming to\nobtain. Self-supervised keypoint discovery is a promising strategy for\nestimating 3D poses without annotations. However, current keypoint discovery\napproaches commonly process single 2D views and do not operate in the 3D space.\nWe propose a new method to perform self-supervised keypoint discovery in 3D\nfrom multi-view videos of behaving agents, without any keypoint or bounding box\nsupervision in 2D or 3D. Our method uses an encoder-decoder architecture with a\n3D volumetric heatmap, trained to reconstruct spatiotemporal differences across\nmultiple views, in addition to joint length constraints on a learned 3D\nskeleton of the subject. In this way, we discover keypoints without requiring\nmanual supervision in videos of humans and rats, demonstrating the potential of\n3D keypoint discovery for studying behavior.\n","authors":["Jennifer J. Sun","Pierre Karashchuk","Amil Dravid","Serim Ryou","Sonia Fereidooni","John Tuthill","Aggelos Katsaggelos","Bingni W. Brunton","Georgia Gkioxari","Ann Kennedy","Yisong Yue","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2212.07401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07398v1","updated":"2022-12-14T18:31:47Z","published":"2022-12-14T18:31:47Z","title":"Self-Play and Self-Describe: Policy Adaptation with Vision-Language\n  Foundation Models","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. To adapt the policy to unseen tasks and\nenvironments, we explore a new paradigm on leveraging the pre-trained\nfoundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the\ntrained policy to a new task or a new environment, we first let the policy\nself-play with randomly generated instructions to record the demonstrations.\nWhile the execution could be wrong, we can use the pre-trained foundation\nmodels to accurately self-describe (i.e., re-label or classify) the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nSPLAYD improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/SPLAYD/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v1.pdf","comment":"Project page: https://geyuying.github.io/SPLAYD/"},{"id":"http://arxiv.org/abs/2212.07388v1","updated":"2022-12-14T18:16:41Z","published":"2022-12-14T18:16:41Z","title":"NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior","summary":"  Training a Neural Radiance Field (NeRF) without pre-computed camera poses is\nchallenging. Recent advances in this direction demonstrate the possibility of\njointly optimising a NeRF and camera poses in forward-facing scenes. However,\nthese methods still face difficulties during dramatic camera movement. We\ntackle this challenging problem by incorporating undistorted monocular depth\npriors. These priors are generated by correcting scale and shift parameters\nduring training, with which we are then able to constrain the relative poses\nbetween consecutive frames. This constraint is achieved using our proposed\nnovel loss functions. Experiments on real-world indoor and outdoor scenes show\nthat our method can handle challenging camera trajectories and outperforms\nexisting methods in terms of novel view rendering quality and pose estimation\naccuracy.\n","authors":["Wenjing Bian","Zirui Wang","Kejie Li","Jia-Wang Bian","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2212.07388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16667v2","updated":"2022-12-14T18:09:41Z","published":"2022-11-30T01:22:25Z","title":"Dynamic Sparse Training via More Exploration","summary":"  Over-parameterization of deep neural networks (DNNs) has shown high\nprediction accuracy for many applications. Although effective, the large number\nof parameters hinders its popularity on resource-limited devices and has an\noutsize environmental impact. Sparse training (using a fixed number of nonzero\nweights in each iteration) could significantly mitigate the training costs by\nreducing the model size. However, existing sparse training methods mainly use\neither random-based or greedy-based drop-and-grow strategies, resulting in\nlocal minimal and low accuracy. In this work, we consider the dynamic sparse\ntraining as a sparse connectivity search problem and design an exploitation and\nexploration acquisition function to escape from local optima and saddle points.\nWe further design an acquisition function and provide the theoretical\nguarantees for the proposed method and clarify its convergence property.\nExperimental results show that sparse models (up to 98\\% sparsity) obtained by\nour proposed method outperform the SOTA sparse training methods on a wide\nvariety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10,\nResNet-50 / CIFAR-100, our method has even higher accuracy than dense models.\nOn ResNet-50 / ImageNet, the proposed method has up to 8.2\\% accuracy\nimprovement compared to SOTA sparse training methods.\n","authors":["Shaoyi Huang","Bowen Lei","Dongkuan Xu","Hongwu Peng","Yue Sun","Mimi Xie","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2211.16667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07378v1","updated":"2022-12-14T17:59:03Z","published":"2022-12-14T17:59:03Z","title":"3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation","summary":"  We present 3DHumanGAN, a 3D-aware generative adversarial network (GAN) that\nsynthesizes images of full-body humans with consistent appearances under\ndifferent view-angles and body-poses. To tackle the representational and\ncomputational challenges in synthesizing the articulated structure of human\nbodies, we propose a novel generator architecture in which a 2D convolutional\nbackbone is modulated by a 3D pose mapping network. The 3D pose mapping network\nis formulated as a renderable implicit function conditioned on a posed 3D human\nmesh. This design has several merits: i) it allows us to harness the power of\n2D GANs to generate photo-realistic images; ii) it generates consistent images\nunder varying view-angles and specifiable poses; iii) the model can benefit\nfrom the 3D human prior. Our model is adversarially learned from a collection\nof web images needless of manual annotation.\n","authors":["Zhuoqian Yang","Shikai Li","Wayne Wu","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2212.07378v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.07372v1","updated":"2022-12-14T17:50:39Z","published":"2022-12-14T17:50:39Z","title":"Image Compression with Product Quantized Masked Image Modeling","summary":"  Recent neural compression methods have been based on the popular hyperprior\nframework. It relies on Scalar Quantization and offers a very strong\ncompression performance. This contrasts from recent advances in image\ngeneration and representation learning, where Vector Quantization is more\ncommonly employed. In this work, we attempt to bring these lines of research\ncloser by revisiting vector quantization for image compression. We build upon\nthe VQ-VAE framework and introduce several modifications. First, we replace the\nvanilla vector quantizer by a product quantizer. This intermediate solution\nbetween vector and scalar quantization allows for a much wider set of\nrate-distortion points: It implicitly defines high-quality quantizers that\nwould otherwise require intractably large codebooks. Second, inspired by the\nsuccess of Masked Image Modeling (MIM) in the context of self-supervised\nlearning and generative image models, we propose a novel conditional entropy\nmodel which improves entropy coding by modelling the co-dependencies of the\nquantized latent codes. The resulting PQ-MIM model is surprisingly effective:\nits compression performance on par with recent hyperprior methods. It also\noutperforms HiFiC in terms of FID and KID metrics when optimized with\nperceptual losses (e.g. adversarial). Finally, since PQ-MIM is compatible with\nimage generation frameworks, we show qualitatively that it can operate under a\nhybrid mode between compression and generation, with no further training or\nfinetuning. As a result, we explore the extreme compression regime where an\nimage is compressed into 200 bytes, i.e., less than a tweet.\n","authors":["Alaaeldin El-Nouby","Matthew J. Muckley","Karen Ullrich","Ivan Laptev","Jakob Verbeek","Hervé Jégou"],"pdf_url":"https://arxiv.org/pdf/2212.07372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08403v2","updated":"2022-12-14T17:49:54Z","published":"2022-11-15T18:45:26Z","title":"REPAIR: REnormalizing Permuted Activations for Interpolation Repair","summary":"  In this paper we look into the conjecture of Entezari et al. (2021) which\nstates that if the permutation invariance of neural networks is taken into\naccount, then there is likely no loss barrier to the linear interpolation\nbetween SGD solutions. First, we observe that neuron alignment methods alone\nare insufficient to establish low-barrier linear connectivity between SGD\nsolutions due to a phenomenon we call variance collapse: interpolated deep\nnetworks suffer a collapse in the variance of their activations, causing poor\nperformance. Next, we propose REPAIR (REnormalizing Permuted Activations for\nInterpolation Repair) which mitigates variance collapse by rescaling the\npreactivations of such interpolated networks. We explore the interaction\nbetween our method and the choice of normalization layer, network width, and\ndepth, and demonstrate that using REPAIR on top of neuron alignment methods\nleads to 60%-100% relative barrier reduction across a wide variety of\narchitecture families and tasks. In particular, we report a 74% barrier\nreduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on\nCIFAR10.\n","authors":["Keller Jordan","Hanie Sedghi","Olga Saukh","Rahim Entezari","Behnam Neyshabur"],"pdf_url":"https://arxiv.org/pdf/2211.08403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09213v2","updated":"2022-12-14T17:38:03Z","published":"2022-10-14T13:01:25Z","title":"Segmentation-guided Domain Adaptation for Efficient Depth Completion","summary":"  Complete depth information and efficient estimators have become vital\ningredients in scene understanding for automated driving tasks. A major problem\nfor LiDAR-based depth completion is the inefficient utilization of convolutions\ndue to the lack of coherent information as provided by the sparse nature of\nuncorrelated LiDAR point clouds, which often leads to complex and\nresource-demanding networks. The problem is reinforced by the expensive\naquisition of depth data for supervised training. In this work, we propose an\nefficient depth completion model based on a vgg05-like CNN architecture and\npropose a semi-supervised domain adaptation approach to transfer knowledge from\nsynthetic to real world data to improve data-efficiency and reduce the need for\na large database. In order to boost spatial coherence, we guide the learning\nprocess using segmentations as additional source of information. The efficiency\nand accuracy of our approach is evaluated on the KITTI dataset. Our approach\nimproves on previous efficient and low parameter state of the art approaches\nwhile having a noticeably lower computational footprint.\n","authors":["Fabian Märkert","Martin Sunkel","Anselm Haselhoff","Stefan Rudolph"],"pdf_url":"https://arxiv.org/pdf/2210.09213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07352v1","updated":"2022-12-14T17:26:35Z","published":"2022-12-14T17:26:35Z","title":"Bi-Noising Diffusion: Towards Conditional Diffusion Models with\n  Generative Restoration Priors","summary":"  Conditional diffusion probabilistic models can model the distribution of\nnatural images and can generate diverse and realistic samples based on given\nconditions. However, oftentimes their results can be unrealistic with\nobservable color shifts and textures. We believe that this issue results from\nthe divergence between the probabilistic distribution learned by the model and\nthe distribution of natural images. The delicate conditions gradually enlarge\nthe divergence during each sampling timestep. To address this issue, we\nintroduce a new method that brings the predicted samples to the training data\nmanifold using a pretrained unconditional diffusion model. The unconditional\nmodel acts as a regularizer and reduces the divergence introduced by the\nconditional model at each sampling step. We perform comprehensive experiments\nto demonstrate the effectiveness of our approach on super-resolution,\ncolorization, turbulence removal, and image-deraining tasks. The improvements\nobtained by our method suggest that the priors can be incorporated as a general\nplugin for improving conditional diffusion models.\n","authors":["Kangfu Mei","Nithin Gopalakrishnan Nair","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2212.07352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07350v1","updated":"2022-12-14T17:22:48Z","published":"2022-12-14T17:22:48Z","title":"A Fast Geometric Regularizer to Mitigate Event Collapse in the Contrast\n  Maximization Framework","summary":"  Event cameras are emerging vision sensors and their advantages are suitable\nfor various applications such as autonomous robots. Contrast maximization\n(CMax), which provides state-of-the-art accuracy on motion estimation using\nevents, may suffer from an overfitting problem called event collapse. Prior\nworks are computationally expensive or cannot alleviate the overfitting, which\nundermines the benefits of the CMax framework. We propose a novel,\ncomputationally efficient regularizer based on geometric principles to mitigate\nevent collapse. The experiments show that the proposed regularizer achieves\nstate-of-the-art accuracy results, while its reduced computational complexity\nmakes it two to four times faster than previous approaches. To the best of our\nknowledge, our regularizer is the only effective solution for event collapse\nwithout trading off runtime. We hope our work opens the door for future\napplications that unlocks the advantages of event cameras.\n","authors":["Shintaro Shiba","Yoshimitsu Aoki","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2212.07350v1.pdf","comment":"10 pages, 7 figures, 4 tables. Project page:\n  https://github.com/tub-rip/event collapse"},{"id":"http://arxiv.org/abs/2212.07346v1","updated":"2022-12-14T17:17:10Z","published":"2022-12-14T17:17:10Z","title":"Learning useful representations for shifting tasks and distributions","summary":"  Does the dominant approach to learn representations (as a side effect of\noptimizing an expected cost for a single training distribution) remain a good\napproach when we are dealing with multiple distributions. Our thesis is that\nsuch scenarios are better served by representations that are \"richer\" than\nthose obtained with a single optimization episode. This is supported by a\ncollection of empirical results obtained with an apparently na\\\"ive ensembling\ntechnique: concatenating the representations obtained with multiple training\nepisodes using the same data, model, algorithm, and hyper-parameters, but\ndifferent random seeds. These independently trained networks perform similarly.\nYet, in a number of scenarios involving new distributions, the concatenated\nrepresentation performs substantially better than an equivalently sized network\ntrained from scratch. This proves that the representations constructed by\nmultiple training episodes are in fact different. Although their concatenation\ncarries little additional information about the training task under the\ntraining distribution, it becomes substantially more informative when tasks or\ndistributions change. Meanwhile, a single training episode is unlikely to yield\nsuch a redundant representation because the optimization process has no reason\nto accumulate features that do not incrementally improve the training\nperformance.\n","authors":["Jianyu Zhang","Léon Bottou"],"pdf_url":"https://arxiv.org/pdf/2212.07346v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2212.07339v1","updated":"2022-12-14T17:02:16Z","published":"2022-12-14T17:02:16Z","title":"Mitigating Artifacts in Real-World Video Super-Resolution Models","summary":"  The recurrent structure is a prevalent framework for the task of video\nsuper-resolution, which models the temporal dependency between frames via\nhidden states. When applied to real-world scenarios with unknown and complex\ndegradations, hidden states tend to contain unpleasant artifacts and propagate\nthem to restored frames. In this circumstance, our analyses show that such\nartifacts can be largely alleviated when the hidden state is replaced with a\ncleaner counterpart. Based on the observations, we propose a Hidden State\nAttention (HSA) module to mitigate artifacts in real-world video\nsuper-resolution. Specifically, we first adopt various cheap filters to produce\na hidden state pool. For example, Gaussian blur filters are for smoothing\nartifacts while sharpening filters are for enhancing details. To aggregate a\nnew hidden state that contains fewer artifacts from the hidden state pool, we\ndevise a Selective Cross Attention (SCA) module, in which the attention between\ninput features and each hidden state is calculated. Equipped with HSA, our\nproposed method, namely FastRealVSR, is able to achieve 2x speedup while\nobtaining better performance than Real-BasicVSR. Codes will be available at\nhttps://github.com/TencentARC/FastRealVSR\n","authors":["Liangbin Xie","Xintao Wang","Shuwei Shi","Jinjin Gu","Chao Dong","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2212.07339v1.pdf","comment":"Accepted by AAAI 2023. Codes will be available at\n  https://github.com/TencentARC/FastRealVSR"},{"id":"http://arxiv.org/abs/2212.07328v1","updated":"2022-12-14T16:48:21Z","published":"2022-12-14T16:48:21Z","title":"Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture\n  of Stochastic Expert","summary":"  Equipping predicted segmentation with calibrated uncertainty is essential for\nsafety-critical applications. In this work, we focus on capturing the\ndata-inherent uncertainty (aka aleatoric uncertainty) in segmentation,\ntypically when ambiguities exist in input images. Due to the high-dimensional\noutput space and potential multiple modes in segmenting ambiguous images, it\nremains challenging to predict well-calibrated uncertainty for segmentation. To\ntackle this problem, we propose a novel mixture of stochastic experts (MoSE)\nmodel, where each expert network estimates a distinct mode of the aleatoric\nuncertainty and a gating network predicts the probabilities of an input image\nbeing segmented in those modes. This yields an efficient two-level uncertainty\nrepresentation. To learn the model, we develop a Wasserstein-like loss that\ndirectly minimizes the distribution distance between the MoSE and ground truth\nannotations. The loss can easily integrate traditional segmentation quality\nmeasures and be efficiently optimized via constraint relaxation. We validate\nour method on the LIDC-IDRI dataset and a modified multimodal Cityscapes\ndataset. Results demonstrate that our method achieves the state-of-the-art or\ncompetitive performance on all metrics.\n","authors":["Zhitong Gao","Yucong Chen","Chuyu Zhang","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2212.07328v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2212.07326v1","updated":"2022-12-14T16:46:54Z","published":"2022-12-14T16:46:54Z","title":"Mathematical model of printing-imaging channel for blind detection of\n  fake copy detection patterns","summary":"  Nowadays, copy detection patterns (CDP) appear as a very promising\nanti-counterfeiting technology for physical object protection. However, the\nadvent of deep learning as a powerful attacking tool has shown that the general\nauthentication schemes are unable to compete and fail against such attacks. In\nthis paper, we propose a new mathematical model of printing-imaging channel for\nthe authentication of CDP together with a new detection scheme based on it. The\nresults show that even deep learning created copy fakes unknown at the training\nstage can be reliably authenticated based on the proposed approach and using\nonly digital references of CDP during authentication.\n","authors":["Joakim Tutt","Olga Taran","Roman Chaban","Brian Pulfer","Yury Belousov","Taras Holotyak","Slava Voloshynovskiy"],"pdf_url":"https://arxiv.org/pdf/2212.07326v1.pdf","comment":"Paper accepted at the IEEE International Workshop on Information\n  Forensics and Security (WIFS) 2022"},{"id":"http://arxiv.org/abs/2212.07312v1","updated":"2022-12-14T16:17:48Z","published":"2022-12-14T16:17:48Z","title":"Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection","summary":"  High-definition (HD) map change detection is the task of determining when\nsensor data and map data are no longer in agreement with one another due to\nreal-world changes. We collect the first dataset for the task, which we entitle\nthe Trust, but Verify (TbV) dataset, by mining thousands of hours of data from\nover 9 months of autonomous vehicle fleet operations. We present learning-based\nformulations for solving the problem in the bird's eye view and ego-view.\nBecause real map changes are infrequent and vector maps are easy to\nsynthetically manipulate, we lean on simulated data to train our model. Perhaps\nsurprisingly, we show that such models can generalize to real world\ndistributions. The dataset, consisting of maps and logs collected in six North\nAmerican cities, is one of the largest AV datasets to date with more than 7.8\nmillion images. We make the data available to the public at\nhttps://www.argoverse.org/av2.html#mapchange-link, along with code and models\nat https://github.com/johnwlambert/tbv under the the CC BY-NC-SA 4.0 license.\n","authors":["John Lambert","James Hays"],"pdf_url":"https://arxiv.org/pdf/2212.07312v1.pdf","comment":"NeurIPS 2021, Track on Datasets and Benchmarks. Project page:\n  https://tbv-dataset.github.io/"},{"id":"http://arxiv.org/abs/2212.07299v1","updated":"2022-12-14T16:02:45Z","published":"2022-12-14T16:02:45Z","title":"Child PalmID: Contactless Palmprint Recognition","summary":"  Developing and least developed countries face the dire challenge of ensuring\nthat each child in their country receives required doses of vaccination,\nadequate nutrition and proper medication. International agencies such as\nUNICEF, WHO and WFP, among other organizations, strive to find innovative\nsolutions to determine which child has received the benefits and which have\nnot. Biometric recognition systems have been sought out to help solve this\nproblem. To that end, this report establishes a baseline accuracy of a\ncommercial contactless palmprint recognition system that may be deployed for\nrecognizing children in the age group of one to five years old. On a database\nof contactless palmprint images of one thousand unique palms from 500 children,\nwe establish SOTA authentication accuracy of 90.85% @ FAR of 0.01%, rank-1\nidentification accuracy of 99.0% (closed set), and FPIR=0.01 @ FNIR=0.3 for\nopen-set identification using PalmMobile SDK from Armatura.\n","authors":["Anil K. Jain","Akash Godbole","Anjoo Bhatnagar","Prem Sewak Sudhish"],"pdf_url":"https://arxiv.org/pdf/2212.07299v1.pdf","comment":"9 pages, 14 figures"},{"id":"http://arxiv.org/abs/2205.12639v2","updated":"2022-12-14T16:01:52Z","published":"2022-05-25T10:33:55Z","title":"TreEnhance: A Tree Search Method For Low-Light Image Enhancement","summary":"  In this paper we present TreEnhance, an automatic method for low-light image\nenhancement capable of improving the quality of digital images. The method\ncombines tree search theory, and in particular the Monte Carlo Tree Search\n(MCTS) algorithm, with deep reinforcement learning. Given as input a low-light\nimage, TreEnhance produces as output its enhanced version together with the\nsequence of image editing operations used to obtain it. During the training\nphase, the method repeatedly alternates two main phases: a generation phase,\nwhere a modified version of MCTS explores the space of image editing operations\nand selects the most promising sequence, and an optimization phase, where the\nparameters of a neural network, implementing the enhancement policy, are\nupdated.\n  Two different inference solutions are proposed for the enhancement of new\nimages: one is based on MCTS and is more accurate but more time and memory\nconsuming; the other directly applies the learned policy and is faster but\nslightly less precise. As a further contribution, we propose a guided search\nstrategy that \"reverses\" the enhancement procedure that a photo editor applied\nto a given input image. Unlike other methods from the state of the art,\nTreEnhance does not pose any constraint on the image resolution and can be used\nin a variety of scenarios with minimal tuning. We tested the method on two\ndatasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good\nresults from both a qualitative and a quantitative point of view.\n","authors":["Marco Cotogni","Claudio Cusano"],"pdf_url":"https://arxiv.org/pdf/2205.12639v2.pdf","comment":"Accepted in Pattern Recognition"},{"id":"http://arxiv.org/abs/2212.07292v1","updated":"2022-12-14T15:54:15Z","published":"2022-12-14T15:54:15Z","title":"One-Shot Domain Adaptive and Generalizable Semantic Segmentation with\n  Class-Aware Cross-Domain Transformers","summary":"  Unsupervised sim-to-real domain adaptation (UDA) for semantic segmentation\naims to improve the real-world test performance of a model trained on simulated\ndata. It can save the cost of manually labeling data in real-world applications\nsuch as robot vision and autonomous driving. Traditional UDA often assumes that\nthere are abundant unlabeled real-world data samples available during training\nfor the adaptation. However, such an assumption does not always hold in\npractice owing to the collection difficulty and the scarcity of the data. Thus,\nwe aim to relieve this need on a large number of real data, and explore the\none-shot unsupervised sim-to-real domain adaptation (OSUDA) and generalization\n(OSDG) problem, where only one real-world data sample is available. To remedy\nthe limited real data knowledge, we first construct the pseudo-target domain by\nstylizing the simulated data with the one-shot real data. To mitigate the\nsim-to-real domain gap on both the style and spatial structure level and\nfacilitate the sim-to-real adaptation, we further propose to use class-aware\ncross-domain transformers with an intermediate domain randomization strategy to\nextract the domain-invariant knowledge, from both the simulated and\npseudo-target data. We demonstrate the effectiveness of our approach for OSUDA\nand OSDG on different benchmarks, outperforming the state-of-the-art methods by\na large margin, 10.87, 9.59, 13.05 and 15.91 mIoU on GTA,\nSYNTHIA$\\rightarrow$Cityscapes, Foggy Cityscapes, respectively.\n","authors":["Rui Gong","Qin Wang","Dengxin Dai","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2212.07292v1.pdf","comment":"15 pages, 6 figures, 10 Tables"},{"id":"http://arxiv.org/abs/2212.07289v1","updated":"2022-12-14T15:44:12Z","published":"2022-12-14T15:44:12Z","title":"ConQueR: Query Contrast Voxel-DETR for 3D Object Detection","summary":"  Although DETR-based 3D detectors can simplify the detection pipeline and\nachieve direct sparse predictions, their performance still lags behind dense\ndetectors with post-processing for 3D object detection from point clouds. DETRs\nusually adopt a larger number of queries than GTs (e.g., 300 queries v.s. 40\nobjects in Waymo) in a scene, which inevitably incur many false positives\nduring inference. In this paper, we propose a simple yet effective sparse 3D\ndetector, named Query Contrast Voxel-DETR (ConQueR), to eliminate the\nchallenging false positives, and achieve more accurate and sparser predictions.\nWe observe that most false positives are highly overlapping in local regions,\ncaused by the lack of explicit supervision to discriminate locally similar\nqueries. We thus propose a Query Contrast mechanism to explicitly enhance\nqueries towards their best-matched GTs over all unmatched query predictions.\nThis is achieved by the construction of positive and negative GT-query pairs\nfor each GT, and a contrastive loss to enhance positive GT-query pairs against\nnegative ones based on feature similarities. ConQueR closes the gap of sparse\nand dense 3D detectors, and reduces up to ~60% false positives. Our\nsingle-frame ConQueR achieves new state-of-the-art (sota) 71.6 mAPH/L2 on the\nchallenging Waymo Open Dataset validation set, outperforming previous sota\nmethods (e.g., PV-RCNN++) by over 2.0 mAPH/L2.\n","authors":["Benjin Zhu","Zhe Wang","Shaoshuai Shi","Hang Xu","Lanqing Hong","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2212.07289v1.pdf","comment":"Project page: https://benjin.me/projects/2022_conquer/"},{"id":"http://arxiv.org/abs/2209.12075v2","updated":"2022-12-14T15:41:22Z","published":"2022-09-24T19:26:46Z","title":"S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction","summary":"  The technology of hyperspectral imaging (HSI) records the visual information\nupon long-range-distributed spectral wavelengths. A representative\nhyperspectral image acquisition procedure conducts a 3D-to-2D encoding by the\ncoded aperture snapshot spectral imager (CASSI) and requires a software decoder\nfor the 3D signal reconstruction. By observing this physical encoding\nprocedure, two major challenges stand in the way of a high-fidelity\nreconstruction. (i) To obtain 2D measurements, CASSI dislocates multiple\nchannels by disperser-titling and squeezes them onto the same spatial region,\nyielding an entangled data loss. (ii) The physical coded aperture leads to a\nmasked data loss by selectively blocking the pixel-wise light exposure. To\ntackle these challenges, we propose a spatial-spectral (S^2-) Transformer\nnetwork with a mask-aware learning strategy. First, we simultaneously leverage\nspatial and spectral attention modeling to disentangle the blended information\nin the 2D measurement along both two dimensions. A series of Transformer\nstructures are systematically designed to fully investigate the spatial and\nspectral informative properties of the hyperspectral data. Second, the masked\npixels will induce higher prediction difficulty and should be treated\ndifferently from unmasked ones. Thereby, we adaptively prioritize the loss\npenalty attributing to the mask structure by inferring the pixel-wise\nreconstruction difficulty upon the mask-encoded prediction. We theoretically\ndiscusses the distinct convergence tendencies between masked/unmasked regions\nof the proposed learning strategy. Extensive experiments demonstrates that the\nproposed method achieves superior reconstruction performance. Additionally, we\nempirically elaborate the behaviour of spatial and spectral attentions under\nthe proposed architecture, and comprehensively examine the impact of the\nmask-aware learning.\n","authors":["Jiamian Wang","Kunpeng Li","Yulun Zhang","Xin Yuan","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2209.12075v2.pdf","comment":"11 pages, 16 figures, 6 tables, Code:\n  https://github.com/Jiamian-Wang/S2-transformer-HSI"},{"id":"http://arxiv.org/abs/2212.07283v1","updated":"2022-12-14T15:33:11Z","published":"2022-12-14T15:33:11Z","title":"Generative Robust Classification","summary":"  Training adversarially robust discriminative (i.e., softmax) classifier has\nbeen the dominant approach to robust classification. Building on recent work on\nadversarial training (AT)-based generative models, we investigate using AT to\nlearn unnormalized class-conditional density models and then performing\ngenerative robust classification. Our result shows that, under the condition of\nsimilar model capacities, the generative robust classifier achieves comparable\nperformance to a baseline softmax robust classifier when the test data is clean\nor when the test perturbation is of limited size, and much better performance\nwhen the test perturbation size exceeds the training perturbation size. The\ngenerative classifier is also able to generate samples or counterfactuals that\nmore closely resemble the training data, suggesting that the generative\nclassifier can better capture the class-conditional distributions. In contrast\nto standard discriminative adversarial training where advanced data\naugmentation techniques are only effective when combined with weight averaging,\nwe find it straightforward to apply advanced data augmentation to achieve\nbetter robustness in our approach. Our result suggests that the generative\nclassifier is a competitive alternative to robust classification, especially\nfor problems with limited number of classes.\n","authors":["Xuwang Yin"],"pdf_url":"https://arxiv.org/pdf/2212.07283v1.pdf","comment":"Report"},{"id":"http://arxiv.org/abs/2212.07277v1","updated":"2022-12-14T15:22:13Z","published":"2022-12-14T15:22:13Z","title":"ContraFeat: Contrasting Deep Features for Semantic Discovery","summary":"  StyleGAN has shown strong potential for disentangled semantic control, thanks\nto its special design of multi-layer intermediate latent variables. However,\nexisting semantic discovery methods on StyleGAN rely on manual selection of\nmodified latent layers to obtain satisfactory manipulation results, which is\ntedious and demanding. In this paper, we propose a model that automates this\nprocess and achieves state-of-the-art semantic discovery performance. The model\nconsists of an attention-equipped navigator module and losses contrasting\ndeep-feature changes. We propose two model variants, with one contrasting\nsamples in a binary manner, and another one contrasting samples with learned\nprototype variation patterns. The proposed losses are defined with pretrained\ndeep features, based on our assumption that the features can implicitly reveal\nthe desired semantic structure including consistency and orthogonality.\nAdditionally, we design two metrics to quantitatively evaluate the performance\nof semantic discovery methods on FFHQ dataset, and also show that disentangled\nrepresentations can be derived via a simple training process. Experimentally,\nour models can obtain state-of-the-art semantic discovery results without\nrelying on latent layer-wise manual selection, and these discovered semantics\ncan be used to manipulate real-world images.\n","authors":["Xinqi Zhu","Chang Xu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2212.07277v1.pdf","comment":"AAAI23"},{"id":"http://arxiv.org/abs/2212.07276v1","updated":"2022-12-14T15:19:06Z","published":"2022-12-14T15:19:06Z","title":"M-GenSeg: Domain Adaptation For Target Modality Tumor Segmentation With\n  Annotation-Efficient Supervision","summary":"  Automated medical image segmentation using deep neural networks typically\nrequires substantial supervised training. However, these models fail to\ngeneralize well across different imaging modalities. This shortcoming,\namplified by the limited availability of annotated data, has been hampering the\ndeployment of such methods at a larger scale across modalities. To address\nthese issues, we propose M-GenSeg, a new semi-supervised training strategy for\naccurate cross-modality tumor segmentation on unpaired bi-modal datasets. Based\non image-level labels, a first unsupervised objective encourages the model to\nperform diseased to healthy translation by disentangling tumors from the\nbackground, which encompasses the segmentation task. Then, teaching the model\nto translate between image modalities enables the synthesis of target images\nfrom a source modality, thus leveraging the pixel-level annotations from the\nsource modality to enforce generalization to the target modality images. We\nevaluated the performance on a brain tumor segmentation datasets composed of\nfour different contrast sequences from the public BraTS 2020 challenge dataset.\nWe report consistent improvement in Dice scores on both source and unannotated\ntarget modalities. On all twelve distinct domain adaptation experiments, the\nproposed model shows a clear improvement over state-of-the-art domain-adaptive\nbaselines, with absolute Dice gains on the target modality reaching 0.15.\n","authors":["Malo Alefsen de Boisredon d'Assier","Eugene Vorontsov","Samuel Kadoury"],"pdf_url":"https://arxiv.org/pdf/2212.07276v1.pdf","comment":"12 pages and 7 figures"},{"id":"http://arxiv.org/abs/2212.07275v1","updated":"2022-12-14T15:17:46Z","published":"2022-12-14T15:17:46Z","title":"PhoMoH: Implicit Photorealistic 3D Models of Human Heads","summary":"  We present PhoMoH, a neural network methodology to construct generative\nmodels of photorealistic 3D geometry and appearance of human heads including\nhair, beards, clothing and accessories. In contrast to prior work, PhoMoH\nmodels the human head using neural fields, thus supporting complex topology.\nInstead of learning a head model from scratch, we propose to augment an\nexisting expressive head model with new features. Concretely, we learn a highly\ndetailed geometry network layered on top of a mid-resolution head model\ntogether with a detailed, local geometry-aware, and disentangled color field.\nOur proposed architecture allows us to learn photorealistic human head models\nfrom relatively little data. The learned generative geometry and appearance\nnetworks can be sampled individually and allow the creation of diverse and\nrealistic human heads. Extensive experiments validate our method qualitatively\nand across different metrics.\n","authors":["Mihai Zanfir","Thiemo Alldieck","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2212.07275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.02831v3","updated":"2022-12-14T15:09:54Z","published":"2022-01-08T14:00:34Z","title":"CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation\n  techniques for Vestibular Schwannoma and Cochlea Segmentation","summary":"  Domain Adaptation (DA) has recently raised strong interests in the medical\nimaging community. While a large variety of DA techniques has been proposed for\nimage segmentation, most of these techniques have been validated either on\nprivate datasets or on small publicly available datasets. Moreover, these\ndatasets mostly addressed single-class problems. To tackle these limitations,\nthe Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in\nconjunction with the 24th International Conference on Medical Image Computing\nand Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large\nand multi-class benchmark for unsupervised cross-modality DA. The challenge's\ngoal is to segment two key brain structures involved in the follow-up and\ntreatment planning of vestibular schwannoma (VS): the VS and the cochleas.\nCurrently, the diagnosis and surveillance in patients with VS are performed\nusing contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in\nusing non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,\nwe created an unsupervised cross-modality segmentation benchmark. The training\nset provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).\nThe aim was to automatically perform unilateral VS and bilateral cochlea\nsegmentation on hrT2 as provided in the testing set (N=137). A total of 16\nteams submitted their algorithm for the evaluation phase. The level of\nperformance reached by the top-performing teams is strikingly high (best median\nDice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -\nVS:92.5%; Cochleas:87.7%). All top-performing methods made use of an\nimage-to-image translation approach to transform the source-domain images into\npseudo-target-domain images. A segmentation network was then trained using\nthese generated images and the manual annotations provided for the source\nimage.\n","authors":["Reuben Dorent","Aaron Kujawa","Marina Ivory","Spyridon Bakas","Nicola Rieke","Samuel Joutard","Ben Glocker","Jorge Cardoso","Marc Modat","Kayhan Batmanghelich","Arseniy Belkov","Maria Baldeon Calisto","Jae Won Choi","Benoit M. Dawant","Hexin Dong","Sergio Escalera","Yubo Fan","Lasse Hansen","Mattias P. Heinrich","Smriti Joshi","Victoriya Kashtanova","Hyeon Gyu Kim","Satoshi Kondo","Christian N. Kruse","Susana K. Lai-Yuen","Hao Li","Han Liu","Buntheng Ly","Ipek Oguz","Hyungseob Shin","Boris Shirokikh","Zixian Su","Guotai Wang","Jianghao Wu","Yanwu Xu","Kai Yao","Li Zhang","Sebastien Ourselin","Jonathan Shapey","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2201.02831v3.pdf","comment":"In Medical Image Analysis"},{"id":"http://arxiv.org/abs/2211.16950v2","updated":"2022-12-14T15:00:10Z","published":"2022-11-30T12:48:17Z","title":"DSNet: a simple yet efficient network with dual-stream attention for\n  lesion segmentation","summary":"  Lesion segmentation requires both speed and accuracy. In this paper, we\npropose a simple yet efficient network DSNet, which consists of a encoder based\non Transformer and a convolutional neural network(CNN)-based distinct pyramid\ndecoder containing three dual-stream attention (DSA) modules. Specifically, the\nDSA module fuses features from two adjacent levels through the false positive\nstream attention (FPSA) branch and the false negative stream attention (FNSA)\nbranch to obtain features with diversified contextual information. We compare\nour method with various state-of-the-art (SOTA) lesion segmentation methods\nwith several public datasets, including CVC-ClinicDB, Kvasir-SEG, and ISIC-2018\nTask 1. The experimental results show that our method achieves SOTA performance\nin terms of mean Dice coefficient (mDice) and mean Intersection over Union\n(mIoU) with low model complexity and memory consumption.\n","authors":["Yunxiao Liu"],"pdf_url":"https://arxiv.org/pdf/2211.16950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.08003v3","updated":"2022-12-14T14:40:39Z","published":"2022-07-16T19:25:41Z","title":"SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video\n  Anomaly Detection","summary":"  A self-supervised multi-task learning (SSMTL) framework for video anomaly\ndetection was recently introduced in literature. Due to its highly accurate\nresults, the method attracted the attention of many researchers. In this work,\nwe revisit the self-supervised multi-task learning framework, proposing several\nupdates to the original method. First, we study various detection methods, e.g.\nbased on detecting high-motion regions using optical flow or background\nsubtraction, since we believe the currently used pre-trained YOLOv3 is\nsuboptimal, e.g. objects in motion or objects from unknown classes are never\ndetected. Second, we modernize the 3D convolutional backbone by introducing\nmulti-head self-attention modules, inspired by the recent success of vision\ntransformers. As such, we alternatively introduce both 2D and 3D convolutional\nvision transformer (CvT) blocks. Third, in our attempt to further improve the\nmodel, we study additional self-supervised learning tasks, such as predicting\nsegmentation maps through knowledge distillation, solving jigsaw puzzles,\nestimating body pose through knowledge distillation, predicting masked regions\n(inpainting), and adversarial learning with pseudo-anomalies. We conduct\nexperiments to assess the performance impact of the introduced changes. Upon\nfinding more promising configurations of the framework, dubbed SSMTL++v1 and\nSSMTL++v2, we extend our preliminary experiments to more data sets,\ndemonstrating that our performance gains are consistent across all data sets.\nIn most cases, our results on Avenue, ShanghaiTech and UBnormal raise the\nstate-of-the-art performance bar to a new level.\n","authors":["Antonio Barbalau","Radu Tudor Ionescu","Mariana-Iuliana Georgescu","Jacob Dueholm","Bharathkumar Ramachandra","Kamal Nasrollahi","Fahad Shahbaz Khan","Thomas B. Moeslund","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2207.08003v3.pdf","comment":"Under consideration at Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2203.10421v2","updated":"2022-12-14T14:28:33Z","published":"2022-03-20T00:52:45Z","title":"CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot\n  Object Navigation","summary":"  For robots to be generally useful, they must be able to find arbitrary\nobjects described by people (i.e., be language-driven) even without expensive\nnavigation training on in-domain data (i.e., perform zero-shot inference). We\nexplore these capabilities in a unified setting: language-driven zero-shot\nobject navigation (L-ZSON). Inspired by the recent success of open-vocabulary\nmodels for image classification, we investigate a straightforward framework,\nCLIP on Wheels (CoW), to adapt open-vocabulary models to this task without\nfine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark,\nwhich considers finding uncommon objects, objects described by spatial and\nappearance attributes, and hidden objects described relative to visible\nobjects. We conduct an in-depth empirical study by directly deploying 21 CoW\nbaselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k\nnavigation episodes and find that (1) CoW baselines often struggle to leverage\nlanguage descriptions, but are proficient at finding uncommon objects. (2) A\nsimple CoW, with CLIP-based object localization and classical exploration --\nand no additional training -- matches the navigation efficiency of a\nstate-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This\nsame CoW provides a 15.6 percentage point improvement in success over a\nstate-of-the-art RoboTHOR ZSON model.\n","authors":["Samir Yitzhak Gadre","Mitchell Wortsman","Gabriel Ilharco","Ludwig Schmidt","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2203.10421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07242v1","updated":"2022-12-14T14:24:00Z","published":"2022-12-14T14:24:00Z","title":"HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics","summary":"  We propose a method that leverages graph neural networks, multi-level message\npassing, and unsupervised training to enable real-time prediction of realistic\nclothing dynamics. Whereas existing methods based on linear blend skinning must\nbe trained for specific garments, our method is agnostic to body shape and\napplies to tight-fitting garments as well as loose, free-flowing clothing. Our\nmethod furthermore handles changes in topology (e.g., garments with buttons or\nzippers) and material properties at inference time. As one key contribution, we\npropose a hierarchical message-passing scheme that efficiently propagates stiff\nstretching modes while preserving local detail. We empirically show that our\nmethod outperforms strong baselines quantitatively and that its results are\nperceived as more realistic than state-of-the-art methods.\n","authors":["Artur Grigorev","Bernhard Thomaszewski","Michael J. Black","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2212.07242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09934v3","updated":"2022-12-14T14:08:57Z","published":"2022-07-20T14:20:35Z","title":"DeepIPC: Deeply Integrated Perception and Control for an Autonomous\n  Vehicle in Real Environments","summary":"  We propose DeepIPC, an end-to-end autonomous driving model that handles both\nperception and control tasks in driving a vehicle. The model consists of two\nmain parts, perception and controller modules. The perception module takes an\nRGBD image to perform semantic segmentation and bird's eye view (BEV) semantic\nmapping along with providing their encoded features. Meanwhile, the controller\nmodule processes these features with the measurement of GNSS locations and\nangular speed to estimate waypoints that come with latent features. Then, two\ndifferent agents are used to translate waypoints and latent features into a set\nof navigational controls to drive the vehicle. The model is evaluated by\npredicting driving records and performing automated driving under various\nconditions in real environments. The experimental results show that DeepIPC\nachieves the best drivability and multi-task performance even with fewer\nparameters compared to the other models.\n","authors":["Oskar Natan","Jun Miura"],"pdf_url":"https://arxiv.org/pdf/2207.09934v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2212.05056v2","updated":"2022-12-14T13:53:10Z","published":"2022-12-07T14:48:25Z","title":"Testing Human Ability To Detect Deepfake Images of Human Faces","summary":"  Deepfakes are computationally-created entities that falsely represent\nreality. They can take image, video, and audio modalities, and pose a threat to\nmany areas of systems and societies, comprising a topic of interest to various\naspects of cybersecurity and cybersafety. In 2020 a workshop consulting AI\nexperts from academia, policing, government, the private sector, and state\nsecurity agencies ranked deepfakes as the most serious AI threat. These experts\nnoted that since fake material can propagate through many uncontrolled routes,\nchanges in citizen behaviour may be the only effective defence. This study aims\nto assess human ability to identify image deepfakes of human faces\n(StyleGAN2:FFHQ) from nondeepfake images (FFHQ), and to assess the\neffectiveness of simple interventions intended to improve detection accuracy.\nUsing an online survey, 280 participants were randomly allocated to one of four\ngroups: a control group, and 3 assistance interventions. Each participant was\nshown a sequence of 20 images randomly selected from a pool of 50 deepfake and\n50 real images of human faces. Participants were asked if each image was\nAI-generated or not, to report their confidence, and to describe the reasoning\nbehind each response. Overall detection accuracy was only just above chance and\nnone of the interventions significantly improved this. Participants' confidence\nin their answers was high and unrelated to accuracy. Assessing the results on a\nper-image basis reveals participants consistently found certain images harder\nto label correctly, but reported similarly high confidence regardless of the\nimage. Thus, although participant accuracy was 62% overall, this accuracy\nacross images ranged quite evenly between 85% and 30%, with an accuracy of\nbelow 50% for one in every five images. We interpret the findings as suggesting\nthat there is a need for an urgent call to action to address this threat.\n","authors":["Sergi D. Bray","Shane D. Johnson","Bennett Kleinberg"],"pdf_url":"https://arxiv.org/pdf/2212.05056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07211v1","updated":"2022-12-14T13:19:40Z","published":"2022-12-14T13:19:40Z","title":"RAGO: Recurrent Graph Optimizer For Multiple Rotation Averaging","summary":"  This paper proposes a deep recurrent Rotation Averaging Graph Optimizer\n(RAGO) for Multiple Rotation Averaging (MRA). Conventional optimization-based\nmethods usually fail to produce accurate results due to corrupted and noisy\nrelative measurements. Recent learning-based approaches regard MRA as a\nregression problem, while these methods are sensitive to initialization due to\nthe gauge freedom problem. To handle these problems, we propose a learnable\niterative graph optimizer minimizing a gauge-invariant cost function with an\nedge rectification strategy to mitigate the effect of inaccurate measurements.\nOur graph optimizer iteratively refines the global camera rotations by\nminimizing each node's single rotation objective function. Besides, our\napproach iteratively rectifies relative rotations to make them more consistent\nwith the current camera orientations and observed relative rotations.\nFurthermore, we employ a gated recurrent unit to improve the result by tracing\nthe temporal information of the cost graph. Our framework is a real-time\nlearning-to-optimize rotation averaging graph optimizer with a tiny size\ndeployed for real-world applications. RAGO outperforms previous traditional and\ndeep methods on real-world and synthetic datasets. The code is available at\nhttps://github.com/sfu-gruvi-3dv/RAGO\n","authors":["Heng Li","Zhaopeng Cui","Shuaicheng Liu","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2212.07211v1.pdf","comment":"Accepted by CVPR 2022"},{"id":"http://arxiv.org/abs/2212.07207v1","updated":"2022-12-14T13:10:27Z","published":"2022-12-14T13:10:27Z","title":"MAELi -- Masked Autoencoder for Large-Scale LiDAR Point Clouds","summary":"  We show how the inherent, but often neglected, properties of large-scale\nLiDAR point clouds can be exploited for effective self-supervised\nrepresentation learning. To this end, we design a highly data-efficient feature\npre-training backbone that significantly reduces the amount of tedious 3D\nannotations to train state-of-the-art object detectors. In particular, we\npropose a Masked AutoEncoder (MAELi) that intuitively utilizes the sparsity of\nthe LiDAR point clouds in both, the encoder and the decoder, during\nreconstruction. This results in more expressive and useful features, directly\napplicable to downstream perception tasks, such as 3D object detection for\nautonomous driving. In a novel reconstruction scheme, MAELi distinguishes\nbetween free and occluded space and leverages a new masking strategy which\ntargets the LiDAR's inherent spherical projection. To demonstrate the potential\nof MAELi, we pre-train one of the most widespread 3D backbones, in an\nend-to-end fashion and show the merit of our fully unsupervised pre-trained\nfeatures on several 3D object detection architectures. Given only a tiny\nfraction of labeled frames to fine-tune such detectors, we achieve significant\nperformance improvements. For example, with only $\\sim800$ labeled frames,\nMAELi features improve a SECOND model by +10.09APH/LEVEL 2 on Waymo Vehicles.\n","authors":["Georg Krispel","David Schinagl","Christian Fruhwirth-Reisinger","Horst Possegger","Horst Bischof"],"pdf_url":"https://arxiv.org/pdf/2212.07207v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2203.03610v2","updated":"2022-12-14T12:34:44Z","published":"2022-03-07T18:59:03Z","title":"ZippyPoint: Fast Interest Point Detection, Description, and Matching\n  through Mixed Precision Discretization","summary":"  Efficient detection and description of geometric regions in images is a\nprerequisite in visual systems for localization and mapping. Such systems still\nrely on traditional hand-crafted methods for efficient generation of\nlightweight descriptors, a common limitation of the more powerful neural\nnetwork models that come with high compute and specific hardware requirements.\nIn this paper, we focus on the adaptations required by detection and\ndescription neural networks to enable their use in computationally limited\nplatforms such as robots, mobile, and augmented reality devices. To that end,\nwe investigate and adapt network quantization techniques to accelerate\ninference and enable its use on compute limited platforms. In addition, we\nrevisit common practices in descriptor quantization and propose the use of a\nbinary descriptor normalization layer, enabling the generation of distinctive\nbinary descriptors with a constant number of ones. ZippyPoint, our efficient\nquantized network with binary descriptors, improves the network runtime speed,\nthe descriptor matching speed, and the 3D model size, by at least an order of\nmagnitude when compared to full-precision counterparts. These improvements come\nat a minor performance degradation as evaluated on the tasks of homography\nestimation, visual localization, and map-free visual relocalization. Code and\ntrained models will be released upon acceptance.\n","authors":["Menelaos Kanakis","Simon Maurer","Matteo Spallanzani","Ajad Chhatkuli","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.03610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07187v1","updated":"2022-12-14T12:30:03Z","published":"2022-12-14T12:30:03Z","title":"Design-time Fashion Popularity Forecasting in VR Environments","summary":"  Being able to forecast the popularity of new garment designs is very\nimportant in an industry as fast paced as fashion, both in terms of\nprofitability and reducing the problem of unsold inventory. Here, we attempt to\naddress this task in order to provide informative forecasts to fashion\ndesigners within a virtual reality designer application that will allow them to\nfine tune their creations based on current consumer preferences within an\ninteractive and immersive environment. To achieve this we have to deal with the\nfollowing central challenges: (1) the proposed method should not hinder the\ncreative process and thus it has to rely only on the garment's visual\ncharacteristics, (2) the new garment lacks historical data from which to\nextrapolate their future popularity and (3) fashion trends in general are\nhighly dynamical. To this end, we develop a computer vision pipeline fine tuned\non fashion imagery in order to extract relevant visual features along with the\ncategory and attributes of the garment. We propose a hierarchical label sharing\n(HLS) pipeline for automatically capturing hierarchical relations among fashion\ncategories and attributes. Moreover, we propose MuQAR, a Multimodal\nQuasi-AutoRegressive neural network that forecasts the popularity of new\ngarments by combining their visual features and categorical features while an\nautoregressive neural network is modelling the popularity time series of the\ngarment's category and attributes. Both the proposed HLS and MuQAR prove\ncapable of surpassing the current state-of-the-art in key benchmark datasets,\nDeepFashion for image classification and VISUELLE for new garment sales\nforecasting.\n","authors":["Stefanos-Iordanis Papadopoulos","Christos Koutlis","Anastasios Papazoglou-Chalikias","Symeon Papadopoulos","Spiros Nikolopoulos"],"pdf_url":"https://arxiv.org/pdf/2212.07187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01772v3","updated":"2022-12-14T12:28:07Z","published":"2022-05-03T20:41:21Z","title":"The Brazilian Data at Risk in the Age of AI?","summary":"  Advances in image processing and analysis as well as machine learning\ntechniques have contributed to the use of biometric recognition systems in\ndaily people tasks. These tasks range from simple access to mobile devices to\ntagging friends in photos shared on social networks and complex financial\noperations on self-service devices for banking transactions. In China, the use\nof these systems goes beyond personal use becoming a country's government\npolicy with the objective of monitoring the behavior of its population. On July\n05th 2021, the Brazilian government announced acquisition of a biometric\nrecognition system to be used nationwide. In the opposite direction to China,\nEurope and some American cities have already started the discussion about the\nlegality of using biometric systems in public places, even banning this\npractice in their territory. In order to open a deeper discussion about the\nrisks and legality of using these systems, this work exposes the\nvulnerabilities of biometric recognition systems, focusing its efforts on the\nface modality. Furthermore, it shows how it is possible to fool a biometric\nsystem through a well-known presentation attack approach in the literature\ncalled morphing. Finally, a list of ten concerns was created to start the\ndiscussion about the security of citizen data and data privacy law in the Age\nof Artificial Intelligence (AI).\n","authors":["Raoni F. da S. Teixeira","Rafael B. Januzi","Fabio A. Faria"],"pdf_url":"https://arxiv.org/pdf/2205.01772v3.pdf","comment":"8 pages in Portuguese and 5 figures, Top 3 among the best papers at\n  the ENIAC 2022"},{"id":"http://arxiv.org/abs/2212.07181v1","updated":"2022-12-14T12:12:29Z","published":"2022-12-14T12:12:29Z","title":"Event-based YOLO Object Detection: Proof of Concept for Forward\n  Perception System","summary":"  Neuromorphic vision or event vision is an advanced vision technology, where\nin contrast to the visible camera that outputs pixels, the event vision\ngenerates neuromorphic events every time there is a brightness change which\nexceeds a specific threshold in the field of view (FOV). This study focuses on\nleveraging neuromorphic event data for roadside object detection. This is a\nproof of concept towards building artificial intelligence (AI) based pipelines\nwhich can be used for forward perception systems for advanced vehicular\napplications. The focus is on building efficient state-of-the-art object\ndetection networks with better inference results for fast-moving forward\nperception using an event camera. In this article, the event-simulated A2D2\ndataset is manually annotated and trained on two different YOLOv5 networks\n(small and large variants). To further assess its robustness, single model\ntesting and ensemble model testing are carried out.\n","authors":["Waseem Shariff","Muhammad Ali Farooq","Joe Lemley","Peter Corcoran"],"pdf_url":"https://arxiv.org/pdf/2212.07181v1.pdf","comment":"7 pages, 9 figures, ICMV conference 2022"},{"id":"http://arxiv.org/abs/2206.06715v2","updated":"2022-12-14T11:43:57Z","published":"2022-06-14T09:40:17Z","title":"Semi-signed prioritized neural fitting for surface reconstruction from\n  unoriented point clouds","summary":"  Reconstructing 3D geometry from \\emph{unoriented} point clouds can benefit\nmany downstream tasks. Recent shape modeling methods mostly adopt implicit\nneural representation to fit a signed distance field (SDF) and optimize the\nnetwork by \\emph{unsigned} supervision. However, these methods occasionally\nhave difficulty in finding the coarse shape for complicated objects, especially\nsuffering from the ``ghost'' surfaces (\\ie, fake surfaces that should not\nexist). To guide the network quickly fit the coarse shape, we propose to\nutilize the signed supervision in regions that are obviously outside the object\nand can be easily determined, resulting in our semi-signed supervision. To\nbetter recover high-fidelity details, a novel importance sampling based on\ntracked region losses and a progressive positional encoding (PE) prioritize the\noptimization towards underfitting and complicated regions. Specifically, we\nvoxelize and partition the object space into \\emph{sign-known} and\n\\emph{sign-uncertain} regions, in which different supervisions are applied.\nBesides, we adaptively adjust the sampling rate of each voxel according to the\ntracked reconstruction loss, so that the network can focus more on the\ncomplicated under-fitting regions. To this end, we propose our semi-signed\nprioritized (SSP) neural fitting, and conduct extensive experiments to\ndemonstrate that SSP achieves state-of-the-art performance on multiple datasets\nincluding the ABC subset and various challenging data. The code will be\nreleased upon the publication.\n","authors":["Runsong Zhu","Di Kang","Ka-Hei Hui","Yue Qian","Xuefei Zhe","Zhen Dong","Linchao Bao","Pheng-Ann Heng","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2206.06715v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13558v2","updated":"2022-12-14T11:28:25Z","published":"2022-09-27T17:16:20Z","title":"FreeSeg: Free Mask from Interpretable Contrastive Language-Image\n  Pretraining for Semantic Segmentation","summary":"  Fully supervised semantic segmentation learns from dense masks, which\nrequires heavy annotation cost for closed set. In this paper, we use natural\nlanguage as supervision without any pixel-level annotation for open world\nsegmentation. We call the proposed framework as FreeSeg, where the mask is\nfreely available from raw feature map of pretraining model. Compared with\nzero-shot or openset segmentation, FreeSeg doesn't require any annotated masks,\nand it widely predicts categories beyond class-agnostic unsupervised\nsegmentation. Specifically, FreeSeg obtains free mask from Image-Text\nSimilarity Map (ITSM) of Interpretable Contrastive Language-Image Pretraining\n(ICLIP). And our core improvements are the smoothed min pooling for dense\nICLIP, with the partial label and pixel strategies for segmentation.\nFurthermore, FreeSeg is very straight forward without complex design like\ngrouping, clustering or retrieval. Besides the simplicity, the performances of\nFreeSeg surpass previous state-of-the-art at large margins, e.g. 13.4% higher\nat mIoU on VOC dataset in the same settings.\n","authors":["Yi Li","Huifeng Yao","Hualiang Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2209.13558v2.pdf","comment":"This paper contains some immature results"},{"id":"http://arxiv.org/abs/2212.07158v1","updated":"2022-12-14T11:20:24Z","published":"2022-12-14T11:20:24Z","title":"Establishing a stronger baseline for lightweight contrastive models","summary":"  Recent research has reported a performance degradation in self-supervised\ncontrastive learning for specially designed efficient networks, such as\nMobileNet and EfficientNet. A common practice to address this problem is to\nintroduce a pretrained contrastive teacher model and train the lightweight\nnetworks with distillation signals generated by the teacher. However, it is\ntime and resource consuming to pretrain a teacher model when it is not\navailable. In this work, we aim to establish a stronger baseline for\nlightweight contrastive models without using a pretrained teacher model.\nSpecifically, we show that the optimal recipe for efficient models is different\nfrom that of larger models, and using the same training settings as ResNet50,\nas previous research does, is inappropriate. Additionally, we observe a common\nissu e in contrastive learning where either the positive or negative views can\nbe noisy, and propose a smoothed version of InfoNCE loss to alleviate this\nproblem. As a result, we successfully improve the linear evaluation results\nfrom 36.3\\% to 62.3\\% for MobileNet-V3-Large and from 42.2\\% to 65.8\\% for\nEfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with\n$5\\times$ fewer parameters. We hope our research will facilitate the usage of\nlightweight contrastive models.\n","authors":["Wenye Lin","Yifeng Ding","Zhixiong Cao","Hai-tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2212.07158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11053v2","updated":"2022-12-14T10:42:14Z","published":"2022-04-23T11:09:43Z","title":"Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial\n  Expression Recognition","summary":"  High-quality annotated images are significant to deep facial expression\nrecognition (FER) methods. However, uncertain labels, mostly existing in\nlarge-scale public datasets, often mislead the training process. In this paper,\nwe achieve uncertain label correction of facial expressions using auxiliary\naction unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization\nmodule is introduced to highlight valid samples and suppress category imbalance\nin every batch. Based on the latent dependency between emotions and AUs, an\nauxiliary branch using graph convolutional layers is added to extract the\nsemantic information from graph topologies. Finally, a re-labeling strategy\ncorrects the ambiguous annotations by comparing their feature similarities with\nsemantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57%\naccuracy on RAF-DB and AffectNet datasets, respectively, outperforming the\nbaseline and state-of-the-art methods.\n","authors":["Yang Liu","Xingming Zhang","Janne Kauttonen","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2204.11053v2.pdf","comment":"7 pages, 7 figures, accecpted by ICPR 2022"},{"id":"http://arxiv.org/abs/2212.07146v1","updated":"2022-12-14T10:40:35Z","published":"2022-12-14T10:40:35Z","title":"Fully complex-valued deep learning model for visual perception","summary":"  Deep learning models operating in the complex domain are used due to their\nrich representation capacity. However, most of these models are either\nrestricted to the first quadrant of the complex plane or project the\ncomplex-valued data into the real domain, causing a loss of information. This\npaper proposes that operating entirely in the complex domain increases the\noverall performance of complex-valued models. A novel, fully complex-valued\nlearning scheme is proposed to train a Fully Complex-valued Convolutional\nNeural Network (FC-CNN) using a newly proposed complex-valued loss function and\ntraining strategy. Benchmarked on CIFAR-10, SVHN, and CIFAR-100, FC-CNN has a\n4-10% gain compared to its real-valued counterpart, maintaining the model\ncomplexity. With fewer parameters, it achieves comparable performance to\nstate-of-the-art complex-valued models on CIFAR-10 and SVHN. For the CIFAR-100\ndataset, it achieves state-of-the-art performance with 25% fewer parameters.\nFC-CNN shows better training efficiency and much faster convergence than all\nthe other models.\n","authors":["Aniruddh Sikdar","Sumanth Udupa","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2212.07146v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2212.07144v1","updated":"2022-12-14T10:28:08Z","published":"2022-12-14T10:28:08Z","title":"Uncertain Facial Expression Recognition via Multi-task Assisted\n  Correction","summary":"  Deep models for facial expression recognition achieve high performance by\ntraining on large-scale labeled data. However, publicly available datasets\ncontain uncertain facial expressions caused by ambiguous annotations or\nconfusing emotions, which could severely decline the robustness. Previous\nstudies usually follow the bias elimination method in general tasks without\nconsidering the uncertainty problem from the perspective of different\ncorresponding sources. In this paper, we propose a novel method of multi-task\nassisted correction in addressing uncertain facial expression recognition\ncalled MTAC. Specifically, a confidence estimation block and a weighted\nregularization module are applied to highlight solid samples and suppress\nuncertain samples in every batch. In addition, two auxiliary tasks, i.e.,\naction unit detection and valence-arousal measurement, are introduced to learn\nsemantic distributions from a data-driven AU graph and mitigate category\nimbalance based on latent dependencies between discrete and continuous\nemotions, respectively. Moreover, a re-labeling strategy guided by\nfeature-level similarity constraint further generates new labels for identified\nuncertain samples to promote model learning. The proposed method can flexibly\ncombine with existing frameworks in a fully-supervised or weakly-supervised\nmanner. Experiments on RAF-DB, AffectNet, and AffWild2 datasets demonstrate\nthat the MTAC obtains substantial improvements over baselines when facing\nsynthetic and real uncertainties and outperforms the state-of-the-art methods.\n","authors":["Yang Liu","Xingming Zhang","Janne Kauttonen","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2212.07144v1.pdf","comment":"12 pages, 10 figures, 5 tables. arXiv admin note: text overlap with\n  arXiv:2204.11053"},{"id":"http://arxiv.org/abs/2212.07143v1","updated":"2022-12-14T10:24:50Z","published":"2022-12-14T10:24:50Z","title":"Reproducible scaling laws for contrastive language-image learning","summary":"  Scaling up neural networks has led to remarkable performance across a wide\nrange of tasks. Moreover, performance often follows reliable scaling laws as a\nfunction of training set size, model size, and compute, which offers valuable\nguidance as large-scale experiments are becoming increasingly expensive.\nHowever, previous work on scaling laws has primarily used private data \\&\nmodels or focused on uni-modal language or vision learning. To address these\nlimitations, we investigate scaling laws for contrastive language-image\npre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP\nrepository. Our large-scale experiments involve models trained on up to two\nbillion image-text pairs and identify power law scaling for multiple downstream\ntasks including zero-shot classification, retrieval, linear probing, and\nend-to-end fine-tuning. We find that the training distribution plays a key role\nin scaling laws as the OpenAI and OpenCLIP models exhibit different scaling\nbehavior despite identical model architectures and similar training recipes. We\nopen-source our evaluation workflow and all models, including the largest\npublic CLIP models, to ensure reproducibility and make scaling laws research\nmore accessible. Source code and instructions to reproduce this study will be\navailable at https://github.com/LAION-AI/scaling-laws-openclip\n","authors":["Mehdi Cherti","Romain Beaumont","Ross Wightman","Mitchell Wortsman","Gabriel Ilharco","Cade Gordon","Christoph Schuhmann","Ludwig Schmidt","Jenia Jitsev"],"pdf_url":"https://arxiv.org/pdf/2212.07143v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/1909.10811v3","updated":"2022-12-14T10:07:24Z","published":"2019-09-24T10:59:44Z","title":"Image Recognition using Region Creep","summary":"  This paper describes a new type of auto-associative image classifier that\nuses a shallow architecture with a very quick learning phase. The image is\nparsed into smaller areas and each area is saved directly for a region, along\nwith the related output category. When a new image is presented, a direct match\nwith each region is made and the best matching areas returned. Each area stores\na list of the categories it belongs to, where there is a one-to-many relation\nbetween the input region and the output categories. The image classification\nprocess sums the category lists to return a preferred category for the whole\nimage. These areas can overlap with each other and when moving from a region to\nits neighbours, there is likely to be only small changes in the area image\npart. It would therefore be possible to guess what the best image area is for\none region by cumulating the results of its neighbours. This associative\nfeature is being called 'Region Creep' and the cumulated region can be compared\nwith train cases instead, when a suitable match is not found. Rules can be\nincluded and state that: if one set of pixels are present, another set should\neither be removed or should also be present, where this is across the whole\nimage. The memory problems with a traditional auto-associative network may be\nless with this version and tests on a set of hand-written numbers have produced\nstate-of-the-art results.\n","authors":["Kieran Greer"],"pdf_url":"https://arxiv.org/pdf/1909.10811v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07116v1","updated":"2022-12-14T09:11:19Z","published":"2022-12-14T09:11:19Z","title":"Blood Oxygen Saturation Estimation from Facial Video via DC and AC\n  components of Spatio-temporal Map","summary":"  Peripheral blood oxygen saturation (SpO2), an indicator of oxygen levels in\nthe blood, is one of the most important physiological parameters. Although SpO2\nis usually measured using a pulse oximeter, non-contact SpO2 estimation methods\nfrom facial or hand videos have been attracting attention in recent years. In\nthis paper, we propose an SpO2 estimation method from facial videos based on\nconvolutional neural networks (CNN). Our method constructs CNN models that\nconsider the direct current (DC) and alternating current (AC) components\nextracted from the RGB signals of facial videos, which are important in the\nprinciple of SpO2 estimation. Specifically, we extract the DC and AC components\nfrom the spatio-temporal map using filtering processes and train CNN models to\npredict SpO2 from these components. We also propose an end-to-end model that\npredicts SpO2 directly from the spatio-temporal map by extracting the DC and AC\ncomponents via convolutional layers. Experiments using facial videos and SpO2\ndata from 50 subjects demonstrate that the proposed method achieves a better\nestimation performance than current state-of-the-art SpO2 estimation methods.\n","authors":["Yusuke Akamatsu","Yoshifumi Onishi","Hitoshi Imaoka"],"pdf_url":"https://arxiv.org/pdf/2212.07116v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2010.01823v3","updated":"2022-12-14T09:08:49Z","published":"2020-10-05T07:16:40Z","title":"Quantifying Statistical Significance of Neural Network-based Image\n  Segmentation by Selective Inference","summary":"  Although a vast body of literature relates to image segmentation methods that\nuse deep neural networks (DNNs), less attention has been paid to assessing the\nstatistical reliability of segmentation results. In this study, we interpret\nthe segmentation results as hypotheses driven by DNN (called DNN-driven\nhypotheses) and propose a method by which to quantify the reliability of these\nhypotheses within a statistical hypothesis testing framework. Specifically, we\nconsider a statistical hypothesis test for the difference between the object\nand background regions. This problem is challenging, as the difference would be\nfalsely large because of the adaptation of the DNN to the data. To overcome\nthis difficulty, we introduce a conditional selective inference (SI) framework\n-- a new statistical inference framework for data-driven hypotheses that has\nrecently received considerable attention -- to compute exact (non-asymptotic)\nvalid p-values for the segmentation results. To use the conditional SI\nframework for DNN-based segmentation, we develop a new SI algorithm based on\nthe homotopy method, which enables us to derive the exact (non-asymptotic)\nsampling distribution of DNN-driven hypothesis. We conduct experiments on both\nsynthetic and real-world datasets, through which we offer evidence that our\nproposed method can successfully control the false positive rate, has good\nperformance in terms of computational efficiency, and provides good results\nwhen applied to medical image data.\n","authors":["Vo Nguyen Le Duy","Shogo Iwazaki","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2010.01823v3.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2208.07039v3","updated":"2022-12-14T09:03:35Z","published":"2022-08-15T07:29:31Z","title":"Hierarchical Attention Network for Few-Shot Object Detection via\n  Meta-Contrastive Learning","summary":"  Few-shot object detection (FSOD) aims to classify and detect few images of\nnovel categories. Existing meta-learning methods insufficiently exploit\nfeatures between support and query images owing to structural limitations. We\npropose a hierarchical attention network with sequentially large receptive\nfields to fully exploit the query and support images. In addition,\nmeta-learning does not distinguish the categories well because it determines\nwhether the support and query images match. In other words, metric-based\nlearning for classification is ineffective because it does not work directly.\nThus, we propose a contrastive learning method called meta-contrastive\nlearning, which directly helps achieve the purpose of the meta-learning\nstrategy. Finally, we establish a new state-of-the-art network, by realizing\nsignificant margins. Our method brings 2.3, 1.0, 1.3, 3.4 and 2.4% AP\nimprovements for 1-30 shots object detection on COCO dataset. Our code is\navailable at: https://github.com/infinity7428/hANMCL\n","authors":["Dongwoo Park","Jong-Min Lee"],"pdf_url":"https://arxiv.org/pdf/2208.07039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07102v1","updated":"2022-12-14T08:48:37Z","published":"2022-12-14T08:48:37Z","title":"Artificial intelligence-driven digital twin of a modern house\n  demonstrated in virtual reality","summary":"  A digital twin is defined as a virtual representation of a physical asset\nenabled through data and simulators for real-time prediction, optimization,\nmonitoring, controlling, and improved decision-making. Unfortunately, the term\nremains vague and says little about its capability. Recently, the concept of\ncapability level has been introduced to address this issue. Based on its\ncapability, the concept states that a digital twin can be categorized on a\nscale from zero to five, referred to as standalone, descriptive, diagnostic,\npredictive, prescriptive, and autonomous, respectively. The current work\nintroduces the concept in the context of the built environment. It demonstrates\nthe concept by using a modern house as a use case. The house is equipped with\nan array of sensors that collect timeseries data regarding the internal state\nof the house. Together with physics-based and data-driven models, these data\nare used to develop digital twins at different capability levels demonstrated\nin virtual reality. The work, in addition to presenting a blueprint for\ndeveloping digital twins, also provided future research directions to enhance\nthe technology.\n","authors":["Elias Mohammed Elfarri","Adil Rasheed","Omer San"],"pdf_url":"https://arxiv.org/pdf/2212.07102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07101v1","updated":"2022-12-14T08:46:46Z","published":"2022-12-14T08:46:46Z","title":"Domain Generalization by Learning and Removing Domain-specific Features","summary":"  Deep Neural Networks (DNNs) suffer from domain shift when the test dataset\nfollows a distribution different from the training dataset. Domain\ngeneralization aims to tackle this issue by learning a model that can\ngeneralize to unseen domains. In this paper, we propose a new approach that\naims to explicitly remove domain-specific features for domain generalization.\nFollowing this approach, we propose a novel framework called Learning and\nRemoving Domain-specific features for Generalization (LRDG) that learns a\ndomain-invariant model by tactically removing domain-specific features from the\ninput images. Specifically, we design a classifier to effectively learn the\ndomain-specific features for each source domain, respectively. We then develop\nan encoder-decoder network to map each input image into a new image space where\nthe learned domain-specific features are removed. With the images output by the\nencoder-decoder network, another classifier is designed to learn the\ndomain-invariant features to conduct image classification. Extensive\nexperiments demonstrate that our framework achieves superior performance\ncompared with state-of-the-art methods.\n","authors":["Yu Ding","Lei Wang","Bin Liang","Shuming Liang","Yang Wang","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07101v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.07098v1","updated":"2022-12-14T08:45:51Z","published":"2022-12-14T08:45:51Z","title":"Interactive Sketching of Mannequin Poses","summary":"  It can be easy and even fun to sketch humans in different poses. In contrast,\ncreating those same poses on a 3D graphics \"mannequin\" is comparatively\ntedious. Yet 3D body poses are necessary for various downstream applications.\nWe seek to preserve the convenience of 2D sketching while giving users of\ndifferent skill levels the flexibility to accurately and more quickly\npose\\slash refine a 3D mannequin.\n  At the core of the interactive system, we propose a machine-learning model\nfor inferring the 3D pose of a CG mannequin from sketches of humans drawn in a\ncylinder-person style. Training such a model is challenging because of artist\nvariability, a lack of sketch training data with corresponding ground truth 3D\nposes, and the high dimensionality of human pose-space. Our unique approach to\nsynthesizing vector graphics training data underpins our integrated\nML-and-kinematics system. We validate the system by tightly coupling it with a\nuser interface, and by performing a user study, in addition to quantitative\ncomparisons.\n","authors":["Gizem Unlu","Mohamed Sayed","Gabriel Brostow"],"pdf_url":"https://arxiv.org/pdf/2212.07098v1.pdf","comment":"accepted and published at 3DV 2022"},{"id":"http://arxiv.org/abs/2212.07086v1","updated":"2022-12-14T08:19:30Z","published":"2022-12-14T08:19:30Z","title":"NLIP: Noise-robust Language-Image Pre-training","summary":"  Large-scale cross-modal pre-training paradigms have recently shown ubiquitous\nsuccess on a wide range of downstream tasks, e.g., zero-shot classification,\nretrieval and image captioning. However, their successes highly rely on the\nscale and quality of web-crawled data that naturally contain incomplete and\nnoisy information (e.g., wrong or irrelevant content). Existing works either\ndesign manual rules to clean data or generate pseudo-targets as auxiliary\nsignals for reducing noise impact, which do not explicitly tackle both the\nincorrect and incomplete challenges simultaneously. In this paper, to\nautomatically mitigate the impact of noise by solely mining over existing data,\nwe propose a principled Noise-robust Language-Image Pre-training framework\n(NLIP) to stabilize pre-training via two schemes: noise-harmonization and\nnoise-completion. First, in noise-harmonization scheme, NLIP estimates the\nnoise probability of each pair according to the memorization effect of\ncross-modal transformers, then adopts noise-adaptive regularization to\nharmonize the cross-modal alignments with varying degrees. Second, in\nnoise-completion scheme, to enrich the missing object information of text, NLIP\ninjects a concept-conditioned cross-modal decoder to obtain semantic-consistent\nsynthetic captions to complete noisy ones, which uses the retrieved visual\nconcepts (i.e., objects' names) for the corresponding image to guide captioning\ngeneration. By collaboratively optimizing noise-harmonization and\nnoise-completion schemes, our NLIP can alleviate the common noise effects\nduring image-text pre-training in a more efficient way. Extensive experiments\nshow the significant performance improvements of our NLIP using only 26M data\nover existing pre-trained models (e.g., CLIP, FILIP and BLIP) on 12 zero-shot\nclassification datasets, MSCOCO image captioning and zero-shot image-text\nretrieval tasks.\n","authors":["Runhui Huang","Yanxin Long","Jianhua Han","Hang Xu","Xiwen Liang","Chunjing Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2212.07086v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2212.07084v1","updated":"2022-12-14T08:17:39Z","published":"2022-12-14T08:17:39Z","title":"Fully Complex-valued Fully Convolutional Multi-feature Fusion Network\n  (FC2MFN) for Building Segmentation of InSAR images","summary":"  Building segmentation in high-resolution InSAR images is a challenging task\nthat can be useful for large-scale surveillance. Although complex-valued deep\nlearning networks perform better than their real-valued counterparts for\ncomplex-valued SAR data, phase information is not retained throughout the\nnetwork, which causes a loss of information. This paper proposes a Fully\nComplex-valued, Fully Convolutional Multi-feature Fusion Network(FC2MFN) for\nbuilding semantic segmentation on InSAR images using a novel, fully\ncomplex-valued learning scheme. The network learns multi-scale features,\nperforms multi-feature fusion, and has a complex-valued output. For the\nparticularity of complex-valued InSAR data, a new complex-valued pooling layer\nis proposed that compares complex numbers considering their magnitude and\nphase. This helps the network retain the phase information even through the\npooling layer. Experimental results on the simulated InSAR dataset show that\nFC2MFN achieves better results compared to other state-of-the-art methods in\nterms of segmentation performance and model complexity.\n","authors":["Aniruddh Sikdar","Sumanth Udupa","Suresh Sundaram","Narasimhan Sundararajan"],"pdf_url":"https://arxiv.org/pdf/2212.07084v1.pdf","comment":"Accepted for publication in IEEE Symposium Series On Computational\n  Intelligence 2022, 8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.07079v1","updated":"2022-12-14T08:10:40Z","published":"2022-12-14T08:10:40Z","title":"A novel state connection strategy for quantum computing to represent and\n  compress digital images","summary":"  Quantum image processing draws a lot of attention due to faster data\ncomputation and storage compared to classical data processing systems.\nConverting classical image data into the quantum domain and state label\npreparation complexity is still a challenging issue. The existing techniques\nnormally connect the pixel values and the state position directly. Recently,\nthe EFRQI (efficient flexible representation of the quantum image) approach\nuses an auxiliary qubit that connects the pixel-representing qubits to the\nstate position qubits via Toffoli gates to reduce state connection. Due to the\ntwice use of Toffoli gates for each pixel connection still it requires a\nsignificant number of bits to connect each pixel value. In this paper, we\npropose a new SCMFRQI (state connection modification FRQI) approach for further\nreducing the required bits by modifying the state connection using a reset gate\nrather than repeating the use of the same Toffoli gate connection as a reset\ngate. Moreover, unlike other existing methods, we compress images using\nblock-level for further reduction of required qubits. The experimental results\nconfirm that the proposed method outperforms the existing methods in terms of\nboth image representation and compression points of view.\n","authors":["Md Ershadul Haque","Manoranjan Paul","Tanmoy Debnath"],"pdf_url":"https://arxiv.org/pdf/2212.07079v1.pdf","comment":"8 pages, conference"},{"id":"http://arxiv.org/abs/2212.07075v1","updated":"2022-12-14T07:52:36Z","published":"2022-12-14T07:52:36Z","title":"Cross-Modal Similarity-Based Curriculum Learning for Image Captioning","summary":"  Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.\n","authors":["Hongkuan Zhang","Saku Sugawara","Akiko Aizawa","Lei Zhou","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2212.07075v1.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.07070v1","updated":"2022-12-14T07:35:20Z","published":"2022-12-14T07:35:20Z","title":"Deep Negative Correlation Classification","summary":"  Ensemble learning serves as a straightforward way to improve the performance\nof almost any machine learning algorithm. Existing deep ensemble methods\nusually naively train many different models and then aggregate their\npredictions. This is not optimal in our view from two aspects: i) Naively\ntraining multiple models adds much more computational burden, especially in the\ndeep learning era; ii) Purely optimizing each base model without considering\ntheir interactions limits the diversity of ensemble and performance gains. We\ntackle these issues by proposing deep negative correlation classification\n(DNCC), in which the accuracy and diversity trade-off is systematically\ncontrolled by decomposing the loss function seamlessly into individual accuracy\nand the correlation between individual models and the ensemble. DNCC yields a\ndeep classification ensemble where the individual estimator is both accurate\nand negatively correlated. Thanks to the optimized diversities, DNCC works well\neven when utilizing a shared network backbone, which significantly improves its\nefficiency when compared with most existing ensemble systems. Extensive\nexperiments on multiple benchmark datasets and network structures demonstrate\nthe superiority of the proposed method.\n","authors":["Le Zhang","Qibin Hou","Yun Liu","Jia-Wang Bian","Xun Xu","Joey Tianyi Zhou","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.07070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06431v2","updated":"2022-12-14T07:33:12Z","published":"2022-12-13T08:42:39Z","title":"Object-fabrication Targeted Attack for Object Detection","summary":"  Recent researches show that the deep learning based object detection is\nvulnerable to adversarial examples. Generally, the adversarial attack for\nobject detection contains targeted attack and untargeted attack. According to\nour detailed investigations, the research on the former is relatively fewer\nthan the latter and all the existing methods for the targeted attack follow the\nsame mode, i.e., the object-mislabeling mode that misleads detectors to\nmislabel the detected object as a specific wrong label. However, this mode has\nlimited attack success rate, universal and generalization performances. In this\npaper, we propose a new object-fabrication targeted attack mode which can\nmislead detectors to `fabricate' extra false objects with specific target\nlabels. Furthermore, we design a dual attention based targeted feature space\nattack method to implement the proposed targeted attack mode. The attack\nperformances of the proposed mode and method are evaluated on MS COCO and\nBDD100K datasets using FasterRCNN and YOLOv5. Evaluation results demonstrate\nthat, the proposed object-fabrication targeted attack mode and the\ncorresponding targeted feature space attack method show significant\nimprovements in terms of image-specific attack, universal performance and\ngeneralization capability, compared with the previous targeted attack for\nobject detection. Code will be made available.\n","authors":["Xuchong Zhang","Changfeng Sun","Haoliang Han","Hang Wang","Hongbin Sun","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2212.06431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13307v3","updated":"2022-12-14T07:27:54Z","published":"2021-11-26T03:57:46Z","title":"Self-supervised Correlation Mining Network for Person Image Generation","summary":"  Person image generation aims to perform non-rigid deformation on source\nimages, which generally requires unaligned data pairs for training. Recently,\nself-supervised methods express great prospects in this task by merging the\ndisentangled representations for self-reconstruction. However, such methods\nfail to exploit the spatial correlation between the disentangled features. In\nthis paper, we propose a Self-supervised Correlation Mining Network (SCM-Net)\nto rearrange the source images in the feature space, in which two collaborative\nmodules are integrated, Decomposed Style Encoder (DSE) and Correlation Mining\nModule (CMM). Specifically, the DSE first creates unaligned pairs at the\nfeature level. Then, the CMM establishes the spatial correlation field for\nfeature rearrangement. Eventually, a translation module transforms the\nrearranged features to realistic results. Meanwhile, for improving the fidelity\nof cross-scale pose transformation, we propose a graph based Body Structure\nRetaining Loss (BSR Loss) to preserve reasonable body structures on half body\nto full body generation. Extensive experiments conducted on DeepFashion dataset\ndemonstrate the superiority of our method compared with other supervised and\nunsupervised approaches. Furthermore, satisfactory results on face generation\nshow the versatility of our method in other deformation tasks.\n","authors":["Zijian Wang","Xingqun Qi","Kun Yuan","Muyi Sun"],"pdf_url":"https://arxiv.org/pdf/2111.13307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07066v1","updated":"2022-12-14T07:22:44Z","published":"2022-12-14T07:22:44Z","title":"Improving Warped Planar Object Detection Network For Automatic License\n  Plate Recognition","summary":"  This paper aims to improve the Warping Planer Object Detection Network\n(WPOD-Net) using feature engineering to increase accuracy. What problems are\nsolved using the Warping Object Detection Network using feature engineering?\nMore specifically, we think that it makes sense to add knowledge about edges in\nthe image to enhance the information for determining the license plate contour\nof the original WPOD-Net model. The Sobel filter has been selected\nexperimentally and acts as a Convolutional Neural Network layer, the edge\ninformation is combined with the old information of the original network to\ncreate the final embedding vector. The proposed model was compared with the\noriginal model on a set of data that we collected for evaluation. The results\nare evaluated through the Quadrilateral Intersection over Union value and\ndemonstrate that the model has a significant improvement in performance.\n","authors":["Nguyen Dinh Tra","Nguyen Cong Tri","Phan Duy Hung"],"pdf_url":"https://arxiv.org/pdf/2212.07066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07065v1","updated":"2022-12-14T07:21:45Z","published":"2022-12-14T07:21:45Z","title":"CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled\n  Videos","summary":"  Recent years have seen progress beyond domain-specific sound separation for\nspeech or music towards universal sound separation for arbitrary sounds. Prior\nwork on universal sound separation has investigated separating a target sound\nout of an audio mixture given a text query. Such text-queried sound separation\nsystems provide a natural and scalable interface for specifying arbitrary\ntarget sounds. However, supervised text-queried sound separation systems\nrequire costly labeled audio-text pairs for training. Moreover, the audio\nprovided in existing datasets is often recorded in a controlled environment,\ncausing a considerable generalization gap to noisy audio in the wild. In this\nwork, we aim to approach text-queried universal sound separation by using only\nunlabeled data. We propose to leverage the visual modality as a bridge to learn\nthe desired audio-textual correspondence. The proposed CLIPSep model first\nencodes the input query into a query vector using the contrastive\nlanguage-image pretraining (CLIP) model, and the query vector is then used to\ncondition an audio separation model to separate out the target sound. While the\nmodel is trained on image-audio pairs extracted from unlabeled videos, at test\ntime we can instead query the model with text inputs in a zero-shot setting,\nthanks to the joint language-image embedding learned by the CLIP model.\nFurther, videos in the wild often contain off-screen sounds and background\nnoise that may hinder the model from learning the desired audio-textual\ncorrespondence. To address this problem, we further propose an approach called\nnoise invariant training for training a query-based sound separation model on\nnoisy data. Experimental results show that the proposed models successfully\nlearn text-queried universal sound separation using only noisy unlabeled\nvideos, even achieving competitive performance against a supervised model in\nsome settings.\n","authors":["Hao-Wen Dong","Naoya Takahashi","Yuki Mitsufuji","Julian McAuley","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2212.07065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07060v1","updated":"2022-12-14T07:03:23Z","published":"2022-12-14T07:03:23Z","title":"VINet: Lightweight, Scalable, and Heterogeneous Cooperative Perception\n  for 3D Object Detection","summary":"  Utilizing the latest advances in Artificial Intelligence (AI), the computer\nvision community is now witnessing an unprecedented evolution in all kinds of\nperception tasks, particularly in object detection. Based on multiple spatially\nseparated perception nodes, Cooperative Perception (CP) has emerged to\nsignificantly advance the perception of automated driving. However, current\ncooperative object detection methods mainly focus on ego-vehicle efficiency\nwithout considering the practical issues of system-wide costs. In this paper,\nwe introduce VINet, a unified deep learning-based CP network for scalable,\nlightweight, and heterogeneous cooperative 3D object detection. VINet is the\nfirst CP method designed from the standpoint of large-scale system-level\nimplementation and can be divided into three main phases: 1) Global\nPre-Processing and Lightweight Feature Extraction which prepare the data into\nglobal style and extract features for cooperation in a lightweight manner; 2)\nTwo-Stream Fusion which fuses the features from scalable and heterogeneous\nperception nodes; and 3) Central Feature Backbone and 3D Detection Head which\nfurther process the fused features and generate cooperative detection results.\nA cooperative perception platform is designed and developed for CP dataset\nacquisition and several baselines are compared during the experiments. The\nexperimental analysis shows that VINet can achieve remarkable improvements for\npedestrians and cars with 2x less system-wide computational costs and 12x less\nsystem-wide communicational costs.\n","authors":["Zhengwei Bai","Guoyuan Wu","Matthew J. Barth","Yongkang Liu","Emrah Akin Sisbot","Kentaro Oguchi"],"pdf_url":"https://arxiv.org/pdf/2212.07060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07058v1","updated":"2022-12-14T07:00:31Z","published":"2022-12-14T07:00:31Z","title":"Explainable Artificial Intelligence in Retinal Imaging for the detection\n  of Systemic Diseases","summary":"  Explainable Artificial Intelligence (AI) in the form of an interpretable and\nsemiautomatic approach to stage grading ocular pathologies such as Diabetic\nretinopathy, Hypertensive retinopathy, and other retinopathies on the backdrop\nof major systemic diseases. The experimental study aims to evaluate an\nexplainable staged grading process without using deep Convolutional Neural\nNetworks (CNNs) directly. Many current CNN-based deep neural networks used for\ndiagnosing retinal disorders might have appreciable performance but fail to\npinpoint the basis driving their decisions. To improve these decisions'\ntransparency, we have proposed a clinician-in-the-loop assisted intelligent\nworkflow that performs a retinal vascular assessment on the fundus images to\nderive quantifiable and descriptive parameters. The retinal vessel parameters\nmeta-data serve as hyper-parameters for better interpretation and\nexplainability of decisions. The semiautomatic methodology aims to have a\nfederated approach to AI in healthcare applications with more inputs and\ninterpretations from clinicians. The baseline process involved in the machine\nlearning pipeline through image processing techniques for optic disc detection,\nvessel segmentation, and arteriole/venule identification.\n","authors":["Ayushi Raj Bhatt","Rajkumar Vaghashiya","Meghna Kulkarni","Dr Prakash Kamaraj"],"pdf_url":"https://arxiv.org/pdf/2212.07058v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.07055v1","updated":"2022-12-14T06:51:39Z","published":"2022-12-14T06:51:39Z","title":"Dual-branch Cross-Patch Attention Learning for Group Affect Recognition","summary":"  Group affect refers to the subjective emotion that is evoked by an external\nstimulus in a group, which is an important factor that shapes group behavior\nand outcomes. Recognizing group affect involves identifying important\nindividuals and salient objects among a crowd that can evoke emotions. Most of\nthe existing methods are proposed to detect faces and objects using pre-trained\ndetectors and summarize the results into group emotions by specific rules.\nHowever, such affective region selection mechanisms are heuristic and\nsusceptible to imperfect faces and objects from the pre-trained detectors.\nMoreover, faces and objects on group-level images are often contextually\nrelevant. There is still an open question about how important faces and objects\ncan be interacted with. In this work, we incorporate the psychological concept\ncalled Most Important Person (MIP). It represents the most noteworthy face in\nthe crowd and has an affective semantic meaning. We propose the Dual-branch\nCross-Patch Attention Transformer (DCAT) which uses global image and MIP\ntogether as inputs. Specifically, we first learn the informative facial regions\nproduced by the MIP and the global context separately. Then, the Cross-Patch\nAttention module is proposed to fuse the features of MIP and global context\ntogether to complement each other. With parameters less than 10x, the proposed\nDCAT outperforms state-of-the-art methods on two datasets of group valence\nprediction, GAF 3.0 and GroupEmoW datasets. Moreover, our proposed model can be\ntransferred to another group affect task, group cohesion, and shows comparable\nresults.\n","authors":["Hongxia Xie","Ming-Xian Lee","Tzu-Jui Chen","Hung-Jen Chen","Hou-I Liu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.07055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12468v2","updated":"2022-12-14T06:09:21Z","published":"2022-09-26T07:18:00Z","title":"RetiFluidNet: A Self-Adaptive and Multi-Attention Deep Convolutional\n  Network for Retinal OCT Fluid Segmentation","summary":"  Optical coherence tomography (OCT) helps ophthalmologists assess macular\nedema, accumulation of fluids, and lesions at microscopic resolution.\nQuantification of retinal fluids is necessary for OCT-guided treatment\nmanagement, which relies on a precise image segmentation step. As manual\nanalysis of retinal fluids is a time-consuming, subjective, and error-prone\ntask, there is increasing demand for fast and robust automatic solutions. In\nthis study, a new convolutional neural architecture named RetiFluidNet is\nproposed for multi-class retinal fluid segmentation. The model benefits from\nhierarchical representation learning of textural, contextual, and edge features\nusing a new self-adaptive dual-attention (SDA) module, multiple self-adaptive\nattention-based skip connections (SASC), and a novel multi-scale deep self\nsupervision learning (DSL) scheme. The attention mechanism in the proposed SDA\nmodule enables the model to automatically extract deformation-aware\nrepresentations at different levels, and the introduced SASC paths further\nconsider spatial-channel interdependencies for concatenation of counterpart\nencoder and decoder units, which improve representational capability.\nRetiFluidNet is also optimized using a joint loss function comprising a\nweighted version of dice overlap and edge-preserved connectivity-based losses,\nwhere several hierarchical stages of multi-scale local losses are integrated\ninto the optimization process. The model is validated based on three publicly\navailable datasets: RETOUCH, OPTIMA, and DUKE, with comparisons against several\nbaselines. Experimental results on the datasets prove the effectiveness of the\nproposed model in retinal OCT fluid segmentation and reveal that the suggested\nmethod is more effective than existing state-of-the-art fluid segmentation\nalgorithms in adapting to retinal OCT scans recorded by various image scanning\ninstruments.\n","authors":["Reza Rasti","Armin Biglari","Mohammad Rezapourian","Ziyun Yang","Sina Farsiu"],"pdf_url":"https://arxiv.org/pdf/2209.12468v2.pdf","comment":"11 pages, Early Access Version, IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2212.07050v1","updated":"2022-12-14T06:04:18Z","published":"2022-12-14T06:04:18Z","title":"Significantly improving zero-shot X-ray pathology classification via\n  fine-tuning pre-trained image-text encoders","summary":"  Deep neural networks have been successfully adopted to diverse domains\nincluding pathology classification based on medical images. However,\nlarge-scale and high-quality data to train powerful neural networks are rare in\nthe medical domain as the labeling must be done by qualified experts.\nResearchers recently tackled this problem with some success by taking advantage\nof models pre-trained on large-scale general domain data. Specifically,\nresearchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it\nwith chest X-ray images and paired reports to perform zero-shot pathology\nclassification, thus completely removing the need for pathology-annotated\nimages to train a classification model. Existing studies, however, fine-tuned\nthe pre-trained model with the same contrastive learning objective, and failed\nto exploit the multi-labeled nature of medical image-report pairs. In this\npaper, we propose a new fine-tuning strategy based on sentence sampling and\npositive-pair loss relaxation for improving the downstream zero-shot pathology\nclassification performance, which can be applied to any pre-trained contrastive\nimage-text encoders. Our method consistently showed dramatically improved\nzero-shot pathology classification performance on four different chest X-ray\ndatasets and 3 different pre-trained models (5.77% average AUROC increase). In\nparticular, fine-tuning CLIP with our method showed much comparable or\nmarginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1\nscore and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent\ndiseases from the CheXpert dataset.\n","authors":["Jongseong Jang","Daeun Kyung","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2212.07050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07048v1","updated":"2022-12-14T05:48:58Z","published":"2022-12-14T05:48:58Z","title":"PD-Quant: Post-Training Quantization based on Prediction Difference\n  Metric","summary":"  As a neural network compression technique, post-training quantization (PTQ)\ntransforms a pre-trained model into a quantized model using a lower-precision\ndata type. However, the prediction accuracy will decrease because of the\nquantization noise, especially in extremely low-bit settings. How to determine\nthe appropriate quantization parameters (e.g., scaling factors and rounding of\nweights) is the main problem facing now. Many existing methods determine the\nquantization parameters by minimizing the distance between features before and\nafter quantization. Using this distance as the metric to optimize the\nquantization parameters only considers local information. We analyze the\nproblem of minimizing local metrics and indicate that it would not result in\noptimal quantization parameters. Furthermore, the quantized model suffers from\noverfitting due to the small number of calibration samples in PTQ. In this\npaper, we propose PD-Quant to solve the problems. PD-Quant uses the information\nof differences between network prediction before and after quantization to\ndetermine the quantization parameters. To mitigate the overfitting problem,\nPD-Quant adjusts the distribution of activations in PTQ. Experiments show that\nPD-Quant leads to better quantization parameters and improves the prediction\naccuracy of quantized models, especially in low-bit settings. For example,\nPD-Quant pushes the accuracy of ResNet-18 up to 53.08% and RegNetX-600MF up to\n40.92% in weight 2-bit activation 2-bit. The code will be released at\nhttps://github.com/hustvl/PD-Quant.\n","authors":["Jiawei Liu","Lin Niu","Zhihang Yuan","Dawei Yang","Xinggang Wang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2212.07048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07047v1","updated":"2022-12-14T05:47:52Z","published":"2022-12-14T05:47:52Z","title":"Shared Coupling-bridge for Weakly Supervised Local Feature Learning","summary":"  Sparse local feature extraction is usually believed to be of important\nsignificance in typical vision tasks such as simultaneous localization and\nmapping, image matching and 3D reconstruction. At present, it still has some\ndeficiencies needing further improvement, mainly including the discrimination\npower of extracted local descriptors, the localization accuracy of detected\nkeypoints, and the efficiency of local feature learning. This paper focuses on\npromoting the currently popular sparse local feature learning with camera pose\nsupervision. Therefore, it pertinently proposes a Shared Coupling-bridge scheme\nwith four light-weight yet effective improvements for weakly-supervised local\nfeature (SCFeat) learning. It mainly contains: i) the\n\\emph{Feature-Fusion-ResUNet Backbone} (F2R-Backbone) for local descriptors\nlearning, ii) a shared coupling-bridge normalization to improve the decoupling\ntraining of description network and detection network, iii) an improved\ndetection network with peakiness measurement to detect keypoints and iv) the\nfundamental matrix error as a reward factor to further optimize feature\ndetection training. Extensive experiments prove that our SCFeat improvement is\neffective. It could often obtain a state-of-the-art performance on classic\nimage matching and visual localization. In terms of 3D reconstruction, it could\nstill achieve competitive results. For sharing and communication, our source\ncodes are available at https://github.com/sunjiayuanro/SCFeat.git.\n","authors":["Jiayuan Sun","Jiewen Zhu","Luping Ji"],"pdf_url":"https://arxiv.org/pdf/2212.07047v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2212.07044v1","updated":"2022-12-14T05:37:50Z","published":"2022-12-14T05:37:50Z","title":"3D Neuron Morphology Analysis","summary":"  We consider the problem of finding an accurate representation of neuron\nshapes, extracting sub-cellular features, and classifying neurons based on\nneuron shapes. In neuroscience research, the skeleton representation is often\nused as a compact and abstract representation of neuron shapes. However,\nexisting methods are limited to getting and analyzing \"curve\" skeletons which\ncan only be applied for tubular shapes. This paper presents a 3D neuron\nmorphology analysis method for more general and complex neuron shapes. First,\nwe introduce the concept of skeleton mesh to represent general neuron shapes\nand propose a novel method for computing mesh representations from 3D surface\npoint clouds. A skeleton graph is then obtained from skeleton mesh and is used\nto extract sub-cellular features. Finally, an unsupervised learning method is\nused to embed the skeleton graph for neuron classification. Extensive\nexperiment results are provided and demonstrate the robustness of our method to\nanalyze neuron morphology.\n","authors":["Jiaxiang Jiang","Michael Goebel","Cezar Borba","William Smith","B. S. Manjunath"],"pdf_url":"https://arxiv.org/pdf/2212.07044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07039v1","updated":"2022-12-14T05:14:02Z","published":"2022-12-14T05:14:02Z","title":"Multi-Modal Domain Fusion for Multi-modal Aerial View Object\n  Classification","summary":"  Object detection and classification using aerial images is a challenging task\nas the information regarding targets are not abundant. Synthetic Aperture\nRadar(SAR) images can be used for Automatic Target Recognition(ATR) systems as\nit can operate in all-weather conditions and in low light settings. But, SAR\nimages contain salt and pepper noise(speckle noise) that cause hindrance for\nthe deep learning models to extract meaningful features. Using just aerial view\nElectro-optical(EO) images for ATR systems may also not result in high accuracy\nas these images are of low resolution and also do not provide ample information\nin extreme weather conditions. Therefore, information from multiple sensors can\nbe used to enhance the performance of Automatic Target Recognition(ATR)\nsystems. In this paper, we explore a methodology to use both EO and SAR sensor\ninformation to effectively improve the performance of the ATR systems by\nhandling the shortcomings of each of the sensors. A novel Multi-Modal Domain\nFusion(MDF) network is proposed to learn the domain invariant features from\nmulti-modal data and use it to accurately classify the aerial view objects. The\nproposed MDF network achieves top-10 performance in the Track-1 with an\naccuracy of 25.3 % and top-5 performance in Track-2 with an accuracy of 34.26 %\nin the test phase on the PBVS MAVOC Challenge dataset [18].\n","authors":["Sumanth Udupa","Aniruddh Sikdar","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2212.07039v1.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/1901.00109v4","updated":"2022-12-14T04:48:24Z","published":"2019-01-01T07:52:24Z","title":"Morphological Network: How Far Can We Go with Morphological Neurons?","summary":"  Morphological neurons, that is morphological operators such as dilation and\nerosion with learnable structuring elements, have intrigued researchers for\nquite some time because of the power these operators bring to the table despite\ntheir simplicity. These operators are known to be powerful nonlinear tools, but\nfor a given problem coming up with a sequence of operations and their\nstructuring element is a non-trivial task. So, the existing works have mainly\nfocused on this part of the problem without delving deep into their\napplicability as generic operators. A few works have tried to utilize\nmorphological neurons as a part of classification (and regression) networks\nwhen the input is a feature vector. However, these methods mainly focus on a\nspecific problem, without going into generic theoretical analysis. In this\nwork, we have theoretically analyzed morphological neurons and have shown that\nthese are far more powerful than previously anticipated. Our proposed\nmorphological block, containing dilation and erosion followed by their linear\ncombination, represents a sum of hinge functions. Existing works show that\nhinge functions perform quite well in classification and regression problems.\nTwo morphological blocks can even approximate any continuous function. However,\nto facilitate the theoretical analysis that we have done in this paper, we have\nrestricted ourselves to the 1D version of the operators, where the structuring\nelement operates on the whole input. Experimental evaluations also indicate the\neffectiveness of networks built with morphological neurons, over similarly\nstructured neural networks.\n","authors":["Ranjan Mondal","Sanchayan Santra","Soumendu Sundar Mukherjee","Bhabatosh Chanda"],"pdf_url":"https://arxiv.org/pdf/1901.00109v4.pdf","comment":"Accepted at BMVC 2022"},{"id":"http://arxiv.org/abs/2212.07026v1","updated":"2022-12-14T04:40:50Z","published":"2022-12-14T04:40:50Z","title":"Improving group robustness under noisy labels using predictive\n  uncertainty","summary":"  The standard empirical risk minimization (ERM) can underperform on certain\nminority groups (i.e., waterbirds in lands or landbirds in water) due to the\nspurious correlation between the input and its label. Several studies have\nimproved the worst-group accuracy by focusing on the high-loss samples. The\nhypothesis behind this is that such high-loss samples are\n\\textit{spurious-cue-free} (SCF) samples. However, these approaches can be\nproblematic since the high-loss samples may also be samples with noisy labels\nin the real-world scenarios. To resolve this issue, we utilize the predictive\nuncertainty of a model to improve the worst-group accuracy under noisy labels.\nTo motivate this, we theoretically show that the high-uncertainty samples are\nthe SCF samples in the binary classification problem. This theoretical result\nimplies that the predictive uncertainty is an adequate indicator to identify\nSCF samples in a noisy label setting. Motivated from this, we propose a novel\nENtropy based Debiasing (END) framework that prevents models from learning the\nspurious cues while being robust to the noisy labels. In the END framework, we\nfirst train the \\textit{identification model} to obtain the SCF samples from a\ntraining set using its predictive uncertainty. Then, another model is trained\non the dataset augmented with an oversampled SCF set. The experimental results\nshow that our END framework outperforms other strong baselines on several\nreal-world benchmarks that consider both the noisy labels and the\nspurious-cues.\n","authors":["Dongpin Oh","Dae Lee","Jeunghyun Byun","Bonggun Shin"],"pdf_url":"https://arxiv.org/pdf/2212.07026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07023v1","updated":"2022-12-14T04:26:32Z","published":"2022-12-14T04:26:32Z","title":"Unsupervised Domain Adaptation for Automated Knee Osteoarthritis\n  Phenotype Classification","summary":"  Purpose: The aim of this study was to demonstrate the utility of unsupervised\ndomain adaptation (UDA) in automated knee osteoarthritis (OA) phenotype\nclassification using a small dataset (n=50). Materials and Methods: For this\nretrospective study, we collected 3,166 three-dimensional (3D) double-echo\nsteady-state magnetic resonance (MR) images from the Osteoarthritis Initiative\ndataset and 50 3D turbo/fast spin-echo MR images from our institute (in 2020\nand 2021) as the source and target datasets, respectively. For each patient,\nthe degree of knee OA was initially graded according to the MRI Osteoarthritis\nKnee Score (MOAKS) before being converted to binary OA phenotype labels. The\nproposed UDA pipeline included (a) pre-processing, which involved automatic\nsegmentation and region-of-interest cropping; (b) source classifier training,\nwhich involved pre-training phenotype classifiers on the source dataset; (c)\ntarget encoder adaptation, which involved unsupervised adaption of the source\nencoder to the target encoder and (d) target classifier validation, which\ninvolved statistical analysis of the target classification performance\nevaluated by the area under the receiver operating characteristic curve\n(AUROC), sensitivity, specificity and accuracy. Additionally, a classifier was\ntrained without UDA for comparison. Results: The target classifier trained with\nUDA achieved improved AUROC, sensitivity, specificity and accuracy for both\nknee OA phenotypes compared with the classifier trained without UDA.\nConclusion: The proposed UDA approach improves the performance of automated\nknee OA phenotype classification for small target datasets by utilising a\nlarge, high-quality source dataset for training. The results successfully\ndemonstrated the advantages of the UDA approach in classification on small\ndatasets.\n","authors":["Junru Zhong","Yongcheng Yao","Donal G. Cahill","Fan Xiao","Siyue Li","Jack Lee","Kevin Ki-Wai Ho","Michael Tim-Yun Ong","James F. Griffith","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07023v1.pdf","comment":"Junru Zhong and Yongcheng Yao share the same contribution. 17 pages,\n  4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.07020v1","updated":"2022-12-14T04:19:45Z","published":"2022-12-14T04:19:45Z","title":"Object Delineation in Satellite Images","summary":"  Machine learning is being widely applied to analyze satellite data with\nproblems such as classification and feature detection. Unlike traditional image\nprocessing algorithms, geospatial applications need to convert the detected\nobjects from a raster form to a geospatial vector form to further analyze it.\nThis gem delivers a simple and light-weight algorithm for delineating the\npixels that are marked by ML algorithms to extract geospatial objects from\nsatellite images. The proposed algorithm is exact and users can further apply\nsimplification and approximation based on the application needs.\n","authors":["Zhuocheng Shang","Ahmed Eldawy"],"pdf_url":"https://arxiv.org/pdf/2212.07020v1.pdf","comment":"7 Pages, 4 Figures, 1 Table, to be submitted to the 4th ACM\n  SIGSPATIAL International Workshop on Spatial Gems (SpatialGems 2022)"},{"id":"http://arxiv.org/abs/2212.07016v1","updated":"2022-12-14T04:08:56Z","published":"2022-12-14T04:08:56Z","title":"Understanding Zero-Shot Adversarial Robustness for Large-Scale Models","summary":"  Pretrained large-scale vision-language models like CLIP have exhibited strong\ngeneralization over unseen tasks. Yet imperceptible adversarial perturbations\ncan significantly reduce CLIP's performance on new tasks. In this work, we\nidentify and explore the problem of \\emph{adapting large-scale models for\nzero-shot adversarial robustness}. We first identify two key factors during\nmodel adaption -- training losses and adaptation methods -- that affect the\nmodel's zero-shot adversarial robustness. We then propose a text-guided\ncontrastive adversarial training loss, which aligns the text embeddings and the\nadversarial visual features with contrastive learning on a small set of\ntraining data. We apply this training loss to two adaption methods, model\nfinetuning and visual prompt tuning. We find that visual prompt tuning is more\neffective in the absence of texts, while finetuning wins in the existence of\ntext guidance. Overall, our approach significantly improves the zero-shot\nadversarial robustness over CLIP, seeing an average improvement of over 31\npoints over ImageNet and 15 zero-shot datasets. We hope this work can shed\nlight on understanding the zero-shot adversarial robustness of large-scale\nmodels.\n","authors":["Chengzhi Mao","Scott Geng","Junfeng Yang","Xin Wang","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2212.07016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07010v1","updated":"2022-12-14T03:48:00Z","published":"2022-12-14T03:48:00Z","title":"Cross-Domain Video Anomaly Detection without Target Domain Adaptation","summary":"  Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume\nthat at least few task-relevant target domain training data are available for\nadaptation from the source to the target domain. However, this requires\nlaborious model-tuning by the end-user who may prefer to have a system that\nworks ``out-of-the-box.\" To address such practical scenarios, we identify a\nnovel target domain (inference-time) VAD task where no target domain training\ndata are available. To this end, we propose a new `Zero-shot Cross-domain Video\nAnomaly Detection (zxvad)' framework that includes a future-frame prediction\ngenerative model setup. Different from prior future-frame prediction models,\nour model uses a novel Normalcy Classifier module to learn the features of\nnormal event videos by learning how such features are different ``relatively\"\nto features in pseudo-abnormal examples. A novel Untrained Convolutional Neural\nNetwork based Anomaly Synthesis module crafts these pseudo-abnormal examples by\nadding foreign objects in normal video frames with no extra training cost. With\nour novel relative normalcy feature learning strategy, zxvad generalizes and\nlearns to distinguish between normal and abnormal frames in a new target domain\nwithout adaptation during inference. Through evaluations on common datasets, we\nshow that zxvad outperforms the state-of-the-art (SOTA), regardless of whether\ntask-relevant (i.e., VAD) source training data are available or not. Lastly,\nzxvad also beats the SOTA methods in inference-time efficiency metrics\nincluding the model size, total parameters, GPU energy consumption, and GMACs.\n","authors":["Abhishek Aich","Kuan-Chuan Peng","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2212.07010v1.pdf","comment":"Accepted at WACV 2023; Includes Supplementary Material"},{"id":"http://arxiv.org/abs/2212.06344v2","updated":"2022-12-14T03:17:20Z","published":"2022-12-13T02:54:54Z","title":"DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization","summary":"  We present a neural technique for learning to select a local sub-region\naround a point which can be used for mesh parameterization. The motivation for\nour framework is driven by interactive workflows used for decaling, texturing,\nor painting on surfaces. Our key idea is to incorporate segmentation\nprobabilities as weights of a classical parameterization method, implemented as\na novel differentiable parameterization layer within a neural network\nframework. We train a segmentation network to select 3D regions that are\nparameterized into 2D and penalized by the resulting distortion, giving rise to\nsegmentations which are distortion-aware. Following training, a user can use\nour system to interactively select a point on the mesh and obtain a large,\nmeaningful region around the selection which induces a low-distortion\nparameterization. Our code and project page are currently available.\n","authors":["Richard Liu","Noam Aigerman","Vladimir G. Kim","Rana Hanocka"],"pdf_url":"https://arxiv.org/pdf/2212.06344v2.pdf","comment":"Project page: https://threedle.github.io/DA-Wand/ Code:\n  https://github.com/threedle/DA-Wand"},{"id":"http://arxiv.org/abs/2107.05583v3","updated":"2022-12-14T03:11:01Z","published":"2021-07-12T17:01:11Z","title":"Few-shot Learning with Global Relatedness Decoupled-Distillation","summary":"  Despite the success that metric learning based approaches have achieved in\nfew-shot learning, recent works reveal the ineffectiveness of their episodic\ntraining mode. In this paper, we point out two potential reasons for this\nproblem: 1) the random episodic labels can only provide limited supervision\ninformation, while the relatedness information between the query and support\nsamples is not fully exploited; 2) the meta-learner is usually constrained by\nthe limited contextual information of the local episode. To overcome these\nproblems, we propose a new Global Relatedness Decoupled-Distillation (GRDD)\nmethod using the global category knowledge and the Relatedness\nDecoupled-Distillation (RDD) strategy. Our GRDD learns new visual concepts\nquickly by imitating the habit of humans, i.e. learning from the deep knowledge\ndistilled from the teacher. More specifically, we first train a global learner\non the entire base subset using category labels as supervision to leverage the\nglobal context information of the categories. Then, the well-trained global\nlearner is used to simulate the query-support relatedness in global\ndependencies. Finally, the distilled global query-support relatedness is\nexplicitly used to train the meta-learner using the RDD strategy, with the goal\nof making the meta-learner more discriminative. The RDD strategy aims to\ndecouple the dense query-support relatedness into the groups of sparse\ndecoupled relatedness. Moreover, only the relatedness of a single support\nsample with other query samples is considered in each group. By distilling the\nsparse decoupled relatedness group by group, sharper relatedness can be\neffectively distilled to the meta-learner, thereby facilitating the learning of\na discriminative meta-learner. We conduct extensive experiments on the\nminiImagenet and CIFAR-FS datasets, which show the state-of-the-art performance\nof our GRDD method.\n","authors":["Yuan Zhou","Yanrong Guo","Shijie Hao","Richang Hong","Zhengjun Zha","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2107.05583v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2211.07218v3","updated":"2022-12-14T02:22:06Z","published":"2022-11-14T09:20:48Z","title":"SA-DPSGD: Differentially Private Stochastic Gradient Descent based on\n  Simulated Annealing","summary":"  Differential privacy (DP) provides a formal privacy guarantee that prevents\nadversaries with access to machine learning models from extracting information\nabout individual training points. Differentially private stochastic gradient\ndescent (DPSGD) is the most popular training method with differential privacy\nin image recognition. However, existing DPSGD schemes lead to significant\nperformance degradation, which prevents the application of differential\nprivacy. In this paper, we propose a simulated annealing-based differentially\nprivate stochastic gradient descent scheme (SA-DPSGD) which accepts a candidate\nupdate with a probability that depends both on the update quality and on the\nnumber of iterations. Through this random update screening, we make the\ndifferentially private gradient descent proceed in the right direction in each\niteration, and result in a more accurate model finally. In our experiments,\nunder the same hyperparameters, our scheme achieves test accuracies 98.35%,\n87.41% and 60.92% on datasets MNIST, FashionMNIST and CIFAR10, respectively,\ncompared to the state-of-the-art result of 98.12%, 86.33% and 59.34%. Under the\nfreely adjusted hyperparameters, our scheme achieves even higher accuracies,\n98.89%, 88.50% and 64.17%. We believe that our method has a great contribution\nfor closing the accuracy gap between private and non-private image\nclassification.\n","authors":["Jie Fu","Zhili Chen","XinPeng Ling"],"pdf_url":"https://arxiv.org/pdf/2211.07218v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05481v4","updated":"2022-12-14T02:13:16Z","published":"2022-08-10T14:04:20Z","title":"High-Frequency Space Diffusion Models for Accelerated MRI","summary":"  Diffusion models with continuous stochastic differential equations (SDEs)\nhave shown superior performances in image generation. It can be used as a deep\ngenerative prior to solve the inverse problem in MR reconstruction. However,\nthe existing VP-SDE can be treated as maximizing the energy of the MR image to\nbe reconstructed and may lead to SDE sequence divergence. The VE-SDE based MR\nreconstruction is not consistent with actual diffusion process. In addition,\nboth VE- and VP-SDEs-based models suffer from a time-consuming sampling\nprocedure, resulting long reconstruction time. In this study, a new SDE\nfocusing on the diffusion process in high-frequency space is designed\nspecifically for robust MR reconstruction based on diffusion models.\nExperiments on the publicly fastMRI dataset show that HFS-SDE based\nreconstruction method outperforms the parallel imaging, supervised deep\nlearning, and existing VE- and VP-SDEs-based methods in terms of reconstruction\naccuracy. It also improves the stability of MR reconstruction and accelerates\nsampling procedure of reverse diffusion.\n","authors":["Chentao Cao","Zhuo-Xu Cui","Shaonan Liu","Hairong Zheng","Dong Liang","Yanjie Zhu"],"pdf_url":"https://arxiv.org/pdf/2208.05481v4.pdf","comment":"submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2212.00648v2","updated":"2022-12-14T02:12:27Z","published":"2022-12-01T16:49:53Z","title":"One-shot recognition of any material anywhere using contrastive learning\n  with physics-based rendering","summary":"  We present MatSim: a synthetic dataset, a benchmark, and a method for\ncomputer vision based recognition of similarities and transitions between\nmaterials and textures, focusing on identifying any material under any\nconditions using one or a few examples (one-shot learning). The visual\nrecognition of materials is essential to everything from examining food while\ncooking to inspecting agriculture, chemistry, and industrial products. In this\nwork, we utilize giant repositories used by computer graphics artists to\ngenerate a new CGI dataset for material similarity. We use physics-based\nrendering (PBR) repositories for visual material simulation, assign these\nmaterials random 3D objects, and render images with a vast range of backgrounds\nand illumination conditions (HDRI). We add a gradual transition between\nmaterials to support applications with a smooth transition between states (like\ngradually cooked food). We also render materials inside transparent containers\nto support beverage and chemistry lab use cases. We then train a contrastive\nlearning network to generate a descriptor that identifies unfamiliar materials\nusing a single image. We also present a new benchmark for a few-shot material\nrecognition that contains a wide range of real-world examples, including the\nstate of a chemical reaction, rotten/fresh fruits, states of food, different\ntypes of construction materials, types of ground, and many other use cases\ninvolving material states, transitions and subclasses. We show that a network\ntrained on the MatSim synthetic dataset outperforms state-of-the-art models\nlike Clip on the benchmark, despite being tested on material classes that were\nnot seen during training. The dataset, benchmark, code and trained models are\navailable online.\n","authors":["Manuel S. Drehwald","Sagi Eppel","Jolina Li","Han Hao","Alan Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2212.00648v2.pdf","comment":"for associated code and dataset, see\n  https://zenodo.org/record/7390166#.Y5ku6mHMJH4 or\n  https://e1.pcloud.link/publink/show?code=kZIiSQZCYU5M4HOvnQykql9jxF4h0KiC5MX\n  and https://icedrive.net/s/A13FWzZ8V2aP9T4ufGQ1N3fBZxDF"},{"id":"http://arxiv.org/abs/2212.06971v1","updated":"2022-12-14T01:37:16Z","published":"2022-12-14T01:37:16Z","title":"Find Someone Who: Visual Commonsense Understanding in Human-Centric\n  Grounding","summary":"  From a visual scene containing multiple people, human is able to distinguish\neach individual given the context descriptions about what happened before,\ntheir mental/physical states or intentions, etc. Above ability heavily relies\non human-centric commonsense knowledge and reasoning. For example, if asked to\nidentify the \"person who needs healing\" in an image, we need to first know that\nthey usually have injuries or suffering expressions, then find the\ncorresponding visual clues before finally grounding the person. We present a\nnew commonsense task, Human-centric Commonsense Grounding, that tests the\nmodels' ability to ground individuals given the context descriptions about what\nhappened before, and their mental/physical states or intentions. We further\ncreate a benchmark, HumanCog, a dataset with 130k grounded commonsensical\ndescriptions annotated on 67k images, covering diverse types of commonsense and\nvisual scenes. We set up a context-object-aware method as a strong baseline\nthat outperforms previous pre-trained and non-pretrained models. Further\nanalysis demonstrates that rich visual commonsense and powerful integration of\nmulti-modal commonsense are essential, which sheds light on future works. Data\nand code will be available https://github.com/Hxyou/HumanCog.\n","authors":["Haoxuan You","Rui Sun","Zhecan Wang","Kai-Wei Chang","Shih-Fu Chang"],"pdf_url":"https://arxiv.org/pdf/2212.06971v1.pdf","comment":"11 pages, 7 figures. EMNLP 2022-findings"},{"id":"http://arxiv.org/abs/2212.06969v1","updated":"2022-12-14T01:28:12Z","published":"2022-12-14T01:28:12Z","title":"Localizing Objects in 3D from Egocentric Videos with Visual Queries","summary":"  With the recent advances in video and 3D understanding, novel 4D\nspatio-temporal challenges fusing both concepts have emerged. Towards this\ndirection, the Ego4D Episodic Memory Benchmark proposed a task for Visual\nQueries with 3D Localization (VQ3D). Given an egocentric video clip and an\nimage crop depicting a query object, the goal is to localize the 3D position of\nthe center of that query object with respect to the camera pose of a query\nframe. Current methods tackle the problem of VQ3D by lifting the 2D\nlocalization results of the sister task Visual Queries with 2D Localization\n(VQ2D) into a 3D reconstruction. Yet, we point out that the low number of\nQueries with Poses (QwP) from previous VQ3D methods severally hinders their\noverall success rate and highlights the need for further effort in 3D modeling\nto tackle the VQ3D task. In this work, we formalize a pipeline that better\nentangles 3D multiview geometry with 2D object retrieval from egocentric\nvideos. We estimate more robust camera poses, leading to more successful object\nqueries and substantially improved VQ3D performance. In practice, our method\nreaches a top-1 overall success rate of 86.36% on the Ego4D Episodic Memory\nBenchmark VQ3D, a 10x improvement over the previous state-of-the-art. In\naddition, we provide a complete empirical study highlighting the remaining\nchallenges in VQ3D.\n","authors":["Jinjie Mai","Abdullah Hamdi","Silvio Giancola","Chen Zhao","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2212.06969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06958v1","updated":"2022-12-14T00:30:09Z","published":"2022-12-14T00:30:09Z","title":"A Novel Active Solution for Two-Dimensional Face Presentation Attack\n  Detection","summary":"  Identity authentication is the process of verifying one's identity. There are\nseveral identity authentication methods, among which biometric authentication\nis of utmost importance. Facial recognition is a sort of biometric\nauthentication with various applications, such as unlocking mobile phones and\naccessing bank accounts. However, presentation attacks pose the greatest threat\nto facial recognition. A presentation attack is an attempt to present a\nnon-live face, such as a photo, video, mask, and makeup, to the camera.\nPresentation attack detection is a countermeasure that attempts to identify\nbetween a genuine user and a presentation attack. Several industries, such as\nfinancial services, healthcare, and education, use biometric authentication\nservices on various devices. This illustrates the significance of presentation\nattack detection as the verification step. In this paper, we study\nstate-of-the-art to cover the challenges and solutions related to presentation\nattack detection in a single place. We identify and classify different\npresentation attack types and identify the state-of-the-art methods that could\nbe used to detect each of them. We compare the state-of-the-art literature\nregarding attack types, evaluation metrics, accuracy, and datasets and discuss\nresearch and industry challenges of presentation attack detection. Most\npresentation attack detection approaches rely on extensive data training and\nquality, making them difficult to implement. We introduce an efficient active\npresentation attack detection approach that overcomes weaknesses in the\nexisting literature. The proposed approach does not require training data, is\nCPU-light, can process low-quality images, has been tested with users of\nvarious ages and is shown to be user-friendly and highly robust to\n2-dimensional presentation attacks.\n","authors":["Matineh Pooshideh"],"pdf_url":"https://arxiv.org/pdf/2212.06958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10785v2","updated":"2022-12-14T00:01:30Z","published":"2022-09-22T05:04:09Z","title":"Deep Lake: a Lakehouse for Deep Learning","summary":"  Traditional data lakes provide critical data infrastructure for analytical\nworkloads by enabling time travel, running SQL queries, ingesting data with\nACID transactions, and visualizing petabyte-scale datasets on cloud storage.\nThey allow organizations to break down data silos, unlock data-driven\ndecision-making, improve operational efficiency, and reduce costs. However, as\ndeep learning usage increases, traditional data lakes are not well-designed for\napplications such as natural language processing (NLP), audio processing,\ncomputer vision, and applications involving non-tabular datasets. This paper\npresents Deep Lake, an open-source lakehouse for deep learning applications\ndeveloped at Activeloop. Deep Lake maintains the benefits of a vanilla data\nlake with one key difference: it stores complex data, such as images, videos,\nannotations, as well as tabular data, in the form of tensors and rapidly\nstreams the data over the network to (a) Tensor Query Language, (b) in-browser\nvisualization engine, or (c) deep learning frameworks without sacrificing GPU\nutilization. Datasets stored in Deep Lake can be accessed from PyTorch,\nTensorFlow, JAX, and integrate with numerous MLOps tools.\n","authors":["Sasun Hambardzumyan","Abhinav Tuli","Levon Ghukasyan","Fariz Rahman","Hrant Topchyan","David Isayan","Mark McQuade","Mikayel Harutyunyan","Tatevik Hakobyan","Ivo Stranic","Davit Buniatyan"],"pdf_url":"https://arxiv.org/pdf/2209.10785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07555v1","updated":"2022-12-14T23:59:24Z","published":"2022-12-14T23:59:24Z","title":"IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object\n  Interactions","summary":"  Can we make virtual characters in a scene interact with their surrounding\nobjects through simple instructions? Is it possible to synthesize such motion\nplausibly with a diverse set of objects and instructions? Inspired by these\nquestions, we present the first framework to synthesize the full-body motion of\nvirtual human characters performing specified actions with 3D objects placed\nwithin their reach. Our system takes as input textual instructions specifying\nthe objects and the associated intentions of the virtual characters and outputs\ndiverse sequences of full-body motions. This is in contrast to existing work,\nwhere full-body action synthesis methods generally do not consider object\ninteractions, and human-object interaction methods focus mainly on synthesizing\nhand or finger movements for grasping objects. We accomplish our objective by\ndesigning an intent-driven full-body motion generator, which uses a pair of\ndecoupled conditional variational autoencoders (CVAE) to learn the motion of\nthe body parts in an autoregressive manner. We also optimize for the positions\nof the objects with six degrees of freedom (6DoF) such that they plausibly fit\nwithin the hands of the synthesized characters. We compare our proposed method\nwith the existing methods of motion synthesis and establish a new and stronger\nstate-of-the-art for the task of intent-driven motion synthesis. Through a user\nstudy, we further show that our synthesized full-body motions appear more\nrealistic to the participants in more than 80% of scenarios compared to the\ncurrent state-of-the-art methods, and are perceived to be as good as the ground\ntruth on several occasions.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2212.07555v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2207.09771v2","updated":"2022-12-14T23:24:55Z","published":"2022-07-20T09:26:29Z","title":"Localization supervision of chest x-ray classifiers using label-specific\n  eye-tracking annotation","summary":"  Convolutional neural networks (CNNs) have been successfully applied to chest\nx-ray (CXR) images. Moreover, annotated bounding boxes have been shown to\nimprove the interpretability of a CNN in terms of localizing abnormalities.\nHowever, only a few relatively small CXR datasets containing bounding boxes are\navailable, and collecting them is very costly. Opportunely, eye-tracking (ET)\ndata can be collected in a non-intrusive way during the clinical workflow of a\nradiologist. We use ET data recorded from radiologists while dictating CXR\nreports to train CNNs. We extract snippets from the ET data by associating them\nwith the dictation of keywords and use them to supervise the localization of\nspecific abnormalities. We show that this method improves a model's\ninterpretability without impacting its image-level classification.\n","authors":["Ricardo Bigolin Lanfredi","Joyce D. Schroeder","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2207.09771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07527v1","updated":"2022-12-14T22:22:45Z","published":"2022-12-14T22:22:45Z","title":"Plastic Contaminant Detection in Aerial Imagery of Cotton Fields with\n  Deep Learning","summary":"  Plastic shopping bags that get carried away from the side of roads and\ntangled on cotton plants can end up at cotton gins if not removed before the\nharvest. Such bags may not only cause problem in the ginning process but might\nalso get embodied in cotton fibers reducing its quality and marketable value.\nTherefore, it is required to detect, locate, and remove the bags before cotton\nis harvested. Manually detecting and locating these bags in cotton fields is\nlabor intensive, time-consuming and a costly process. To solve these\nchallenges, we present application of four variants of YOLOv5 (YOLOv5s,\nYOLOv5m, YOLOv5l and YOLOv5x) for detecting plastic shopping bags using\nUnmanned Aircraft Systems (UAS)-acquired RGB (Red, Green, and Blue) images. We\nalso show fixed effect model tests of color of plastic bags as well as\nYOLOv5-variant on average precision (AP), mean average precision (mAP@50) and\naccuracy. In addition, we also demonstrate the effect of height of plastic bags\non the detection accuracy. It was found that color of bags had significant\neffect (p < 0.001) on accuracy across all the four variants while it did not\nshow any significant effect on the AP with YOLOv5m (p = 0.10) and YOLOv5x (p =\n0.35) at 95% confidence level. Similarly, YOLOv5-variant did not show any\nsignificant effect on the AP (p = 0.11) and accuracy (p = 0.73) of white bags,\nbut it had significant effects on the AP (p = 0.03) and accuracy (p = 0.02) of\nbrown bags including on the mAP@50 (p = 0.01) and inference speed (p < 0.0001).\nAdditionally, height of plastic bags had significant effect (p < 0.0001) on\noverall detection accuracy. The findings reported in this paper can be useful\nin speeding up removal of plastic bags from cotton fields before harvest and\nthereby reducing the amount of contaminants that end up at cotton gins.\n","authors":["Pappu Kumar Yadav","J. Alex Thomasson","Robert G. Hardin","Stephen W. Searcy","Ulisses Braga-Neto","Sorin C. Popescu","Roberto Rodriguez","Daniel E Martin","Juan Enciso","Karem Meza","Emma L. White"],"pdf_url":"https://arxiv.org/pdf/2212.07527v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2212.07501v1","updated":"2022-12-14T20:46:50Z","published":"2022-12-14T20:46:50Z","title":"Diffusion Probabilistic Models beat GANs on Medical Images","summary":"  The success of Deep Learning applications critically depends on the quality\nand scale of the underlying training data. Generative adversarial networks\n(GANs) can generate arbitrary large datasets, but diversity and fidelity are\nlimited, which has recently been addressed by denoising diffusion probabilistic\nmodels (DDPMs) whose superiority has been demonstrated on natural images. In\nthis study, we propose Medfusion, a conditional latent DDPM for medical images.\nWe compare our DDPM-based model against GAN-based models, which constitute the\ncurrent state-of-the-art in the medical domain. Medfusion was trained and\ncompared with (i) StyleGan-3 on n=101,442 images from the AIROGS challenge\ndataset to generate fundoscopies with and without glaucoma, (ii) ProGAN on\nn=191,027 from the CheXpert dataset to generate radiographs with and without\ncardiomegaly and (iii) wGAN on n=19,557 images from the CRCMS dataset to\ngenerate histopathological images with and without microsatellite stability. In\nthe AIROGS, CRMCS, and CheXpert datasets, Medfusion achieved lower (=better)\nFID than the GANs (11.63 versus 20.43, 30.03 versus 49.26, and 17.28 versus\n84.31). Also, fidelity (precision) and diversity (recall) were higher (=better)\nfor Medfusion in all three datasets. Our study shows that DDPM are a superior\nalternative to GANs for image synthesis in the medical domain.\n","authors":["Gustav Müller-Franzes","Jan Moritz Niehues","Firas Khader","Soroosh Tayebi Arasteh","Christoph Haarburger","Christiane Kuhl","Tianci Wang","Tianyu Han","Sven Nebelung","Jakob Nikolas Kather","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2212.07501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07497v1","updated":"2022-12-14T20:31:43Z","published":"2022-12-14T20:31:43Z","title":"Towards fully automated deep-learning-based brain tumor segmentation: is\n  brain extraction still necessary?","summary":"  State-of-the-art brain tumor segmentation is based on deep learning models\napplied to multi-modal MRIs. Currently, these models are trained on images\nafter a preprocessing stage that involves registration, interpolation, brain\nextraction (BE, also known as skull-stripping) and manual correction by an\nexpert. However, for clinical practice, this last step is tedious and\ntime-consuming and, therefore, not always feasible, resulting in\nskull-stripping faults that can negatively impact the tumor segmentation\nquality. Still, the extent of this impact has never been measured for any of\nthe many different BE methods available. In this work, we propose an automatic\nbrain tumor segmentation pipeline and evaluate its performance with multiple BE\nmethods. Our experiments show that the choice of a BE method can compromise up\nto 15.7% of the tumor segmentation performance. Moreover, we propose training\nand testing tumor segmentation models on non-skull-stripped images, effectively\ndiscarding the BE step from the pipeline. Our results show that this approach\nleads to a competitive performance at a fraction of the time. We conclude that,\nin contrast to the current paradigm, training tumor segmentation models on\nnon-skull-stripped images can be the best option when high performance in\nclinical practice is desired.\n","authors":["Bruno Machado Pacheco","Guilherme de Souza e Cassia","Danilo Silva"],"pdf_url":"https://arxiv.org/pdf/2212.07497v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.07495v1","updated":"2022-12-14T20:28:50Z","published":"2022-12-14T20:28:50Z","title":"SAIF: Sparse Adversarial and Interpretable Attack Framework","summary":"  Adversarial attacks hamper the decision-making ability of neural networks by\nperturbing the input signal. The addition of calculated small distortion to\nimages, for instance, can deceive a well-trained image classification network.\nIn this work, we propose a novel attack technique called Sparse Adversarial and\nInterpretable Attack Framework (SAIF). Specifically, we design imperceptible\nattacks that contain low-magnitude perturbations at a small number of pixels\nand leverage these sparse attacks to reveal the vulnerability of classifiers.\nWe use the Frank-Wolfe (conditional gradient) algorithm to simultaneously\noptimize the attack perturbations for bounded magnitude and sparsity with\n$O(1/\\sqrt{T})$ convergence. Empirical results show that SAIF computes highly\nimperceptible and interpretable adversarial examples, and outperforms\nstate-of-the-art sparse attack methods on the ImageNet dataset.\n","authors":["Tooba Imtiaz","Morgan Kohler","Jared Miller","Zifeng Wang","Mario Sznaier","Octavia Camps","Jennifer Dy"],"pdf_url":"https://arxiv.org/pdf/2212.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07476v1","updated":"2022-12-14T19:50:35Z","published":"2022-12-14T19:50:35Z","title":"The Infinite Index: Information Retrieval on Generative Text-To-Image\n  Models","summary":"  The text-to-image model Stable Diffusion has recently become very popular.\nOnly weeks after its open source release, millions are experimenting with image\ngeneration. This is due to its ease of use, since all it takes is a brief\ndescription of the desired image to \"prompt\" the generative model. Rarely do\nthe images generated for a new prompt immediately meet the user's expectations.\nUsually, an iterative refinement of the prompt (\"prompt engineering\") is\nnecessary for satisfying images. As a new perspective, we recast image prompt\nengineering as interactive image retrieval - on an \"infinite index\". Thereby, a\nprompt corresponds to a query and prompt engineering to query refinement.\nSelected image-prompt pairs allow direct relevance feedback, as the model can\nmodify an image for the refined prompt. This is a form of one-sided interactive\nretrieval, where the initiative is on the user side, whereas the server side\nremains stateless. In light of an extensive literature review, we develop these\nparallels in detail and apply the findings to a case study of a creative search\ntask on such a model. We note that the uncertainty in searching an infinite\nindex is virtually never-ending. We also discuss future research opportunities\nrelated to retrieval models specialized for generative models and interactive\ngenerative image retrieval. The application of IR technology, such as query\nreformulation and relevance feedback, will contribute to improved workflows\nwhen using generative models, while the notion of an infinite index raises new\nchallenges in IR research.\n","authors":["Niklas Deckers","Maik Fröbe","Johannes Kiesel","Gianluca Pandolfo","Christopher Schröder","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2212.07476v1.pdf","comment":"Accepted at CHIIR 2023"},{"id":"http://arxiv.org/abs/2212.07784v1","updated":"2022-12-14T18:50:20Z","published":"2022-12-14T18:50:20Z","title":"RTMDet: An Empirical Study of Designing Real-Time Object Detectors","summary":"  In this paper, we aim to design an efficient real-time object detector that\nexceeds the YOLO series and is easily extensible for many object recognition\ntasks such as instance segmentation and rotated object detection. To obtain a\nmore efficient model architecture, we explore an architecture that has\ncompatible capacities in the backbone and neck, constructed by a basic building\nblock that consists of large-kernel depth-wise convolutions. We further\nintroduce soft labels when calculating matching costs in the dynamic label\nassignment to improve accuracy. Together with better training techniques, the\nresulting object detector, named RTMDet, achieves 52.8% AP on COCO with 300+\nFPS on an NVIDIA 3090 GPU, outperforming the current mainstream industrial\ndetectors. RTMDet achieves the best parameter-accuracy trade-off with\ntiny/small/medium/large/extra-large model sizes for various application\nscenarios, and obtains new state-of-the-art performance on real-time instance\nsegmentation and rotated object detection. We hope the experimental results can\nprovide new insights into designing versatile real-time object detectors for\nmany object recognition tasks. Code and models are released at\nhttps://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet.\n","authors":["Chengqi Lyu","Wenwei Zhang","Haian Huang","Yue Zhou","Yudong Wang","Yanyi Liu","Shilong Zhang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07784v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.07786v1","updated":"2022-12-14T17:34:03Z","published":"2022-12-14T17:34:03Z","title":"Convergent Data-driven Regularizations for CT Reconstruction","summary":"  The reconstruction of images from their corresponding noisy Radon transform\nis a typical example of an ill-posed linear inverse problem as arising in the\napplication of computerized tomography (CT). As the (na\\\"{\\i}ve) solution does\nnot depend on the measured data continuously, regularization is needed to\nre-establish a continuous dependence. In this work, we investigate simple, but\nyet still provably convergent approaches to learning linear regularization\nmethods from data. More specifically, we analyze two approaches: One generic\nlinear regularization that learns how to manipulate the singular values of the\nlinear operator in an extension of [1], and one tailored approach in the\nFourier domain that is specific to CT-reconstruction. We prove that such\napproaches become convergent regularization methods as well as the fact that\nthe reconstructions they provide are typically much smoother than the training\ndata they were trained on. Finally, we compare the spectral as well as the\nFourier-based approaches for CT-reconstruction numerically, discuss their\nadvantages and disadvantages and investigate the effect of discretization\nerrors at different resolutions.\n","authors":["Samira Kabri","Alexander Auras","Danilo Riccio","Hartmut Bauermeister","Martin Benning","Michael Moeller","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2212.07786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10571v2","updated":"2022-12-14T15:37:48Z","published":"2022-06-21T17:50:29Z","title":"Toward Unpaired Multi-modal Medical Image Segmentation via Learning\n  Structured Semantic Consistency","summary":"  Integrating multi-modal data to improve medical image analysis has received\ngreat attention recently. However, due to the modal discrepancy, how to use a\nsingle model to process the data from multiple modalities is still an open\nissue. In this paper, we propose a novel scheme to achieve better pixel-level\nsegmentation for unpaired multi-modal medical images. Different from previous\nmethods which adopted both modality-specific and modality-shared modules to\naccommodate the appearance variance of different modalities while extracting\nthe common semantic information, our method is based on a single Transformer\nwith a carefully designed External Attention Module (EAM) to learn the\nstructured semantic consistency (i.e. semantic class representations and their\ncorrelations) between modalities in the training phase. In practice, the\nabove-mentioned structured semantic consistency across modalities can be\nprogressively achieved by implementing the consistency regularization at the\nmodality-level and image-level respectively. The proposed EAMs are adopted to\nlearn the semantic consistency for different scale representations and can be\ndiscarded once the model is optimized. Therefore, during the testing phase, we\nonly need to maintain one Transformer for all modal predictions, which nicely\nbalances the model's ease of use and simplicity. To demonstrate the\neffectiveness of the proposed method, we conduct the experiments on two medical\nimage segmentation scenarios: (1) cardiac structure segmentation, and (2)\nabdominal multi-organ segmentation. Extensive results show that the proposed\nmethod outperforms the state-of-the-art methods by a wide margin, and even\nachieves competitive performance with extremely limited training samples (e.g.,\n1 or 3 annotated CT or MRI images) for one specific modality.\n","authors":["Jie Yang","Ye Zhu","Ruimao Zhang","Chaoqun Wang","Zhen Li","Xiang Wan"],"pdf_url":"https://arxiv.org/pdf/2206.10571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07431v1","updated":"2022-12-14T13:21:37Z","published":"2022-12-14T13:21:37Z","title":"Projection-Domain Self-Supervision for Volumetric Helical CT\n  Reconstruction","summary":"  We propose a deep learning method for three-dimensional reconstruction in\nlow-dose helical cone-beam computed tomography. We reconstruct the volume\ndirectly, i.e., not from 2D slices, guaranteeing consistency along all axes. In\na crucial step beyond prior work, we train our model in a self-supervised\nmanner in the projection domain using noisy 2D projection data, without relying\non 3D reference data or the output of a reference reconstruction method. This\nmeans the fidelity of our results is not limited by the quality and\navailability of such data. We evaluate our method on real helical cone-beam\nprojections and simulated phantoms. Our reconstructions are sharper and less\nnoisy than those of previous methods, and several decibels better in\nquantitative PSNR measurements. When applied to full-dose data, our method\nproduces high-quality results orders of magnitude faster than iterative\ntechniques.\n","authors":["Onni Kosomaa","Samuli Laine","Tero Karras","Miika Aittala","Jaakko Lehtinen"],"pdf_url":"https://arxiv.org/pdf/2212.07431v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.07194v1","updated":"2022-12-14T12:39:47Z","published":"2022-12-14T12:39:47Z","title":"Traffic Flow Prediction via Variational Bayesian Inference-based\n  Encoder-Decoder Framework","summary":"  Accurate traffic flow prediction, a hotspot for intelligent transportation\nresearch, is the prerequisite for mastering traffic and making travel plans.\nThe speed of traffic flow can be affected by roads condition, weather,\nholidays, etc. Furthermore, the sensors to catch the information about traffic\nflow will be interfered with by environmental factors such as illumination,\ncollection time, occlusion, etc. Therefore, the traffic flow in the practical\ntransportation system is complicated, uncertain, and challenging to predict\naccurately. This paper proposes a deep encoder-decoder prediction framework\nbased on variational Bayesian inference. A Bayesian neural network is\nconstructed by combining variational inference with gated recurrent units (GRU)\nand used as the deep neural network unit of the encoder-decoder framework to\nmine the intrinsic dynamics of traffic flow. Then, the variational inference is\nintroduced into the multi-head attention mechanism to avoid noise-induced\ndeterioration of prediction accuracy. The proposed model achieves superior\nprediction performance on the Guangzhou urban traffic flow dataset over the\nbenchmarks, particularly when the long-term prediction.\n","authors":["Jianlei Kong","Xiaomeng Fan","Xue-Bo Jin","Min Zuo"],"pdf_url":"https://arxiv.org/pdf/2212.07194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07177v1","updated":"2022-12-14T12:04:57Z","published":"2022-12-14T12:04:57Z","title":"How Researchers Could Obtain Quick and Cheap User Feedback on their\n  Algorithms Without Having to Operate their Own Recommender System","summary":"  The majority of recommendation algorithms are evaluated on the basis of\nhistoric benchmark datasets. Evaluation on historic benchmark datasets is quick\nand cheap to conduct, yet excludes the viewpoint of users who actually consume\nrecommendations. User feedback is seldom collected, since it requires access to\nan operational recommender system. Establishing and maintaining an operational\nrecommender system imposes a timely and financial burden that a majority of\nresearchers cannot shoulder. We aim to reduce this burden in order to promote\nwidespread user-centric evaluations of recommendation algorithms, in particular\nfor novice researchers in the field. We present work in progress on an\nevaluation tool that implements a novel paradigm that enables user-centric\nevaluations of recommendation algorithms without access to an operational\nrecommender system. Finally, we sketch the experiments we plan to conduct with\nthe help of the evaluation tool.\n","authors":["Tobias Eichinger","Ananta Lamichhane"],"pdf_url":"https://arxiv.org/pdf/2212.07177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07126v1","updated":"2022-12-14T09:25:49Z","published":"2022-12-14T09:25:49Z","title":"Explainability of Text Processing and Retrieval Methods: A Critical\n  Survey","summary":"  Deep Learning and Machine Learning based models have become extremely popular\nin text processing and information retrieval. However, the non-linear\nstructures present inside the networks make these models largely inscrutable. A\nsignificant body of research has focused on increasing the transparency of\nthese models. This article provides a broad overview of research on the\nexplainability and interpretability of natural language processing and\ninformation retrieval methods. More specifically, we survey approaches that\nhave been applied to explain word embeddings, sequence modeling, attention\nmodules, transformers, BERT, and document ranking. The concluding section\nsuggests some possible directions for future research on this topic.\n","authors":["Sourav Saha","Debapriyo Majumdar","Mandar Mitra"],"pdf_url":"https://arxiv.org/pdf/2212.07126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07112v1","updated":"2022-12-14T09:05:14Z","published":"2022-12-14T09:05:14Z","title":"DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service\n  Chatlog","summary":"  Harvesting question-answer (QA) pairs from customer service chatlog in the\nwild is an efficient way to enrich the knowledge base for customer service\nchatbots in the cold start or continuous integration scenarios. Prior work\nattempts to obtain 1-to-1 QA pairs from growing customer service chatlog, which\nfails to integrate the incomplete utterances from the dialog context for\ncomposite QA retrieval. In this paper, we propose N-to-N QA extraction task in\nwhich the derived questions and corresponding answers might be separated across\ndifferent utterances. We introduce a suite of generative/discriminative tagging\nbased methods with end-to-end and two-stage variants that perform well on 5\ncustomer service datasets and for the first time setup a benchmark for N-to-N\nDialogQAE with utterance and session level evaluation metrics. With a deep dive\ninto extracted QA pairs, we find that the relations between and inside the QA\npairs can be indicators to analyze the dialogue structure, e.g. information\nseeking, clarification, barge-in and elaboration. We also show that the\nproposed models can adapt to different domains and languages, and reduce the\nlabor cost of knowledge accumulation in the real-world product dialogue\nplatform.\n","authors":["Xin Zheng","Tianyu Liu","Haoran Meng","Xu Wang","Yufan Jiang","Mengliang Rao","Binghuai Lin","Zhifang Sui","Yunbo Cao"],"pdf_url":"https://arxiv.org/pdf/2212.07112v1.pdf","comment":"Preprint version; The first three authors contribute equally"},{"id":"http://arxiv.org/abs/2204.08146v2","updated":"2022-12-14T02:02:26Z","published":"2022-04-18T03:25:15Z","title":"PrivateRec: Differentially Private Training and Serving for Federated\n  News Recommendation","summary":"  Collecting and training over sensitive personal data raise severe privacy\nconcerns in personalized recommendation systems, and federated learning can\npotentially alleviate the problem by training models over decentralized user\ndata.However, a theoretically private solution in both the training and serving\nstages of federated recommendation is essential but still lacking.Furthermore,\nnaively applying differential privacy (DP) to the two stages in federated\nrecommendation would fail to achieve a satisfactory trade-off between privacy\nand utility due to the high-dimensional characteristics of model gradients and\nhidden representations.In this work, we propose a federated news recommendation\nmethod for achieving a better utility in model training and online serving\nunder a DP guarantee.We first clarify the DP definition over behavior data for\neach round in the life-circle of federated recommendation systems.Next, we\npropose a privacy-preserving online serving mechanism under this definition\nbased on the idea of decomposing user embeddings with public basic vectors and\nperturbing the lower-dimensional combination coefficients. We apply a random\nbehavior padding mechanism to reduce the required noise intensity for better\nutility. Besides, we design a federated recommendation model training method,\nwhich can generate effective and public basic vectors for serving while\nproviding DP for training participants. We avoid the dimension-dependent noise\nfor large models via label permutation and differentially private attention\nmodules. Experiments on real-world news recommendation datasets validate that\nour method achieves superior utility under a DP guarantee in both training and\nserving of federated news recommendations.\n","authors":["Ruixuan Liu","Yanlin Wang","Yang Cao","Lingjuan Lyu","Weike Pan","Yun Chen","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2204.08146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01328v2","updated":"2022-12-14T23:42:24Z","published":"2022-06-02T22:55:51Z","title":"Augmenting Scientific Creativity with Retrieval across Knowledge Domains","summary":"  Exposure to ideas in domains outside a scientist's own may benefit her in\nreformulating existing research problems in novel ways and discovering new\napplication domains for existing solution ideas. While improved performance in\nscholarly search engines can help scientists efficiently identify relevant\nadvances in domains they may already be familiar with, it may fall short of\nhelping them explore diverse ideas \\textit{outside} such domains. In this paper\nwe explore the design of systems aimed at augmenting the end-user ability in\ncross-domain exploration with flexible query specification. To this end, we\ndevelop an exploratory search system in which end-users can select a portion of\ntext core to their interest from a paper abstract and retrieve papers that have\na high similarity to the user-selected core aspect but differ in terms of\ndomains. Furthermore, end-users can `zoom in' to specific domain clusters to\nretrieve more papers from them and understand nuanced differences within the\nclusters. Our case studies with scientists uncover opportunities and design\nimplications for systems aimed at facilitating cross-domain exploration and\ninspiration.\n","authors":["Hyeonsu B. Kang","Sheshera Mysore","Kevin Huang","Haw-Shiuan Chang","Thorben Prein","Andrew McCallum","Aniket Kittur","Elsa Olivetti"],"pdf_url":"https://arxiv.org/pdf/2206.01328v2.pdf","comment":"NLP+HCI Workshop at NAACL 2022"},{"id":"http://arxiv.org/abs/2212.07476v1","updated":"2022-12-14T19:50:35Z","published":"2022-12-14T19:50:35Z","title":"The Infinite Index: Information Retrieval on Generative Text-To-Image\n  Models","summary":"  The text-to-image model Stable Diffusion has recently become very popular.\nOnly weeks after its open source release, millions are experimenting with image\ngeneration. This is due to its ease of use, since all it takes is a brief\ndescription of the desired image to \"prompt\" the generative model. Rarely do\nthe images generated for a new prompt immediately meet the user's expectations.\nUsually, an iterative refinement of the prompt (\"prompt engineering\") is\nnecessary for satisfying images. As a new perspective, we recast image prompt\nengineering as interactive image retrieval - on an \"infinite index\". Thereby, a\nprompt corresponds to a query and prompt engineering to query refinement.\nSelected image-prompt pairs allow direct relevance feedback, as the model can\nmodify an image for the refined prompt. This is a form of one-sided interactive\nretrieval, where the initiative is on the user side, whereas the server side\nremains stateless. In light of an extensive literature review, we develop these\nparallels in detail and apply the findings to a case study of a creative search\ntask on such a model. We note that the uncertainty in searching an infinite\nindex is virtually never-ending. We also discuss future research opportunities\nrelated to retrieval models specialized for generative models and interactive\ngenerative image retrieval. The application of IR technology, such as query\nreformulation and relevance feedback, will contribute to improved workflows\nwhen using generative models, while the notion of an infinite index raises new\nchallenges in IR research.\n","authors":["Niklas Deckers","Maik Fröbe","Johannes Kiesel","Gianluca Pandolfo","Christopher Schröder","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2212.07476v1.pdf","comment":"Accepted at CHIIR 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2212.06835v1","updated":"2022-12-14T18:59:07Z","published":"2022-12-14T18:59:07Z","title":"Simulating 2+1D Lattice Quantum Electrodynamics at Finite Density with\n  Neural Flow Wavefunctions","summary":"  We present a neural flow wavefunction, Gauge-Fermion FlowNet, and use it to\nsimulate 2+1D lattice compact quantum electrodynamics with finite density\ndynamical fermions. The gauge field is represented by a neural network which\nparameterizes a discretized flow-based transformation of the amplitude while\nthe fermionic sign structure is represented by a neural net backflow. This\napproach directly represents the $U(1)$ degree of freedom without any\ntruncation, obeys Guass's law by construction, samples autoregressively\navoiding any equilibration time, and variationally simulates Gauge-Fermion\nsystems with sign problems accurately. In this model, we investigate\nconfinement and string breaking phenomena in different fermion density and\nhopping regimes. We study the phase transition from the charge crystal phase to\nthe vacuum phase at zero density, and observe the phase seperation and the net\ncharge penetration blocking effect under magnetic interaction at finite\ndensity. In addition, we investigate a magnetic phase transition due to the\ncompetition effect between the kinetic energy of fermions and the magnetic\nenergy of the gauge field. With our method, we further note potential\ndifferences on the order of the phase transitions between a continuous $U(1)$\nsystem and one with finite truncation. Our state-of-the-art neural network\napproach opens up new possibilities to study different gauge theories coupled\nto dynamical matter in higher dimensions.\n","authors":["Zhuo Chen","Di Luo","Kaiwen Hu","Bryan K. Clark"],"pdf_url":"https://arxiv.org/pdf/2212.06835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.00626v2","updated":"2022-12-14T18:58:01Z","published":"2022-08-30T02:12:47Z","title":"The alignment problem from a deep learning perspective","summary":"  Within the coming decades, artificial general intelligence (AGI) may surpass\nhuman capabilities at a wide range of important tasks. We outline a case for\nexpecting that, without substantial effort to prevent it, AGIs could learn to\npursue goals which are very undesirable (in other words, misaligned) from a\nhuman perspective. We argue that AGIs trained in similar ways as today's most\ncapable models could learn to act deceptively to receive higher reward; learn\ninternally-represented goals which generalize beyond their training\ndistributions; and pursue those goals using power-seeking strategies. We\noutline how the deployment of misaligned AGIs might irreversibly undermine\nhuman control over the world, and briefly review research directions aimed at\npreventing these problems.\n","authors":["Richard Ngo","Lawrence Chan","Sören Mindermann"],"pdf_url":"https://arxiv.org/pdf/2209.00626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.06812v2","updated":"2022-12-14T18:55:27Z","published":"2022-05-13T17:59:23Z","title":"Principal-Agent Hypothesis Testing","summary":"  Consider the relationship between a regulator (the principal) and a\npharmaceutical company (the agent). The pharmaceutical company wishes to sell a\nproduct to make a profit, and the FDA wishes to ensure that only efficacious\ndrugs are released to the public. The efficacy of the drug is not known to the\nFDA, so the pharmaceutical company must run a costly trial to prove efficacy to\nthe FDA. Critically, the statistical protocol used to establish efficacy\naffects the behavior of a strategic, self-interested pharmaceutical company; a\nlower standard of statistical evidence incentivizes the pharmaceutical company\nto run more trials for drugs that are less likely to be effective, since the\ndrug may pass the trial by chance, resulting in large profits. The interaction\nbetween the statistical protocol and the incentives of the pharmaceutical\ncompany is crucial to understanding this system and designing protocols with\nhigh social utility. In this work, we discuss how the principal and agent can\nenter into a contract with payoffs based on statistical evidence. When there is\nstronger evidence for the quality of the product, the principal allows the\nagent to make a larger profit. We show how to design contracts that are robust\nto an agent's strategic actions, and derive the optimal contract in the\npresence of strategic behavior.\n","authors":["Stephen Bates","Michael I. Jordan","Michael Sklar","Jake A. Soloff"],"pdf_url":"https://arxiv.org/pdf/2205.06812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07414v1","updated":"2022-12-14T18:54:46Z","published":"2022-12-14T18:54:46Z","title":"Hierarchical Over-the-Air FedGradNorm","summary":"  Multi-task learning (MTL) is a learning paradigm to learn multiple related\ntasks simultaneously with a single shared network where each task has a\ndistinct personalized header network for fine-tuning. MTL can be integrated\ninto a federated learning (FL) setting if tasks are distributed across clients\nand clients have a single shared network, leading to personalized federated\nlearning (PFL). To cope with statistical heterogeneity in the federated setting\nacross clients which can significantly degrade the learning performance, we use\na distributed dynamic weighting approach. To perform the communication between\nthe remote parameter server (PS) and the clients efficiently over the noisy\nchannel in a power and bandwidth-limited regime, we utilize over-the-air (OTA)\naggregation and hierarchical federated learning (HFL). Thus, we propose\nhierarchical over-the-air (HOTA) PFL with a dynamic weighting strategy which we\ncall HOTA-FedGradNorm. Our algorithm considers the channel conditions during\nthe dynamic weight selection process. We conduct experiments on a wireless\ncommunication system dataset (RadComDynamic). The experimental results\ndemonstrate that the training speed with HOTA-FedGradNorm is faster compared to\nthe algorithms with a naive static equal weighting strategy. In addition,\nHOTA-FedGradNorm provides robustness against the negative channel effects by\ncompensating for the channel conditions during the dynamic weight selection\nprocess.\n","authors":["Cemil Vahapoglu","Matin Mortaheb","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2212.07414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07407v1","updated":"2022-12-14T18:46:14Z","published":"2022-12-14T18:46:14Z","title":"Cross-Domain Transfer via Semantic Skill Imitation","summary":"  We propose an approach for semantic imitation, which uses demonstrations from\na source domain, e.g. human videos, to accelerate reinforcement learning (RL)\nin a different target domain, e.g. a robotic manipulator in a simulated\nkitchen. Instead of imitating low-level actions like joint velocities, our\napproach imitates the sequence of demonstrated semantic skills like \"opening\nthe microwave\" or \"turning on the stove\". This allows us to transfer\ndemonstrations across environments (e.g. real-world to simulated kitchen) and\nagent embodiments (e.g. bimanual human demonstration to robotic arm). We\nevaluate on three challenging cross-domain learning problems and match the\nperformance of demonstration-accelerated RL approaches that require in-domain\ndemonstrations. In a simulated kitchen environment, our approach learns\nlong-horizon robot manipulation tasks, using less than 3 minutes of human video\ndemonstrations from a real-world kitchen. This enables scaling robot learning\nvia the reuse of demonstrations, e.g. collected as human videos, for learning\nin any number of target domains.\n","authors":["Karl Pertsch","Ruta Desai","Vikash Kumar","Franziska Meier","Joseph J. Lim","Dhruv Batra","Akshara Rai"],"pdf_url":"https://arxiv.org/pdf/2212.07407v1.pdf","comment":"Project website: https://kpertsch.github.io/star"},{"id":"http://arxiv.org/abs/2212.07398v1","updated":"2022-12-14T18:31:47Z","published":"2022-12-14T18:31:47Z","title":"Self-Play and Self-Describe: Policy Adaptation with Vision-Language\n  Foundation Models","summary":"  Recent progress on vision-language foundation models have brought significant\nadvancement to building general-purpose robots. By using the pre-trained models\nto encode the scene and instructions as inputs for decision making, the\ninstruction-conditioned policy can generalize across different objects and\ntasks. While this is encouraging, the policy still fails in most cases given an\nunseen task or environment. To adapt the policy to unseen tasks and\nenvironments, we explore a new paradigm on leveraging the pre-trained\nfoundation models with Self-PLAY and Self-Describe (SPLAYD). When deploying the\ntrained policy to a new task or a new environment, we first let the policy\nself-play with randomly generated instructions to record the demonstrations.\nWhile the execution could be wrong, we can use the pre-trained foundation\nmodels to accurately self-describe (i.e., re-label or classify) the\ndemonstrations. This automatically provides new pairs of\ndemonstration-instruction data for policy fine-tuning. We evaluate our method\non a broad range of experiments with the focus on generalization on unseen\nobjects, unseen tasks, unseen environments, and sim-to-real transfer. We show\nSPLAYD improves baselines by a large margin in all cases. Our project page is\navailable at https://geyuying.github.io/SPLAYD/\n","authors":["Yuying Ge","Annabella Macaluso","Li Erran Li","Ping Luo","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07398v1.pdf","comment":"Project page: https://geyuying.github.io/SPLAYD/"},{"id":"http://arxiv.org/abs/2106.02735v2","updated":"2022-12-14T18:29:41Z","published":"2021-06-04T22:00:53Z","title":"Learning particle swarming models from data with Gaussian processes","summary":"  Interacting particle or agent systems that display a rich variety of swarming\nbehaviours are ubiquitous in science and engineering. A fundamental and\nchallenging goal is to understand the link between individual interaction rules\nand swarming. In this paper, we study the data-driven discovery of a\nsecond-order particle swarming model that describes the evolution of $N$\nparticles in $\\mathbb{R}^d$ under radial interactions. We propose a learning\napproach that models the latent radial interaction function as Gaussian\nprocesses, which can simultaneously fulfill two inference goals: one is the\nnonparametric inference of {the} interaction function with pointwise\nuncertainty quantification, and the other one is the inference of unknown\nscalar parameters in the non-collective friction forces of the system. We\nformulate the learning problem as a statistical inverse problem and provide a\ndetailed analysis of recoverability conditions, establishing that a coercivity\ncondition is sufficient for recoverability. Given data collected from $M$ i.i.d\ntrajectories with independent Gaussian observational noise, we provide a\nfinite-sample analysis, showing that our posterior mean estimator converges in\na Reproducing kernel Hilbert space norm, at an optimal rate in $M$ equal to the\none in the classical 1-dimensional Kernel Ridge regression. As a byproduct, we\nshow we can obtain a parametric learning rate in $M$ for the posterior marginal\nvariance using $L^{\\infty}$ norm, and the rate could also involve $N$ and $L$\n(the number of observation time instances for each trajectory), depending on\nthe condition number of the inverse problem. Numerical results on systems that\nexhibit different swarming behaviors demonstrate efficient learning of our\napproach from scarce noisy trajectory data.\n","authors":["Jinchao Feng","Charles Kulick","Yunxiang Ren","Sui Tang"],"pdf_url":"https://arxiv.org/pdf/2106.02735v2.pdf","comment":"36 pages; Appendix 5 pages"},{"id":"http://arxiv.org/abs/2212.07397v1","updated":"2022-12-14T18:27:58Z","published":"2022-12-14T18:27:58Z","title":"Hierarchical Strategies for Cooperative Multi-Agent Reinforcement\n  Learning","summary":"  Adequate strategizing of agents behaviors is essential to solving cooperative\nMARL problems. One intuitively beneficial yet uncommon method in this domain is\npredicting agents future behaviors and planning accordingly. Leveraging this\npoint, we propose a two-level hierarchical architecture that combines a novel\ninformation-theoretic objective with a trajectory prediction model to learn a\nstrategy. To this end, we introduce a latent policy that learns two types of\nlatent strategies: individual $z_A$, and relational $z_R$ using a modified\nGraph Attention Network module to extract interaction features. We encourage\neach agent to behave according to the strategy by conditioning its local $Q$\nfunctions on $z_A$, and we further equip agents with a shared $Q$ function that\nconditions on $z_R$. Additionally, we introduce two regularizers to allow\npredicted trajectories to be accurate and rewarding. Empirical results on\nGoogle Research Football (GRF) and StarCraft (SC) II micromanagement tasks show\nthat our method establishes a new state of the art being, to the best of our\nknowledge, the first MARL algorithm to solve all super hard SC II scenarios as\nwell as the GRF full game with a win rate higher than $95\\%$, thus\noutperforming all existing methods. Videos and brief overview of the methods\nand results are available at:\nhttps://sites.google.com/view/hier-strats-marl/home.\n","authors":["Majd Ibrahim","Ammar Fayad"],"pdf_url":"https://arxiv.org/pdf/2212.07397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.05729v2","updated":"2022-12-14T18:26:28Z","published":"2021-12-10T18:33:12Z","title":"Learning soft interventions in complex equilibrium systems","summary":"  Complex systems often contain feedback loops that can be described as cyclic\ncausal models. Intervening in such systems may lead to counterintuitive\neffects, which cannot be inferred directly from the graph structure. After\nestablishing a framework for differentiable soft interventions based on Lie\ngroups, we take advantage of modern automatic differentiation techniques and\ntheir application to implicit functions in order to optimize interventions in\ncyclic causal models. We illustrate the use of this framework by investigating\nscenarios of transition to sustainable economies.\n","authors":["Michel Besserve","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2112.05729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16667v2","updated":"2022-12-14T18:09:41Z","published":"2022-11-30T01:22:25Z","title":"Dynamic Sparse Training via More Exploration","summary":"  Over-parameterization of deep neural networks (DNNs) has shown high\nprediction accuracy for many applications. Although effective, the large number\nof parameters hinders its popularity on resource-limited devices and has an\noutsize environmental impact. Sparse training (using a fixed number of nonzero\nweights in each iteration) could significantly mitigate the training costs by\nreducing the model size. However, existing sparse training methods mainly use\neither random-based or greedy-based drop-and-grow strategies, resulting in\nlocal minimal and low accuracy. In this work, we consider the dynamic sparse\ntraining as a sparse connectivity search problem and design an exploitation and\nexploration acquisition function to escape from local optima and saddle points.\nWe further design an acquisition function and provide the theoretical\nguarantees for the proposed method and clarify its convergence property.\nExperimental results show that sparse models (up to 98\\% sparsity) obtained by\nour proposed method outperform the SOTA sparse training methods on a wide\nvariety of deep learning tasks. On VGG-19 / CIFAR-100, ResNet-50 / CIFAR-10,\nResNet-50 / CIFAR-100, our method has even higher accuracy than dense models.\nOn ResNet-50 / ImageNet, the proposed method has up to 8.2\\% accuracy\nimprovement compared to SOTA sparse training methods.\n","authors":["Shaoyi Huang","Bowen Lei","Dongkuan Xu","Hongwu Peng","Yue Sun","Mimi Xie","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2211.16667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07383v1","updated":"2022-12-14T18:08:42Z","published":"2022-12-14T18:08:42Z","title":"Sequential Kernelized Independence Testing","summary":"  Independence testing is a fundamental and classical statistical problem that\nhas been extensively studied in the batch setting when one fixes the sample\nsize before collecting data. However, practitioners often prefer procedures\nthat adapt to the complexity of a problem at hand instead of setting sample\nsize in advance. Ideally, such procedures should (a) allow stopping earlier on\neasy tasks (and later on harder tasks), hence making better use of available\nresources, and (b) continuously monitor the data and efficiently incorporate\nstatistical evidence after collecting new data, while controlling the false\nalarm rate. It is well known that classical batch tests are not tailored for\nstreaming data settings, since valid inference after data peeking requires\ncorrecting for multiple testing, but such corrections generally result in low\npower. In this paper, we design sequential kernelized independence tests\n(SKITs) that overcome such shortcomings based on the principle of testing by\nbetting. We exemplify our broad framework using bets inspired by kernelized\ndependence measures such as the Hilbert-Schmidt independence criterion (HSIC)\nand the constrained-covariance criterion (COCO). Importantly, we also\ngeneralize the framework to non-i.i.d. time-varying settings, for which there\nexist no batch tests. We demonstrate the power of our approaches on both\nsimulated and real data.\n","authors":["Aleksandr Podkopaev","Patrick Blöbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2212.07383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04454v3","updated":"2022-12-14T18:03:04Z","published":"2022-12-08T18:23:59Z","title":"XRand: Differentially Private Defense against Explanation-Guided Attacks","summary":"  Recent development in the field of explainable artificial intelligence (XAI)\nhas helped improve trust in Machine-Learning-as-a-Service (MLaaS) systems, in\nwhich an explanation is provided together with the model prediction in response\nto each query. However, XAI also opens a door for adversaries to gain insights\ninto the black-box models in MLaaS, thereby making the models more vulnerable\nto several attacks. For example, feature-based explanations (e.g., SHAP) could\nexpose the top important features that a black-box model focuses on. Such\ndisclosure has been exploited to craft effective backdoor triggers against\nmalware classifiers. To address this trade-off, we introduce a new concept of\nachieving local differential privacy (LDP) in the explanations, and from that\nwe establish a defense, called XRand, against such attacks. We show that our\nmechanism restricts the information that the adversary can learn about the top\nimportant features, while maintaining the faithfulness of the explanations.\n","authors":["Truc Nguyen","Phung Lai","NhatHai Phan","My T. Thai"],"pdf_url":"https://arxiv.org/pdf/2212.04454v3.pdf","comment":"To be published at AAAI 2023"},{"id":"http://arxiv.org/abs/2211.08403v2","updated":"2022-12-14T17:49:54Z","published":"2022-11-15T18:45:26Z","title":"REPAIR: REnormalizing Permuted Activations for Interpolation Repair","summary":"  In this paper we look into the conjecture of Entezari et al. (2021) which\nstates that if the permutation invariance of neural networks is taken into\naccount, then there is likely no loss barrier to the linear interpolation\nbetween SGD solutions. First, we observe that neuron alignment methods alone\nare insufficient to establish low-barrier linear connectivity between SGD\nsolutions due to a phenomenon we call variance collapse: interpolated deep\nnetworks suffer a collapse in the variance of their activations, causing poor\nperformance. Next, we propose REPAIR (REnormalizing Permuted Activations for\nInterpolation Repair) which mitigates variance collapse by rescaling the\npreactivations of such interpolated networks. We explore the interaction\nbetween our method and the choice of normalization layer, network width, and\ndepth, and demonstrate that using REPAIR on top of neuron alignment methods\nleads to 60%-100% relative barrier reduction across a wide variety of\narchitecture families and tasks. In particular, we report a 74% barrier\nreduction for ResNet50 on ImageNet and 90% barrier reduction for ResNet18 on\nCIFAR10.\n","authors":["Keller Jordan","Hanie Sedghi","Olga Saukh","Rahim Entezari","Behnam Neyshabur"],"pdf_url":"https://arxiv.org/pdf/2211.08403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07368v1","updated":"2022-12-14T17:46:17Z","published":"2022-12-14T17:46:17Z","title":"Reconstruction of Multivariate Sparse Signals from Mismatched Samples","summary":"  Erroneous correspondences between samples and their respective channel or\ntarget commonly arise in several real-world applications. For instance,\nwhole-brain calcium imaging of freely moving organisms, multiple target\ntracking or multi-person contactless vital sign monitoring may be severely\naffected by mismatched sample-channel assignments. To systematically address\nthis fundamental problem, we pose it as a signal reconstruction problem where\nwe have lost correspondences between the samples and their respective channels.\nWe show that under the assumption that the signals of interest admit a sparse\nrepresentation over an overcomplete dictionary, unique signal recovery is\npossible. Our derivations reveal that the problem is equivalent to a structured\nunlabeled sensing problem without precise knowledge of the sensing matrix.\nUnfortunately, existing methods are neither robust to errors in the regressors\nnor do they exploit the structure of the problem. Therefore, we propose a novel\nrobust two-step approach for the reconstruction of shuffled sparse signals. The\nperformance and robustness of the proposed approach is illustrated in an\napplication of whole-brain calcium imaging in computational neuroscience. The\nproposed framework can be generalized to sparse signal representations other\nthan the ones considered in this work to be applied in a variety of real-world\nproblems with imprecise measurement or channel assignment.\n","authors":["Taulant Koka","Michael Muma","Benjamín Béjar Haro"],"pdf_url":"https://arxiv.org/pdf/2212.07368v1.pdf","comment":"13 pages, 9 figures, submitted to Transactions on Signal Processing"},{"id":"http://arxiv.org/abs/2211.07357v2","updated":"2022-12-14T17:45:20Z","published":"2022-11-11T17:48:13Z","title":"Controlling Commercial Cooling Systems Using Reinforcement Learning","summary":"  This paper is a technical overview of DeepMind and Google's recent work on\nreinforcement learning for controlling commercial cooling systems. Building on\nexpertise that began with cooling Google's data centers more efficiently, we\nrecently conducted live experiments on two real-world facilities in partnership\nwith Trane Technologies, a building management system provider. These live\nexperiments had a variety of challenges in areas such as evaluation, learning\nfrom offline data, and constraint satisfaction. Our paper describes these\nchallenges in the hope that awareness of them will benefit future applied RL\nwork. We also describe the way we adapted our RL system to deal with these\nchallenges, resulting in energy savings of approximately 9% and 13%\nrespectively at the two live experiment sites.\n","authors":["Jerry Luo","Cosmin Paduraru","Octavian Voicu","Yuri Chervonyi","Scott Munns","Jerry Li","Crystal Qian","Praneet Dutta","Jared Quincy Davis","Ningjia Wu","Xingwei Yang","Chu-Ming Chang","Ted Li","Rob Rose","Mingyan Fan","Hootan Nakhost","Tinglin Liu","Brian Kirkman","Frank Altamura","Lee Cline","Patrick Tonker","Joel Gouker","Dave Uden","Warren Buddy Bryan","Jason Law","Deeni Fatiha","Neil Satra","Juliet Rothenberg","Mandeep Waraich","Molly Carlin","Satish Tallapaka","Sims Witherspoon","David Parish","Peter Dolan","Chenyu Zhao","Daniel J. Mankowitz"],"pdf_url":"https://arxiv.org/pdf/2211.07357v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2212.07365v1","updated":"2022-12-14T17:40:00Z","published":"2022-12-14T17:40:00Z","title":"Learning Invariant Subspaces of Koopman Operators--Part 2: Heterogeneous\n  Dictionary Mixing to Approximate Subspace Invariance","summary":"  This work builds on the models and concepts presented in part 1 to learn\napproximate dictionary representations of Koopman operators from data. Part I\nof this paper presented a methodology for arguing the subspace invariance of a\nKoopman dictionary. This methodology was demonstrated on the state-inclusive\nlogistic lifting (SILL) basis. This is an affine basis augmented with\nconjunctive logistic functions. The SILL dictionary's nonlinear functions are\nhomogeneous, a norm in data-driven dictionary learning of Koopman operators. In\nthis paper, we discover that structured mixing of heterogeneous dictionary\nfunctions drawn from different classes of nonlinear functions achieve the same\naccuracy and dimensional scaling as the deep-learning-based deepDMD algorithm.\nWe specifically show this by building a heterogeneous dictionary comprised of\nSILL functions and conjunctive radial basis functions (RBFs). This mixed\ndictionary achieves the same accuracy and dimensional scaling as deepDMD with\nan order of magnitude reduction in parameters, while maintaining geometric\ninterpretability. These results strengthen the viability of dictionary-based\nKoopman models to solving high-dimensional nonlinear learning problems.\n","authors":["Charles A. Johnson","Shara Balakrishnan","Enoch Yeung"],"pdf_url":"https://arxiv.org/pdf/2212.07365v1.pdf","comment":"16 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2206.13585"},{"id":"http://arxiv.org/abs/2210.09213v2","updated":"2022-12-14T17:38:03Z","published":"2022-10-14T13:01:25Z","title":"Segmentation-guided Domain Adaptation for Efficient Depth Completion","summary":"  Complete depth information and efficient estimators have become vital\ningredients in scene understanding for automated driving tasks. A major problem\nfor LiDAR-based depth completion is the inefficient utilization of convolutions\ndue to the lack of coherent information as provided by the sparse nature of\nuncorrelated LiDAR point clouds, which often leads to complex and\nresource-demanding networks. The problem is reinforced by the expensive\naquisition of depth data for supervised training. In this work, we propose an\nefficient depth completion model based on a vgg05-like CNN architecture and\npropose a semi-supervised domain adaptation approach to transfer knowledge from\nsynthetic to real world data to improve data-efficiency and reduce the need for\na large database. In order to boost spatial coherence, we guide the learning\nprocess using segmentations as additional source of information. The efficiency\nand accuracy of our approach is evaluated on the KITTI dataset. Our approach\nimproves on previous efficient and low parameter state of the art approaches\nwhile having a noticeably lower computational footprint.\n","authors":["Fabian Märkert","Martin Sunkel","Anselm Haselhoff","Stefan Rudolph"],"pdf_url":"https://arxiv.org/pdf/2210.09213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07359v1","updated":"2022-12-14T17:34:11Z","published":"2022-12-14T17:34:11Z","title":"Post-hoc Uncertainty Learning using a Dirichlet Meta-Model","summary":"  It is known that neural networks have the problem of being over-confident\nwhen directly using the output label distribution to generate uncertainty\nmeasures. Existing methods mainly resolve this issue by retraining the entire\nmodel to impose the uncertainty quantification capability so that the learned\nmodel can achieve desired performance in accuracy and uncertainty prediction\nsimultaneously. However, training the model from scratch is computationally\nexpensive and may not be feasible in many situations. In this work, we consider\na more practical post-hoc uncertainty learning setting, where a well-trained\nbase model is given, and we focus on the uncertainty quantification task at the\nsecond stage of training. We propose a novel Bayesian meta-model to augment\npre-trained models with better uncertainty quantification abilities, which is\neffective and computationally efficient. Our proposed method requires no\nadditional training data and is flexible enough to quantify different\nuncertainties and easily adapt to different application settings, including\nout-of-domain data detection, misclassification detection, and trustworthy\ntransfer learning. We demonstrate our proposed meta-model approach's\nflexibility and superior empirical performance on these applications over\nmultiple representative image classification benchmarks.\n","authors":["Maohao Shen","Yuheng Bu","Prasanna Sattigeri","Soumya Ghosh","Subhro Das","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2212.07359v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2212.07358v1","updated":"2022-12-14T17:33:52Z","published":"2022-12-14T17:33:52Z","title":"Learning Invariant Subspaces of Koopman Operators--Part 1: A Methodology\n  for Demonstrating a Dictionary's Approximate Subspace Invariance","summary":"  Koopman operators model nonlinear dynamics as a linear dynamic system acting\non a nonlinear function as the state. This nonstandard state is often called a\nKoopman observable and is usually approximated numerically by a superposition\nof functions drawn from a dictionary. In a widely used algorithm, Extended\nDynamic Mode Decomposition, the dictionary functions are drawn from a fixed\nclass of functions. Recently, deep learning combined with EDMD has been used to\nlearn novel dictionary functions in an algorithm called deep dynamic mode\ndecomposition (deepDMD). The learned representation both (1) accurately models\nand (2) scales well with the dimension of the original nonlinear system. In\nthis paper we analyze the learned dictionaries from deepDMD and explore the\ntheoretical basis for their strong performance. We explore State-Inclusive\nLogistic Lifting (SILL) dictionary functions to approximate Koopman\nobservables. Error analysis of these dictionary functions show they satisfy a\nproperty of subspace approximation, which we define as uniform finite\napproximate closure. Our results provide a hypothesis to explain the success of\ndeep neural networks in learning numerical approximations to Koopman operators.\nPart 2 of this paper will extend this explanation by demonstrating the subspace\ninvariant of heterogeneous dictionaries and presenting a head-to-head numerical\ncomparison of deepDMD and low-parameter heterogeneous dictionary learning.\n","authors":["Charles A. Johnson","Shara Balakrishnan","Enoch Yeung"],"pdf_url":"https://arxiv.org/pdf/2212.07358v1.pdf","comment":"13 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2206.13585"},{"id":"http://arxiv.org/abs/2212.07356v1","updated":"2022-12-14T17:33:01Z","published":"2022-12-14T17:33:01Z","title":"Scheduling and Aggregation Design for Asynchronous Federated Learning\n  over Wireless Networks","summary":"  Federated Learning (FL) is a collaborative machine learning (ML) framework\nthat combines on-device training and server-based aggregation to train a common\nML model among distributed agents. In this work, we propose an asynchronous FL\ndesign with periodic aggregation to tackle the straggler issue in FL systems.\nConsidering limited wireless communication resources, we investigate the effect\nof different scheduling policies and aggregation designs on the convergence\nperformance. Driven by the importance of reducing the bias and variance of the\naggregated model updates, we propose a scheduling policy that jointly considers\nthe channel quality and training data representation of user devices. The\neffectiveness of our channel-aware data-importance-based scheduling policy,\ncompared with state-of-the-art methods proposed for synchronous FL, is\nvalidated through simulations. Moreover, we show that an \"age-aware\"\naggregation weighting design can significantly improve the learning performance\nin an asynchronous FL setting.\n","authors":["Chung-Hsuan Hu","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2212.07356v1.pdf","comment":"Accepted by JSAC special issue: Communication-Efficient Distributed\n  Learning over Networks; to appear"},{"id":"http://arxiv.org/abs/2212.07347v1","updated":"2022-12-14T17:19:46Z","published":"2022-12-14T17:19:46Z","title":"Lorentz Group Equivariant Autoencoders","summary":"  There has been significant work recently in developing machine learning\nmodels in high energy physics (HEP), for tasks such as classification,\nsimulation, and anomaly detection. Typically, these models are adapted from\nthose designed for datasets in computer vision or natural language processing\nwithout necessarily incorporating inductive biases suited to HEP data, such as\nrespecting its inherent symmetries. Such inductive biases can make the model\nmore performant and interpretable, and reduce the amount of training data\nneeded. To that end, we develop the Lorentz group autoencoder (LGAE), an\nautoencoder model equivariant with respect to the proper, orthochronous Lorentz\ngroup $\\mathrm{SO}^+(3,1)$, with a latent space living in the representations\nof the group. We present our architecture and several experimental results on\njets at the LHC and find it significantly outperforms a non-Lorentz-equivariant\ngraph neural network baseline on compression and reconstruction, and anomaly\ndetection. We also demonstrate the advantage of such an equivariant model in\nanalyzing the latent space of the autoencoder, which can have a significant\nimpact on the explainability of anomalies found by such black-box machine\nlearning models.\n","authors":["Zichun Hao","Raghav Kansal","Javier Duarte","Nadezda Chernyavskaya"],"pdf_url":"https://arxiv.org/pdf/2212.07347v1.pdf","comment":"7 pages, 6 figures, 4 tables, and a 2 page appendix"},{"id":"http://arxiv.org/abs/2212.07346v1","updated":"2022-12-14T17:17:10Z","published":"2022-12-14T17:17:10Z","title":"Learning useful representations for shifting tasks and distributions","summary":"  Does the dominant approach to learn representations (as a side effect of\noptimizing an expected cost for a single training distribution) remain a good\napproach when we are dealing with multiple distributions. Our thesis is that\nsuch scenarios are better served by representations that are \"richer\" than\nthose obtained with a single optimization episode. This is supported by a\ncollection of empirical results obtained with an apparently na\\\"ive ensembling\ntechnique: concatenating the representations obtained with multiple training\nepisodes using the same data, model, algorithm, and hyper-parameters, but\ndifferent random seeds. These independently trained networks perform similarly.\nYet, in a number of scenarios involving new distributions, the concatenated\nrepresentation performs substantially better than an equivalently sized network\ntrained from scratch. This proves that the representations constructed by\nmultiple training episodes are in fact different. Although their concatenation\ncarries little additional information about the training task under the\ntraining distribution, it becomes substantially more informative when tasks or\ndistributions change. Meanwhile, a single training episode is unlikely to yield\nsuch a redundant representation because the optimization process has no reason\nto accumulate features that do not incrementally improve the training\nperformance.\n","authors":["Jianyu Zhang","Léon Bottou"],"pdf_url":"https://arxiv.org/pdf/2212.07346v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2110.13194v3","updated":"2022-12-14T17:08:37Z","published":"2021-10-25T18:20:04Z","title":"Covariance-Generalized Matching Component Analysis for Data Fusion and\n  Transfer Learning","summary":"  In order to encode additional statistical information in data fusion and\ntransfer learning applications, we introduce a generalized covariance\nconstraint for the matching component analysis (MCA) transfer learning\ntechnique. We provide a closed-form solution to the resulting\ncovariance-generalized optimization problem and an algorithm for its\ncomputation. We call the resulting technique -- applicable to both data fusion\nand transfer learning -- covariance-generalized MCA (CGMCA). We also\ndemonstrate via numerical experiments that CGMCA is capable of meaningfully\nencoding into its maps more information than MCA.\n","authors":["Nick Lorenzo","Sean O'Rourke","Theresa Scarnati"],"pdf_url":"https://arxiv.org/pdf/2110.13194v3.pdf","comment":"v3: Made major organizational changes; added numerical results;\n  eliminated statement and proof of lemma in favor of offering a solution\n  achieving a cited bound"},{"id":"http://arxiv.org/abs/2212.07313v1","updated":"2022-12-14T16:19:51Z","published":"2022-12-14T16:19:51Z","title":"Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility\n  on Demand Systems","summary":"  We consider the sequential decision-making problem of making proactive\nrequest assignment and rejection decisions for a profit-maximizing operator of\nan autonomous mobility on demand system. We formalize this problem as a Markov\ndecision process and propose a novel combination of multi-agent Soft\nActor-Critic and weighted bipartite matching to obtain an anticipative control\npolicy. Thereby, we factorize the operator's otherwise intractable action\nspace, but still obtain a globally coordinated decision. Experiments based on\nreal-world taxi data show that our method outperforms state of the art\nbenchmarks with respect to performance, stability, and computational\ntractability.\n","authors":["Tobias Enders","James Harrison","Marco Pavone","Maximilian Schiffer"],"pdf_url":"https://arxiv.org/pdf/2212.07313v1.pdf","comment":"20 pages, 7 figures, extended version of paper submitted to the 5th\n  Learning for Dynamics & Control Conference (L4DC 2023)"},{"id":"http://arxiv.org/abs/2212.07311v1","updated":"2022-12-14T16:16:47Z","published":"2022-12-14T16:16:47Z","title":"Bayesian data fusion with shared priors","summary":"  The integration of data and knowledge from several sources is known as data\nfusion. When data is available in a distributed fashion or when different\nsensors are used to infer a quantity of interest, data fusion becomes\nessential. In Bayesian settings, a priori information of the unknown quantities\nis available and, possibly, shared among the distributed estimators. When the\nlocal estimates are fused, such prior might be overused unless it is accounted\nfor. This paper explores the effects of shared priors in Bayesian data fusion\ncontexts, providing fusion rules and analysis to understand the performance of\nsuch fusion as a function of the number of collaborative agents and the\nuncertainty of the priors. Analytical results are corroborated through\nexperiments in a variety of estimation and classification problems.\n","authors":["Peng Wu","Tales Imbiriba","Victor Elvira","Pau Closas"],"pdf_url":"https://arxiv.org/pdf/2212.07311v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2211.06545v3","updated":"2022-12-14T16:01:03Z","published":"2022-11-12T02:01:46Z","title":"Self-Supervised Graph Structure Refinement for Graph Neural Networks","summary":"  Graph structure learning (GSL), which aims to learn the adjacency matrix for\ngraph neural networks (GNNs), has shown great potential in boosting the\nperformance of GNNs. Most existing GSL works apply a joint learning framework\nwhere the estimated adjacency matrix and GNN parameters are optimized for\ndownstream tasks. However, as GSL is essentially a link prediction task, whose\ngoal may largely differ from the goal of the downstream task. The inconsistency\nof these two goals limits the GSL methods to learn the potential optimal graph\nstructure. Moreover, the joint learning framework suffers from scalability\nissues in terms of time and space during the process of estimation and\noptimization of the adjacency matrix. To mitigate these issues, we propose a\ngraph structure refinement (GSR) framework with a pretrain-finetune pipeline.\nSpecifically, The pre-training phase aims to comprehensively estimate the\nunderlying graph structure by a multi-view contrastive learning framework with\nboth intra- and inter-view link prediction tasks. Then, the graph structure is\nrefined by adding and removing edges according to the edge probabilities\nestimated by the pre-trained model. Finally, the fine-tuning GNN is initialized\nby the pre-trained model and optimized toward downstream tasks. With the\nrefined graph structure remaining static in the fine-tuning space, GSR avoids\nestimating and optimizing graph structure in the fine-tuning phase which enjoys\ngreat scalability and efficiency. Moreover, the fine-tuning GNN is boosted by\nboth migrating knowledge and refining graphs. Extensive experiments are\nconducted to evaluate the effectiveness (best performance on six benchmark\ndatasets), efficiency, and scalability (13.8x faster using 32.8% GPU memory\ncompared to the best GSL baseline on Cora) of the proposed model.\n","authors":["Jianan Zhao","Qianlong Wen","Mingxuan Ju","Chuxu Zhang","Yanfang Ye"],"pdf_url":"https://arxiv.org/pdf/2211.06545v3.pdf","comment":"there are some issues of the paper, we need to revise the paper"},{"id":"http://arxiv.org/abs/2205.12934v3","updated":"2022-12-14T16:00:51Z","published":"2022-05-25T17:37:08Z","title":"Amortized Inference for Causal Structure Learning","summary":"  Inferring causal structure poses a combinatorial search problem that\ntypically involves evaluating structures with a score or independence test. The\nresulting search is costly, and designing suitable scores or tests that capture\nprior knowledge is difficult. In this work, we propose to amortize causal\nstructure learning. Rather than searching over structures, we train a\nvariational inference model to directly predict the causal structure from\nobservational or interventional data. This allows our inference model to\nacquire domain-specific inductive biases for causal discovery solely from data\ngenerated by a simulator, bypassing both the hand-engineering of suitable score\nfunctions and the search over graphs. The architecture of our inference model\nemulates permutation invariances that are crucial for statistical efficiency in\nstructure learning, which facilitates generalization to significantly larger\nproblem instances than seen during training. On synthetic data and\nsemisynthetic gene expression data, our models exhibit robust generalization\ncapabilities when subject to substantial distribution shifts and significantly\noutperform existing algorithms, especially in the challenging genomics domain.\nOur code and models are publicly available at:\nhttps://github.com/larslorch/avici.\n","authors":["Lars Lorch","Scott Sussex","Jonas Rothfuss","Andreas Krause","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2205.12934v3.pdf","comment":"NeurIPS 2022, minor formatting changes"},{"id":"http://arxiv.org/abs/2212.07295v1","updated":"2022-12-14T15:58:37Z","published":"2022-12-14T15:58:37Z","title":"Maximal Initial Learning Rates in Deep ReLU Networks","summary":"  Training a neural network requires choosing a suitable learning rate,\ninvolving a trade-off between speed and effectiveness of convergence. While\nthere has been considerable theoretical and empirical analysis of how large the\nlearning rate can be, most prior work focuses only on late-stage training. In\nthis work, we introduce the maximal initial learning rate $\\eta^{\\ast}$ - the\nlargest learning rate at which a randomly initialized neural network can\nsuccessfully begin training and achieve (at least) a given threshold accuracy.\nUsing a simple approach to estimate $\\eta^{\\ast}$, we observe that in\nconstant-width fully-connected ReLU networks, $\\eta^{\\ast}$ demonstrates\ndifferent behavior to the maximum learning rate later in training.\nSpecifically, we find that $\\eta^{\\ast}$ is well predicted as a power of\n$(\\text{depth} \\times \\text{width})$, provided that (i) the width of the\nnetwork is sufficiently large compared to the depth, and (ii) the input layer\nof the network is trained at a relatively small learning rate. We further\nanalyze the relationship between $\\eta^{\\ast}$ and the sharpness $\\lambda_{1}$\nof the network at initialization, indicating that they are closely though not\ninversely related. We formally prove bounds for $\\lambda_{1}$ in terms of\n$(\\text{depth} \\times \\text{width})$ that align with our empirical results.\n","authors":["Gaurav Iyer","Boris Hanin","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2212.07295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07429v2","updated":"2022-12-14T15:49:09Z","published":"2022-11-11T18:48:59Z","title":"Accounting for Temporal Variability in Functional Magnetic Resonance\n  Imaging Improves Prediction of Intelligence","summary":"  Neuroimaging-based prediction methods for intelligence and cognitive\nabilities have seen a rapid development in literature. Among different\nneuroimaging modalities, prediction based on functional connectivity (FC) has\nshown great promise. Most literature has focused on prediction using static FC,\nbut there are limited investigations on the merits of such analysis compared to\nprediction based on dynamic FC or region level functional magnetic resonance\nimaging (fMRI) times series that encode temporal variability. To account for\nthe temporal dynamics in fMRI data, we propose a deep neural network involving\nbi-directional long short-term memory (bi-LSTM) approach that also incorporates\nfeature selection mechanism. The proposed pipeline is implemented via an\nefficient GPU computation framework and applied to predict intelligence scores\nbased on region level fMRI time series as well as dynamic FC. We compare the\nprediction performance for different intelligence measures based on static FC,\ndynamic FC, and region level time series acquired from the Adolescent Brain\nCognitive Development (ABCD) study involving close to 7000 individuals. Our\ndetailed analysis illustrates that static FC consistently has inferior\nprediction performance compared to region level time series or dynamic FC for\nunimodal rest and task fMRI experiments, and in almost all cases using a\ncombination of task and rest features. In addition, the proposed bi-LSTM\npipeline based on region level time series identifies several shared and\ndifferential important brain regions across task and rest fMRI experiments that\ndrive intelligence prediction. A test-retest analysis of the selected features\nshows strong reliability across cross-validation folds. Given the large sample\nsize from ABCD study, our results provide strong evidence that superior\nprediction of intelligence can be achieved by accounting for temporal\nvariations in fMRI.\n","authors":["Yang Li","Xin Ma","Raj Sunderraman","Shihao Ji","Suprateek Kundu"],"pdf_url":"https://arxiv.org/pdf/2211.07429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.07018v2","updated":"2022-12-14T15:38:08Z","published":"2022-03-14T11:56:47Z","title":"A review of Generative Adversarial Networks for Electronic Health\n  Records: applications, evaluation measures and data sources","summary":"  Electronic Health Records (EHRs) are a valuable asset to facilitate clinical\nresearch and point of care applications; however, many challenges such as data\nprivacy concerns impede its optimal utilization. Deep generative models,\nparticularly, Generative Adversarial Networks (GANs) show great promise in\ngenerating synthetic EHR data by learning underlying data distributions while\nachieving excellent performance and addressing these challenges. This work aims\nto review the major developments in various applications of GANs for EHRs and\nprovides an overview of the proposed methodologies. For this purpose, we\ncombine perspectives from healthcare applications and machine learning\ntechniques in terms of source datasets and the fidelity and privacy evaluation\nof the generated synthetic datasets. We also compile a list of the metrics and\ndatasets used by the reviewed works, which can be utilized as benchmarks for\nfuture research in the field. We conclude by discussing challenges in GANs for\nEHRs development and proposing recommended practices. We hope that this work\nmotivates novel research development directions in the intersection of\nhealthcare and machine learning.\n","authors":["Ghadeer Ghosheh","Jin Li","Tingting Zhu"],"pdf_url":"https://arxiv.org/pdf/2203.07018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07283v1","updated":"2022-12-14T15:33:11Z","published":"2022-12-14T15:33:11Z","title":"Generative Robust Classification","summary":"  Training adversarially robust discriminative (i.e., softmax) classifier has\nbeen the dominant approach to robust classification. Building on recent work on\nadversarial training (AT)-based generative models, we investigate using AT to\nlearn unnormalized class-conditional density models and then performing\ngenerative robust classification. Our result shows that, under the condition of\nsimilar model capacities, the generative robust classifier achieves comparable\nperformance to a baseline softmax robust classifier when the test data is clean\nor when the test perturbation is of limited size, and much better performance\nwhen the test perturbation size exceeds the training perturbation size. The\ngenerative classifier is also able to generate samples or counterfactuals that\nmore closely resemble the training data, suggesting that the generative\nclassifier can better capture the class-conditional distributions. In contrast\nto standard discriminative adversarial training where advanced data\naugmentation techniques are only effective when combined with weight averaging,\nwe find it straightforward to apply advanced data augmentation to achieve\nbetter robustness in our approach. Our result suggests that the generative\nclassifier is a competitive alternative to robust classification, especially\nfor problems with limited number of classes.\n","authors":["Xuwang Yin"],"pdf_url":"https://arxiv.org/pdf/2212.07283v1.pdf","comment":"Report"},{"id":"http://arxiv.org/abs/2212.07282v1","updated":"2022-12-14T15:30:56Z","published":"2022-12-14T15:30:56Z","title":"Directional Direct Feedback Alignment: Estimating Backpropagation Paths\n  for Efficient Learning on Neural Processors","summary":"  The error Backpropagation algorithm (BP) is a key method for training deep\nneural networks. While performant, it is also resource-demanding in terms of\ncomputation, memory usage and energy. This makes it unsuitable for online\nlearning on edge devices that require a high processing rate and low energy\nconsumption. More importantly, BP does not take advantage of the parallelism\nand local characteristics offered by dedicated neural processors. There is\ntherefore a demand for alternative algorithms to BP that could improve the\nlatency, memory requirements, and energy footprint of neural networks on\nhardware. In this work, we propose a novel method based on Direct Feedback\nAlignment (DFA) which uses Forward-Mode Automatic Differentiation to estimate\nbackpropagation paths and learn feedback connections in an online manner. We\nexperimentally show that Directional DFA achieves performances that are closer\nto BP than other feedback methods on several benchmark datasets and\narchitectures while benefiting from the locality and parallelization\ncharacteristics of DFA. Moreover, we show that, unlike other feedback learning\nalgorithms, our method provides stable learning for convolution layers.\n","authors":["Florian Bacho","Dominique Chu"],"pdf_url":"https://arxiv.org/pdf/2212.07282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.00839v2","updated":"2022-12-14T15:24:32Z","published":"2021-06-01T22:32:02Z","title":"Algorithmic Insurance","summary":"  As machine learning algorithms start to get integrated into the\ndecision-making process of companies and organizations, insurance products are\nbeing developed to protect their owners from liability risk. Algorithmic\nliability differs from human liability since it is based on a single model\ncompared to multiple heterogeneous decision-makers and its performance is known\na priori for a given set of data. Traditional actuarial tools for human\nliability do not take these properties into consideration, primarily focusing\non the distribution of historical claims. We propose, for the first time, a\nquantitative framework to estimate the risk exposure of insurance contracts for\nmachine-driven liability, introducing the concept of algorithmic insurance.\nSpecifically, we present an optimization formulation to estimate the risk\nexposure of a binary classification model given a pre-defined range of\npremiums. We adjust the formulation to account for uncertainty in the resulting\nlosses using robust optimization. Our approach outlines how properties of the\nmodel, such as accuracy, interpretability, and generalizability, can influence\nthe insurance contract evaluation. To showcase a practical implementation of\nthe proposed framework, we present a case study of medical malpractice in the\ncontext of breast cancer detection. Our analysis focuses on measuring the\neffect of the model parameters on the expected financial loss and identifying\nthe aspects of algorithmic performance that predominantly affect the risk of\nthe contract.\n","authors":["Dimitris Bertsimas","Agni Orfanoudaki"],"pdf_url":"https://arxiv.org/pdf/2106.00839v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07278v1","updated":"2022-12-14T15:22:32Z","published":"2022-12-14T15:22:32Z","title":"Backdoor Mitigation in Deep Neural Networks via Strategic Retraining","summary":"  Deep Neural Networks (DNN) are becoming increasingly more important in\nassisted and automated driving. Using such entities which are obtained using\nmachine learning is inevitable: tasks such as recognizing traffic signs cannot\nbe developed reasonably using traditional software development methods. DNN\nhowever do have the problem that they are mostly black boxes and therefore hard\nto understand and debug. One particular problem is that they are prone to\nhidden backdoors. This means that the DNN misclassifies its input, because it\nconsiders properties that should not be decisive for the output. Backdoors may\neither be introduced by malicious attackers or by inappropriate training. In\nany case, detecting and removing them is important in the automotive area, as\nthey might lead to safety violations with potentially severe consequences. In\nthis paper, we introduce a novel method to remove backdoors. Our method works\nfor both intentional as well as unintentional backdoors. We also do not require\nprior knowledge about the shape or distribution of backdoors. Experimental\nevidence shows that our method performs well on several medium-sized examples.\n","authors":["Akshay Dhonthi","Ernst Moritz Hahn","Vahid Hashemi"],"pdf_url":"https://arxiv.org/pdf/2212.07278v1.pdf","comment":"13 Pages, 7 Tables, 4 Figures. Accepted at the International\n  Symposium of Formal Methods 2023 (FM 2023)"},{"id":"http://arxiv.org/abs/2207.05705v2","updated":"2022-12-14T15:12:54Z","published":"2022-07-12T17:27:18Z","title":"Conservative SPDEs as fluctuating mean field limits of stochastic\n  gradient descent","summary":"  The convergence of stochastic interacting particle systems in the mean-field\nlimit to solutions of conservative stochastic partial differential equations is\nestablished, with optimal rate of convergence. As a second main result, a\nquantitative central limit theorem for such SPDEs is derived, again, with\noptimal rate of convergence.\n  The results apply, in particular, to the convergence in the mean-field\nscaling of stochastic gradient descent dynamics in overparametrized, shallow\nneural networks to solutions of SPDEs. It is shown that the inclusion of\nfluctuations in the limiting SPDE improves the rate of convergence, and retains\ninformation about the fluctuations of stochastic gradient descent in the\ncontinuum limit.\n","authors":["Benjamin Gess","Rishabh S. Gvalani","Vitalii Konarovskyi"],"pdf_url":"https://arxiv.org/pdf/2207.05705v2.pdf","comment":"65 pages"},{"id":"http://arxiv.org/abs/2004.11722v6","updated":"2022-12-14T14:58:01Z","published":"2020-04-22T07:42:30Z","title":"Counterfactual Learning of Stochastic Policies with Continuous Actions:\n  from Models to Offline Evaluation","summary":"  Counterfactual reasoning from logged data has become increasingly important\nfor many applications such as web advertising or healthcare. In this paper, we\naddress the problem of learning stochastic policies with continuous actions\nfrom the viewpoint of counterfactual risk minimization (CRM). While the CRM\nframework is appealing and well studied for discrete actions, the continuous\naction case raises new challenges about modelization, optimization, and~offline\nmodel selection with real data which turns out to be particularly challenging.\nOur paper contributes to these three aspects of the CRM estimation pipeline.\nFirst, we introduce a modelling strategy based on a joint kernel embedding of\ncontexts and actions, which overcomes the shortcomings of previous\ndiscretization approaches. Second, we empirically show that the optimization\naspect of counterfactual learning is important, and we demonstrate the benefits\nof proximal point algorithms and differentiable estimators. Finally, we propose\nan evaluation protocol for offline policies in real-world logged systems, which\nis challenging since policies cannot be replayed on test data, and we release a\nnew large-scale dataset along with multiple synthetic, yet realistic,\nevaluation setups.\n","authors":["Houssam Zenati","Alberto Bietti","Matthieu Martin","Eustache Diemert","Pierre Gaillard","Julien Mairal"],"pdf_url":"https://arxiv.org/pdf/2004.11722v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.09553v3","updated":"2022-12-14T14:52:16Z","published":"2021-06-17T14:33:55Z","title":"Large-Scale Chemical Language Representations Capture Molecular\n  Structure and Properties","summary":"  Models based on machine learning can enable accurate and fast molecular\nproperty predictions, which is of interest in drug discovery and material\ndesign. Various supervised machine learning models have demonstrated promising\nperformance, but the vast chemical space and the limited availability of\nproperty labels make supervised learning challenging. Recently, unsupervised\ntransformer-based language models pretrained on a large unlabelled corpus have\nproduced state-of-the-art results in many downstream natural language\nprocessing tasks. Inspired by this development, we present molecular embeddings\nobtained by training an efficient transformer encoder model, MoLFormer, which\nuses rotary positional embeddings. This model employs a linear attention\nmechanism, coupled with highly distributed training, on SMILES sequences of 1.1\nbillion unlabelled molecules from the PubChem and ZINC datasets. We show that\nthe learned molecular representation outperforms existing baselines, including\nsupervised and self-supervised graph neural networks and language models, on\nseveral downstream tasks from ten benchmark datasets. They perform\ncompetitively on two others. Further analyses, specifically through the lens of\nattention, demonstrate that MoLFormer trained on chemical SMILES indeed learns\nthe spatial relationships between atoms within a molecule. These results\nprovide encouraging evidence that large-scale molecular language models can\ncapture sufficient chemical and structural information to predict various\ndistinct molecular properties, including quantum-chemical properties.\n","authors":["Jerret Ross","Brian Belgodere","Vijil Chenthamarakshan","Inkit Padhi","Youssef Mroueh","Payel Das"],"pdf_url":"https://arxiv.org/pdf/2106.09553v3.pdf","comment":"NMI 2022"},{"id":"http://arxiv.org/abs/2207.08003v3","updated":"2022-12-14T14:40:39Z","published":"2022-07-16T19:25:41Z","title":"SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video\n  Anomaly Detection","summary":"  A self-supervised multi-task learning (SSMTL) framework for video anomaly\ndetection was recently introduced in literature. Due to its highly accurate\nresults, the method attracted the attention of many researchers. In this work,\nwe revisit the self-supervised multi-task learning framework, proposing several\nupdates to the original method. First, we study various detection methods, e.g.\nbased on detecting high-motion regions using optical flow or background\nsubtraction, since we believe the currently used pre-trained YOLOv3 is\nsuboptimal, e.g. objects in motion or objects from unknown classes are never\ndetected. Second, we modernize the 3D convolutional backbone by introducing\nmulti-head self-attention modules, inspired by the recent success of vision\ntransformers. As such, we alternatively introduce both 2D and 3D convolutional\nvision transformer (CvT) blocks. Third, in our attempt to further improve the\nmodel, we study additional self-supervised learning tasks, such as predicting\nsegmentation maps through knowledge distillation, solving jigsaw puzzles,\nestimating body pose through knowledge distillation, predicting masked regions\n(inpainting), and adversarial learning with pseudo-anomalies. We conduct\nexperiments to assess the performance impact of the introduced changes. Upon\nfinding more promising configurations of the framework, dubbed SSMTL++v1 and\nSSMTL++v2, we extend our preliminary experiments to more data sets,\ndemonstrating that our performance gains are consistent across all data sets.\nIn most cases, our results on Avenue, ShanghaiTech and UBnormal raise the\nstate-of-the-art performance bar to a new level.\n","authors":["Antonio Barbalau","Radu Tudor Ionescu","Mariana-Iuliana Georgescu","Jacob Dueholm","Bharathkumar Ramachandra","Kamal Nasrollahi","Fahad Shahbaz Khan","Thomas B. Moeslund","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2207.08003v3.pdf","comment":"Under consideration at Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2212.07249v1","updated":"2022-12-14T14:34:15Z","published":"2022-12-14T14:34:15Z","title":"APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning","summary":"  Long-form numerical reasoning in financial analysis aims to generate a\nreasoning program to calculate the correct answer for a given question.\nPrevious work followed a retriever-generator framework, where the retriever\nselects key facts from a long-form document, and the generator generates a\nreasoning program based on retrieved facts. However, they treated all facts\nequally without considering the different contributions of facts with and\nwithout numbers. Meanwhile, the program consistency were ignored under\nsupervised training, resulting in lower training accuracy and diversity. To\nsolve these problems, we proposed APOLLO to improve the long-form numerical\nreasoning framework. For the retriever, we adopt a number-aware negative\nsampling strategy to enable the retriever to be more discriminative on key\nnumerical facts. For the generator, we design consistency-based reinforcement\nlearning and target program augmentation strategy based on the consistency of\nprogram execution results. Experimental results on the FinQA and ConvFinQA\nleaderboard verify the effectiveness of our proposed method, achieving the new\nstate-of-the-art.\n","authors":["Jiashuo Sun","Hang Zhang","Chen Lin","Yeyun Gong","Jian Guo","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2212.07249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.10421v2","updated":"2022-12-14T14:28:33Z","published":"2022-03-20T00:52:45Z","title":"CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot\n  Object Navigation","summary":"  For robots to be generally useful, they must be able to find arbitrary\nobjects described by people (i.e., be language-driven) even without expensive\nnavigation training on in-domain data (i.e., perform zero-shot inference). We\nexplore these capabilities in a unified setting: language-driven zero-shot\nobject navigation (L-ZSON). Inspired by the recent success of open-vocabulary\nmodels for image classification, we investigate a straightforward framework,\nCLIP on Wheels (CoW), to adapt open-vocabulary models to this task without\nfine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark,\nwhich considers finding uncommon objects, objects described by spatial and\nappearance attributes, and hidden objects described relative to visible\nobjects. We conduct an in-depth empirical study by directly deploying 21 CoW\nbaselines across Habitat, RoboTHOR, and Pasture. In total, we evaluate over 90k\nnavigation episodes and find that (1) CoW baselines often struggle to leverage\nlanguage descriptions, but are proficient at finding uncommon objects. (2) A\nsimple CoW, with CLIP-based object localization and classical exploration --\nand no additional training -- matches the navigation efficiency of a\nstate-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This\nsame CoW provides a 15.6 percentage point improvement in success over a\nstate-of-the-art RoboTHOR ZSON model.\n","authors":["Samir Yitzhak Gadre","Mitchell Wortsman","Gabriel Ilharco","Ludwig Schmidt","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2203.10421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07231v1","updated":"2022-12-14T14:06:27Z","published":"2022-12-14T14:06:27Z","title":"Cutting Plane Selection with Analytic Centers and Multiregression","summary":"  Cutting planes are a crucial component of state-of-the-art mixed-integer\nprogramming solvers, with the choice of which subset of cuts to add being vital\nfor solver performance. We propose new distance-based measures to qualify the\nvalue of a cut by quantifying the extent to which it separates relevant parts\nof the relaxed feasible set. For this purpose, we use the analytic centers of\nthe relaxation polytope or of its optimal face, as well as alternative optimal\nsolutions of the linear programming relaxation. We assess the impact of the\nchoice of distance measure on root node performance and throughout the whole\nbranch-and-bound tree, comparing our measures against those prevalent in the\nliterature. Finally, by a multi-output regression, we predict the relative\nperformance of each measure, using static features readily available before the\nseparation process. Our results indicate that analytic center-based methods\nhelp to significantly reduce the number of branch-and-bound nodes needed to\nexplore the search space and that our multiregression approach can further\nimprove on any individual method.\n","authors":["Mark Turner","Timo Berthold","Mathieu Besançon","Thorsten Koch"],"pdf_url":"https://arxiv.org/pdf/2212.07231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07224v1","updated":"2022-12-14T13:57:01Z","published":"2022-12-14T13:57:01Z","title":"FedSkip: Combatting Statistical Heterogeneity with Federated Skip\n  Aggregation","summary":"  The statistical heterogeneity of the non-independent and identically\ndistributed (non-IID) data in local clients significantly limits the\nperformance of federated learning. Previous attempts like FedProx, SCAFFOLD,\nMOON, FedNova and FedDyn resort to an optimization perspective, which requires\nan auxiliary term or re-weights local updates to calibrate the learning bias or\nthe objective inconsistency. However, in addition to previous explorations for\nimprovement in federated averaging, our analysis shows that another critical\nbottleneck is the poorer optima of client models in more heterogeneous\nconditions. We thus introduce a data-driven approach called FedSkip to improve\nthe client optima by periodically skipping federated averaging and scattering\nlocal models to the cross devices. We provide theoretical analysis of the\npossible benefit from FedSkip and conduct extensive experiments on a range of\ndatasets to demonstrate that FedSkip achieves much higher accuracy, better\naggregation efficiency and competing communication efficiency. Source code is\navailable at: https://github.com/MediaBrain-SJTU/FedSkip.\n","authors":["Ziqing Fan","Yanfeng Wang","Jiangchao Yao","Lingjuan Lyu","Ya Zhang","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2212.07224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10997v2","updated":"2022-12-14T13:27:55Z","published":"2022-09-22T13:27:21Z","title":"Counterfactual Explanations Using Optimization With Constraint Learning","summary":"  To increase the adoption of counterfactual explanations in practice, several\ncriteria that these should adhere to have been put forward in the literature.\nWe propose counterfactual explanations using optimization with constraint\nlearning (CE-OCL), a generic and flexible approach that addresses all these\ncriteria and allows room for further extensions. Specifically, we discuss how\nwe can leverage an optimization with constraint learning framework for the\ngeneration of counterfactual explanations, and how components of this framework\nreadily map to the criteria. We also propose two novel modeling approaches to\naddress data manifold closeness and diversity, which are two key criteria for\npractical counterfactual explanations. We test CE-OCL on several datasets and\npresent our results in a case study. Compared against the current\nstate-of-the-art methods, CE-OCL allows for more flexibility and has an overall\nsuperior performance in terms of several evaluation metrics proposed in related\nwork.\n","authors":["Donato Maragno","Tabea E. Röber","Ilker Birbil"],"pdf_url":"https://arxiv.org/pdf/2209.10997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07201v1","updated":"2022-12-14T12:59:25Z","published":"2022-12-14T12:59:25Z","title":"Toroidal Coordinates: Decorrelating Circular Coordinates With Lattice\n  Reduction","summary":"  The circular coordinates algorithm of de Silva, Morozov, and\nVejdemo-Johansson takes as input a dataset together with a cohomology class\nrepresenting a $1$-dimensional hole in the data; the output is a map from the\ndata into the circle that captures this hole, and that is of minimum energy in\na suitable sense. However, when applied to several cohomology classes, the\noutput circle-valued maps can be \"geometrically correlated\" even if the chosen\ncohomology classes are linearly independent. It is shown in the original work\nthat less correlated maps can be obtained with suitable integer linear\ncombinations of the cohomology classes, with the linear combinations being\nchosen by inspection. In this paper, we identify a formal notion of geometric\ncorrelation between circle-valued maps which, in the Riemannian manifold case,\ncorresponds to the Dirichlet form, a bilinear form derived from the Dirichlet\nenergy. We describe a systematic procedure for constructing low energy\ntorus-valued maps on data, starting from a set of linearly independent\ncohomology classes. We showcase our procedure with computational examples. Our\nmain algorithm is based on the Lenstra--Lenstra--Lov\\'asz algorithm from\ncomputational number theory.\n","authors":["Luis Scoccola","Hitesh Gakhar","Johnathan Bush","Nikolas Schonsheck","Tatum Rask","Ling Zhou","Jose A. Perea"],"pdf_url":"https://arxiv.org/pdf/2212.07201v1.pdf","comment":"21 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.07194v1","updated":"2022-12-14T12:39:47Z","published":"2022-12-14T12:39:47Z","title":"Traffic Flow Prediction via Variational Bayesian Inference-based\n  Encoder-Decoder Framework","summary":"  Accurate traffic flow prediction, a hotspot for intelligent transportation\nresearch, is the prerequisite for mastering traffic and making travel plans.\nThe speed of traffic flow can be affected by roads condition, weather,\nholidays, etc. Furthermore, the sensors to catch the information about traffic\nflow will be interfered with by environmental factors such as illumination,\ncollection time, occlusion, etc. Therefore, the traffic flow in the practical\ntransportation system is complicated, uncertain, and challenging to predict\naccurately. This paper proposes a deep encoder-decoder prediction framework\nbased on variational Bayesian inference. A Bayesian neural network is\nconstructed by combining variational inference with gated recurrent units (GRU)\nand used as the deep neural network unit of the encoder-decoder framework to\nmine the intrinsic dynamics of traffic flow. Then, the variational inference is\nintroduced into the multi-head attention mechanism to avoid noise-induced\ndeterioration of prediction accuracy. The proposed model achieves superior\nprediction performance on the Guangzhou urban traffic flow dataset over the\nbenchmarks, particularly when the long-term prediction.\n","authors":["Jianlei Kong","Xiaomeng Fan","Xue-Bo Jin","Min Zuo"],"pdf_url":"https://arxiv.org/pdf/2212.07194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.03610v2","updated":"2022-12-14T12:34:44Z","published":"2022-03-07T18:59:03Z","title":"ZippyPoint: Fast Interest Point Detection, Description, and Matching\n  through Mixed Precision Discretization","summary":"  Efficient detection and description of geometric regions in images is a\nprerequisite in visual systems for localization and mapping. Such systems still\nrely on traditional hand-crafted methods for efficient generation of\nlightweight descriptors, a common limitation of the more powerful neural\nnetwork models that come with high compute and specific hardware requirements.\nIn this paper, we focus on the adaptations required by detection and\ndescription neural networks to enable their use in computationally limited\nplatforms such as robots, mobile, and augmented reality devices. To that end,\nwe investigate and adapt network quantization techniques to accelerate\ninference and enable its use on compute limited platforms. In addition, we\nrevisit common practices in descriptor quantization and propose the use of a\nbinary descriptor normalization layer, enabling the generation of distinctive\nbinary descriptors with a constant number of ones. ZippyPoint, our efficient\nquantized network with binary descriptors, improves the network runtime speed,\nthe descriptor matching speed, and the 3D model size, by at least an order of\nmagnitude when compared to full-precision counterparts. These improvements come\nat a minor performance degradation as evaluated on the tasks of homography\nestimation, visual localization, and map-free visual relocalization. Code and\ntrained models will be released upon acceptance.\n","authors":["Menelaos Kanakis","Simon Maurer","Matteo Spallanzani","Ajad Chhatkuli","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2203.03610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07753v3","updated":"2022-12-14T12:24:01Z","published":"2022-07-15T21:03:11Z","title":"Do Not Sleep on Traditional Machine Learning: Simple and Interpretable\n  Techniques Are Competitive to Deep Learning for Sleep Scoring","summary":"  Over the last few years, research in automatic sleep scoring has mainly\nfocused on developing increasingly complex deep learning architectures.\nHowever, recently these approaches achieved only marginal improvements, often\nat the expense of requiring more data and more expensive training procedures.\nDespite all these efforts and their satisfactory performance, automatic sleep\nstaging solutions are not widely adopted in a clinical context yet. We argue\nthat most deep learning solutions for sleep scoring are limited in their\nreal-world applicability as they are hard to train, deploy, and reproduce.\nMoreover, these solutions lack interpretability and transparency, which are\noften key to increase adoption rates. In this work, we revisit the problem of\nsleep stage classification using classical machine learning. Results show that\ncompetitive performance can be achieved with a conventional machine learning\npipeline consisting of preprocessing, feature extraction, and a simple machine\nlearning model. In particular, we analyze the performance of a linear model and\na non-linear (gradient boosting) model. Our approach surpasses state-of-the-art\n(that uses the same data) on two public datasets: Sleep-EDF SC-20 (MF1 0.810)\nand Sleep-EDF ST (MF1 0.795), while achieving competitive results on Sleep-EDF\nSC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that, for the sleep stage\nscoring task, the expressiveness of an engineered feature vector is on par with\nthe internally learned representations of deep learning models. This\nobservation opens the door to clinical adoption, as a representative feature\nvector allows to leverage both the interpretability and successful track record\nof traditional machine learning models.\n","authors":["Jeroen Van Der Donckt","Jonas Van Der Donckt","Emiel Deprost","Nicolas Vandenbussche","Michael Rademaker","Gilles Vandewiele","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2207.07753v3.pdf","comment":"The first two authors contributed equally. Accepted to Biomedical\n  Signal Processing and Control"},{"id":"http://arxiv.org/abs/2212.07179v1","updated":"2022-12-14T12:08:30Z","published":"2022-12-14T12:08:30Z","title":"FLAGS Framework for Comparative Analysis of Federated Learning\n  Algorithms","summary":"  Federated Learning (FL) has become a key choice for distributed machine\nlearning. Initially focused on centralized aggregation, recent works in FL have\nemphasized greater decentralization to adapt to the highly heterogeneous\nnetwork edge. Among these, Hierarchical, Device-to-Device and Gossip Federated\nLearning (HFL, D2DFL \\& GFL respectively) can be considered as foundational FL\nalgorithms employing fundamental aggregation strategies. A number of FL\nalgorithms were subsequently proposed employing multiple fundamental\naggregation schemes jointly. Existing research, however, subjects the FL\nalgorithms to varied conditions and gauges the performance of these algorithms\nmainly against Federated Averaging (FedAvg) only. This work consolidates the FL\nlandscape and offers an objective analysis of the major FL algorithms through a\ncomprehensive cross-evaluation for a wide range of operating conditions. In\naddition to the three foundational FL algorithms, this work also analyzes six\nderived algorithms. To enable a uniform assessment, a multi-FL framework named\nFLAGS: Federated Learning AlGorithms Simulation has been developed for rapid\nconfiguration of multiple FL algorithms. Our experiments indicate that fully\ndecentralized FL algorithms achieve comparable accuracy under multiple\noperating conditions, including asynchronous aggregation and the presence of\nstragglers. Furthermore, decentralized FL can also operate in noisy\nenvironments and with a comparably higher local update rate. However, the\nimpact of extremely skewed data distributions on decentralized FL is much more\nadverse than on centralized variants. The results indicate that it may not be\nnecessary to restrict the devices to a single FL algorithm; rather, multi-FL\nnodes may operate with greater efficiency.\n","authors":["Ahnaf Hannan Lodhi","Barış Akgün","Öznur Özkasap"],"pdf_url":"https://arxiv.org/pdf/2212.07179v1.pdf","comment":"39 pages, 10 figures. Accepted for publication in Elsevier 'Internet\n  of Things'"},{"id":"http://arxiv.org/abs/2201.05818v2","updated":"2022-12-14T11:47:37Z","published":"2022-01-15T10:04:05Z","title":"Two Measures of Non-Probabilistic Uncertainty","summary":"  There are two reasons why uncertainty about the future yield of investments\nmay not be adequately described by Probability Theory. The first one is due to\nunique or nearly-unique events, that either never realized or occurred too\nseldom for probabilities to be reliable. The second one arises when when one\nfears that something may happen, that one is not even able to figure out, e.g.,\nif one asks: \"Climate change, financial crises, pandemic, war, what next?\"\n  In both cases, simple one-to-one causal mappings between available\nalternatives and possible consequences eventually melt down. However, such\ndestructions reflect into the changing narratives of business executives,\nemployees and other stakeholders in specific, identifiable and differential\nways. In particular, texts such as consultants' reports or letters to\nshareholders can be analysed in order to detect the impact of both sorts of\nuncertainty onto the causal relations that normally guide decision-making.\n  We propose structural measures of causal mappings as a means to measure\nnon-probabilistic uncertainty, eventually suggesting that automated text\nanalysis can greatly augment the possibilities offered by these techniques.\nProspective applications may concern statistical institutes, stock market\ntraders, as well as businesses wishing to compare their own vision to those\nprevailing in their industry.\n","authors":["Florian Ellsaesser","Guido Fioretti","Gail E. James"],"pdf_url":"https://arxiv.org/pdf/2201.05818v2.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2202.11028v2","updated":"2022-12-14T11:45:18Z","published":"2022-02-22T16:59:35Z","title":"Generating Synthetic Mobility Networks with Generative Adversarial\n  Networks","summary":"  The increasingly crucial role of human displacements in complex societal\nphenomena, such as traffic congestion, segregation, and the diffusion of\nepidemics, is attracting the interest of scientists from several disciplines.\nIn this article, we address mobility network generation, i.e., generating a\ncity's entire mobility network, a weighted directed graph in which nodes are\ngeographic locations and weighted edges represent people's movements between\nthose locations, thus describing the entire mobility set flows within a city.\nOur solution is MoGAN, a model based on Generative Adversarial Networks (GANs)\nto generate realistic mobility networks. We conduct extensive experiments on\npublic datasets of bike and taxi rides to show that MoGAN outperforms the\nclassical Gravity and Radiation models regarding the realism of the generated\nnetworks. Our model can be used for data augmentation and performing\nsimulations and what-if analysis.\n","authors":["Giovanni Mauro","Massimiliano Luca","Antonio Longa","Bruno Lepri","Luca Pappalardo"],"pdf_url":"https://arxiv.org/pdf/2202.11028v2.pdf","comment":"19 pages, 5 figures. Supplementary 9 pages and 5 figures"},{"id":"http://arxiv.org/abs/2110.15288v5","updated":"2022-12-14T11:44:37Z","published":"2021-10-28T16:48:15Z","title":"Hyper-Representations: Self-Supervised Representation Learning on Neural\n  Network Weights for Model Characteristic Prediction","summary":"  Self-Supervised Learning (SSL) has been shown to learn useful and\ninformation-preserving representations. Neural Networks (NNs) are widely\napplied, yet their weight space is still not fully understood. Therefore, we\npropose to use SSL to learn hyper-representations of the weights of populations\nof NNs. To that end, we introduce domain specific data augmentations and an\nadapted attention architecture. Our empirical evaluation demonstrates that\nself-supervised representation learning in this domain is able to recover\ndiverse NN model characteristics. Further, we show that the proposed learned\nrepresentations outperform prior work for predicting hyper-parameters, test\naccuracy, and generalization gap as well as transfer to out-of-distribution\nsettings.\n","authors":["Konstantin Schürholt","Dimche Kostadinov","Damian Borth"],"pdf_url":"https://arxiv.org/pdf/2110.15288v5.pdf","comment":"Published at 35th Conference on Neural Information Processing Systems\n  (NeurIPS 2021), Sydney, Australia. 31 Pages, 14 figures"},{"id":"http://arxiv.org/abs/2108.12175v2","updated":"2022-12-14T11:42:14Z","published":"2021-08-27T08:40:08Z","title":"Grammar Based Speaker Role Identification for Air Traffic Control Speech\n  Recognition","summary":"  Automatic Speech Recognition (ASR) for air traffic control is generally\ntrained by pooling Air Traffic Controller (ATCO) and pilot data into one set.\nThis is motivated by the fact that pilot's voice communications are more scarce\nthan ATCOs. Due to this data imbalance and other reasons (e.g., varying\nacoustic conditions), the speech from ATCOs is usually recognized more\naccurately than from pilots. Automatically identifying the speaker roles is a\nchallenging task, especially in the case of the noisy voice recordings\ncollected using Very High Frequency (VHF) receivers or due to the\nunavailability of the push-to-talk (PTT) signal, i.e., both audio channels are\nmixed. In this work, we propose to (1) automatically segment the ATCO and pilot\ndata based on an intuitive approach exploiting ASR transcripts and (2)\nsubsequently consider an automatic recognition of ATCOs' and pilots' voice as\ntwo separate tasks. Our work is performed on VHF audio data with high noise\nlevels, i.e., signal-to-noise (SNR) ratios below 15 dB, as this data is\nrecognized to be helpful for various speech-based machine-learning tasks.\nSpecifically, for the speaker role identification task, the module is\nrepresented by a simple yet efficient knowledge-based system exploiting a\ngrammar defined by the International Civil Aviation Organization (ICAO). The\nsystem accepts text as the input, either manually verified annotations or\nautomatically generated transcripts. The developed approach provides an average\naccuracy in speaker role identification of about 83%. Finally, we show that\ntraining an acoustic model for ASR tasks separately (i.e., separate models for\nATCOs and pilots) or using a multitask approach is well suited for the noisy\ndata and outperforms the traditional ASR system where all data is pooled\ntogether.\n","authors":["Amrutha Prasad","Juan Zuluaga-Gomez","Petr Motlicek","Saeed Sarfjoo","Iuliia Nigmatulina","Oliver Ohneiser","Hartmut Helmke"],"pdf_url":"https://arxiv.org/pdf/2108.12175v2.pdf","comment":"Presented at Sesar Innovation Days - 2022. See\n  https://www.sesarju.eu/sesarinnovationdays"},{"id":"http://arxiv.org/abs/2205.08115v2","updated":"2022-12-14T11:38:47Z","published":"2022-05-17T06:26:54Z","title":"Fast and Provably Convergent Algorithms for Gromov-Wasserstein in Graph\n  Data","summary":"  In this paper, we study the design and analysis of a class of efficient\nalgorithms for computing the Gromov-Wasserstein (GW) distance tailored to\nlarge-scale graph learning tasks. Armed with the Luo-Tseng error bound\ncondition~\\citep{luo1992error}, two proposed algorithms, called Bregman\nAlternating Projected Gradient (BAPG) and hybrid Bregman Proximal Gradient\n(hBPG) enjoy the convergence guarantees. Upon task-specific properties, our\nanalysis further provides novel theoretical insights to guide how to select the\nbest-fit method. As a result, we are able to provide comprehensive experiments\nto validate the effectiveness of our methods on a host of tasks, including\ngraph alignment, graph partition, and shape matching. In terms of both\nwall-clock time and modeling performance, the proposed methods achieve\nstate-of-the-art results.\n","authors":["Jiajin Li","Jianheng Tang","Lemin Kong","Huikang Liu","Jia Li","Anthony Man-Cho So","Jose Blanchet"],"pdf_url":"https://arxiv.org/pdf/2205.08115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07164v1","updated":"2022-12-14T11:34:59Z","published":"2022-12-14T11:34:59Z","title":"Speech and Natural Language Processing Technologies for Pseudo-Pilot\n  Simulator","summary":"  This paper describes a simple yet efficient repetition-based modular system\nfor speeding up air-traffic controllers (ATCos) training. E.g., a human pilot\nis still required in EUROCONTROL's ESCAPE lite simulator (see\nhttps://www.eurocontrol.int/simulator/escape) during ATCo training. However,\nthis need can be substituted by an automatic system that could act as a pilot.\nIn this paper, we aim to develop and integrate a pseudo-pilot agent into the\nATCo training pipeline by merging diverse artificial intelligence (AI) powered\nmodules. The system understands the voice communications issued by the ATCo,\nand, in turn, it generates a spoken prompt that follows the pilot's phraseology\nto the initial communication. Our system mainly relies on open-source AI tools\nand air traffic control (ATC) databases, thus, proving its simplicity and ease\nof replicability. The overall pipeline is composed of the following: (1) a\nsubmodule that receives and pre-processes the input stream of raw audio, (2) an\nautomatic speech recognition (ASR) system that transforms audio into a sequence\nof words; (3) a high-level ATC-related entity parser, which extracts relevant\ninformation from the communication, i.e., callsigns and commands, and finally,\n(4) a speech synthesizer submodule that generates responses based on the\nhigh-level ATC entities previously extracted. Overall, we show that this system\ncould pave the way toward developing a real proof-of-concept pseudo-pilot\nsystem. Hence, speeding up the training of ATCos while drastically reducing its\noverall cost.\n","authors":["Amrutha Prasad","Juan Zuluaga-Gomez","Petr Motlicek","Saeed Sarfjoo","Iuliia Nigmatulina","Karel Vesely"],"pdf_url":"https://arxiv.org/pdf/2212.07164v1.pdf","comment":"Presented at Sesar Innovation Days 2022.\n  https://www.sesarju.eu/sesarinnovationdays"},{"id":"http://arxiv.org/abs/2212.07158v1","updated":"2022-12-14T11:20:24Z","published":"2022-12-14T11:20:24Z","title":"Establishing a stronger baseline for lightweight contrastive models","summary":"  Recent research has reported a performance degradation in self-supervised\ncontrastive learning for specially designed efficient networks, such as\nMobileNet and EfficientNet. A common practice to address this problem is to\nintroduce a pretrained contrastive teacher model and train the lightweight\nnetworks with distillation signals generated by the teacher. However, it is\ntime and resource consuming to pretrain a teacher model when it is not\navailable. In this work, we aim to establish a stronger baseline for\nlightweight contrastive models without using a pretrained teacher model.\nSpecifically, we show that the optimal recipe for efficient models is different\nfrom that of larger models, and using the same training settings as ResNet50,\nas previous research does, is inappropriate. Additionally, we observe a common\nissu e in contrastive learning where either the positive or negative views can\nbe noisy, and propose a smoothed version of InfoNCE loss to alleviate this\nproblem. As a result, we successfully improve the linear evaluation results\nfrom 36.3\\% to 62.3\\% for MobileNet-V3-Large and from 42.2\\% to 65.8\\% for\nEfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with\n$5\\times$ fewer parameters. We hope our research will facilitate the usage of\nlightweight contrastive models.\n","authors":["Wenye Lin","Yifeng Ding","Zhixiong Cao","Hai-tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2212.07158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07143v1","updated":"2022-12-14T10:24:50Z","published":"2022-12-14T10:24:50Z","title":"Reproducible scaling laws for contrastive language-image learning","summary":"  Scaling up neural networks has led to remarkable performance across a wide\nrange of tasks. Moreover, performance often follows reliable scaling laws as a\nfunction of training set size, model size, and compute, which offers valuable\nguidance as large-scale experiments are becoming increasingly expensive.\nHowever, previous work on scaling laws has primarily used private data \\&\nmodels or focused on uni-modal language or vision learning. To address these\nlimitations, we investigate scaling laws for contrastive language-image\npre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP\nrepository. Our large-scale experiments involve models trained on up to two\nbillion image-text pairs and identify power law scaling for multiple downstream\ntasks including zero-shot classification, retrieval, linear probing, and\nend-to-end fine-tuning. We find that the training distribution plays a key role\nin scaling laws as the OpenAI and OpenCLIP models exhibit different scaling\nbehavior despite identical model architectures and similar training recipes. We\nopen-source our evaluation workflow and all models, including the largest\npublic CLIP models, to ensure reproducibility and make scaling laws research\nmore accessible. Source code and instructions to reproduce this study will be\navailable at https://github.com/LAION-AI/scaling-laws-openclip\n","authors":["Mehdi Cherti","Romain Beaumont","Ross Wightman","Mitchell Wortsman","Gabriel Ilharco","Cade Gordon","Christoph Schuhmann","Ludwig Schmidt","Jenia Jitsev"],"pdf_url":"https://arxiv.org/pdf/2212.07143v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2210.08753v4","updated":"2022-12-14T10:20:53Z","published":"2022-10-17T05:16:23Z","title":"MCP: Self-supervised Pre-training for Personalized Chatbots with\n  Multi-level Contrastive Sampling","summary":"  Personalized chatbots focus on endowing the chatbots with a consistent\npersonality to behave like real users and further act as personal assistants.\nPrevious studies have explored generating implicit user profiles from the\nuser's dialogue history for building personalized chatbots. However, these\nstudies only use the response generation loss to train the entire model, thus\nit is prone to suffer from the problem of data sparsity. Besides, they\noveremphasize the final generated response's quality while ignoring the\ncorrelations and fusions between the user's dialogue history, leading to rough\ndata representations and performance degradation. To tackle these problems, we\npropose a self-supervised learning framework MCP for capturing better\nrepresentations from users' dialogue history for personalized chatbots.\nSpecifically, we apply contrastive sampling methods to leverage the supervised\nsignals hidden in user dialog history, and generate the pre-training samples\nfor enhancing the model. We design three pre-training tasks based on three\ntypes of contrastive pairs from user dialogue history, namely response pairs,\nsequence augmentation pairs, and user pairs. We pre-train the utterance encoder\nand the history encoder towards the contrastive objectives and use these\npre-trained encoders for generating user profiles while personalized response\ngeneration. Experimental results on two real-world datasets show a significant\nimprovement in our proposed model MCP compared with the existing methods.\n","authors":["Zhaoheng Huang","Zhicheng Dou","Yutao Zhu","Zhengyi Ma"],"pdf_url":"https://arxiv.org/pdf/2210.08753v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07138v1","updated":"2022-12-14T10:00:53Z","published":"2022-12-14T10:00:53Z","title":"Approximating Optimal Estimation of Time Offset Synchronization with\n  Temperature Variations","summary":"  The paper addresses the problem of time offset synchronization in the\npresence of temperature variations, which lead to a non-Gaussian environment.\nIn this context, regular Kalman filtering reveals to be suboptimal. A\nfunctional optimization approach is developed in order to approximate optimal\nestimation of the clock offset between master and slave. A numerical\napproximation is provided to this aim, based on regular neural network\ntraining. Other heuristics are provided as well, based on spline regression. An\nextensive performance evaluation highlights the benefits of the proposed\ntechniques, which can be easily generalized to several clock synchronization\nprotocols and operating environments.\n","authors":["Maurizio Mongelli","Stefano Scanzio"],"pdf_url":"https://arxiv.org/pdf/2212.07138v1.pdf","comment":"preprint, 9 pages"},{"id":"http://arxiv.org/abs/2212.07127v1","updated":"2022-12-14T09:26:07Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07123v1","updated":"2022-12-14T09:20:42Z","published":"2022-12-14T09:20:42Z","title":"Reinforcement Learning in System Identification","summary":"  System identification, also known as learning forward models, transfer\nfunctions, system dynamics, etc., has a long tradition both in science and\nengineering in different fields. Particularly, it is a recurring theme in\nReinforcement Learning research, where forward models approximate the state\ntransition function of a Markov Decision Process by learning a mapping function\nfrom current state and action to the next state. This problem is commonly\ndefined as a Supervised Learning problem in a direct way. This common approach\nfaces several difficulties due to the inherent complexities of the dynamics to\nlearn, for example, delayed effects, high non-linearity, non-stationarity,\npartial observability and, more important, error accumulation when using\nbootstrapped predictions (predictions based on past predictions), over large\ntime horizons. Here we explore the use of Reinforcement Learning in this\nproblem. We elaborate on why and how this problem fits naturally and sound as a\nReinforcement Learning problem, and present some experimental results that\ndemonstrate RL is a promising technique to solve these kind of problems.\n","authors":["Jose Antonio Martin H.","Oscar Fernandez Vicente","Sergio Perez","Anas Belfadil","Cristina Ibanez-Llano","Freddy Jose Perozo Rondon","Jose Javier Valle","Javier Arechalde Pelaz"],"pdf_url":"https://arxiv.org/pdf/2212.07123v1.pdf","comment":"Accepted in Neurips Deep Reinforcement Learning Workshop 2022:\n  https://openreview.net/forum?id=fGcbpWQIJZV"},{"id":"http://arxiv.org/abs/2212.07118v1","updated":"2022-12-14T09:12:30Z","published":"2022-12-14T09:12:30Z","title":"Uncertainty Quantification for Deep Neural Networks: An Empirical\n  Comparison and Usage Guidelines","summary":"  Deep Neural Networks (DNN) are increasingly used as components of larger\nsoftware systems that need to process complex data, such as images, written\ntexts, audio/video signals. DNN predictions cannot be assumed to be always\ncorrect for several reasons, among which the huge input space that is dealt\nwith, the ambiguity of some inputs data, as well as the intrinsic properties of\nlearning algorithms, which can provide only statistical warranties. Hence,\ndevelopers have to cope with some residual error probability. An architectural\npattern commonly adopted to manage failure-prone components is the supervisor,\nan additional component that can estimate the reliability of the predictions\nmade by untrusted (e.g., DNN) components and can activate an automated healing\nprocedure when these are likely to fail, ensuring that the Deep Learning based\nSystem (DLS) does not cause damages, despite its main functionality being\nsuspended.\n  In this paper, we consider DLS that implement a supervisor by means of\nuncertainty estimation. After overviewing the main approaches to uncertainty\nestimation and discussing their pros and cons, we motivate the need for a\nspecific empirical assessment method that can deal with the experimental\nsetting in which supervisors are used, where the accuracy of the DNN matters\nonly as long as the supervisor lets the DLS continue to operate. Then we\npresent a large empirical study conducted to compare the alternative approaches\nto uncertainty estimation. We distilled a set of guidelines for developers that\nare useful to incorporate a supervisor based on uncertainty monitoring into a\nDLS.\n","authors":["Michael Weiss","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2212.07118v1.pdf","comment":"Accepted for publication at the Journal of Software: Testing,\n  Verification and Reliability. arXiv admin note: substantial text overlap with\n  arXiv:2102.00902"},{"id":"http://arxiv.org/abs/2010.01823v3","updated":"2022-12-14T09:08:49Z","published":"2020-10-05T07:16:40Z","title":"Quantifying Statistical Significance of Neural Network-based Image\n  Segmentation by Selective Inference","summary":"  Although a vast body of literature relates to image segmentation methods that\nuse deep neural networks (DNNs), less attention has been paid to assessing the\nstatistical reliability of segmentation results. In this study, we interpret\nthe segmentation results as hypotheses driven by DNN (called DNN-driven\nhypotheses) and propose a method by which to quantify the reliability of these\nhypotheses within a statistical hypothesis testing framework. Specifically, we\nconsider a statistical hypothesis test for the difference between the object\nand background regions. This problem is challenging, as the difference would be\nfalsely large because of the adaptation of the DNN to the data. To overcome\nthis difficulty, we introduce a conditional selective inference (SI) framework\n-- a new statistical inference framework for data-driven hypotheses that has\nrecently received considerable attention -- to compute exact (non-asymptotic)\nvalid p-values for the segmentation results. To use the conditional SI\nframework for DNN-based segmentation, we develop a new SI algorithm based on\nthe homotopy method, which enables us to derive the exact (non-asymptotic)\nsampling distribution of DNN-driven hypothesis. We conduct experiments on both\nsynthetic and real-world datasets, through which we offer evidence that our\nproposed method can successfully control the false positive rate, has good\nperformance in terms of computational efficiency, and provides good results\nwhen applied to medical image data.\n","authors":["Vo Nguyen Le Duy","Shogo Iwazaki","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2010.01823v3.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07103v1","updated":"2022-12-14T08:49:02Z","published":"2022-12-14T08:49:02Z","title":"Simplification of Forest Classifiers and Regressors","summary":"  We study the problem of sharing as many branching conditions of a given\nforest classifier or regressor as possible while keeping classification\nperformance. As a constraint for preventing from accuracy degradation, we first\nconsider the one that the decision paths of all the given feature vectors must\nnot change. For a branching condition that a value of a certain feature is at\nmost a given threshold, the set of values satisfying such constraint can be\nrepresented as an interval. Thus, the problem is reduced to the problem of\nfinding the minimum set intersecting all the constraint-satisfying intervals\nfor each set of branching conditions on the same feature. We propose an\nalgorithm for the original problem using an algorithm solving this problem\nefficiently. The constraint is relaxed later to promote further sharing of\nbranching conditions by allowing decision path change of a certain ratio of the\ngiven feature vectors or allowing a certain number of non-intersected\nconstraint-satisfying intervals. We also extended our algorithm for both the\nrelaxations. The effectiveness of our method is demonstrated through\ncomprehensive experiments using 21 datasets (13 classification and 8 regression\ndatasets in UCI machine learning repository) and 4 classifiers/regressors\n(random forest, extremely randomized trees, AdaBoost and gradient boosting).\n","authors":["Atsuyoshi Nakamura","Kento Sakurada"],"pdf_url":"https://arxiv.org/pdf/2212.07103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07101v1","updated":"2022-12-14T08:46:46Z","published":"2022-12-14T08:46:46Z","title":"Domain Generalization by Learning and Removing Domain-specific Features","summary":"  Deep Neural Networks (DNNs) suffer from domain shift when the test dataset\nfollows a distribution different from the training dataset. Domain\ngeneralization aims to tackle this issue by learning a model that can\ngeneralize to unseen domains. In this paper, we propose a new approach that\naims to explicitly remove domain-specific features for domain generalization.\nFollowing this approach, we propose a novel framework called Learning and\nRemoving Domain-specific features for Generalization (LRDG) that learns a\ndomain-invariant model by tactically removing domain-specific features from the\ninput images. Specifically, we design a classifier to effectively learn the\ndomain-specific features for each source domain, respectively. We then develop\nan encoder-decoder network to map each input image into a new image space where\nthe learned domain-specific features are removed. With the images output by the\nencoder-decoder network, another classifier is designed to learn the\ndomain-invariant features to conduct image classification. Extensive\nexperiments demonstrate that our framework achieves superior performance\ncompared with state-of-the-art methods.\n","authors":["Yu Ding","Lei Wang","Bin Liang","Shuming Liang","Yang Wang","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07101v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2204.01387v2","updated":"2022-12-14T08:45:26Z","published":"2022-04-04T11:08:21Z","title":"Anti-Spoofing Using Transfer Learning with Variational Information\n  Bottleneck","summary":"  Recent advances in sophisticated synthetic speech generated from\ntext-to-speech (TTS) or voice conversion (VC) systems cause threats to the\nexisting automatic speaker verification (ASV) systems. Since such synthetic\nspeech is generated from diverse algorithms, generalization ability with using\nlimited training data is indispensable for a robust anti-spoofing system. In\nthis work, we propose a transfer learning scheme based on the wav2vec 2.0\npretrained model with variational information bottleneck (VIB) for speech\nanti-spoofing task. Evaluation on the ASVspoof 2019 logical access (LA)\ndatabase shows that our method improves the performance of distinguishing\nunseen spoofed and genuine speech, outperforming current state-of-the-art\nanti-spoofing systems. Furthermore, we show that the proposed system improves\nperformance in low-resource and cross-dataset settings of anti-spoofing task\nsignificantly, demonstrating that our system is also robust in terms of data\nsize and data distribution.\n","authors":["Youngsik Eom","Yeonghyeon Lee","Ji Sub Um","Hoirin Kim"],"pdf_url":"https://arxiv.org/pdf/2204.01387v2.pdf","comment":"Accepted to Interspeech 2022"},{"id":"http://arxiv.org/abs/2211.09008v2","updated":"2022-12-14T08:40:56Z","published":"2022-11-15T18:56:55Z","title":"Normalizing Flows for Hierarchical Bayesian Analysis: A Gravitational\n  Wave Population Study","summary":"  We propose parameterizing the population distribution of the gravitational\nwave population modeling framework (Hierarchical Bayesian Analysis) with a\nnormalizing flow. We first demonstrate the merit of this method on illustrative\nexperiments and then analyze four parameters of the latest LIGO/Virgo data\nrelease: primary mass, secondary mass, redshift, and effective spin. Our\nresults show that despite the small and notoriously noisy dataset, the\nposterior predictive distributions (assuming a prior over the parameters of the\nflow) of the observed gravitational wave population recover structure that\nagrees with robust previous phenomenological modeling results while being less\nsusceptible to biases introduced by less-flexible distribution models.\nTherefore, the method forms a promising flexible, reliable replacement for\npopulation inference distributions, even when data is highly noisy.\n","authors":["David Ruhe","Kaze Wong","Miles Cranmer","Patrick Forré"],"pdf_url":"https://arxiv.org/pdf/2211.09008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.05905v3","updated":"2022-12-14T08:24:03Z","published":"2020-11-11T16:50:08Z","title":"ShadowNet: A Secure and Efficient On-device Model Inference System for\n  Convolutional Neural Networks","summary":"  With the increased usage of AI accelerators on mobile and edge devices,\non-device machine learning (ML) is gaining popularity. Thousands of proprietary\nML models are being deployed today on billions of untrusted devices. This\nraises serious security concerns about model privacy. However, protecting model\nprivacy without losing access to the untrusted AI accelerators is a challenging\nproblem. In this paper, we present a novel on-device model inference system,\nShadowNet. ShadowNet protects the model privacy with Trusted Execution\nEnvironment (TEE) while securely outsourcing the heavy linear layers of the\nmodel to the untrusted hardware accelerators. ShadowNet achieves this by\ntransforming the weights of the linear layers before outsourcing them and\nrestoring the results inside the TEE. The non-linear layers are also kept\nsecure inside the TEE. ShadowNet's design ensures efficient transformation of\nthe weights and the subsequent restoration of the results. We build a ShadowNet\nprototype based on TensorFlow Lite and evaluate it on five popular CNNs,\nnamely, MobileNet, ResNet-44, MiniVGG, ResNet-404, and YOLOv4-tiny. Our\nevaluation shows that ShadowNet achieves strong security guarantees with\nreasonable performance, offering a practical solution for secure on-device\nmodel inference.\n","authors":["Zhichuang Sun","Ruimin Sun","Changming Liu","Amrita Roy Chowdhury","Long Lu","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2011.05905v3.pdf","comment":"17 pages, 12 figures, IEEE Security & Privacy, Oakland'23"},{"id":"http://arxiv.org/abs/2108.01644v2","updated":"2022-12-14T08:13:21Z","published":"2021-08-03T17:33:38Z","title":"The Devil is in the GAN: Backdoor Attacks and Defenses in Deep\n  Generative Models","summary":"  Deep Generative Models (DGMs) are a popular class of deep learning models\nwhich find widespread use because of their ability to synthesize data from\ncomplex, high-dimensional manifolds. However, even with their increasing\nindustrial adoption, they haven't been subject to rigorous security and privacy\nanalysis. In this work we examine one such aspect, namely backdoor attacks on\nDGMs which can significantly limit the applicability of pre-trained models\nwithin a model supply chain and at the very least cause massive reputation\ndamage for companies outsourcing DGMs form third parties.\n  While similar attacks scenarios have been studied in the context of classical\nprediction models, their manifestation in DGMs hasn't received the same\nattention. To this end we propose novel training-time attacks which result in\ncorrupted DGMs that synthesize regular data under normal operations and\ndesignated target outputs for inputs sampled from a trigger distribution. These\nattacks are based on an adversarial loss function that combines the dual\nobjectives of attack stealth and fidelity. We systematically analyze these\nattacks, and show their effectiveness for a variety of approaches like\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nwell as different data domains including images and audio. Our experiments show\nthat - even for large-scale industry-grade DGMs (like StyleGAN) - our attacks\ncan be mounted with only modest computational effort. We also motivate suitable\ndefenses based on static/dynamic model and output inspections, demonstrate\ntheir usefulness, and prescribe a practical and comprehensive defense strategy\nthat paves the way for safe usage of DGMs.\n","authors":["Ambrish Rawat","Killian Levacher","Mathieu Sinn"],"pdf_url":"https://arxiv.org/pdf/2108.01644v2.pdf","comment":"17 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2211.03827v2","updated":"2022-12-14T08:05:41Z","published":"2022-11-07T19:23:51Z","title":"Lower Bounds for the Convergence of Tensor Power Iteration on Random\n  Overcomplete Models","summary":"  Tensor decomposition serves as a powerful primitive in statistics and machine\nlearning. In this paper, we focus on using power iteration to decompose an\novercomplete random tensor. Past work studying the properties of tensor power\niteration either requires a non-trivial data-independent initialization, or is\nrestricted to the undercomplete regime. Moreover, several papers implicitly\nsuggest that logarithmically many iterations (in terms of the input dimension)\nare sufficient for the power method to recover one of the tensor components. In\nthis paper, we analyze the dynamics of tensor power iteration from random\ninitialization in the overcomplete regime. Surprisingly, we show that\npolynomially many steps are necessary for convergence of tensor power iteration\nto any of the true component, which refutes the previous conjecture. On the\nother hand, our numerical experiments suggest that tensor power iteration\nsuccessfully recovers tensor components for a broad range of parameters,\ndespite that it takes at least polynomially many steps to converge. To further\ncomplement our empirical evidence, we prove that a popular objective function\nfor tensor decomposition is strictly increasing along the power iteration path.\nOur proof is based on the Gaussian conditioning technique, which has been\napplied to analyze the approximate message passing (AMP) algorithm. The major\ningredient of our argument is a conditioning lemma that allows us to generalize\nAMP-type analysis to non-proportional limit and polynomially many iterations of\nthe power method.\n","authors":["Yuchen Wu","Kangjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2211.03827v2.pdf","comment":"40 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.07072v1","updated":"2022-12-14T07:48:42Z","published":"2022-12-14T07:48:42Z","title":"SMSMix: Sense-Maintained Sentence Mixup for Word Sense Disambiguation","summary":"  Word Sense Disambiguation (WSD) is an NLP task aimed at determining the\ncorrect sense of a word in a sentence from discrete sense choices. Although\ncurrent systems have attained unprecedented performances for such tasks, the\nnonuniform distribution of word senses during training generally results in\nsystems performing poorly on rare senses. To this end, we consider data\naugmentation to increase the frequency of these least frequent senses (LFS) to\nreduce the distributional bias of senses during training. We propose\nSense-Maintained Sentence Mixup (SMSMix), a novel word-level mixup method that\nmaintains the sense of a target word. SMSMix smoothly blends two sentences\nusing mask prediction while preserving the relevant span determined by saliency\nscores to maintain a specific word's sense. To the best of our knowledge, this\nis the first attempt to apply mixup in NLP while preserving the meaning of a\nspecific word. With extensive experiments, we validate that our augmentation\nmethod can effectively give more information about rare senses during training\nwith maintained target sense label.\n","authors":["Hee Suk Yoon","Eunseop Yoon","John Harvill","Sunjae Yoon","Mark Hasegawa-Johnson","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2212.07072v1.pdf","comment":"Accepted to Findings of EMNLP2022"},{"id":"http://arxiv.org/abs/2212.07070v1","updated":"2022-12-14T07:35:20Z","published":"2022-12-14T07:35:20Z","title":"Deep Negative Correlation Classification","summary":"  Ensemble learning serves as a straightforward way to improve the performance\nof almost any machine learning algorithm. Existing deep ensemble methods\nusually naively train many different models and then aggregate their\npredictions. This is not optimal in our view from two aspects: i) Naively\ntraining multiple models adds much more computational burden, especially in the\ndeep learning era; ii) Purely optimizing each base model without considering\ntheir interactions limits the diversity of ensemble and performance gains. We\ntackle these issues by proposing deep negative correlation classification\n(DNCC), in which the accuracy and diversity trade-off is systematically\ncontrolled by decomposing the loss function seamlessly into individual accuracy\nand the correlation between individual models and the ensemble. DNCC yields a\ndeep classification ensemble where the individual estimator is both accurate\nand negatively correlated. Thanks to the optimized diversities, DNCC works well\neven when utilizing a shared network backbone, which significantly improves its\nefficiency when compared with most existing ensemble systems. Extensive\nexperiments on multiple benchmark datasets and network structures demonstrate\nthe superiority of the proposed method.\n","authors":["Le Zhang","Qibin Hou","Yun Liu","Jia-Wang Bian","Xun Xu","Joey Tianyi Zhou","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.07070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07069v1","updated":"2022-12-14T07:35:11Z","published":"2022-12-14T07:35:11Z","title":"AI-enabled exploration of Instagram profiles predicts soft skills and\n  personality traits to empower hiring decisions","summary":"  It does not matter whether it is a job interview with Tech Giants, Wall\nStreet firms, or a small startup; all candidates want to demonstrate their best\nselves or even present themselves better than they really are. Meanwhile,\nrecruiters want to know the candidates' authentic selves and detect soft skills\nthat prove an expert candidate would be a great fit in any company. Recruiters\nworldwide usually struggle to find employees with the highest level of these\nskills. Digital footprints can assist recruiters in this process by providing\ncandidates' unique set of online activities, while social media delivers one of\nthe largest digital footprints to track people. In this study, for the first\ntime, we show that a wide range of behavioral competencies consisting of 16\nin-demand soft skills can be automatically predicted from Instagram profiles\nbased on the following lists and other quantitative features using machine\nlearning algorithms. We also provide predictions on Big Five personality\ntraits. Models were built based on a sample of 400 Iranian volunteer users who\nanswered an online questionnaire and provided their Instagram usernames which\nallowed us to crawl the public profiles. We applied several machine learning\nalgorithms to the uniformed data. Deep learning models mostly outperformed by\ndemonstrating 70% and 69% average Accuracy in two-level and three-level\nclassifications respectively. Creating a large pool of people with the highest\nlevel of soft skills, and making more accurate evaluations of job candidates is\npossible with the application of AI on social media user-generated data.\n","authors":["Mercedeh Harirchian","Fereshteh Amin","Saeed Rouhani","Aref Aligholipour","Vahid Amiri Lord"],"pdf_url":"https://arxiv.org/pdf/2212.07069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.09590v2","updated":"2022-12-14T07:21:37Z","published":"2020-06-17T01:23:00Z","title":"Deep Learning with Functional Inputs","summary":"  We present a methodology for integrating functional data into deep densely\nconnected feed-forward neural networks. The model is defined for scalar\nresponses with multiple functional and scalar covariates. A by-product of the\nmethod is a set of dynamic functional weights that can be visualized during the\noptimization process. This visualization leads to greater interpretability of\nthe relationship between the covariates and the response relative to\nconventional neural networks. The model is shown to perform well in a number of\ncontexts including prediction of new data and recovery of the true underlying\nfunctional weights; these results were confirmed through real applications and\nsimulation studies. A forthcoming R package is developed on top of a popular\ndeep learning library (Keras) allowing for general use of the approach.\n","authors":["Barinder Thind","Kevin Multani","Jiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2006.09590v2.pdf","comment":"28 pages, 6 figures, accepted & published by JCGS; please note that\n  the version here is outdated -- look for the updated version using the DOI"},{"id":"http://arxiv.org/abs/2211.01214v4","updated":"2022-12-14T07:16:31Z","published":"2022-11-02T15:55:46Z","title":"Time-aware Random Walk Diffusion to Improve Dynamic Graph Learning","summary":"  How can we augment a dynamic graph for improving the performance of dynamic\ngraph neural networks? Graph augmentation has been widely utilized to boost the\nlearning performance of GNN-based models. However, most existing approaches\nonly enhance spatial structure within an input static graph by transforming the\ngraph, and do not consider dynamics caused by time such as temporal locality,\ni.e., recent edges are more influential than earlier ones, which remains\nchallenging for dynamic graph augmentation. In this work, we propose TiaRa\n(Time-aware Random Walk Diffusion), a novel diffusion-based method for\naugmenting a dynamic graph represented as a discrete-time sequence of graph\nsnapshots. For this purpose, we first design a time-aware random walk proximity\nso that a surfer can walk along the time dimension as well as edges, resulting\nin spatially and temporally localized scores. We then derive our diffusion\nmatrices based on the time-aware random walk, and show they become enhanced\nadjacency matrices that both spatial and temporal localities are augmented.\nThroughout extensive experiments, we demonstrate that TiaRa effectively\naugments a given dynamic graph, and leads to significant improvements in\ndynamic GNN models for various graph datasets and tasks.\n","authors":["Jong-whi Lee","Jinhong Jung"],"pdf_url":"https://arxiv.org/pdf/2211.01214v4.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2008.06595v3","updated":"2022-12-14T07:03:49Z","published":"2020-08-14T22:44:26Z","title":"Decision-making at Unsignalized Intersection for Autonomous Vehicles:\n  Left-turn Maneuver with Deep Reinforcement Learning","summary":"  Decision-making module enables autonomous vehicles to reach appropriate\nmaneuvers in the complex urban environments, especially the intersection\nsituations. This work proposes a deep reinforcement learning (DRL) based\nleft-turn decision-making framework at unsignalized intersection for autonomous\nvehicles. The objective of the studied automated vehicle is to make an\nefficient and safe left-turn maneuver at a four-way unsignalized intersection.\nThe exploited DRL methods include deep Q-learning (DQL) and double DQL.\nSimulation results indicate that the presented decision-making strategy could\nefficaciously reduce the collision rate and improve transport efficiency. This\nwork also reveals that the constructed left-turn control structure has a great\npotential to be applied in real-time.\n","authors":["Feng Wang","Dongjie Shi","Teng Liu","Xiaolin Tang"],"pdf_url":"https://arxiv.org/pdf/2008.06595v3.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2212.07058v1","updated":"2022-12-14T07:00:31Z","published":"2022-12-14T07:00:31Z","title":"Explainable Artificial Intelligence in Retinal Imaging for the detection\n  of Systemic Diseases","summary":"  Explainable Artificial Intelligence (AI) in the form of an interpretable and\nsemiautomatic approach to stage grading ocular pathologies such as Diabetic\nretinopathy, Hypertensive retinopathy, and other retinopathies on the backdrop\nof major systemic diseases. The experimental study aims to evaluate an\nexplainable staged grading process without using deep Convolutional Neural\nNetworks (CNNs) directly. Many current CNN-based deep neural networks used for\ndiagnosing retinal disorders might have appreciable performance but fail to\npinpoint the basis driving their decisions. To improve these decisions'\ntransparency, we have proposed a clinician-in-the-loop assisted intelligent\nworkflow that performs a retinal vascular assessment on the fundus images to\nderive quantifiable and descriptive parameters. The retinal vessel parameters\nmeta-data serve as hyper-parameters for better interpretation and\nexplainability of decisions. The semiautomatic methodology aims to have a\nfederated approach to AI in healthcare applications with more inputs and\ninterpretations from clinicians. The baseline process involved in the machine\nlearning pipeline through image processing techniques for optic disc detection,\nvessel segmentation, and arteriole/venule identification.\n","authors":["Ayushi Raj Bhatt","Rajkumar Vaghashiya","Meghna Kulkarni","Dr Prakash Kamaraj"],"pdf_url":"https://arxiv.org/pdf/2212.07058v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2212.07056v1","updated":"2022-12-14T06:55:32Z","published":"2022-12-14T06:55:32Z","title":"On the Probability of Necessity and Sufficiency of Explaining Graph\n  Neural Networks: A Lower Bound Optimization Approach","summary":"  Explainability of Graph Neural Networks (GNNs) is critical to various GNN\napplications but remains an open challenge. A convincing explanation should be\nboth necessary and sufficient simultaneously. However, existing GNN explaining\napproaches focus on only one of the two aspects, necessity or sufficiency, or a\ntrade-off between the two. To search for the most necessary and sufficient\nexplanation, the Probability of Necessity and Sufficiency (PNS) can be applied\nsince it can mathematically quantify the necessity and sufficiency of an\nexplanation. Nevertheless, the difficulty of obtaining PNS due to\nnon-monotonicity and the challenge of counterfactual estimation limits its wide\nuse. To address the non-identifiability of PNS, we resort to a lower bound of\nPNS that can be optimized via counterfactual estimation, and propose Necessary\nand Sufficient Explanation for GNN (NSEG) via optimizing that lower bound.\nSpecifically, we employ nearest neighbor matching to generate counterfactual\nsamples for the features, which is different from the random perturbation. In\nparticular, NSEG combines the edges and node features to generate an\nexplanation, where the common edge explanation is a special case of the\ncombined explanation. Empirical study shows that NSEG achieves excellent\nperformance in generating the most necessary and sufficient explanations among\na series of state-of-the-art methods.\n","authors":["Ruichu Cai","Yuxuan Zhu","Xuexin Chen","Yuan Fang","Min Wu","Jie Qiao","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2212.07056v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2202.10887v4","updated":"2022-12-14T06:44:53Z","published":"2022-02-22T13:38:14Z","title":"Policy Evaluation for Temporal and/or Spatial Dependent Experiments in\n  Ride-sourcing Platforms","summary":"  The aim of this paper is to establish causal relationship between\nride-sharing platform's policies and outcomes of interest under complex\ntemporal and/or spatial dependent experiments. We propose a\ntemporal/spatio-temporal varying coefficient decision process (VCDP) model to\ncapture the dynamic treatment effects in temporal/spatio-temporal dependent\nexperiments. We characterize the average treatment effect by decomposing it as\nthe sum of direct effect (DE) and indirect effect (IE) and develop estimation\nand inference procedures for both DE and IE. We also establish the statistical\nproperties (e.g., weak convergence and asymptotic power) of our models. We\nconduct extensive simulations and real data analyses to verify the usefulness\nof the proposed method.\n","authors":["Shikai Luo","Ying Yang","Chengchun Shi","Fang Yao","Jieping Ye","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2202.10887v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01432v3","updated":"2022-12-14T06:07:21Z","published":"2022-05-03T11:47:36Z","title":"ARCADE: Adversarially Regularized Convolutional Autoencoder for Network\n  Anomaly Detection","summary":"  As the number of heterogenous IP-connected devices and traffic volume\nincrease, so does the potential for security breaches. The undetected\nexploitation of these breaches can bring severe cybersecurity and privacy\nrisks. Anomaly-based \\acp{IDS} play an essential role in network security. In\nthis paper, we present a practical unsupervised anomaly-based deep learning\ndetection system called ARCADE (Adversarially Regularized Convolutional\nAutoencoder for unsupervised network anomaly DEtection). With a convolutional\n\\ac{AE}, ARCADE automatically builds a profile of the normal traffic using a\nsubset of raw bytes of a few initial packets of network flows so that potential\nnetwork anomalies and intrusions can be efficiently detected before they cause\nmore damage to the network. ARCADE is trained exclusively on normal traffic. An\nadversarial training strategy is proposed to regularize and decrease the\n\\ac{AE}'s capabilities to reconstruct network flows that are out-of-the-normal\ndistribution, thereby improving its anomaly detection capabilities. The\nproposed approach is more effective than state-of-the-art deep learning\napproaches for network anomaly detection. Even when examining only two initial\npackets of a network flow, ARCADE can effectively detect malware infection and\nnetwork attacks. ARCADE presents 20 times fewer parameters than baselines,\nachieving significantly faster detection speed and reaction time.\n","authors":["Willian T. Lunardi","Martin Andreoni Lopez","Jean-Pierre Giacalone"],"pdf_url":"https://arxiv.org/pdf/2205.01432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07050v1","updated":"2022-12-14T06:04:18Z","published":"2022-12-14T06:04:18Z","title":"Significantly improving zero-shot X-ray pathology classification via\n  fine-tuning pre-trained image-text encoders","summary":"  Deep neural networks have been successfully adopted to diverse domains\nincluding pathology classification based on medical images. However,\nlarge-scale and high-quality data to train powerful neural networks are rare in\nthe medical domain as the labeling must be done by qualified experts.\nResearchers recently tackled this problem with some success by taking advantage\nof models pre-trained on large-scale general domain data. Specifically,\nresearchers took contrastive image-text encoders (e.g., CLIP) and fine-tuned it\nwith chest X-ray images and paired reports to perform zero-shot pathology\nclassification, thus completely removing the need for pathology-annotated\nimages to train a classification model. Existing studies, however, fine-tuned\nthe pre-trained model with the same contrastive learning objective, and failed\nto exploit the multi-labeled nature of medical image-report pairs. In this\npaper, we propose a new fine-tuning strategy based on sentence sampling and\npositive-pair loss relaxation for improving the downstream zero-shot pathology\nclassification performance, which can be applied to any pre-trained contrastive\nimage-text encoders. Our method consistently showed dramatically improved\nzero-shot pathology classification performance on four different chest X-ray\ndatasets and 3 different pre-trained models (5.77% average AUROC increase). In\nparticular, fine-tuning CLIP with our method showed much comparable or\nmarginally outperformed to board-certified radiologists (0.619 vs 0.625 in F1\nscore and 0.530 vs 0.544 in MCC) in zero-shot classification of five prominent\ndiseases from the CheXpert dataset.\n","authors":["Jongseong Jang","Daeun Kyung","Seung Hwan Kim","Honglak Lee","Kyunghoon Bae","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2212.07050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07043v1","updated":"2022-12-14T05:36:18Z","published":"2022-12-14T05:36:18Z","title":"AsPOS: Assamese Part of Speech Tagger using Deep Learning Approach","summary":"  Part of Speech (POS) tagging is crucial to Natural Language Processing (NLP).\nIt is a well-studied topic in several resource-rich languages. However, the\ndevelopment of computational linguistic resources is still in its infancy\ndespite the existence of numerous languages that are historically and literary\nrich. Assamese, an Indian scheduled language, spoken by more than 25 million\npeople, falls under this category. In this paper, we present a Deep Learning\n(DL)-based POS tagger for Assamese. The development process is divided into two\nstages. In the first phase, several pre-trained word embeddings are employed to\ntrain several tagging models. This allows us to evaluate the performance of the\nword embeddings in the POS tagging task. The top-performing model from the\nfirst phase is employed to annotate another set of new sentences. In the second\nphase, the model is trained further using the fresh dataset. Finally, we attain\na tagging accuracy of 86.52% in F1 score. The model may serve as a baseline for\nfurther study on DL-based Assamese POS tagging.\n","authors":["Dhrubajyoti Pathak","Sukumar Nandi","Priyankoo Sarmah"],"pdf_url":"https://arxiv.org/pdf/2212.07043v1.pdf","comment":"Accepted in AICCSA 2022"},{"id":"http://arxiv.org/abs/2206.04734v3","updated":"2022-12-14T05:09:28Z","published":"2022-06-09T19:14:52Z","title":"Fast Bayesian Inference with Batch Bayesian Quadrature via Kernel\n  Recombination","summary":"  Calculation of Bayesian posteriors and model evidences typically requires\nnumerical integration. Bayesian quadrature (BQ), a surrogate-model-based\napproach to numerical integration, is capable of superb sample efficiency, but\nits lack of parallelisation has hindered its practical applications. In this\nwork, we propose a parallelised (batch) BQ method, employing techniques from\nkernel quadrature, that possesses an empirically exponential convergence rate.\nAdditionally, just as with Nested Sampling, our method permits simultaneous\ninference of both posteriors and model evidence. Samples from our BQ surrogate\nmodel are re-selected to give a sparse set of samples, via a kernel\nrecombination algorithm, requiring negligible additional time to increase the\nbatch size. Empirically, we find that our approach significantly outperforms\nthe sampling efficiency of both state-of-the-art BQ techniques and Nested\nSampling in various real-world datasets, including lithium-ion battery\nanalytics.\n","authors":["Masaki Adachi","Satoshi Hayakawa","Martin Jørgensen","Harald Oberhauser","Michael A. Osborne"],"pdf_url":"https://arxiv.org/pdf/2206.04734v3.pdf","comment":"38 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.07035v1","updated":"2022-12-14T05:04:10Z","published":"2022-12-14T05:04:10Z","title":"MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning","summary":"  Contrastive learning (CL), which can extract the information shared between\ndifferent contrastive views, has become a popular paradigm for vision\nrepresentation learning. Inspired by the success in computer vision, recent\nwork introduces CL into graph modeling, dubbed as graph contrastive learning\n(GCL). However, generating contrastive views in graphs is more challenging than\nthat in images, since we have little prior knowledge on how to significantly\naugment a graph without changing its labels. We argue that typical data\naugmentation techniques (e.g., edge dropping) in GCL cannot generate diverse\nenough contrastive views to filter out noises. Moreover, previous GCL methods\nemploy two view encoders with exactly the same neural architecture and tied\nparameters, which further harms the diversity of augmented views. To address\nthis limitation, we propose a novel paradigm named model augmented GCL\n(MA-GCL), which will focus on manipulating the architectures of view encoders\ninstead of perturbing graph inputs. Specifically, we present three\neasy-to-implement model augmentation tricks for GCL, namely asymmetric, random\nand shuffling, which can respectively help alleviate high- frequency noises,\nenrich training instances and bring safer augmentations. All three tricks are\ncompatible with typical data augmentations. Experimental results show that\nMA-GCL can achieve state-of-the-art performance on node classification\nbenchmarks by applying the three tricks on a simple base model. Extensive\nstudies also validate our motivation and the effectiveness of each trick.\n(Code, data and appendix are available at https://github.com/GXM1141/MA-GCL. )\n","authors":["Xumeng Gong","Cheng Yang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2212.07035v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/1901.00109v4","updated":"2022-12-14T04:48:24Z","published":"2019-01-01T07:52:24Z","title":"Morphological Network: How Far Can We Go with Morphological Neurons?","summary":"  Morphological neurons, that is morphological operators such as dilation and\nerosion with learnable structuring elements, have intrigued researchers for\nquite some time because of the power these operators bring to the table despite\ntheir simplicity. These operators are known to be powerful nonlinear tools, but\nfor a given problem coming up with a sequence of operations and their\nstructuring element is a non-trivial task. So, the existing works have mainly\nfocused on this part of the problem without delving deep into their\napplicability as generic operators. A few works have tried to utilize\nmorphological neurons as a part of classification (and regression) networks\nwhen the input is a feature vector. However, these methods mainly focus on a\nspecific problem, without going into generic theoretical analysis. In this\nwork, we have theoretically analyzed morphological neurons and have shown that\nthese are far more powerful than previously anticipated. Our proposed\nmorphological block, containing dilation and erosion followed by their linear\ncombination, represents a sum of hinge functions. Existing works show that\nhinge functions perform quite well in classification and regression problems.\nTwo morphological blocks can even approximate any continuous function. However,\nto facilitate the theoretical analysis that we have done in this paper, we have\nrestricted ourselves to the 1D version of the operators, where the structuring\nelement operates on the whole input. Experimental evaluations also indicate the\neffectiveness of networks built with morphological neurons, over similarly\nstructured neural networks.\n","authors":["Ranjan Mondal","Sanchayan Santra","Soumendu Sundar Mukherjee","Bhabatosh Chanda"],"pdf_url":"https://arxiv.org/pdf/1901.00109v4.pdf","comment":"Accepted at BMVC 2022"},{"id":"http://arxiv.org/abs/2212.07026v1","updated":"2022-12-14T04:40:50Z","published":"2022-12-14T04:40:50Z","title":"Improving group robustness under noisy labels using predictive\n  uncertainty","summary":"  The standard empirical risk minimization (ERM) can underperform on certain\nminority groups (i.e., waterbirds in lands or landbirds in water) due to the\nspurious correlation between the input and its label. Several studies have\nimproved the worst-group accuracy by focusing on the high-loss samples. The\nhypothesis behind this is that such high-loss samples are\n\\textit{spurious-cue-free} (SCF) samples. However, these approaches can be\nproblematic since the high-loss samples may also be samples with noisy labels\nin the real-world scenarios. To resolve this issue, we utilize the predictive\nuncertainty of a model to improve the worst-group accuracy under noisy labels.\nTo motivate this, we theoretically show that the high-uncertainty samples are\nthe SCF samples in the binary classification problem. This theoretical result\nimplies that the predictive uncertainty is an adequate indicator to identify\nSCF samples in a noisy label setting. Motivated from this, we propose a novel\nENtropy based Debiasing (END) framework that prevents models from learning the\nspurious cues while being robust to the noisy labels. In the END framework, we\nfirst train the \\textit{identification model} to obtain the SCF samples from a\ntraining set using its predictive uncertainty. Then, another model is trained\non the dataset augmented with an oversampled SCF set. The experimental results\nshow that our END framework outperforms other strong baselines on several\nreal-world benchmarks that consider both the noisy labels and the\nspurious-cues.\n","authors":["Dongpin Oh","Dae Lee","Jeunghyun Byun","Bonggun Shin"],"pdf_url":"https://arxiv.org/pdf/2212.07026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.00115v4","updated":"2022-12-14T04:34:34Z","published":"2021-09-30T22:54:46Z","title":"Comparing Sequential Forecasters","summary":"  Consider two forecasters, each making a single prediction for a sequence of\nevents over time. We ask a relatively basic question: how might we compare\nthese forecasters, either online or post-hoc, while avoiding unverifiable\nassumptions on how the forecasts and outcomes were generated? In this paper, we\npresent a rigorous answer to this question by designing novel sequential\ninference procedures for estimating the time-varying difference in forecast\nscores. To do this, we employ confidence sequences (CS), which are sequences of\nconfidence intervals that can be continuously monitored and are valid at\narbitrary data-dependent stopping times (\"anytime-valid\"). The widths of our\nCSs are adaptive to the underlying variance of the score differences.\nUnderlying their construction is a game-theoretic statistical framework, in\nwhich we further identify e-processes and p-processes for sequentially testing\na weak null hypothesis -- whether one forecaster outperforms another on average\n(rather than always). Our methods do not make distributional assumptions on the\nforecasts or outcomes; our main theorems apply to any bounded scores, and we\nlater provide alternative methods for unbounded scores. We empirically validate\nour approaches by comparing real-world baseball and weather forecasters.\n","authors":["Yo Joong Choe","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2110.00115v4.pdf","comment":"Under revision. Code and data sources available at\n  https://github.com/yjchoe/ComparingForecasters"},{"id":"http://arxiv.org/abs/2212.07013v1","updated":"2022-12-14T04:01:19Z","published":"2022-12-14T04:01:19Z","title":"Learning and Predicting Multimodal Vehicle Action Distributions in a\n  Unified Probabilistic Model Without Labels","summary":"  We present a unified probabilistic model that learns a representative set of\ndiscrete vehicle actions and predicts the probability of each action given a\nparticular scenario. Our model also enables us to estimate the distribution\nover continuous trajectories conditioned on a scenario, representing what each\ndiscrete action would look like if executed in that scenario. While our primary\nobjective is to learn representative action sets, these capabilities combine to\nproduce accurate multimodal trajectory predictions as a byproduct. Although our\nlearned action representations closely resemble semantically meaningful\ncategories (e.g., \"go straight\", \"turn left\", etc.), our method is entirely\nself-supervised and does not utilize any manually generated labels or\ncategories. Our method builds upon recent advances in variational inference and\ndeep unsupervised clustering, resulting in full distribution estimates based on\ndeterministic model evaluations.\n","authors":["Charles Richter","Patrick R. Barragán","Sertac Karaman"],"pdf_url":"https://arxiv.org/pdf/2212.07013v1.pdf","comment":"Presented at the Fresh Perspectives on the Future of Autonomous\n  Driving workshop, ICRA 2022"},{"id":"http://arxiv.org/abs/2103.11435v3","updated":"2022-12-14T03:54:07Z","published":"2021-03-21T16:39:27Z","title":"A deep learning approach to data-driven model-free pricing and to\n  martingale optimal transport","summary":"  We introduce a novel and highly tractable supervised learning approach based\non neural networks that can be applied for the computation of model-free price\nbounds of, potentially high-dimensional, financial derivatives and for the\ndetermination of optimal hedging strategies attaining these bounds. In\nparticular, our methodology allows to train a single neural network offline and\nthen to use it online for the fast determination of model-free price bounds of\na whole class of financial derivatives with current market data. We show the\napplicability of this approach and highlight its accuracy in several examples\ninvolving real market data. Further, we show how a neural network can be\ntrained to solve martingale optimal transport problems involving fixed marginal\ndistributions instead of financial market data.\n","authors":["Ariel Neufeld","Julian Sester"],"pdf_url":"https://arxiv.org/pdf/2103.11435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.09534v3","updated":"2022-12-14T03:51:15Z","published":"2020-06-16T21:53:20Z","title":"Towards improving discriminative reconstruction via simultaneous dense\n  and sparse coding","summary":"  Discriminative features extracted from the sparse coding model have been\nshown to perform well for classification. Recent deep learning architectures\nhave further improved reconstruction in inverse problems by considering new\ndense priors learned from data. We propose a novel dense and sparse coding\nmodel that integrates both representation capability and discriminative\nfeatures. The model studies the problem of recovering a dense vector\n$\\mathbf{x}$ and a sparse vector $\\mathbf{u}$ given measurements of the form\n$\\mathbf{y} = \\mathbf{A}\\mathbf{x}+\\mathbf{B}\\mathbf{u}$. Our first analysis\nproposes a geometric condition based on the minimal angle between spanning\nsubspaces corresponding to the matrices $\\mathbf{A}$ and $\\mathbf{B}$ that\nguarantees unique solution to the model. The second analysis shows that, under\nmild assumptions, a convex program recovers the dense and sparse components. We\nvalidate the effectiveness of the model on simulated data and propose a dense\nand sparse autoencoder (DenSaE) tailored to learning the dictionaries from the\ndense and sparse model. We demonstrate that (i) DenSaE denoises natural images\nbetter than architectures derived from the sparse coding model\n($\\mathbf{B}\\mathbf{u}$), (ii) in the presence of noise, training the biases in\nthe latter amounts to implicitly learning the $\\mathbf{A}\\mathbf{x} +\n\\mathbf{B}\\mathbf{u}$ model, (iii) $\\mathbf{A}$ and $\\mathbf{B}$ capture low-\nand high-frequency contents, respectively, and (iv) compared to the sparse\ncoding model, DenSaE offers a balance between discriminative power and\nrepresentation.\n","authors":["Abiy Tasissa","Emmanouil Theodosis","Bahareh Tolooshams","Demba Ba"],"pdf_url":"https://arxiv.org/pdf/2006.09534v3.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2201.03027v2","updated":"2022-12-14T03:42:11Z","published":"2022-01-09T14:51:34Z","title":"Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly\n  Multimedia Traffic in Graynet","summary":"  Identifying anomaly multimedia traffic in cyberspace is a big challenge in\ndistributed service systems, multiple generation networks and future internet\nof everything. This letter explores meta-generalization for a multiparty\nprivacy learning model in graynet to improve the performance of anomaly\nmultimedia traffic identification. The multiparty privacy learning model in\ngraynet is a globally shared model that is partitioned, distributed and trained\nby exchanging multiparty parameters updates with preserving private data. The\nmeta-generalization refers to discovering the inherent attributes of a learning\nmodel to reduce its generalization error. In experiments, three\nmeta-generalization principles are tested as follows. The generalization error\nof the multiparty privacy learning model in graynet is reduced by changing the\ndimension of byte-level imbedding. Following that, the error is reduced by\nadapting the depth for extracting packet-level features. Finally, the error is\nreduced by adjusting the size of support set for preprocessing traffic-level\ndata. Experimental results demonstrate that the proposal outperforms the\nstate-of-the-art learning models for identifying anomaly multimedia traffic.\n","authors":["Satoshi Nato","Yiqiang Sheng"],"pdf_url":"https://arxiv.org/pdf/2201.03027v2.pdf","comment":"Correct some typos"},{"id":"http://arxiv.org/abs/2212.06998v1","updated":"2022-12-14T03:11:25Z","published":"2022-12-14T03:11:25Z","title":"Safety Correction from Baseline: Towards the Risk-aware Policy in\n  Robotics via Dual-agent Reinforcement Learning","summary":"  Learning a risk-aware policy is essential but rather challenging in\nunstructured robotic tasks. Safe reinforcement learning methods open up new\npossibilities to tackle this problem. However, the conservative policy updates\nmake it intractable to achieve sufficient exploration and desirable performance\nin complex, sample-expensive environments. In this paper, we propose a\ndual-agent safe reinforcement learning strategy consisting of a baseline and a\nsafe agent. Such a decoupled framework enables high flexibility, data\nefficiency and risk-awareness for RL-based control. Concretely, the baseline\nagent is responsible for maximizing rewards under standard RL settings. Thus,\nit is compatible with off-the-shelf training techniques of unconstrained\noptimization, exploration and exploitation. On the other hand, the safe agent\nmimics the baseline agent for policy improvement and learns to fulfill safety\nconstraints via off-policy RL tuning. In contrast to training from scratch,\nsafe policy correction requires significantly fewer interactions to obtain a\nnear-optimal policy. The dual policies can be optimized synchronously via a\nshared replay buffer, or leveraging the pre-trained model or the\nnon-learning-based controller as a fixed baseline agent. Experimental results\nshow that our approach can learn feasible skills without prior knowledge as\nwell as deriving risk-averse counterparts from pre-trained unsafe policies. The\nproposed method outperforms the state-of-the-art safe RL algorithms on\ndifficult robot locomotion and manipulation tasks with respect to both safety\nconstraint satisfaction and sample efficiency.\n","authors":["Linrui Zhang","Zichen Yan","Li Shen","Shoujie Li","Xueqian Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2212.06998v1.pdf","comment":"The 2022 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2022)"},{"id":"http://arxiv.org/abs/2212.06988v1","updated":"2022-12-14T02:50:26Z","published":"2022-12-14T02:50:26Z","title":"Efficient Exploration in Resource-Restricted Reinforcement Learning","summary":"  In many real-world applications of reinforcement learning (RL), performing\nactions requires consuming certain types of resources that are\nnon-replenishable in each episode. Typical applications include robotic control\nwith limited energy and video games with consumable items. In tasks with\nnon-replenishable resources, we observe that popular RL methods such as soft\nactor critic suffer from poor sample efficiency. The major reason is that, they\ntend to exhaust resources fast and thus the subsequent exploration is severely\nrestricted due to the absence of resources. To address this challenge, we first\nformalize the aforementioned problem as a resource-restricted reinforcement\nlearning, and then propose a novel resource-aware exploration bonus (RAEB) to\nmake reasonable usage of resources. An appealing feature of RAEB is that, it\ncan significantly reduce unnecessary resource-consuming trials while\neffectively encouraging the agent to explore unvisited states. Experiments\ndemonstrate that the proposed RAEB significantly outperforms state-of-the-art\nexploration strategies in resource-restricted reinforcement learning\nenvironments, improving the sample efficiency by up to an order of magnitude.\n","authors":["Zhihai Wang","Taoxing Pan","Qi Zhou","Jie Wang"],"pdf_url":"https://arxiv.org/pdf/2212.06988v1.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2212.06140v2","updated":"2022-12-14T02:18:09Z","published":"2022-12-08T23:31:06Z","title":"Fairify: Fairness Verification of Neural Networks","summary":"  Fairness of machine learning (ML) software has become a major concern in the\nrecent past. Although recent research on testing and improving fairness have\ndemonstrated impact on real-world software, providing fairness guarantee in\npractice is still lacking. Certification of ML models is challenging because of\nthe complex decision-making process of the models. In this paper, we proposed\nFairify, an SMT-based approach to verify individual fairness property in neural\nnetwork (NN) models. Individual fairness ensures that any two similar\nindividuals get similar treatment irrespective of their protected attributes\ne.g., race, sex, age. Verifying this fairness property is hard because of the\nglobal checking and non-linear computation nodes in NN. We proposed sound\napproach to make individual fairness verification tractable for the developers.\nThe key idea is that many neurons in the NN always remain inactive when a\nsmaller part of the input domain is considered. So, Fairify leverages whitebox\naccess to the models in production and then apply formal analysis based\npruning. Our approach adopts input partitioning and then prunes the NN for each\npartition to provide fairness certification or counterexample. We leveraged\ninterval arithmetic and activation heuristic of the neurons to perform the\npruning as necessary. We evaluated Fairify on 25 real-world neural networks\ncollected from four different sources, and demonstrated the effectiveness,\nscalability and performance over baseline and closely related work. Fairify is\nalso configurable based on the domain and size of the NN. Our novel formulation\nof the problem can answer targeted verification queries with relaxations and\ncounterexamples, which have practical implications.\n","authors":["Sumon Biswas","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2212.06140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05481v4","updated":"2022-12-14T02:13:16Z","published":"2022-08-10T14:04:20Z","title":"High-Frequency Space Diffusion Models for Accelerated MRI","summary":"  Diffusion models with continuous stochastic differential equations (SDEs)\nhave shown superior performances in image generation. It can be used as a deep\ngenerative prior to solve the inverse problem in MR reconstruction. However,\nthe existing VP-SDE can be treated as maximizing the energy of the MR image to\nbe reconstructed and may lead to SDE sequence divergence. The VE-SDE based MR\nreconstruction is not consistent with actual diffusion process. In addition,\nboth VE- and VP-SDEs-based models suffer from a time-consuming sampling\nprocedure, resulting long reconstruction time. In this study, a new SDE\nfocusing on the diffusion process in high-frequency space is designed\nspecifically for robust MR reconstruction based on diffusion models.\nExperiments on the publicly fastMRI dataset show that HFS-SDE based\nreconstruction method outperforms the parallel imaging, supervised deep\nlearning, and existing VE- and VP-SDEs-based methods in terms of reconstruction\naccuracy. It also improves the stability of MR reconstruction and accelerates\nsampling procedure of reverse diffusion.\n","authors":["Chentao Cao","Zhuo-Xu Cui","Shaonan Liu","Hairong Zheng","Dong Liang","Yanjie Zhu"],"pdf_url":"https://arxiv.org/pdf/2208.05481v4.pdf","comment":"submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2208.09218v2","updated":"2022-12-14T01:57:25Z","published":"2022-08-19T08:43:53Z","title":"Demystifying Randomly Initialized Networks for Evaluating Generative\n  Models","summary":"  Evaluation of generative models is mostly based on the comparison between the\nestimated distribution and the ground truth distribution in a certain feature\nspace. To embed samples into informative features, previous works often use\nconvolutional neural networks optimized for classification, which is criticized\nby recent studies. Therefore, various feature spaces have been explored to\ndiscover alternatives. Among them, a surprising approach is to use a randomly\ninitialized neural network for feature embedding. However, the fundamental\nbasis to employ the random features has not been sufficiently justified. In\nthis paper, we rigorously investigate the feature space of models with random\nweights in comparison to that of trained models. Furthermore, we provide an\nempirical evidence to choose networks for random features to obtain consistent\nand reliable results. Our results indicate that the features from random\nnetworks can evaluate generative models well similarly to those from trained\nnetworks, and furthermore, the two types of features can be used together in a\ncomplementary way.\n","authors":["Junghyuk Lee","Jun-Hyuk Kim","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2208.09218v2.pdf","comment":"Accepted in AAAI 2023"},{"id":"http://arxiv.org/abs/2209.09419v3","updated":"2022-12-14T01:57:07Z","published":"2022-09-20T02:31:42Z","title":"Multi-armed Bandit Learning on a Graph","summary":"  The multi-armed bandit(MAB) problem is a simple yet powerful framework that\nhas been extensively studied in the context of decision-making under\nuncertainty. In many real-world applications, such as robotic applications,\nselecting an arm corresponds to a physical action that constrains the choices\nof the next available arms (actions). Motivated by this, we study an extension\nof MAB called the graph bandit, where an agent travels over a graph to maximize\nthe reward collected from different nodes. The graph defines the agent's\nfreedom in selecting the next available nodes at each step. We assume the graph\nstructure is fully available, but the reward distributions are unknown. Built\nupon an offline graph-based planning algorithm and the principle of optimism,\nwe design a learning algorithm, G-UCB, that balances long-term\nexploration-exploitation using the principle of optimism. We show that our\nproposed algorithm achieves $O(\\sqrt{|S|T\\log(T)}+D|S|\\log T)$ learning regret,\nwhere $|S|$ is the number of nodes and $D$ is the diameter of the graph, which\nmatches the theoretical lower bound $\\Omega(\\sqrt{|S|T})$ up to logarithmic\nfactors. To our knowledge, this result is among the first tight regret bounds\nin non-episodic, un-discounted learning problems with known deterministic\ntransitions. Numerical experiments confirm that our algorithm outperforms\nseveral benchmarks.\n","authors":["Tianpeng Zhang","Kasper Johansson","Na Li"],"pdf_url":"https://arxiv.org/pdf/2209.09419v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06968v1","updated":"2022-12-14T01:21:05Z","published":"2022-12-14T01:21:05Z","title":"Particle-Based Score Estimation for State Space Model Learning in\n  Autonomous Driving","summary":"  Multi-object state estimation is a fundamental problem for robotic\napplications where a robot must interact with other moving objects. Typically,\nother objects' relevant state features are not directly observable, and must\ninstead be inferred from observations. Particle filtering can perform such\ninference given approximate transition and observation models. However, these\nmodels are often unknown a priori, yielding a difficult parameter estimation\nproblem since observations jointly carry transition and observation noise. In\nthis work, we consider learning maximum-likelihood parameters using particle\nmethods. Recent methods addressing this problem typically differentiate through\ntime in a particle filter, which requires workarounds to the non-differentiable\nresampling step, that yield biased or high variance gradient estimates. By\ncontrast, we exploit Fisher's identity to obtain a particle-based approximation\nof the score function (the gradient of the log likelihood) that yields a low\nvariance estimate while only requiring stepwise differentiation through the\ntransition and observation models. We apply our method to real data collected\nfrom autonomous vehicles (AVs) and show that it learns better models than\nexisting techniques and is more stable in training, yielding an effective\nsmoother for tracking the trajectories of vehicles around an AV.\n","authors":["Angad Singh","Omar Makhlouf","Maximilian Igl","Joao Messias","Arnaud Doucet","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2212.06968v1.pdf","comment":"Accepted to CoRL 2022"},{"id":"http://arxiv.org/abs/2212.06967v1","updated":"2022-12-14T01:18:45Z","published":"2022-12-14T01:18:45Z","title":"Explaining Agent's Decision-making in a Hierarchical Reinforcement\n  Learning Scenario","summary":"  Reinforcement learning is a machine learning approach based on behavioral\npsychology. It is focused on learning agents that can acquire knowledge and\nlearn to carry out new tasks by interacting with the environment. However, a\nproblem occurs when reinforcement learning is used in critical contexts where\nthe users of the system need to have more information and reliability for the\nactions executed by an agent. In this regard, explainable reinforcement\nlearning seeks to provide to an agent in training with methods in order to\nexplain its behavior in such a way that users with no experience in machine\nlearning could understand the agent's behavior. One of these is the\nmemory-based explainable reinforcement learning method that is used to compute\nprobabilities of success for each state-action pair using an episodic memory.\nIn this work, we propose to make use of the memory-based explainable\nreinforcement learning method in a hierarchical environment composed of\nsub-tasks that need to be first addressed to solve a more complex task. The end\ngoal is to verify if it is possible to provide to the agent the ability to\nexplain its actions in the global task as well as in the sub-tasks. The results\nobtained showed that it is possible to use the memory-based method in\nhierarchical environments with high-level tasks and compute the probabilities\nof success to be used as a basis for explaining the agent's behavior.\n","authors":["Hugo Muñoz","Ernesto Portugal","Angel Ayala","Bruno Fernandes","Francisco Cruz"],"pdf_url":"https://arxiv.org/pdf/2212.06967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06965v1","updated":"2022-12-14T01:15:26Z","published":"2022-12-14T01:15:26Z","title":"Error-Aware B-PINNs: Improving Uncertainty Quantification in Bayesian\n  Physics-Informed Neural Networks","summary":"  Physics-Informed Neural Networks (PINNs) are gaining popularity as a method\nfor solving differential equations. While being more feasible in some contexts\nthan the classical numerical techniques, PINNs still lack credibility. A remedy\nfor that can be found in Uncertainty Quantification (UQ) which is just\nbeginning to emerge in the context of PINNs. Assessing how well the trained\nPINN complies with imposed differential equation is the key to tackling\nuncertainty, yet there is lack of comprehensive methodology for this task. We\npropose a framework for UQ in Bayesian PINNs (B-PINNs) that incorporates the\ndiscrepancy between the B-PINN solution and the unknown true solution. We\nexploit recent results on error bounds for PINNs on linear dynamical systems\nand demonstrate the predictive uncertainty on a class of linear ODEs.\n","authors":["Olga Graf","Pablo Flores","Pavlos Protopapas","Karim Pichara"],"pdf_url":"https://arxiv.org/pdf/2212.06965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12996v2","updated":"2022-12-14T00:22:09Z","published":"2022-11-22T17:46:56Z","title":"Converting OpenStreetMap Data to Road Networks for Downstream\n  Applications","summary":"  We study how to convert OpenStreetMap data to road networks for downstream\napplications. OpenStreetMap data has different formats. Extensible Markup\nLanguage (XML) is one of them. OSM data consist of nodes, ways, and relations.\nWe process OSM XML data to extract the information of nodes and ways to obtain\nthe map of streets of the Memphis area. We can use this map for different\ndownstream applications.\n","authors":["Md Kaisar Ahmed"],"pdf_url":"https://arxiv.org/pdf/2211.12996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06951v1","updated":"2022-12-14T00:04:56Z","published":"2022-12-14T00:04:56Z","title":"AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain\n  Security","summary":"  Blockchain has empowered computer systems to be more secure using a\ndistributed network. However, the current blockchain design suffers from\nfairness issues in transaction ordering. Miners are able to reorder\ntransactions to generate profits, the so-called miner extractable value (MEV).\nExisting research recognizes MEV as a severe security issue and proposes\npotential solutions, including prominent Flashbots. However, previous studies\nhave mostly analyzed blockchain data, which might not capture the impacts of\nMEV in a much broader AI society. Thus, in this research, we applied natural\nlanguage processing (NLP) methods to comprehensively analyze topics in tweets\non MEV. We collected more than 20000 tweets with \\#MEV and \\#Flashbots hashtags\nand analyzed their topics. Our results show that the tweets discussed profound\ntopics of ethical concern, including security, equity, emotional sentiments,\nand the desire for solutions to MEV. We also identify the co-movements of MEV\nactivities on blockchain and social media platforms. Our study contributes to\nthe literature at the interface of blockchain security, MEV solutions, and AI\nethics.\n","authors":["Yihang Fu","Zesen Zhuang","Luyao Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.06951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07555v1","updated":"2022-12-14T23:59:24Z","published":"2022-12-14T23:59:24Z","title":"IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object\n  Interactions","summary":"  Can we make virtual characters in a scene interact with their surrounding\nobjects through simple instructions? Is it possible to synthesize such motion\nplausibly with a diverse set of objects and instructions? Inspired by these\nquestions, we present the first framework to synthesize the full-body motion of\nvirtual human characters performing specified actions with 3D objects placed\nwithin their reach. Our system takes as input textual instructions specifying\nthe objects and the associated intentions of the virtual characters and outputs\ndiverse sequences of full-body motions. This is in contrast to existing work,\nwhere full-body action synthesis methods generally do not consider object\ninteractions, and human-object interaction methods focus mainly on synthesizing\nhand or finger movements for grasping objects. We accomplish our objective by\ndesigning an intent-driven full-body motion generator, which uses a pair of\ndecoupled conditional variational autoencoders (CVAE) to learn the motion of\nthe body parts in an autoregressive manner. We also optimize for the positions\nof the objects with six degrees of freedom (6DoF) such that they plausibly fit\nwithin the hands of the synthesized characters. We compare our proposed method\nwith the existing methods of motion synthesis and establish a new and stronger\nstate-of-the-art for the task of intent-driven motion synthesis. Through a user\nstudy, we further show that our synthesized full-body motions appear more\nrealistic to the participants in more than 80% of scenarios compared to the\ncurrent state-of-the-art methods, and are perceived to be as good as the ground\ntruth on several occasions.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2212.07555v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.07554v1","updated":"2022-12-14T23:57:46Z","published":"2022-12-14T23:57:46Z","title":"Generative structured normalizing flow Gaussian processes applied to\n  spectroscopic data","summary":"  In this work, we propose a novel generative model for mapping inputs to\nstructured, high-dimensional outputs using structured conditional normalizing\nflows and Gaussian process regression. The model is motivated by the need to\ncharacterize uncertainty in the input/output relationship when making\ninferences on new data. In particular, in the physical sciences, limited\ntraining data may not adequately characterize future observed data; it is\ncritical that models adequately indicate uncertainty, particularly when they\nmay be asked to extrapolate. In our proposed model, structured conditional\nnormalizing flows provide parsimonious latent representations that relate to\nthe inputs through a Gaussian process, providing exact likelihood calculations\nand uncertainty that naturally increases away from the training data inputs. We\ndemonstrate the methodology on laser-induced breakdown spectroscopy data from\nthe ChemCam instrument onboard the Mars rover Curiosity. ChemCam was designed\nto recover the chemical composition of rock and soil samples by measuring the\nspectral properties of plasma atomic emissions induced by a laser pulse. We\nshow that our model can generate realistic spectra conditional on a given\nchemical composition and that we can use the model to perform uncertainty\nquantification of chemical compositions for new observed spectra. Based on our\nresults, we anticipate that our proposed modeling approach may be useful in\nother scientific domains with high-dimensional, complex structure where it is\nimportant to quantify predictive uncertainty.\n","authors":["Natalie Klein","Nishant Panda","Patrick Gasda","Diane Oyen"],"pdf_url":"https://arxiv.org/pdf/2212.07554v1.pdf","comment":"Best paper award, 1st Annual AAAI Workshop on AI to Accelerate\n  Science and Engineering (AI2ASE), February 2022"},{"id":"http://arxiv.org/abs/2212.07553v1","updated":"2022-12-14T23:49:53Z","published":"2022-12-14T23:49:53Z","title":"Automated Reachability Analysis of Neural Network-Controlled Systems via\n  Adaptive Polytopes","summary":"  Over-approximating the reachable sets of dynamical systems is a fundamental\nproblem in safety verification and robust control synthesis. The representation\nof these sets is a key factor that affects the computational complexity and the\napproximation error. In this paper, we develop a new approach for\nover-approximating the reachable sets of neural network dynamical systems using\nadaptive template polytopes. We use the singular value decomposition of linear\nlayers along with the shape of the activation functions to adapt the geometry\nof the polytopes at each time step to the geometry of the true reachable sets.\nWe then propose a branch-and-bound method to compute accurate\nover-approximations of the reachable sets by the inferred templates. We\nillustrate the utility of the proposed approach in the reachability analysis of\nlinear systems driven by neural network controllers.\n","authors":["Taha Entesari","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2212.07553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07551v1","updated":"2022-12-14T23:46:23Z","published":"2022-12-14T23:46:23Z","title":"Faster Maximum Inner Product Search in High Dimensions","summary":"  Maximum Inner Product Search (MIPS) is a popular problem in the machine\nlearning literature due to its applicability in a wide array of applications,\nsuch as recommender systems. In high-dimensional settings, however, MIPS\nqueries can become computationally expensive as most existing solutions do not\nscale well with data dimensionality. In this work, we present a\nstate-of-the-art algorithm for the MIPS problem in high dimensions, dubbed\nBanditMIPS. BanditMIPS is a randomized algorithm that borrows techniques from\nmulti-armed bandits to reduce the MIPS problem to a best-arm identification\nproblem. BanditMIPS reduces the complexity of state-of-the-art algorithms from\n$O(\\sqrt{d})$ to $O(\\text{log}d)$, where $d$ is the dimension of the problem\ndata vectors. On high-dimensional real-world datasets, BanditMIPS runs\napproximately 12 times faster than existing approaches and returns the same\nsolution. BanditMIPS requires no preprocessing of the data and includes a\nhyperparameter that practitioners may use to trade off accuracy and runtime. We\nalso propose a variant of our algorithm, named BanditMIPS-$\\alpha$, which\nemploys non-uniform sampling across the data dimensions to provide further\nspeedups.\n","authors":["Mo Tiwari","Ryan Kang","Je-Yong Lee","Luke Lee","Chris Piech","Sebastian Thrun","Ilan Shomorony","Martin Jinye Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.07551v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2212.07549v1","updated":"2022-12-14T23:41:57Z","published":"2022-12-14T23:41:57Z","title":"ReDDIT: Regret Detection and Domain Identification from Text","summary":"  In this paper, we present a study of regret and its expression on social\nmedia platforms. Specifically, we present a novel dataset of Reddit texts that\nhave been classified into three classes: Regret by Action, Regret by Inaction,\nand No Regret. We then use this dataset to investigate the language used to\nexpress regret on Reddit and to identify the domains of text that are most\ncommonly associated with regret. Our findings show that Reddit users are most\nlikely to express regret for past actions, particularly in the domain of\nrelationships. We also found that deep learning models using GloVe embedding\noutperformed other models in all experiments, indicating the effectiveness of\nGloVe for representing the meaning and context of words in the domain of\nregret. Overall, our study provides valuable insights into the nature and\nprevalence of regret on social media, as well as the potential of deep learning\nand word embeddings for analyzing and understanding emotional language in\nonline text. These findings have implications for the development of natural\nlanguage processing algorithms and the design of social media platforms that\nsupport emotional expression and communication.\n","authors":["Fazlourrahman Balouchzahi","Sabur Butt","Grigori Sidorov","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2212.07549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12429v2","updated":"2022-12-14T23:27:16Z","published":"2022-02-24T23:54:12Z","title":"BagPipe: Accelerating Deep Recommendation Model Training","summary":"  Deep learning based recommendation models (DLRM) are widely used in several\nbusiness critical applications. Training such recommendation models efficiently\nis challenging primarily because they consist of billions of embedding-based\nparameters which are often stored remotely leading to significant overheads\nfrom embedding access. By profiling existing DLRM training, we observe that\nonly 8.5% of the iteration time is spent in forward/backward pass while the\nremaining time is spent on embedding and model synchronization. Our key insight\nin this paper is that access to embeddings have a specific structure and\npattern which can be used to accelerate training. We observe that embedding\naccesses are heavily skewed, with almost 1% of embeddings represent more than\n92% of total accesses. Further, we observe that during training we can\nlookahead at future batches to determine exactly which embeddings will be\nneeded at what iteration in the future. Based on these insight, we propose\nBagpipe, a system for training deep recommendation models that uses caching and\nprefetching to overlap remote embedding accesses with the computation. We\ndesigned an Oracle Cacher, a new system component which uses our lookahead\nalgorithm to generate optimal cache update decisions and provide strong\nconsistency guarantees. Our experiments using three datasets and two models\nshows that our approach provides a speed up of up to 6.2x compared to state of\nthe art baselines, while providing the same convergence and reproducibility\nguarantees as synchronous training.\n","authors":["Saurabh Agarwal","Chengpo Yan","Ziyi Zhang","Shivaram Venkataraman"],"pdf_url":"https://arxiv.org/pdf/2202.12429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12638v2","updated":"2022-12-14T22:58:51Z","published":"2022-11-22T23:53:06Z","title":"Projection-free Adaptive Regret with Membership Oracles","summary":"  In the framework of online convex optimization, most iterative algorithms\nrequire the computation of projections onto convex sets, which can be\ncomputationally expensive. To tackle this problem HK12 proposed the study of\nprojection-free methods that replace projections with less expensive\ncomputations. The most common approach is based on the Frank-Wolfe method, that\nuses linear optimization computation in lieu of projections. Recent work by\nGK22 gave sublinear adaptive regret guarantees with projection free algorithms\nbased on the Frank Wolfe approach.\n  In this work we give projection-free algorithms that are based on a different\ntechnique, inspired by Mhammedi22, that replaces projections by set-membership\ncomputations. We propose a simple lazy gradient-based algorithm with a\nMinkowski regularization that attains near-optimal adaptive regret bounds. For\ngeneral convex loss functions we improve previous adaptive regret bounds from\n$O(T^{3/4})$ to $O(\\sqrt{T})$, and further to tight interval dependent bound\n$\\tilde{O}(\\sqrt{I})$ where $I$ denotes the interval length. For strongly\nconvex functions we obtain the first poly-logarithmic adaptive regret bounds\nusing a projection-free algorithm.\n","authors":["Zhou Lu","Nataly Brukhim","Paula Gradu","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2211.12638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.11435v2","updated":"2022-12-14T22:52:52Z","published":"2021-10-21T19:07:04Z","title":"Generating Multivariate Load States Using a Conditional Variational\n  Autoencoder","summary":"  For planning of power systems and for the calibration of operational tools,\nit is essential to analyse system performance in a large range of\nrepresentative scenarios. When the available historical data is limited,\ngenerative models are a promising solution, but modelling high-dimensional\ndependencies is challenging. In this paper, a multivariate load state\ngenerating model on the basis of a conditional variational autoencoder (CVAE)\nneural network is proposed. Going beyond common CVAE implementations, the model\nincludes stochastic variation of output samples under given latent vectors and\nco-optimizes the parameters for this output variability. It is shown that this\nimproves statistical properties of the generated data. The quality of generated\nmultivariate loads is evaluated using univariate and multivariate performance\nmetrics. A generation adequacy case study on the European network is used to\nillustrate model's ability to generate realistic tail distributions. The\nexperiments demonstrate that the proposed generator outperforms other data\ngenerating mechanisms.\n","authors":["Chenguang Wang","Ensieh Sharifnia","Zhi Gao","Simon H. Tindemans","Peter Palensky"],"pdf_url":"https://arxiv.org/pdf/2110.11435v2.pdf","comment":"8 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2212.07536v1","updated":"2022-12-14T22:43:56Z","published":"2022-12-14T22:43:56Z","title":"Robust Policy Optimization in Deep Reinforcement Learning","summary":"  The policy gradient method enjoys the simplicity of the objective where the\nagent optimizes the cumulative reward directly. Moreover, in the continuous\naction domain, parameterized distribution of action distribution allows easy\ncontrol of exploration, resulting from the variance of the representing\ndistribution. Entropy can play an essential role in policy optimization by\nselecting the stochastic policy, which eventually helps better explore the\nenvironment in reinforcement learning (RL). However, the stochasticity often\nreduces as the training progresses; thus, the policy becomes less exploratory.\nAdditionally, certain parametric distributions might only work for some\nenvironments and require extensive hyperparameter tuning. This paper aims to\nmitigate these issues. In particular, we propose an algorithm called Robust\nPolicy Optimization (RPO), which leverages a perturbed distribution. We\nhypothesize that our method encourages high-entropy actions and provides a way\nto represent the action space better. We further provide empirical evidence to\nverify our hypothesis. We evaluated our methods on various continuous control\ntasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed\nthat in many settings, RPO increases the policy entropy early in training and\nthen maintains a certain level of entropy throughout the training period.\nEventually, our agent RPO shows consistently improved performance compared to\nPPO and other techniques: entropy regularization, different distributions, and\ndata augmentation. Furthermore, in several settings, our method stays robust in\nperformance, while other baseline mechanisms fail to improve and even worsen\nthe performance.\n","authors":["Md Masudur Rahman","Yexiang Xue"],"pdf_url":"https://arxiv.org/pdf/2212.07536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07534v1","updated":"2022-12-14T22:36:13Z","published":"2022-12-14T22:36:13Z","title":"Decentralized Nonconvex Optimization with Guaranteed Privacy and\n  Accuracy","summary":"  Privacy protection and nonconvexity are two challenging problems in\ndecentralized optimization and learning involving sensitive data. Despite some\nrecent advances addressing each of the two problems separately, no results have\nbeen reported that have theoretical guarantees on both privacy protection and\nsaddle/maximum avoidance in decentralized nonconvex optimization. We propose a\nnew algorithm for decentralized nonconvex optimization that can enable both\nrigorous differential privacy and saddle/maximum avoiding performance. The new\nalgorithm allows the incorporation of persistent additive noise to enable\nrigorous differential privacy for data samples, gradients, and intermediate\noptimization variables without losing provable convergence, and thus\ncircumventing the dilemma of trading accuracy for privacy in differential\nprivacy design. More interestingly, the algorithm is theoretically proven to be\nable to efficiently { guarantee accuracy by avoiding} convergence to local\nmaxima and saddle points, which has not been reported before in the literature\non decentralized nonconvex optimization. The algorithm is efficient in both\ncommunication (it only shares one variable in each iteration) and computation\n(it is encryption-free), and hence is promising for large-scale nonconvex\noptimization and learning involving high-dimensional optimization parameters.\nNumerical experiments for both a decentralized estimation problem and an\nIndependent Component Analysis (ICA) problem confirm the effectiveness of the\nproposed approach.\n","authors":["Yongqiang Wang","Tamer Basar"],"pdf_url":"https://arxiv.org/pdf/2212.07534v1.pdf","comment":"Accepted as a full paper to Automatica"},{"id":"http://arxiv.org/abs/2212.07530v1","updated":"2022-12-14T22:30:55Z","published":"2022-12-14T22:30:55Z","title":"Causes and Cures for Interference in Multilingual Translation","summary":"  Multilingual machine translation models can benefit from synergy between\ndifferent language pairs, but also suffer from interference. While there is a\ngrowing number of sophisticated methods that aim to eliminate interference, our\nunderstanding of interference as a phenomenon is still limited. This work\nidentifies the main factors that contribute to interference in multilingual\nmachine translation. Through systematic experimentation, we find that\ninterference (or synergy) are primarily determined by model size, data size,\nand the proportion of each language pair within the total dataset. We observe\nthat substantial interference occurs mainly when the model is very small with\nrespect to the available training data, and that using standard transformer\nconfigurations with less than one billion parameters largely alleviates\ninterference and promotes synergy. Moreover, we show that tuning the sampling\ntemperature to control the proportion of each language pair in the data is key\nto balancing the amount of interference between low and high resource language\npairs effectively, and can lead to superior performance overall.\n","authors":["Uri Shaham","Maha Elbayad","Vedanuj Goswami","Omer Levy","Shruti Bhosale"],"pdf_url":"https://arxiv.org/pdf/2212.07530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07525v1","updated":"2022-12-14T22:13:11Z","published":"2022-12-14T22:13:11Z","title":"Efficient Self-supervised Learning with Contextualized Target\n  Representations for Vision, Speech and Language","summary":"  Current self-supervised learning algorithms are often modality-specific and\nrequire large amounts of computational resources. To address these issues, we\nincrease the training efficiency of data2vec, a learning objective that\ngeneralizes across several modalities. We do not encode masked tokens, use a\nfast convolutional decoder and amortize the effort to build teacher\nrepresentations. data2vec 2.0 benefits from the rich contextualized target\nrepresentations introduced in data2vec which enable a fast self-supervised\nlearner. Experiments on ImageNet-1K image classification show that data2vec 2.0\nmatches the accuracy of Masked Autoencoders in 16.4x lower pre-training time,\non Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x\nless time, and on GLUE natural language understanding it matches a retrained\nRoBERTa model in half the time. Trading some speed for accuracy results in\nImageNet-1K top-1 accuracy of 86.8\\% with a ViT-L model trained for 150 epochs.\n","authors":["Alexei Baevski","Arun Babu","Wei-Ning Hsu","Michael Auli"],"pdf_url":"https://arxiv.org/pdf/2212.07525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07524v1","updated":"2022-12-14T22:12:32Z","published":"2022-12-14T22:12:32Z","title":"Invariant Lipschitz Bandits: A Side Observation Approach","summary":"  Symmetry arises in many optimization and decision-making problems, and has\nattracted considerable attention from the optimization community: By utilizing\nthe existence of such symmetries, the process of searching for optimal\nsolutions can be improved significantly. Despite its success in (offline)\noptimization, the utilization of symmetries has not been well examined within\nthe online optimization settings, especially in the bandit literature. As such,\nin this paper we study the invariant Lipschitz bandit setting, a subclass of\nthe Lipschitz bandits where the reward function and the set of arms are\npreserved under a group of transformations. We introduce an algorithm named\n\\texttt{UniformMesh-N}, which naturally integrates side observations using\ngroup orbits into the \\texttt{UniformMesh} algorithm\n(\\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of\narms. Using the side-observation approach, we prove an improved regret upper\nbound, which depends on the cardinality of the group, given that the group is\nfinite. We also prove a matching regret's lower bound for the invariant\nLipschitz bandit class (up to logarithmic factors). We hope that our work will\nignite further investigation of symmetry in bandit theory and sequential\ndecision-making theory in general.\n","authors":["Nam Phuong Tran","The-Anh Ta","Long Tran-Thanh"],"pdf_url":"https://arxiv.org/pdf/2212.07524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07517v1","updated":"2022-12-14T21:46:33Z","published":"2022-12-14T21:46:33Z","title":"Analytical Engines With Context-Rich Processing: Towards Efficient\n  Next-Generation Analytics","summary":"  As modern data pipelines continue to collect, produce, and store a variety of\ndata formats, extracting and combining value from traditional and context-rich\nsources such as strings, text, video, audio, and logs becomes a manual process\nwhere such formats are unsuitable for RDBMS. To tap into the dark data, domain\nexperts analyze and extract insights and integrate them into the data\nrepositories. This process can involve out-of-DBMS, ad-hoc analysis, and\nprocessing resulting in ETL, engineering effort, and suboptimal performance.\nWhile AI systems based on ML models can automate the analysis process, they\noften further generate context-rich answers. Using multiple sources of truth,\nfor either training the models or in the form of knowledge bases, further\nexacerbates the problem of consolidating the data of interest.\n  We envision an analytical engine co-optimized with components that enable\ncontext-rich analysis. Firstly, as the data from different sources or resulting\nfrom model answering cannot be cleaned ahead of time, we propose using online\ndata integration via model-assisted similarity operations. Secondly, we aim for\na holistic pipeline cost- and rule-based optimization across relational and\nmodel-based operators. Thirdly, with increasingly heterogeneous hardware and\nequally heterogeneous workloads ranging from traditional relational analytics\nto generative model inference, we envision a system that just-in-time adapts to\nthe complex analytical query requirements. To solve increasingly complex\nanalytical problems, ML offers attractive solutions that must be combined with\ntraditional analytical processing and benefit from decades of database\ncommunity research to achieve scalability and performance effortless for the\nend user.\n","authors":["Viktor Sanca","Anastasia Ailamaki"],"pdf_url":"https://arxiv.org/pdf/2212.07517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07514v1","updated":"2022-12-14T21:39:15Z","published":"2022-12-14T21:39:15Z","title":"PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal\n  Imputation","summary":"  The promise of Mobile Health (mHealth) is the ability to use wearable sensors\nto monitor participant physiology at high frequencies during daily life to\nenable temporally-precise health interventions. However, a major challenge is\nfrequent missing data. Despite a rich imputation literature, existing\ntechniques are ineffective for the pulsative signals which comprise many\nmHealth applications, and a lack of available datasets has stymied progress. We\naddress this gap with PulseImpute, the first large-scale pulsative signal\nimputation challenge which includes realistic mHealth missingness models, an\nextensive set of baselines, and clinically-relevant downstream tasks. Our\nbaseline models include a novel transformer-based architecture designed to\nexploit the structure of pulsative signals. We hope that PulseImpute will\nenable the ML community to tackle this significant and challenging task.\n","authors":["Maxwell A. Xu","Alexander Moreno","Supriya Nagesh","V. Burak Aydemir","David W. Wetter","Santosh Kumar","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2212.07514v1.pdf","comment":"Presented at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2204.01851v2","updated":"2022-12-14T21:23:04Z","published":"2022-04-04T21:11:00Z","title":"Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic\n  Representation","summary":"  Spatial audio methods are gaining a growing interest due to the spread of\nimmersive audio experiences and applications, such as virtual and augmented\nreality. For these purposes, 3D audio signals are often acquired through arrays\nof Ambisonics microphones, each comprising four capsules that decompose the\nsound field in spherical harmonics. In this paper, we propose a dual quaternion\nrepresentation of the spatial sound field acquired through an array of two\nFirst Order Ambisonics (FOA) microphones. The audio signals are encapsulated in\na dual quaternion that leverages quaternion algebra properties to exploit\ncorrelations among them. This augmented representation with 6 degrees of\nfreedom (6DOF) involves a more accurate coverage of the sound field, resulting\nin a more precise sound localization and a more immersive audio experience. We\nevaluate our approach on a sound event localization and detection (SELD)\nbenchmark. We show that our dual quaternion SELD model with temporal\nconvolution blocks (DualQSELD-TCN) achieves better results with respect to real\nand quaternion-valued baselines thanks to our augmented representation of the\nsound field. Full code is available at:\nhttps://github.com/ispamm/DualQSELD-TCN.\n","authors":["Eleonora Grassucci","Gioia Mancini","Christian Brignone","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2204.01851v2.pdf","comment":"Paper accepted for publication in Elsevier Pattern Recognition\n  Letters"},{"id":"http://arxiv.org/abs/2206.04632v3","updated":"2022-12-14T21:19:07Z","published":"2022-06-09T17:25:22Z","title":"Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from\n  Demonstrations","summary":"  Learning from demonstration (LfD) has succeeded in tasks featuring a long\ntime horizon. However, when the problem complexity also includes\nhuman-in-the-loop perturbations, state-of-the-art approaches do not guarantee\nthe successful reproduction of a task. In this work, we identify the roots of\nthis challenge as the failure of a learned continuous policy to satisfy the\ndiscrete plan implicit in the demonstration. By utilizing modes (rather than\nsubgoals) as the discrete abstraction and motion policies with both mode\ninvariance and goal reachability properties, we prove our learned continuous\npolicy can simulate any discrete plan specified by a linear temporal logic\n(LTL) formula. Consequently, an imitator is robust to both task- and\nmotion-level perturbations and guaranteed to achieve task success. Project\npage: https://yanweiw.github.io/tli/\n","authors":["Yanwei Wang","Nadia Figueroa","Shen Li","Ankit Shah","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2206.04632v3.pdf","comment":"CoRL 2022 Oral Talk"},{"id":"http://arxiv.org/abs/2212.07508v1","updated":"2022-12-14T21:13:48Z","published":"2022-12-14T21:13:48Z","title":"Tensions Between the Proxies of Human Values in AI","summary":"  Motivated by mitigating potentially harmful impacts of technologies, the AI\ncommunity has formulated and accepted mathematical definitions for certain\npillars of accountability: e.g. privacy, fairness, and model transparency. Yet,\nwe argue this is fundamentally misguided because these definitions are\nimperfect, siloed constructions of the human values they hope to proxy, while\ngiving the guise that those values are sufficiently embedded in our\ntechnologies. Under popularized methods, tensions arise when practitioners\nattempt to achieve each pillar of fairness, privacy, and transparency in\nisolation or simultaneously. In this position paper, we push for redirection.\nWe argue that the AI community needs to consider all the consequences of\nchoosing certain formulations of these pillars -- not just the technical\nincompatibilities, but also the effects within the context of deployment. We\npoint towards sociotechnical research for frameworks for the latter, but push\nfor broader efforts into implementing these in practice.\n","authors":["Teresa Datta","Daniel Nissani","Max Cembalest","Akash Khanna","Haley Massa","John P. Dickerson"],"pdf_url":"https://arxiv.org/pdf/2212.07508v1.pdf","comment":"Contributed Talk, NeurIPS 2022 Workshop on Algorithmic Fairness\n  through the Lens of Causality and Privacy; To be published in 2023 IEEE\n  Conference on Secure and Trustworthy Machine Learning (SaTML)"},{"id":"http://arxiv.org/abs/1910.02519v3","updated":"2022-12-14T21:05:09Z","published":"2019-10-06T20:34:52Z","title":"FIS-GAN: GAN with Flow-based Importance Sampling","summary":"  Generative Adversarial Networks (GAN) training process, in most cases, apply\nUniform or Gaussian sampling methods in the latent space, which probably spends\nmost of the computation on examples that can be properly handled and easy to\ngenerate. Theoretically, importance sampling speeds up stochastic optimization\nin supervised learning by prioritizing training examples. In this paper, we\nexplore the possibility of adapting importance sampling into adversarial\nlearning. We use importance sampling to replace Uniform and Gaussian sampling\nmethods in the latent space and employ normalizing flow to approximate latent\nspace posterior distribution by density estimation. Empirically, results on\nMNIST and Fashion-MNIST demonstrate that our method significantly accelerates\nGAN's optimization while retaining visual fidelity in generated samples.\n","authors":["Shiyu Yi","Donglin Zhan","Wenqing Zhang","Denglin Jiang","Kang An","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/1910.02519v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02704v2","updated":"2022-12-14T20:49:47Z","published":"2022-12-06T01:53:50Z","title":"Benchmarking AutoML algorithms on a collection of synthetic\n  classification problems","summary":"  Automated machine learning (AutoML) algorithms have grown in popularity due\nto their high performance and flexibility to adapt to different problems and\ndata sets. With the increasing number of AutoML algorithms, deciding which\nwould best suit a given problem becomes increasingly more work. Therefore, it\nis essential to use complex and challenging benchmarks which would be able to\ndifferentiate the AutoML algorithms from each other. This paper compares the\nperformance of four different AutoML algorithms: Tree-based Pipeline\nOptimization Tool (TPOT), Auto-Sklearn, Auto-Sklearn 2, and H2O AutoML. We use\nthe Diverse and Generative ML benchmark (DIGEN), a diverse set of synthetic\ndatasets derived from generative functions designed to highlight the strengths\nand weaknesses of the performance of common machine learning algorithms. We\nconfirm that AutoML can identify pipelines that perform well on all included\ndatasets. Most AutoML algorithms performed similarly without much room for\nimprovement; however, some were more consistent than others at finding\nhigh-performing solutions for some datasets.\n","authors":["Pedro Henrique Ribeiro","Patryk Orzechowski","Joost Wagenaar","Jason H. Moore"],"pdf_url":"https://arxiv.org/pdf/2212.02704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07492v1","updated":"2022-12-14T20:23:11Z","published":"2022-12-14T20:23:11Z","title":"Machine Learning Coarse-Grained Potentials of Protein Thermodynamics","summary":"  A generalized understanding of protein dynamics is an unsolved scientific\nproblem, the solution of which is critical to the interpretation of the\nstructure-function relationships that govern essential biological processes.\nHere, we approach this problem by constructing coarse-grained molecular\npotentials based on artificial neural networks and grounded in statistical\nmechanics. For training, we build a unique dataset of unbiased all-atom\nmolecular dynamics simulations of approximately 9 ms for twelve different\nproteins with multiple secondary structure arrangements. The coarse-grained\nmodels are capable of accelerating the dynamics by more than three orders of\nmagnitude while preserving the thermodynamics of the systems. Coarse-grained\nsimulations identify relevant structural states in the ensemble with comparable\nenergetics to the all-atom systems. Furthermore, we show that a single\ncoarse-grained potential can integrate all twelve proteins and can capture\nexperimental structural features of mutated proteins. These results indicate\nthat machine learning coarse-grained potentials could provide a feasible\napproach to simulate and understand protein dynamics.\n","authors":["Maciej Majewski","Adrià Pérez","Philipp Thölke","Stefan Doerr","Nicholas E. Charron","Toni Giorgino","Brooke E. Husic","Cecilia Clementi","Frank Noé","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2212.07492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01478v2","updated":"2022-12-14T20:17:10Z","published":"2022-10-25T01:22:41Z","title":"A machine learning model to identify corruption in México's public\n  procurement contracts","summary":"  The costs and impacts of government corruption range from impairing a\ncountry's economic growth to affecting its citizens' well-being and safety.\nPublic contracting between government dependencies and private sector\ninstances, referred to as public procurement, is a fertile land of opportunity\nfor corrupt practices, generating substantial monetary losses worldwide. Thus,\nidentifying and deterring corrupt activities between the government and the\nprivate sector is paramount. However, due to several factors, corruption in\npublic procurement is challenging to identify and track, leading to corrupt\npractices going unnoticed. This paper proposes a machine learning model based\non an ensemble of random forest classifiers, which we call hyper-forest, to\nidentify and predict corrupt contracts in M\\'exico's public procurement data.\nThis method's results correctly detect most of the corrupt and non-corrupt\ncontracts evaluated in the dataset. Furthermore, we found that the most\ncritical predictors considered in the model are those related to the\nrelationship between buyers and suppliers rather than those related to features\nof individual contracts. Also, the method proposed here is general enough to be\ntrained with data from other countries. Overall, our work presents a tool that\ncan help in the decision-making process to identify, predict and analyze\ncorruption in public procurement contracts.\n","authors":["Andrés Aldana","Andrea Falcón-Cortés","Hernán Larralde"],"pdf_url":"https://arxiv.org/pdf/2211.01478v2.pdf","comment":"17 pages, 8 figures. On revision in Government Information Quarterly"},{"id":"http://arxiv.org/abs/2212.07489v1","updated":"2022-12-14T20:15:19Z","published":"2022-12-14T20:15:19Z","title":"SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement\n  Learning","summary":"  The availability of challenging benchmarks has played a key role in the\nrecent progress of machine learning. In cooperative multi-agent reinforcement\nlearning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular\ntestbed for centralised training with decentralised execution. However, after\nyears of sustained improvement on SMAC, algorithms now achieve near-perfect\nperformance. In this work, we conduct new analysis demonstrating that SMAC is\nnot sufficiently stochastic to require complex closed-loop policies. In\nparticular, we show that an open-loop policy conditioned only on the timestep\ncan achieve non-trivial win rates for many SMAC scenarios. To address this\nlimitation, we introduce SMACv2, a new version of the benchmark where scenarios\nare procedurally generated and require agents to generalise to previously\nunseen settings (from the same distribution) during evaluation. We show that\nthese changes ensure the benchmark requires the use of closed-loop policies. We\nevaluate state-of-the-art algorithms on SMACv2 and show that it presents\nsignificant challenges not present in the original benchmark. Our analysis\nillustrates that SMACv2 addresses the discovered deficiencies of SMAC and can\nhelp benchmark the next generation of MARL methods. Videos of training are\navailable at https://sites.google.com/view/smacv2\n","authors":["Benjamin Ellis","Skander Moalla","Mikayel Samvelyan","Mingfei Sun","Anuj Mahajan","Jakob N. Foerster","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2212.07489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07486v1","updated":"2022-12-14T20:07:33Z","published":"2022-12-14T20:07:33Z","title":"Scaling Marginalized Importance Sampling to High-Dimensional\n  State-Spaces via State Abstraction","summary":"  We consider the problem of off-policy evaluation (OPE) in reinforcement\nlearning (RL), where the goal is to estimate the performance of an evaluation\npolicy, $\\pi_e$, using a fixed dataset, $\\mathcal{D}$, collected by one or more\npolicies that may be different from $\\pi_e$. Current OPE algorithms may produce\npoor OPE estimates under policy distribution shift i.e., when the probability\nof a particular state-action pair occurring under $\\pi_e$ is very different\nfrom the probability of that same pair occurring in $\\mathcal{D}$ (Voloshin et\nal. 2021, Fu et al. 2021). In this work, we propose to improve the accuracy of\nOPE estimators by projecting the high-dimensional state-space into a\nlow-dimensional state-space using concepts from the state abstraction\nliterature. Specifically, we consider marginalized importance sampling (MIS)\nOPE algorithms which compute state-action distribution correction ratios to\nproduce their OPE estimate. In the original ground state-space, these ratios\nmay have high variance which may lead to high variance OPE. However, we prove\nthat in the lower-dimensional abstract state-space the ratios can have lower\nvariance resulting in lower variance OPE. We then highlight the challenges that\narise when estimating the abstract ratios from data, identify sufficient\nconditions to overcome these issues, and present a minimax optimization problem\nwhose solution yields these abstract ratios. Finally, our empirical evaluation\non difficult, high-dimensional state-space OPE tasks shows that the abstract\nratios can make MIS OPE estimators achieve lower mean-squared error and more\nrobust to hyperparameter tuning than the ground ratios.\n","authors":["Brahma S. Pavse","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2212.07486v1.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2212.07477v1","updated":"2022-12-14T19:54:46Z","published":"2022-12-14T19:54:46Z","title":"Guiding continuous operator learning through Physics-based boundary\n  constraints","summary":"  Boundary conditions (BCs) are important groups of physics-enforced\nconstraints that are necessary for solutions of Partial Differential Equations\n(PDEs) to satisfy at specific spatial locations. These constraints carry\nimportant physical meaning, and guarantee the existence and the uniqueness of\nthe PDE solution. Current neural-network based approaches that aim to solve\nPDEs rely only on training data to help the model learn BCs implicitly. There\nis no guarantee of BC satisfaction by these models during evaluation. In this\nwork, we propose Boundary enforcing Operator Network (BOON) that enables the BC\nsatisfaction of neural operators by making structural changes to the operator\nkernel. We provide our refinement procedure, and demonstrate the satisfaction\nof physics-based BCs, e.g. Dirichlet, Neumann, and periodic by the solutions\nobtained by BOON. Numerical experiments based on multiple PDEs with a wide\nvariety of applications indicate that the proposed approach ensures\nsatisfaction of BCs, and leads to more accurate solutions over the entire\ndomain. The proposed correction method exhibits a (2X-20X) improvement over a\ngiven operator model in relative $L^2$ error (0.000084 relative $L^2$ error for\nBurgers' equation).\n","authors":["Nadim Saad","Gaurav Gupta","Shima Alizadeh","Danielle C. Maddix"],"pdf_url":"https://arxiv.org/pdf/2212.07477v1.pdf","comment":"Nadim and Gaurav contributed equally in this work. 31 pages, 7\n  figures, 16 tables"},{"id":"http://arxiv.org/abs/2210.10950v2","updated":"2022-12-14T19:50:38Z","published":"2022-10-20T01:44:17Z","title":"ESPNN: A novel electronic stopping power neural-network code built on\n  the IAEA stopping power database. I. Atomic targets","summary":"  The International Atomic Energy Agency (IAEA) stopping power database is a\nhighly valued public resource compiling most of the experimental measurements\npublished over nearly a century. The database-accessible to the global\nscientific community-is continuously updated and has been extensively employed\nin theoretical and experimental research for more than 30 years. This work aims\nto employ machine learning algorithms on the 2021 IAEA database to predict\naccurate electronic stopping power cross sections for any ion and target\ncombination in a wide range of incident energies. Unsupervised machine learning\nmethods are applied to clean the database in an automated manner. These\ntechniques purge the data by removing suspicious outliers and old isolated\nvalues. A large portion of the remaining data is used to train a deep neural\nnetwork, while the rest is set aside, constituting the test set. The present\nwork considers collisional systems only with atomic targets. The first version\nof the ESPNN (electronic stopping power neural-network code), openly available\nto users, is shown to yield predicted values in excellent agreement with the\nexperimental results of the test set.\n","authors":["F. Bivort Haiek","A. M. P. Mendez","C. C. Montanari","D. M. Mitnik"],"pdf_url":"https://arxiv.org/pdf/2210.10950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07473v1","updated":"2022-12-14T19:43:43Z","published":"2022-12-14T19:43:43Z","title":"MABSplit: Faster Forest Training Using Multi-Armed Bandits","summary":"  Random forests are some of the most widely used machine learning models\ntoday, especially in domains that necessitate interpretability. We present an\nalgorithm that accelerates the training of random forests and other popular\ntree-based learning methods. At the core of our algorithm is a novel\nnode-splitting subroutine, dubbed MABSplit, used to efficiently find split\npoints when constructing decision trees. Our algorithm borrows techniques from\nthe multi-armed bandit literature to judiciously determine how to allocate\nsamples and computational power across candidate split points. We provide\ntheoretical guarantees that MABSplit improves the sample complexity of each\nnode split from linear to logarithmic in the number of data points. In some\nsettings, MABSplit leads to 100x faster training (an 99% reduction in training\ntime) without any decrease in generalization performance. We demonstrate\nsimilar speedups when MABSplit is used across a variety of forest-based\nvariants, such as Extremely Random Forests and Random Patches. We also show our\nalgorithm can be used in both classification and regression tasks. Finally, we\nshow that MABSplit outperforms existing methods in generalization performance\nand feature importance calculations under a fixed computational budget. All of\nour experimental results are reproducible via a one-line script at\nhttps://github.com/ThrunGroup/FastForest.\n","authors":["Mo Tiwari","Ryan Kang","Je-Yong Lee","Sebastian Thrun","Chris Piech","Ilan Shomorony","Martin Jinye Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.07473v1.pdf","comment":"Published at NeurIPS 2022, 30 pages"},{"id":"http://arxiv.org/abs/2212.00136v2","updated":"2022-12-14T19:39:24Z","published":"2022-11-30T22:00:24Z","title":"DEL-Dock: Molecular Docking-Enabled Modeling of DNA-Encoded Libraries","summary":"  DNA-Encoded Library (DEL) technology has enabled significant advances in hit\nidentification by enabling efficient testing of combinatorially-generated\nmolecular libraries. DEL screens measure protein binding affinity though\nsequencing reads of molecules tagged with unique DNA-barcodes that survive a\nseries of selection experiments. Computational models have been deployed to\nlearn the latent binding affinities that are correlated to the sequenced count\ndata; however, this correlation is often obfuscated by various sources of noise\nintroduced in its complicated data-generation process. In order to denoise DEL\ncount data and screen for molecules with good binding affinity, computational\nmodels require the correct assumptions in their modeling structure to capture\nthe correct signals underlying the data. Recent advances in DEL models have\nfocused on probabilistic formulations of count data, but existing approaches\nhave thus far been limited to only utilizing 2-D molecule-level\nrepresentations. We introduce a new paradigm, DEL-Dock, that combines\nligand-based descriptors with 3-D spatial information from docked\nprotein-ligand complexes. 3-D spatial information allows our model to learn\nover the actual binding modality rather than using only structured-based\ninformation of the ligand. We show that our model is capable of effectively\ndenoising DEL count data to predict molecule enrichment scores that are better\ncorrelated with experimental binding affinity measurements compared to prior\nworks. Moreover, by learning over a collection of docked poses we demonstrate\nthat our model, trained only on DEL data, implicitly learns to perform good\ndocking pose selection without requiring external supervision from\nexpensive-to-source protein crystal structures.\n","authors":["Kirill Shmilovich","Benson Chen","Theofanis Karaletsos","Mohammad M. Sultan"],"pdf_url":"https://arxiv.org/pdf/2212.00136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07469v1","updated":"2022-12-14T19:27:03Z","published":"2022-12-14T19:27:03Z","title":"Learning threshold neurons via the \"edge of stability\"","summary":"  Existing analyses of neural network training often operate under the\nunrealistic assumption of an extremely small learning rate. This lies in stark\ncontrast to practical wisdom and empirical studies, such as the work of J.\nCohen et al. (ICLR 2021), which exhibit startling new phenomena (the \"edge of\nstability\" or \"unstable convergence\") and potential benefits for generalization\nin the large learning rate regime. Despite a flurry of recent works on this\ntopic, however, the latter effect is still poorly understood. In this paper, we\ntake a step towards understanding genuinely non-convex training dynamics with\nlarge learning rates by performing a detailed analysis of gradient descent for\nsimplified models of two-layer neural networks. For these models, we provably\nestablish the edge of stability phenomenon and discover a sharp phase\ntransition for the step size below which the neural network fails to learn\n\"threshold-like\" neurons (i.e., neurons with a non-zero first-layer bias). This\nelucidates one possible mechanism by which the edge of stability can in fact\nlead to better generalization, as threshold neurons are basic building blocks\nwith useful inductive bias for many tasks.\n","authors":["Kwangjun Ahn","Sébastien Bubeck","Sinho Chewi","Yin Tat Lee","Felipe Suarez","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.07469v1.pdf","comment":"35 pages, 12 figures"},{"id":"http://arxiv.org/abs/2212.06451v2","updated":"2022-12-14T19:26:20Z","published":"2022-12-13T09:47:28Z","title":"Improving generalization in reinforcement learning through forked agents","summary":"  An eco-system of agents each having their own policy with some, but limited,\ngeneralizability has proven to be a reliable approach to increase\ngeneralization across procedurally generated environments. In such an approach,\nnew agents are regularly added to the eco-system when encountering a new\nenvironment that is outside of the scope of the eco-system. The speed of\nadaptation and general effectiveness of the eco-system approach highly depends\non the initialization of new agents. In this paper we propose different\ntechniques for such initialization and study their impact.\n","authors":["Olivier Moulin","Vincent Francois-Lavet","Mark Hoogendoorn"],"pdf_url":"https://arxiv.org/pdf/2212.06451v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2212.07462v1","updated":"2022-12-14T19:13:59Z","published":"2022-12-14T19:13:59Z","title":"Harmonic (Quantum) Neural Networks","summary":"  Harmonic functions are abundant in nature, appearing in limiting cases of\nMaxwell's, Navier-Stokes equations, the heat and the wave equation.\nConsequently, there are many applications of harmonic functions, spanning\napplications from industrial process optimisation to robotic path planning and\nthe calculation of first exit times of random walks. Despite their ubiquity and\nrelevance, there have been few attempts to develop effective means of\nrepresenting harmonic functions in the context of machine learning\narchitectures, either in machine learning on classical computers, or in the\nnascent field of quantum machine learning. Architectures which impose or\nencourage an inductive bias towards harmonic functions would facilitate\ndata-driven modelling and the solution of inverse problems in a range of\napplications. For classical neural networks, it has already been established\nhow leveraging inductive biases can in general lead to improved performance of\nlearning algorithms. The introduction of such inductive biases within a quantum\nmachine learning setting is instead still in its nascent stages. In this work,\nwe derive exactly-harmonic (conventional- and quantum-) neural networks in two\ndimensions for simply-connected domains by leveraging the characteristics of\nholomorphic complex functions. We then demonstrate how these can be\napproximately extended to multiply-connected two-dimensional domains using\ntechniques inspired by domain decomposition in physics-informed neural\nnetworks. We further provide architectures and training protocols to\neffectively impose approximately harmonic constraints in three dimensions and\nhigher, and as a corollary we report divergence-free network architectures in\narbitrary dimensions. Our approaches are demonstrated with applications to heat\ntransfer, electrostatics and robot navigation, with comparisons to\nphysics-informed neural networks included.\n","authors":["Atiyo Ghosh","Antonio A. Gentile","Mario Dagrada","Chul Lee","Seong-hyok Kim","Hyukgeun Cha","Yunjun Choi","Brad Kim","Jeong-il Kye","Vincent E. Elfving"],"pdf_url":"https://arxiv.org/pdf/2212.07462v1.pdf","comment":"16 pages (main), 6 pages (supplementary), 5 figures"},{"id":"http://arxiv.org/abs/2212.07786v1","updated":"2022-12-14T17:34:03Z","published":"2022-12-14T17:34:03Z","title":"Convergent Data-driven Regularizations for CT Reconstruction","summary":"  The reconstruction of images from their corresponding noisy Radon transform\nis a typical example of an ill-posed linear inverse problem as arising in the\napplication of computerized tomography (CT). As the (na\\\"{\\i}ve) solution does\nnot depend on the measured data continuously, regularization is needed to\nre-establish a continuous dependence. In this work, we investigate simple, but\nyet still provably convergent approaches to learning linear regularization\nmethods from data. More specifically, we analyze two approaches: One generic\nlinear regularization that learns how to manipulate the singular values of the\nlinear operator in an extension of [1], and one tailored approach in the\nFourier domain that is specific to CT-reconstruction. We prove that such\napproaches become convergent regularization methods as well as the fact that\nthe reconstructions they provide are typically much smoother than the training\ndata they were trained on. Finally, we compare the spectral as well as the\nFourier-based approaches for CT-reconstruction numerically, discuss their\nadvantages and disadvantages and investigate the effect of discretization\nerrors at different resolutions.\n","authors":["Samira Kabri","Alexander Auras","Danilo Riccio","Hartmut Bauermeister","Martin Benning","Michael Moeller","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2212.07786v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2201.03027v2","updated":"2022-12-14T03:42:11Z","published":"2022-01-09T14:51:34Z","title":"Meta-Generalization for Multiparty Privacy Learning to Identify Anomaly\n  Multimedia Traffic in Graynet","summary":"  Identifying anomaly multimedia traffic in cyberspace is a big challenge in\ndistributed service systems, multiple generation networks and future internet\nof everything. This letter explores meta-generalization for a multiparty\nprivacy learning model in graynet to improve the performance of anomaly\nmultimedia traffic identification. The multiparty privacy learning model in\ngraynet is a globally shared model that is partitioned, distributed and trained\nby exchanging multiparty parameters updates with preserving private data. The\nmeta-generalization refers to discovering the inherent attributes of a learning\nmodel to reduce its generalization error. In experiments, three\nmeta-generalization principles are tested as follows. The generalization error\nof the multiparty privacy learning model in graynet is reduced by changing the\ndimension of byte-level imbedding. Following that, the error is reduced by\nadapting the depth for extracting packet-level features. Finally, the error is\nreduced by adjusting the size of support set for preprocessing traffic-level\ndata. Experimental results demonstrate that the proposal outperforms the\nstate-of-the-art learning models for identifying anomaly multimedia traffic.\n","authors":["Satoshi Nato","Yiqiang Sheng"],"pdf_url":"https://arxiv.org/pdf/2201.03027v2.pdf","comment":"Correct some typos"}]},"2022-12-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.06933v1","updated":"2022-12-13T23:06:20Z","published":"2022-12-13T23:06:20Z","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and\n  Methods","summary":"  The rapid advancement of AI technology has made text generation tools like\nGPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can\npose serious threat to the credibility of various forms of media if these\ntechnologies are used for plagiarism, including scientific literature and news\nsources. Despite the development of automated methods for paraphrase\nidentification, detecting this type of plagiarism remains a challenge due to\nthe disparate nature of the datasets on which these methods are trained. In\nthis study, we review traditional and current approaches to paraphrase\nidentification and propose a refined typology of paraphrases. We also\ninvestigate how this typology is represented in popular datasets and how\nunder-representation of certain types of paraphrases impacts detection\ncapabilities. Finally, we outline new directions for future research and\ndatasets in the pursuit of more effective paraphrase detection using AI.\n","authors":["Chao Zhou","Cheng Qiu","Daniel E. Acuna"],"pdf_url":"https://arxiv.org/pdf/2212.06933v1.pdf","comment":"36 pages, 2 figures, 6 tables, 173 references"},{"id":"http://arxiv.org/abs/2212.06868v1","updated":"2022-12-13T19:24:08Z","published":"2022-12-13T19:24:08Z","title":"Deep Image Style Transfer from Freeform Text","summary":"  This paper creates a novel method of deep neural style transfer by generating\nstyle images from freeform user text input. The language model and style\ntransfer model form a seamless pipeline that can create output images with\nsimilar losses and improved quality when compared to baseline style transfer\nmethods. The language model returns a closely matching image given a style text\nand description input, which is then passed to the style transfer model with an\ninput content image to create a final output. A proof-of-concept tool is also\ndeveloped to integrate the models and demonstrate the effectiveness of deep\nimage style transfer from freeform text.\n","authors":["Tejas Santanam","Mengyang Liu","Jiangyue Yu","Zhaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2212.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.11800v3","updated":"2022-12-13T19:03:38Z","published":"2021-12-22T11:15:49Z","title":"STEREO: Scientific Text Reuse in Open Access Publications","summary":"  We present the Webis-STEREO-21 dataset, a massive collection of Scientific\nText Reuse in Open-access publications. It contains more than 91 million cases\nof reused text passages found in 4.2 million unique open-access publications.\nFeaturing a high coverage of scientific disciplines and varieties of reuse, as\nwell as comprehensive metadata to contextualize each case, our dataset\naddresses the most salient shortcomings of previous ones on scientific writing.\nWebis-STEREO-21 allows for tackling a wide range of research questions from\ndifferent scientific backgrounds, facilitating both qualitative and\nquantitative analysis of the phenomenon as well as a first-time grounding on\nthe base rate of text reuse in scientific publications.\n","authors":["Lukas Gienapp","Wolfgang Kircheis","Bjarne Sievers","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2112.11800v3.pdf","comment":"14 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.06817v1","updated":"2022-12-13T18:55:15Z","published":"2022-12-13T18:55:15Z","title":"RT-1: Robotics Transformer for Real-World Control at Scale","summary":"  By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer.github.io\n","authors":["Anthony Brohan","Noah Brown","Justice Carbajal","Yevgen Chebotar","Joseph Dabis","Chelsea Finn","Keerthana Gopalakrishnan","Karol Hausman","Alex Herzog","Jasmine Hsu","Julian Ibarz","Brian Ichter","Alex Irpan","Tomas Jackson","Sally Jesmonth","Nikhil J Joshi","Ryan Julian","Dmitry Kalashnikov","Yuheng Kuang","Isabel Leal","Kuang-Huei Lee","Sergey Levine","Yao Lu","Utsav Malla","Deeksha Manjunath","Igor Mordatch","Ofir Nachum","Carolina Parada","Jodilyn Peralta","Emily Perez","Karl Pertsch","Jornell Quiambao","Kanishka Rao","Michael Ryoo","Grecia Salazar","Pannag Sanketi","Kevin Sayed","Jaspiar Singh","Sumedh Sontakke","Austin Stone","Clayton Tan","Huong Tran","Vincent Vanhoucke","Steve Vega","Quan Vuong","Fei Xia","Ted Xiao","Peng Xu","Sichun Xu","Tianhe Yu","Brianna Zitkovich"],"pdf_url":"https://arxiv.org/pdf/2212.06817v1.pdf","comment":"See website at robotics-transformer.github.io"},{"id":"http://arxiv.org/abs/2212.05409v2","updated":"2022-12-13T18:47:27Z","published":"2022-12-11T04:45:50Z","title":"IndicXTREME: A Multi-Task Benchmark For Evaluating Indic Languages","summary":"  In this work, we introduce IndicXTREME, a benchmark consisting of nine\ndiverse tasks covering 18 languages from the Indic sub-continent belonging to\nfour different families. Across languages and tasks, IndicXTREME contains a\ntotal of 103 evaluation sets, of which 51 are new contributions to the\nliterature. To maintain high quality, we only use human annotators to curate or\ntranslate our datasets. To the best of our knowledge, this is the first effort\ntoward creating a standard benchmark for Indic languages that aims to test the\nzero-shot capabilities of pretrained language models. We also release IndicCorp\nv2, an updated and much larger version of IndicCorp that contains 20.9 billion\ntokens in 24 languages. We pretrain IndicBERT v2 on IndicCorp v2 and evaluate\nit on IndicXTREME to show that it outperforms existing multilingual language\nmodels such as XLM-R and MuRIL.\n","authors":["Sumanth Doddapaneni","Rahul Aralikatte","Gowtham Ramesh","Shreya Goyal","Mitesh M. Khapra","Anoop Kunchukuttan","Pratyush Kumar"],"pdf_url":"https://arxiv.org/pdf/2212.05409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06801v1","updated":"2022-12-13T18:34:59Z","published":"2022-12-13T18:34:59Z","title":"A fine-grained comparison of pragmatic language understanding in humans\n  and language models","summary":"  Pragmatics is an essential part of communication, but it remains unclear what\nmechanisms underlie human pragmatic communication and whether NLP systems\ncapture pragmatic language understanding. To investigate both these questions,\nwe perform a fine-grained comparison of language models and humans on seven\npragmatic phenomena, using zero-shot prompting on an expert-curated set of\nEnglish materials. We ask whether models (1) select pragmatic interpretations\nof speaker utterances, (2) make similar error patterns as humans, and (3) use\nsimilar linguistic cues as humans to solve the tasks. We find that the largest\nmodels achieve high accuracy and match human error patterns: within incorrect\nresponses, models favor the literal interpretation of an utterance over\nheuristic-based distractors. We also find evidence that models and humans are\nsensitive to similar linguistic cues. Our results suggest that even\nparadigmatic pragmatic phenomena may be solved without explicit representations\nof other agents' mental states, and that artificial models can be used to gain\nmechanistic insights into human pragmatic processing.\n","authors":["Jennifer Hu","Sammy Floyd","Olessia Jouravlev","Evelina Fedorenko","Edward Gibson"],"pdf_url":"https://arxiv.org/pdf/2212.06801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06800v1","updated":"2022-12-13T18:34:15Z","published":"2022-12-13T18:34:15Z","title":"Diverse Demonstrations Improve In-context Compositional Generalization","summary":"  In-context learning has shown great success in i.i.d semantic parsing splits,\nwhere the training and test sets are drawn from the same distribution. In this\nsetup, models are typically prompted with demonstrations that are similar to\nthe input question. However, in the setup of compositional generalization,\nwhere models are tested on outputs with structures that are absent from the\ntraining set, selecting similar demonstrations is insufficient, as often no\nexample will be similar enough to the input. In this work, we propose a method\nto select diverse demonstrations that aims to collectively cover all of the\nstructures required in the output program, in order to encourage the model to\ngeneralize to new structures from these demonstrations. We empirically show\nthat combining diverse demonstrations with in-context learning substantially\nimproves performance across three compositional generalization semantic parsing\ndatasets in the pure in-context learning setup and when combined with\nfinetuning.\n","authors":["Itay Levy","Ben Bogin","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2212.06800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06742v1","updated":"2022-12-13T17:21:44Z","published":"2022-12-13T17:21:44Z","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for\n  Programming Languages","summary":"  Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nwill make our code and pre-trained models publicly available.\n","authors":["Yekun Chai","Shuohuan Wang","Chao Pang","Yu Sun","Hao Tian","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2212.06742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06713v1","updated":"2022-12-13T16:31:21Z","published":"2022-12-13T16:31:21Z","title":"Structured Prompting: Scaling In-Context Learning to 1,000 Examples","summary":"  Large language models have exhibited intriguing in-context learning\ncapability, achieving promising zero- and few-shot performance without updating\nthe parameters. However, conventional in-context learning is usually restricted\nby length constraints, rendering it ineffective to absorb supervision from a\nlarge number of examples. In order to go beyond few shots, we introduce\nstructured prompting that breaks the length limit and scales in-context\nlearning to thousands of examples. Specifically, demonstration examples are\nseparately encoded with well-designed position embeddings, and then they are\njointly attended by the test example using a rescaled attention mechanism. So\nwe can scale the number of exemplars with linear complexity instead of\nquadratic complexity with respect to length. Experimental results on a diverse\nset of tasks show that our approach improves end-task performance and reduces\nevaluation variance over conventional in-context learning as the number of\ndemonstration examples increases. Code has been released at\nhttps://aka.ms/structured-prompting.\n","authors":["Yaru Hao","Yutao Sun","Li Dong","Zhixiong Han","Yuxian Gu","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2212.06713v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2212.06645v1","updated":"2022-12-13T15:28:57Z","published":"2022-12-13T15:28:57Z","title":"Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?","summary":"  Traditional multi-task learning architectures train a single model across\nmultiple tasks through a shared encoder followed by task-specific decoders.\nLearning these models often requires specialized training algorithms that\naddress task-conflict in the shared parameter updates, which otherwise can lead\nto negative transfer. A new type of multi-task learning within NLP homogenizes\nmulti-task architectures as a shared encoder and language model decoder, which\ndoes surprisingly well across a range of diverse tasks. Does this new\narchitecture suffer from task-conflicts that require specialized training\nalgorithms? We study how certain factors in the shift towards text-to-text\nmodels affects multi-task conflict and negative transfer, finding that both\ndirectional conflict and transfer are surprisingly constant across\narchitectures.\n","authors":["David Mueller","Nicholas Andrews","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2212.06645v1.pdf","comment":"Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.06039v2","updated":"2022-12-13T15:26:25Z","published":"2022-11-14T19:01:55Z","title":"Technological taxonomies for hypernym and hyponym retrieval in patent\n  texts","summary":"  This paper presents an automatic approach to creating taxonomies of technical\nterms based on the Cooperative Patent Classification (CPC). The resulting\ntaxonomy contains about 170k nodes in 9 separate technological branches and is\nfreely available. We also show that a Text-to-Text Transfer Transformer (T5)\nmodel can be fine-tuned to generate hypernyms and hyponyms with relatively high\nprecision, confirming the manually assessed quality of the resource. The T5\nmodel opens the taxonomy to any new technological terms for which a hypernym\ncan be generated, thus making the resource updateable with new terms, an\nessential feature for the constantly evolving field of technological\nterminology.\n","authors":["You Zuo","Yixuan Li","Alma Parias García","Kim Gerdes"],"pdf_url":"https://arxiv.org/pdf/2212.06039v2.pdf","comment":"ToTh 2022 - Terminology & Ontology: Theories and applications, Jun\n  2022, Chamb{\\'e}ry, France"},{"id":"http://arxiv.org/abs/2212.06636v1","updated":"2022-12-13T15:12:37Z","published":"2022-12-13T15:12:37Z","title":"Categorical Tools for Natural Language Processing","summary":"  This thesis develops the translation between category theory and\ncomputational linguistics as a foundation for natural language processing. The\nthree chapters deal with syntax, semantics and pragmatics. First, string\ndiagrams provide a unified model of syntactic structures in formal grammars.\nSecond, functors compute semantics by turning diagrams into logical, tensor,\nneural or quantum computation. Third, the resulting functorial models can be\ncomposed to form games where equilibria are the solutions of language\nprocessing tasks. This framework is implemented as part of DisCoPy, the Python\nlibrary for computing with string diagrams. We describe the correspondence\nbetween categorical, linguistic and computational structures, and demonstrate\ntheir applications in compositional natural language processing.\n","authors":["Giovanni de Felice"],"pdf_url":"https://arxiv.org/pdf/2212.06636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09783v4","updated":"2022-12-13T14:57:14Z","published":"2022-11-17T18:54:47Z","title":"UniSumm: Unified Few-shot Summarization with Multi-Task Pre-Training and\n  Prefix-Tuning","summary":"  The diverse demands of different summarization tasks and their high\nannotation costs are driving a need for few-shot summarization. However,\ndespite the emergence of many summarization tasks and datasets, the current\ntraining paradigm for few-shot summarization systems ignores potentially\nshareable knowledge in heterogeneous datasets. To this end, we propose\n\\textsc{UniSumm}, a unified few-shot summarization model pre-trained with\nmultiple summarization tasks and can be prefix-tuned to excel at any few-shot\nsummarization datasets. Meanwhile, to better evaluate few-shot summarization\nsystems, under the principles of diversity and robustness, we assemble and\npublicize a new benchmark \\textsc{SummZoo}. It consists of $8$ diverse\nsummarization tasks with multiple sets of few-shot samples for each task,\ncovering both monologue and dialogue domains. Experimental results and ablation\nstudies show that \\textsc{UniSumm} outperforms strong baseline systems by a\nlarge margin across all tasks in \\textsc{SummZoo} under both automatic and\nhuman evaluations. We release our code and benchmark at\n\\url{https://github.com/microsoft/UniSumm}.\n","authors":["Yulong Chen","Yang Liu","Ruochen Xu","Ziyi Yang","Chenguang Zhu","Michael Zeng","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.09783v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06615v1","updated":"2022-12-13T14:38:57Z","published":"2022-12-13T14:38:57Z","title":"Category Theory for Quantum Natural Language Processing","summary":"  This thesis introduces quantum natural language processing (QNLP) models\nbased on a simple yet powerful analogy between computational linguistics and\nquantum mechanics: grammar as entanglement. The grammatical structure of text\nand sentences connects the meaning of words in the same way that entanglement\nstructure connects the states of quantum systems. Category theory allows to\nmake this language-to-qubit analogy formal: it is a monoidal functor from\ngrammar to vector spaces. We turn this abstract analogy into a concrete\nalgorithm that translates the grammatical structure onto the architecture of\nparameterised quantum circuits. We then use a hybrid classical-quantum\nalgorithm to train the model so that evaluating the circuits computes the\nmeaning of sentences in data-driven tasks.\n  The implementation of QNLP models motivated the development of DisCoPy\n(Distributional Compositional Python), the toolkit for applied category theory\nof which the first chapter gives a comprehensive overview. String diagrams are\nthe core data structure of DisCoPy, they allow to reason about computation at a\nhigh level of abstraction. We show how they can encode both grammatical\nstructures and quantum circuits, but also logical formulae, neural networks or\narbitrary Python code. Monoidal functors allow to translate these abstract\ndiagrams into concrete computation, interfacing with optimised task-specific\nlibraries.\n  The second chapter uses DisCopy to implement QNLP models as parameterised\nfunctors from grammar to quantum circuits. It gives a first proof-of-concept\nfor the more general concept of functorial learning: generalising machine\nlearning from functions to functors by learning from diagram-like data. In\norder to learn optimal functor parameters via gradient descent, we introduce\nthe notion of diagrammatic differentiation: a graphical calculus for computing\nthe gradients of parameterised diagrams.\n","authors":["Alexis Toumi"],"pdf_url":"https://arxiv.org/pdf/2212.06615v1.pdf","comment":"PhD thesis, University of Oxford"},{"id":"http://arxiv.org/abs/2212.03760v2","updated":"2022-12-13T14:14:53Z","published":"2022-12-07T16:31:14Z","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching\n  Task-specific and Task-agnostic Representation Learning","summary":"  Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.\n","authors":["Kyuyong Shin","Hanock Kwak","Wonjae Kim","Jisu Jeong","Seungjae Jung","Kyung-Min Kim","Jung-Woo Ha","Sang-Woo Lee"],"pdf_url":"https://arxiv.org/pdf/2212.03760v2.pdf","comment":"14 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2111.07402v3","updated":"2022-12-13T14:12:02Z","published":"2021-11-14T18:16:42Z","title":"Textless Speech Emotion Conversion using Discrete and Decomposed\n  Representations","summary":"  Speech emotion conversion is the task of modifying the perceived emotion of a\nspeech utterance while preserving the lexical content and speaker identity. In\nthis study, we cast the problem of emotion conversion as a spoken language\ntranslation task. We use a decomposition of the speech signal into discrete\nlearned representations, consisting of phonetic-content units, prosodic\nfeatures, speaker, and emotion. First, we modify the speech content by\ntranslating the phonetic-content units to a target emotion, and then predict\nthe prosodic features based on these units. Finally, the speech waveform is\ngenerated by feeding the predicted representations into a neural vocoder. Such\na paradigm allows us to go beyond spectral and parametric changes of the\nsignal, and model non-verbal vocalizations, such as laughter insertion, yawning\nremoval, etc. We demonstrate objectively and subjectively that the proposed\nmethod is vastly superior to current approaches and even beats text-based\nsystems in terms of perceived emotion and audio quality. We rigorously evaluate\nall components of such a complex system and conclude with an extensive model\nanalysis and ablation study to better emphasize the architectural choices,\nstrengths and weaknesses of the proposed method. Samples are available under\nthe following link: https://speechbot.github.io/emotion.\n","authors":["Felix Kreuk","Adam Polyak","Jade Copet","Eugene Kharitonov","Tu-Anh Nguyen","Morgane Rivière","Wei-Ning Hsu","Abdelrahman Mohamed","Emmanuel Dupoux","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2111.07402v3.pdf","comment":"Paper was published at EMNLP 2022"},{"id":"http://arxiv.org/abs/2211.14477v2","updated":"2022-12-13T14:06:53Z","published":"2022-11-26T04:27:31Z","title":"PCRED: Zero-shot Relation Triplet Extraction with Potential Candidate\n  Relation Selection and Entity Boundary Detection","summary":"  Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation\ntriplets from unstructured texts under the zero-shot setting, where the\nrelation sets at the training and testing stages are disjoint. Previous\nstate-of-the-art method handles this challenging task by leveraging pretrained\nlanguage models to generate data as additional training samples, which\nincreases the training cost and severely constrains the model performance. To\naddress the above issues, we propose a novel method named PCRED for ZeroRTE\nwith Potential Candidate Relation Selection and Entity Boundary Detection. The\nremarkable characteristic of PCRED is that it does not rely on additional data\nand still achieves promising performance. The model adopts a relation-first\nparadigm, recognizing unseen relations through candidate relation selection.\nWith this approach, the semantics of relations are naturally infused in the\ncontext. Entities are extracted based on the context and the semantics of\nrelations subsequently. We evaluate our model on two ZeroRTE datasets. The\nexperiment results show that our method consistently outperforms previous\nworks. Our code will be available at https://anonymous.4open.science/r/PCRED.\n","authors":["Yuquan Lan","Dongxu Li","Yunqi Zhang","Hui Zhao","Gang Zhao"],"pdf_url":"https://arxiv.org/pdf/2211.14477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.16257v2","updated":"2022-12-13T13:52:26Z","published":"2022-10-28T16:47:03Z","title":"Solving Math Word Problem via Cooperative Reasoning induced Language\n  Models","summary":"  Large-scale pre-trained language models (PLMs) bring new opportunities to\nchallenge problems, especially those that need high-level intelligence, such as\nthe math word problem (MWPs). However, directly applying existing PLMs to MWPs\ncan fail as the generation process lacks sufficient supervision and thus lacks\nfast adaptivity as humans. We notice that human reasoning has a dual reasoning\nframework that consists of an immediate reaction system (system 1) and a\ndelicate reasoning system (system 2), where the entire reasoning is determined\nby their interaction. This inspires us to develop a cooperative\nreasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe),\nresulting in a human-like reasoning architecture with system 1 as the generator\nand system 2 as the verifier. In our approach, the generator is responsible for\ngenerating reasoning paths, and the verifiers are used to supervise the\nevaluation in order to obtain reliable feedback for the generator. We evaluate\nour CoRe framework on several mathematical reasoning datasets and achieve\ndecent improvement over state-of-the-art methods, up to 9.8% increase over best\nbaselines.\n","authors":["Xinyu Zhu","Junjie Wang","Lin Zhang","Yuxiang Zhang","Ruyi Gan","Jiaxing Zhang","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2210.16257v2.pdf","comment":"The experimental results are not sufficient"},{"id":"http://arxiv.org/abs/2212.06560v1","updated":"2022-12-13T13:29:47Z","published":"2022-12-13T13:29:47Z","title":"Exploring Fake News Detection with Heterogeneous Social Media Context\n  Graphs","summary":"  Fake news detection has become a research area that goes way beyond a purely\nacademic interest as it has direct implications on our society as a whole.\nRecent advances have primarily focused on textbased approaches. However, it has\nbecome clear that to be effective one needs to incorporate additional,\ncontextual information such as spreading behaviour of news articles and user\ninteraction patterns on social media. We propose to construct heterogeneous\nsocial context graphs around news articles and reformulate the problem as a\ngraph classification task. Exploring the incorporation of different types of\ninformation (to get an idea as to what level of social context is most\neffective) and using different graph neural network architectures indicates\nthat this approach is highly effective with robust results on a common\nbenchmark dataset.\n","authors":["Gregor Donabauer","Udo Kruschwitz"],"pdf_url":"https://arxiv.org/pdf/2212.06560v1.pdf","comment":"Preprint accepted at the 45th European Conference on Information\n  Retrieval (ECIR 2023)"},{"id":"http://arxiv.org/abs/2212.06556v1","updated":"2022-12-13T13:15:20Z","published":"2022-12-13T13:15:20Z","title":"Localized Latent Updates for Fine-Tuning Vision-Language Models","summary":"  Although massive pre-trained vision-language models like CLIP show impressive\ngeneralization capabilities for many tasks, still it often remains necessary to\nfine-tune them for improved performance on specific datasets. When doing so, it\nis desirable that updating the model is fast and that the model does not lose\nits capabilities on data outside of the dataset, as is often the case with\nclassical fine-tuning approaches. In this work we suggest a lightweight\nadapter, that only updates the models predictions close to seen datapoints. We\ndemonstrate the effectiveness and speed of this relatively simple approach in\nthe context of few-shot learning, where our results both on classes seen and\nunseen during training are comparable with or improve on the state of the art.\n","authors":["Moritz Ibing","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2212.06556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06543v1","updated":"2022-12-13T12:56:55Z","published":"2022-12-13T12:56:55Z","title":"Modelling Stance Detection as Textual Entailment Recognition and\n  Leveraging Measurement Knowledge from Social Sciences","summary":"  Stance detection (SD) can be considered a special case of textual entailment\nrecognition (TER), a generic natural language task. Modelling SD as TER may\noffer benefits like more training data and a more general learning scheme. In\nthis paper, we present an initial empirical analysis of this approach. We apply\nit to a difficult but relevant test case where no existing labelled SD dataset\nis available, because this is where modelling SD as TER may be especially\nhelpful. We also leverage measurement knowledge from social sciences to improve\nmodel performance. We discuss our findings and suggest future research\ndirections.\n","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri"],"pdf_url":"https://arxiv.org/pdf/2212.06543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06522v1","updated":"2022-12-13T12:14:09Z","published":"2022-12-13T12:14:09Z","title":"Distantly-Supervised Named Entity Recognition with Adaptive Teacher\n  Learning and Fine-grained Student Ensemble","summary":"  Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates\nthe data scarcity problem in NER by automatically generating training samples.\nUnfortunately, the distant supervision may induce noisy labels, thus\nundermining the robustness of the learned models and restricting the practical\napplication. To relieve this problem, recent works adopt self-training\nteacher-student frameworks to gradually refine the training labels and improve\nthe generalization ability of NER models. However, we argue that the\nperformance of the current self-training frameworks for DS-NER is severely\nunderestimated by their plain designs, including both inadequate student\nlearning and coarse-grained teacher updating. Therefore, in this paper, we make\nthe first attempt to alleviate these issues by proposing: (1) adaptive teacher\nlearning comprised of joint training of two teacher-student networks and\nconsidering both consistent and inconsistent predictions between two teachers,\nthus promoting comprehensive student learning. (2) fine-grained student\nensemble that updates each fragment of the teacher model with a temporal moving\naverage of the corresponding fragment of the student, which enhances consistent\npredictions on each model fragment against noise. To verify the effectiveness\nof our proposed method, we conduct experiments on four DS-NER datasets. The\nexperimental results demonstrate that our method significantly surpasses\nprevious SOTA methods.\n","authors":["Xiaoye Qu","Jun Zeng","Daizong Liu","Zhefeng Wang","Baoxing Huai","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.06522v1.pdf","comment":"Accepted at AAAI 2023"},{"id":"http://arxiv.org/abs/2205.10558v2","updated":"2022-12-13T11:19:18Z","published":"2022-05-21T10:36:22Z","title":"CORAL: Contextual Response Retrievability Loss Function for Training\n  Dialog Generation Models","summary":"  Natural Language Generation (NLG) represents a large collection of tasks in\nthe field of NLP. While many of these tasks have been tackled well by the\ncross-entropy (CE) loss, the task of dialog generation poses a few unique\nchallenges for this loss function. First, CE loss assumes that for any given\ninput, the only possible output is the one available as the ground truth in the\ntraining dataset. In general, this is not true for any task, as there can be\nmultiple semantically equivalent sentences, each with a different surface form.\nThis problem gets exaggerated further for the dialog generation task, as there\ncan be multiple valid responses (for a given context) that not only have\ndifferent surface forms but are also not semantically equivalent. Second, CE\nloss does not take the context into consideration while processing the response\nand, hence, it treats all ground truths with equal importance irrespective of\nthe context. But, we may want our final agent to avoid certain classes of\nresponses (e.g. bland, non-informative or biased responses) and give relatively\nhigher weightage for more context-specific responses. To circumvent these\nshortcomings of the CE loss, in this paper, we propose a novel loss function,\nCORAL, that directly optimizes recently proposed estimates of human preference\nfor generated responses. Using CORAL, we can train dialog generation models\nwithout assuming non-existence of response other than the ground-truth. Also,\nthe CORAL loss is computed based on both the context and the response.\nExtensive comparisons on two benchmark datasets show that the proposed methods\noutperform strong state-of-the-art baseline models of different sizes.\n","authors":["Bishal Santra","Ravi Ghadia","Arpit Dwivedi","Manish Gupta","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2205.10558v2.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.06468v1","updated":"2022-12-13T10:37:10Z","published":"2022-12-13T10:37:10Z","title":"Lisan: Yemenu, Irqi, Libyan, and Sudanese Arabic Dialect Copora with\n  Morphological Annotations","summary":"  This article presents morphologically-annotated Yemeni, Sudanese, Iraqi, and\nLibyan Arabic dialects Lisan corpora. Lisan features around 1.2 million tokens.\nWe collected the content of the corpora from several social media platforms.\nThe Yemeni corpus (~ 1.05M tokens) was collected automatically from Twitter.\nThe corpora of the other three dialects (~ 50K tokens each) came manually from\nFacebook and YouTube posts and comments.\n  Thirty five (35) annotators who are native speakers of the target dialects\ncarried out the annotations. The annotators segemented all words in the four\ncorpora into prefixes, stems and suffixes and labeled each with different\nmorphological features such as part of speech, lemma, and a gloss in English.\nAn Arabic Dialect Annotation Toolkit ADAT was developped for the purpose of the\nannation. The annotators were trained on a set of guidelines and on how to use\nADAT. We developed ADAT to assist the annotators and to ensure compatibility\nwith SAMA and Curras tagsets. The tool is open source, and the four corpora are\nalso available online.\n","authors":["Mustafa Jarrar","Fadi A Zaraket","Tymaa Hammouda","Daanish Masood Alavi","Martin Waahlisch"],"pdf_url":"https://arxiv.org/pdf/2212.06468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05767v2","updated":"2022-12-13T10:35:12Z","published":"2022-12-12T08:40:04Z","title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and\n  Multi-Modal","summary":"  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n","authors":["Ke Liang","Lingyuan Meng","Meng Liu","Yue Liu","Wenxuan Tu","Siwei Wang","Sihang Zhou","Xinwang Liu","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2212.05767v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2210.13964v2","updated":"2022-12-13T10:12:48Z","published":"2022-10-25T12:48:56Z","title":"Learning to Reuse Distractors to support Multiple Choice Question\n  Generation in Education","summary":"  Multiple choice questions (MCQs) are widely used in digital learning systems,\nas they allow for automating the assessment process. However, due to the\nincreased digital literacy of students and the advent of social media\nplatforms, MCQ tests are widely shared online, and teachers are continuously\nchallenged to create new questions, which is an expensive and time-consuming\ntask. A particularly sensitive aspect of MCQ creation is to devise relevant\ndistractors, i.e., wrong answers that are not easily identifiable as being\nwrong. This paper studies how a large existing set of manually created answers\nand distractors for questions over a variety of domains, subjects, and\nlanguages can be leveraged to help teachers in creating new MCQs, by the smart\nreuse of existing distractors. We built several data-driven models based on\ncontext-aware question and distractor representations, and compared them with\nstatic feature-based models. The proposed models are evaluated with automated\nmetrics and in a realistic user test with teachers. Both automatic and human\nevaluations indicate that context-aware models consistently outperform a static\nfeature-based approach. For our best-performing context-aware model, on average\n3 distractors out of the 10 shown to teachers were rated as high-quality\ndistractors. We create a performance benchmark, and make it public, to enable\ncomparison between different approaches and to introduce a more standardized\nevaluation of the task. The benchmark contains a test of 298 educational\nquestions covering multiple subjects & languages and a 77k multilingual pool of\ndistractor vocabulary for future research.\n","authors":["Semere Kiros Bitew","Amir Hadifar","Lucas Sterckx","Johannes Deleu","Chris Develder","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2210.13964v2.pdf","comment":"24 pages and 4 figures Accepted for publication in IEEE Transactions\n  on Learning technologies"},{"id":"http://arxiv.org/abs/2209.07351v2","updated":"2022-12-13T09:51:19Z","published":"2022-09-15T15:06:20Z","title":"Rethinking Round-Trip Translation for Machine Translation Evaluation","summary":"  Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.\n","authors":["Terry Yue Zhuo","Qiongkai Xu","Xuanli He","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2209.07351v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2210.17209v2","updated":"2022-12-13T09:13:26Z","published":"2022-10-31T10:45:18Z","title":"The Effect of Multiple Replies for Natural Language Generation Chatbots","summary":"  In this research, by responding to users' utterances with multiple replies to\ncreate a group chat atmosphere, we alleviate the problem that Natural Language\nGeneration chatbots might reply with inappropriate content, thus causing a bad\nuser experience. Because according to our findings, users tend to pay attention\nto appropriate replies and ignore inappropriate replies. We conducted a 2\n(single reply vs. five replies) x 2 (anonymous avatar vs. anime avatar)\nrepeated measures experiment to compare the chatting experience in different\nconditions. The result shows that users will have a better chatting experience\nwhen receiving multiple replies at once from the NLG model compared to the\nsingle reply. Furthermore, according to the effect size of our result, to\nimprove the chatting experience for NLG chatbots which is single reply and\nanonymous avatar, providing five replies will have more benefits than setting\nan anime avatar.\n","authors":["Eason Chen"],"pdf_url":"https://arxiv.org/pdf/2210.17209v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2212.01393v2","updated":"2022-12-13T07:33:16Z","published":"2022-12-02T18:58:51Z","title":"Continual Learning for On-Device Speech Recognition using Disentangled\n  Conformers","summary":"  Automatic speech recognition research focuses on training and evaluating on\nstatic datasets. Yet, as speech models are increasingly deployed on personal\ndevices, such models encounter user-specific distributional shifts. To simulate\nthis real-world scenario, we introduce LibriContinual, a continual learning\nbenchmark for speaker-specific domain adaptation derived from LibriVox\naudiobooks, with data corresponding to 118 individual speakers and 6 train\nsplits per speaker of different sizes. Additionally, current speech recognition\nmodels and continual learning algorithms are not optimized to be\ncompute-efficient. We adapt a general-purpose training algorithm NetAug for ASR\nand create a novel Conformer variant called the DisConformer (Disentangled\nConformer). This algorithm produces ASR models consisting of a frozen 'core'\nnetwork for general-purpose use and several tunable 'augment' networks for\nspeaker-specific tuning. Using such models, we propose a novel\ncompute-efficient continual learning algorithm called DisentangledCL. Our\nexperiments show that the DisConformer models significantly outperform\nbaselines on general ASR i.e. LibriSpeech (15.58% rel. WER on test-other). On\nspeaker-specific LibriContinual they significantly outperform\ntrainable-parameter-matched baselines (by 20.65% rel. WER on test) and even\nmatch fully finetuned baselines in some settings.\n","authors":["Anuj Diwan","Ching-Feng Yeh","Wei-Ning Hsu","Paden Tomasello","Eunsol Choi","David Harwath","Abdelrahman Mohamed"],"pdf_url":"https://arxiv.org/pdf/2212.01393v2.pdf","comment":"8 pages, 2 figures. Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2211.16773v2","updated":"2022-12-13T06:56:04Z","published":"2022-11-30T06:27:46Z","title":"Keywords Reinforcement LM: Improving End-to-End Response Generation in\n  Task Oriented Dialog","summary":"  In task-oriented dialogs such as MultiWoZ (Budzianowski et al., 2018), an\ninformative and successful system response needs to include key information\nsuch as the phone number of a hotel. Therefore, we hypothesize that by asking\nthe model to focus on generating more key quantities correctly, it can achieve\nbetter overall performance. In this paper, we propose a new training algorithm,\nKeywords Reinforcement Language Modeling (KRLM), that aims to use a\nfine-grained reward function for each token and a new per-token Reinforcement\nLearning procedure to help the model learn keywords generation more robustly\nduring inference. Empirical results show that our proposed KRLM training\nalgorithm can achieve state-of-the-art performance on the inform rate, success\nrate, and combined score in the MultiWoZ benchmark dataset.\n","authors":["Xiao Yu","Qingyang Wu","Kun Qian","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2211.16773v2.pdf","comment":"added algorithm analysis, and supplemental information such as error\n  examples and more training/validation curves"},{"id":"http://arxiv.org/abs/2210.12360v2","updated":"2022-12-13T06:38:58Z","published":"2022-10-22T05:48:02Z","title":"Prompt-Tuning Can Be Much Better Than Fine-Tuning on Cross-lingual\n  Understanding With Multilingual Language Models","summary":"  Pre-trained multilingual language models show significant performance gains\nfor zero-shot cross-lingual model transfer on a wide range of natural language\nunderstanding (NLU) tasks. Previously, for zero-shot cross-lingual evaluation,\npre-trained models are only fine-tuned on English data and tested on a variety\nof target languages. In this paper, we do cross-lingual evaluation on various\nNLU tasks (sentence classification, sequence labeling, question answering)\nusing prompt-tuning and compare it with fine-tuning. The results show that\nprompt tuning achieves much better cross-lingual transfer than fine-tuning\nacross datasets, with only 0.1% to 0.3% tuned parameters. Additionally, we\ndemonstrate through the analysis that prompt tuning can have better\ncross-lingual transferability of representations on downstream tasks with\nbetter aligned decision boundaries.\n","authors":["Lifu Tu","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2210.12360v2.pdf","comment":"EMNLP 2022. Code link is added"},{"id":"http://arxiv.org/abs/2212.06397v1","updated":"2022-12-13T06:26:25Z","published":"2022-12-13T06:26:25Z","title":"Style-Label-Free: Cross-Speaker Style Transfer by Quantized VAE and\n  Speaker-wise Normalization in Speech Synthesis","summary":"  Cross-speaker style transfer in speech synthesis aims at transferring a style\nfrom source speaker to synthesised speech of a target speaker's timbre. Most\nprevious approaches rely on data with style labels, but manually-annotated\nlabels are expensive and not always reliable. In response to this problem, we\npropose Style-Label-Free, a cross-speaker style transfer method, which can\nrealize the style transfer from source speaker to target speaker without style\nlabels. Firstly, a reference encoder structure based on quantized variational\nautoencoder (Q-VAE) and style bottleneck is designed to extract discrete style\nrepresentations. Secondly, a speaker-wise batch normalization layer is proposed\nto reduce the source speaker leakage. In order to improve the style extraction\nability of the reference encoder, a style invariant and contrastive data\naugmentation method is proposed. Experimental results show that the method\noutperforms the baseline. We provide a website with audio samples.\n","authors":["Chunyu Qiang","Peng Yang","Hao Che","Xiaorui Wang","Zhongyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2212.06397v1.pdf","comment":"Published to ISCSLP 2022"},{"id":"http://arxiv.org/abs/2212.06385v1","updated":"2022-12-13T05:46:40Z","published":"2022-12-13T05:46:40Z","title":"TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models\n  of Different Modalities","summary":"  Recently, the success of pre-training in text domain has been fully extended\nto vision, audio, and cross-modal scenarios. The proposed pre-training models\nof different modalities are showing a rising trend of homogeneity in their\nmodel structures, which brings the opportunity to implement different\npre-training models within a uniform framework. In this paper, we present\nTencentPretrain, a toolkit supporting pre-training models of different\nmodalities. The core feature of TencentPretrain is the modular design. The\ntoolkit uniformly divides pre-training models into 5 components: embedding,\nencoder, target embedding, decoder, and target. As almost all of common modules\nare provided in each component, users can choose the desired modules from\ndifferent components to build a complete pre-training model. The modular design\nenables users to efficiently reproduce existing pre-training models or build\nbrand-new one. We test the toolkit on text, vision, and audio benchmarks and\nshow that it can match the performance of the original implementations.\n","authors":["Zhe Zhao","Yudong Li","Cheng Hou","Jing Zhao","Rong Tian","Weijie Liu","Yiren Chen","Ningyuan Sun","Haoyan Liu","Weiquan Mao","Han Guo","Weigang Guo","Taiqiang Wu","Tao Zhu","Wenhang Shi","Chen Chen","Shan Huang","Sihong Chen","Liqun Liu","Feifei Li","Xiaoshuai Chen","Xingwu Sun","Zhanhui Kang","Xiaoyong Du","Linlin Shen","Kimmo Yan"],"pdf_url":"https://arxiv.org/pdf/2212.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.01311v3","updated":"2022-12-13T05:37:14Z","published":"2022-03-02T18:56:20Z","title":"HighMMT: Quantifying Modality & Interaction Heterogeneity for\n  High-Modality Representation Learning","summary":"  Many real-world problems are inherently multimodal, from the communicative\nmodalities humans use to express social and emotional states to the force,\nproprioception, and visual sensors ubiquitous on robots. While there has been\nan explosion of interest in multimodal representation learning, these methods\nare still largely focused on a small set of modalities, primarily in the\nlanguage, vision, and audio space. In order to accelerate generalization\ntowards diverse and understudied modalities, this paper studies efficient\nrepresentation learning for high-modality scenarios. Since adding new models\nfor every new modality or task becomes prohibitively expensive, a critical\ntechnical challenge is heterogeneity quantification: how can we measure which\nmodalities encode similar information and interactions in order to permit\nparameter sharing with previous modalities? We propose two new\ninformation-theoretic metrics for heterogeneity quantification: (1) modality\nheterogeneity studies how similar 2 modalities $\\{X_1,X_2\\}$ are by measuring\nhow much information can be transferred from $X_1$ to $X_2$, while (2)\ninteraction heterogeneity studies how similarly pairs of modalities\n$\\{X_1,X_2\\}, \\{X_3,X_4\\}$ interact by measuring how much interaction\ninformation can be transferred from $\\{X_1,X_2\\}$ to $\\{X_3,X_4\\}$. We show the\nimportance of these proposed metrics in high-modality scenarios as a way to\nautomatically prioritize the fusion of modalities that contain unique\ninformation or interactions. The result is a single model, HighMMT, that scales\nup to $10$ modalities and $15$ tasks from $5$ different research areas. Not\nonly does HighMMT outperform prior methods on the tradeoff between performance\nand efficiency, it also demonstrates a crucial scaling behavior: performance\ncontinues to improve with each modality added, and transfers to entirely new\nmodalities and tasks during fine-tuning.\n","authors":["Paul Pu Liang","Yiwei Lyu","Xiang Fan","Jeffrey Tsaw","Yudong Liu","Shentong Mo","Dani Yogatama","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2203.01311v3.pdf","comment":"Code available at https://github.com/pliang279/HighMMT"},{"id":"http://arxiv.org/abs/2212.06383v1","updated":"2022-12-13T05:36:18Z","published":"2022-12-13T05:36:18Z","title":"Towards a general purpose machine translation system for Sranantongo","summary":"  Machine translation for Sranantongo (Sranan, srn), a low-resource Creole\nlanguage spoken predominantly in Surinam, is virgin territory. In this study we\ncreate a general purpose machine translation system for srn. In order to\nfacilitate this research, we introduce the SRNcorpus, a collection of parallel\nDutch (nl) to srn and monolingual srn data. We experiment with a wide range of\nproven machine translation methods. Our results demonstrate a strong baseline\nmachine translation system for srn.\n","authors":["Just Zwennicker","David Stap"],"pdf_url":"https://arxiv.org/pdf/2212.06383v1.pdf","comment":"Accepted to WiNLP (EMNLP). 2 pages"},{"id":"http://arxiv.org/abs/2212.06373v1","updated":"2022-12-13T05:12:40Z","published":"2022-12-13T05:12:40Z","title":"InferEM: Inferring the Speaker's Intention for Empathetic Dialogue\n  Generation","summary":"  Current approaches to empathetic response generation typically encode the\nentire dialogue history directly and put the output into a decoder to generate\nfriendly feedback. These methods focus on modelling contextual information but\nneglect capturing the direct intention of the speaker. We argue that the last\nutterance in the dialogue empirically conveys the intention of the speaker.\nConsequently, we propose a novel model named InferEM for empathetic response\ngeneration. We separately encode the last utterance and fuse it with the entire\ndialogue through multi-head attention based intention fusion module to capture\nthe speaker's intention. Besides, we utilize previous utterances to predict the\nlast utterance, which simulates human's psychology to guess what the\ninterlocutor may speak in advance. To balance the optimizing rates of the\nutterance prediction and response generation, a multi-task learning strategy is\ndesigned for InferEM. Experimental results demonstrate the plausibility and\nvalidity of InferEM in improving empathetic expression.\n","authors":["Guoqing Lv","Xiaoping Wang","Jiang Li","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2212.06373v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.05857v2","updated":"2022-12-13T04:42:44Z","published":"2022-12-12T12:48:33Z","title":"Automated ICD Coding using Extreme Multi-label Long Text\n  Transformer-based Models","summary":"  Background: Encouraged by the success of pretrained Transformer models in\nmany natural language processing tasks, their use for International\nClassification of Diseases (ICD) coding tasks is now actively being explored.\nIn this study, we investigate three types of Transformer-based models, aiming\nto address the extreme label set and long text classification challenges that\nare posed by automated ICD coding tasks. Methods: The Transformer-based model\nPLM-ICD achieved the current state-of-the-art (SOTA) performance on the ICD\ncoding benchmark dataset MIMIC-III. It was chosen as our baseline model to be\nfurther optimised. XR-Transformer, the new SOTA model in the general extreme\nmulti-label text classification domain, and XR-LAT, a novel adaptation of the\nXR-Transformer model, were also trained on the MIMIC-III dataset. XR-LAT is a\nrecursively trained model chain on a predefined hierarchical code tree with\nlabel-wise attention, knowledge transferring and dynamic negative sampling\nmechanisms. Results: Our optimised PLM-ICD model, which was trained with longer\ntotal and chunk sequence lengths, significantly outperformed the current SOTA\nPLM-ICD model, and achieved the highest micro-F1 score of 60.8%. The\nXR-Transformer model, although SOTA in the general domain, did not perform well\nacross all metrics. The best XR-LAT based model obtained results that were\ncompetitive with the current SOTA PLM-ICD model, including improving the\nmacro-AUC by 2.1%. Conclusion: Our optimised PLM-ICD model is the new SOTA\nmodel for automated ICD coding on the MIMIC-III dataset, while our novel XR-LAT\nmodel performs competitively with the previous SOTA PLM-ICD model.\n","authors":["Leibo Liu","Oscar Perez-Concha","Anthony Nguyen","Vicki Bennett","Louisa Jorm"],"pdf_url":"https://arxiv.org/pdf/2212.05857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.06378v5","updated":"2022-12-13T04:14:48Z","published":"2021-04-13T17:32:51Z","title":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question\n  Answering","summary":"  The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate our model on QA benchmarks in the commonsense (CommonsenseQA,\nOpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing\nLM and LM+KG models, and exhibits capabilities to perform interpretable and\nstructured reasoning, e.g., correctly handling negation in questions.\n","authors":["Michihiro Yasunaga","Hongyu Ren","Antoine Bosselut","Percy Liang","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2104.06378v5.pdf","comment":"NAACL 2021. Code & data available at\n  https://github.com/michiyasunaga/qagnn"},{"id":"http://arxiv.org/abs/2212.05525v2","updated":"2022-12-13T03:19:37Z","published":"2022-12-11T15:45:26Z","title":"Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images","summary":"  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance is\ninefficient, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained Transformer-based instance-level model\nTrOCR with randomly cropped image chunks, and gradually increase the image\nchunk size to generalize the recognition ability from instance images to\nfull-page images. In our experiments on the SROIE receipt OCR dataset, the\nmodel finetuned with our strategy achieved 64.4 F1-score and a 22.8% character\nerror rates (CER) on the word-level and character-level metrics, respectively,\nwhich outperforms the baseline results with 48.5 F1-score and 50.6% CER. The\nbest model, which splits the full image into 15 equally sized chunks, gives\n87.8 F1-score and 4.98% CER with minimal additional pre or post-processing of\nthe output. Moreover, the characters in the generated document-level sequences\nare arranged in the reading order, which is practical for real-world\napplications.\n","authors":["Hongkuan Zhang","Edward Whittaker","Ikuo Kitagishi"],"pdf_url":"https://arxiv.org/pdf/2212.05525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06346v1","updated":"2022-12-13T03:00:36Z","published":"2022-12-13T03:00:36Z","title":"The Massively Multilingual Natural Language Understanding 2022\n  (MMNLU-22) Workshop and Competition","summary":"  Despite recent progress in Natural Language Understanding (NLU), the creation\nof multilingual NLU systems remains a challenge. It is common to have NLU\nsystems limited to a subset of languages due to lack of available data. They\nalso often vary widely in performance. We launch a three-phase approach to\naddress the limitations in NLU and help propel NLU technology to new heights.\nWe release a 52 language dataset called the Multilingual Amazon SLU resource\npackage (SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation, or MASSIVE, in an effort to address parallel data availability for\nvoice assistants. We organize the Massively Multilingual NLU 2022 Challenge to\nprovide a competitive environment and push the state-of-the art in the\ntransferability of models into other languages. Finally, we host the first\nMassively Multilingual NLU workshop which brings these components together. The\nMMNLU workshop seeks to advance the science behind multilingual NLU by\nproviding a platform for the presentation of new research in the field and\nconnecting teams working on this research direction. This paper summarizes the\ndataset, workshop and the competition and the findings of each phase.\n","authors":["Christopher Hench","Charith Peris","Jack FitzGerald","Kay Rottmann"],"pdf_url":"https://arxiv.org/pdf/2212.06346v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2212.06295v1","updated":"2022-12-13T00:29:45Z","published":"2022-12-13T00:29:45Z","title":"Despite \"super-human\" performance, current LLMs are unsuited for\n  decisions about ethics and safety","summary":"  Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.\n","authors":["Joshua Albrecht","Ellie Kitanidis","Abraham J. Fetterman"],"pdf_url":"https://arxiv.org/pdf/2212.06295v1.pdf","comment":"ML Safety Workshop, NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07796v1","updated":"2022-12-13T19:17:36Z","published":"2022-12-13T19:17:36Z","title":"CREPE: Can Vision-Language Foundation Models Reason Compositionally?","summary":"  A fundamental characteristic common to both human vision and natural language\nis their compositional nature. Yet, despite the performance gains contributed\nby large vision and language pretraining, we find that - across 6 architectures\ntrained with 4 algorithms on massive datasets - they exhibit little\ncompositionality. To arrive at this conclusion, we introduce a new\ncompositionality evaluation benchmark CREPE which measures two important\naspects of compositionality identified by cognitive science literature:\nsystematicity and productivity. To measure systematicity, CREPE consists of\nthree test datasets. The three test sets are designed to test models trained on\nthree of the popular training datasets: CC-12M, YFCC-15M, and LAION-400M. They\ncontain 385K, 385K, and 373K image-text pairs and 237K, 210K, and 178K hard\nnegative captions. To test productivity, CREPE contains 17K image-text pairs\nwith nine different complexities plus 246K hard negative captions with atomic,\nswapping, and negation foils. The datasets are generated by repurposing the\nVisual Genome scene graphs and region descriptions and applying handcrafted\ntemplates and GPT-3. For systematicity, we find that model performance\ndecreases consistently when novel compositions dominate the retrieval set, with\nRecall@1 dropping by up to 8%. For productivity, models' retrieval success\ndecays as complexity increases, frequently nearing random chance at high\ncomplexity. These results hold regardless of model and training dataset size.\n","authors":["Zixian Ma","Jerry Hong","Mustafa Omer Gul","Mona Gandhi","Irena Gao","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2212.07796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08072v1","updated":"2022-12-13T19:06:00Z","published":"2022-12-13T19:06:00Z","title":"Foresight -- Deep Generative Modelling of Patient Timelines using\n  Electronic Health Records","summary":"  Electronic Health Records (EHRs) hold detailed longitudinal information about\neach patient's health status and general clinical history, a large portion of\nwhich is stored within the unstructured text. Temporal modelling of this\nmedical history, which considers the sequence of events, can be used to\nforecast and simulate future events, estimate risk, suggest alternative\ndiagnoses or forecast complications. While most prediction approaches use\nmainly structured data or a subset of single-domain forecasts and outcomes, we\nprocessed the entire free-text portion of EHRs for longitudinal modelling. We\npresent Foresight, a novel GPT3-based pipeline that uses NER+L tools (i.e.\nMedCAT) to convert document text into structured, coded concepts, followed by\nproviding probabilistic forecasts for future medical events such as disorders,\nmedications, symptoms and interventions. Since large portions of EHR data are\nin text form, such an approach benefits from a granular and detailed view of a\npatient while introducing modest additional noise. On tests in two large UK\nhospitals (King's College Hospital, South London and Maudsley) and the US\nMIMIC-III dataset precision@10 of 0.80, 0.81 and 0.91 was achieved for\nforecasting the next biomedical concept. Foresight was also validated on 34\nsynthetic patient timelines by 5 clinicians and achieved relevancy of 97% for\nthe top forecasted candidate disorder. Foresight can be easily trained and\ndeployed locally as it only requires free-text data (as a minimum). As a\ngenerative model, it can simulate follow-on disorders, medications and\ninterventions for as many steps as required. Foresight is a general-purpose\nmodel for biomedical concept modelling that can be used for real-world risk\nestimation, virtual trials and clinical research to study the progression of\ndiseases, simulate interventions and counterfactuals, and for educational\npurposes.\n","authors":["Zeljko Kraljevic","Dan Bean","Anthony Shek","Rebecca Bendayan","Joshua Au Yeung","Alexander Deng","Alfie Baston","Jack Ross","Esther Idowu","James T Teo","Richard J Dobson"],"pdf_url":"https://arxiv.org/pdf/2212.08072v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2204.05798v2","updated":"2022-12-13T23:32:39Z","published":"2022-04-12T13:32:31Z","title":"Hypercomplex Neural Architectures for Multi-View Breast Cancer\n  Classification","summary":"  Traditionally, deep learning methods for breast cancer classification perform\na single-view analysis. However, radiologists simultaneously analyze all four\nviews that compose a mammography exam, owing to the correlations contained in\nmammography views, which present crucial information for identifying tumors. In\nlight of this, some studies have started to propose multi-view methods.\nNevertheless, in such existing architectures, mammogram views are processed as\nindependent images by separate convolutional branches, thus losing correlations\namong them. To overcome such limitations, in this paper we propose a novel\napproach for multi-view breast cancer classification based on parameterized\nhypercomplex neural networks. Thanks to hypercomplex algebra properties, our\nnetworks are able to model, and thus leverage, existing correlations between\nthe different views that comprise a mammogram, thus mimicking the reading\nprocess performed by clinicians. The proposed methods are able to handle the\ninformation of a patient altogether without breaking the multi-view nature of\nthe exam. We define architectures designed to process two-view exams, namely\nPHResNets, and four-view exams, i.e., PHYSEnet and PHYBOnet. Through an\nextensive experimental evaluation conducted with publicly available datasets,\nwe demonstrate that our proposed models clearly outperform real-valued\ncounterparts and also state-of-the-art methods, proving that breast cancer\nclassification benefits from the proposed multi-view architectures. We also\nassess the method's robustness beyond mammogram analysis by considering\ndifferent benchmarks, as well as a finer-scaled task such as segmentation. Full\ncode and pretrained models for complete reproducibility of our experiments are\nfreely available at: https://github.com/ispamm/PHBreast.\n","authors":["Eleonora Lopez","Eleonora Grassucci","Martina Valleriani","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2204.05798v2.pdf","comment":"This paper has been submitted to IEEE Transactions on Neural Networks\n  and Learning Systems"},{"id":"http://arxiv.org/abs/2210.13431v2","updated":"2022-12-13T21:47:34Z","published":"2022-10-24T17:46:47Z","title":"InstructRL: Instruction-Following Agents with Jointly Pre-Trained\n  Vision-Language Models","summary":"  Humans are excellent at understanding language and vision to accomplish a\nwide range of tasks. In contrast, creating general instruction-following\nembodied agents remains a difficult challenge. Prior work that uses pure\nlanguage-only models lack visual grounding, making it difficult to connect\nlanguage instructions with visual observations. On the other hand, methods that\nuse pre-trained vision-language models typically come with divided language and\nvisual representations, requiring designing specialized network architecture to\nfuse them together. We propose a simple yet effective model for robots to solve\ninstruction-following tasks in vision-based environments. Our \\ours method\nconsists of a multimodal transformer that encodes visual observations and\nlanguage instructions, and a policy transformer that predicts actions based on\nencoded representations. The multimodal transformer is pre-trained on millions\nof image-text pairs and natural language text, thereby producing generic\ncross-modal representations of observations and instructions. The policy\ntransformer keeps track of the full history of observations and actions, and\npredicts actions autoregressively. We show that this unified transformer model\noutperforms all state-of-the-art pre-trained or trained-from-scratch methods in\nboth single-task and multi-task settings. Our model also shows better model\nscalability and generalization ability than prior work.\n","authors":["Hao Liu","Lisa Lee","Kimin Lee","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2210.13431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06909v1","updated":"2022-12-13T21:25:11Z","published":"2022-12-13T21:25:11Z","title":"Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image\n  Inpainting","summary":"  Text-guided image editing can have a transformative impact in supporting\ncreative applications. A key challenge is to generate edits that are faithful\nto input text prompts, while consistent with input images. We present Imagen\nEditor, a cascaded diffusion model built, by fine-tuning Imagen on text-guided\nimage inpainting. Imagen Editor's edits are faithful to the text prompts, which\nis accomplished by using object detectors to propose inpainting masks during\ntraining. In addition, Imagen Editor captures fine details in the input image\nby conditioning the cascaded pipeline on the original high resolution image. To\nimprove qualitative and quantitative evaluation, we introduce EditBench, a\nsystematic benchmark for text-guided image inpainting. EditBench evaluates\ninpainting edits on natural and generated images exploring objects, attributes,\nand scenes. Through extensive human evaluation on EditBench, we find that\nobject-masking during training leads to across-the-board improvements in\ntext-image alignment -- such that Imagen Editor is preferred over DALL-E 2 and\nStable Diffusion -- and, as a cohort, these models are better at\nobject-rendering than text-rendering, and handle material/color/size attributes\nbetter than count/shape attributes.\n","authors":["Su Wang","Chitwan Saharia","Ceslee Montgomery","Jordi Pont-Tuset","Shai Noy","Stefano Pellegrini","Yasumasa Onoe","Sarah Laszlo","David J. Fleet","Radu Soricut","Jason Baldridge","Mohammad Norouzi","Peter Anderson","William Chan"],"pdf_url":"https://arxiv.org/pdf/2212.06909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06896v1","updated":"2022-12-13T20:48:06Z","published":"2022-12-13T20:48:06Z","title":"In-Season Crop Progress in Unsurveyed Regions using Networks Trained on\n  Synthetic Data","summary":"  Many commodity crops have growth stages during which they are particularly\nvulnerable to stress-induced yield loss. In-season crop progress information is\nuseful for quantifying crop risk, and satellite remote sensing (RS) can be used\nto track progress at regional scales. At present, all existing RS-based crop\nprogress estimation (CPE) methods which target crop-specific stages rely on\nground truth data for training/calibration. This reliance on ground survey data\nconfines CPE methods to surveyed regions, limiting their utility. In this\nstudy, a new method is developed for conducting RS-based in-season CPE in\nunsurveyed regions by combining data from surveyed regions with synthetic crop\nprogress data generated for an unsurveyed region. Corn-growing zones in\nArgentina were used as surrogate 'unsurveyed' regions. Existing weather\ngeneration, crop growth, and optical radiative transfer models were linked to\nproduce synthetic weather, crop progress, and canopy reflectance data. A neural\nnetwork (NN) method based upon bi-directional Long Short-Term Memory was\ntrained separately on surveyed data, synthetic data, and two different\ncombinations of surveyed and synthetic data. A stopping criterion was developed\nwhich uses the weighted divergence of surveyed and synthetic data validation\nloss. Net F1 scores across all crop progress stages increased by 8.7% when\ntrained on a combination of surveyed region and synthetic data, and overall\nperformance was only 21% lower than when the NN was trained on surveyed data\nand applied in the US Midwest. Performance gain from synthetic data was\ngreatest in zones with dual planting windows, while the inclusion of surveyed\nregion data from the US Midwest helped mitigate NN sensitivity to noise in NDVI\ndata. Overall results suggest in-season CPE in other unsurveyed regions may be\npossible with increased quantity and variety of synthetic crop progress data.\n","authors":["George Worrall","Jasmeet Judge"],"pdf_url":"https://arxiv.org/pdf/2212.06896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06872v1","updated":"2022-12-13T19:38:13Z","published":"2022-12-13T19:38:13Z","title":"Examining the Difference Among Transformers and CNNs with Explanation\n  Methods","summary":"  We propose a methodology that systematically applies deep explanation\nalgorithms on a dataset-wide basis, to compare different types of visual\nrecognition backbones, such as convolutional networks (CNNs), global attention\nnetworks, and local attention networks. Examination of both qualitative\nvisualizations and quantitative statistics across the dataset helps us to gain\nintuitions that are not just anecdotal, but are supported by the statistics\ncomputed on the entire dataset. Specifically, we propose two methods. The first\none, sub-explanation counting, systematically searches for minimally-sufficient\nexplanations of all images and count the amount of sub-explanations for each\nnetwork. The second one, called cross-testing, computes salient regions using\none network and then evaluates the performance by only showing these regions as\nan image to other networks. Through a combination of qualitative insights and\nquantitative statistics, we illustrate that 1) there are significant\ndifferences between the salient features of CNNs and attention models; 2) the\nocclusion-robustness in local attention models and global attention models may\ncome from different decision-making mechanisms.\n","authors":["Mingqi Jiang","Saeed Khorram","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2212.06872v1.pdf","comment":"28 pages with 39 figures, uses cvpr.sty"},{"id":"http://arxiv.org/abs/2212.06870v1","updated":"2022-12-13T19:30:03Z","published":"2022-12-13T19:30:03Z","title":"MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare","summary":"  We introduce MegaPose, a method to estimate the 6D pose of novel objects,\nthat is, objects unseen during training. At inference time, the method only\nassumes knowledge of (i) a region of interest displaying the object in the\nimage and (ii) a CAD model of the observed object. The contributions of this\nwork are threefold. First, we present a 6D pose refiner based on a\nrender&compare strategy which can be applied to novel objects. The shape and\ncoordinate system of the novel object are provided as inputs to the network by\nrendering multiple synthetic views of the object's CAD model. Second, we\nintroduce a novel approach for coarse pose estimation which leverages a network\ntrained to classify whether the pose error between a synthetic rendering and an\nobserved image of the same object can be corrected by the refiner. Third, we\nintroduce a large-scale synthetic dataset of photorealistic images of thousands\nof objects with diverse visual and shape properties and show that this\ndiversity is crucial to obtain good generalization performance on novel\nobjects. We train our approach on this large synthetic dataset and apply it\nwithout retraining to hundreds of novel objects in real images from several\npose estimation benchmarks. Our approach achieves state-of-the-art performance\non the ModelNet and YCB-Video datasets. An extensive evaluation on the 7 core\ndatasets of the BOP challenge demonstrates that our approach achieves\nperformance competitive with existing approaches that require access to the\ntarget objects during training. Code, dataset and trained models are available\non the project page: https://megapose6d.github.io/.\n","authors":["Yann Labbé","Lucas Manuelli","Arsalan Mousavian","Stephen Tyree","Stan Birchfield","Jonathan Tremblay","Justin Carpentier","Mathieu Aubry","Dieter Fox","Josef Sivic"],"pdf_url":"https://arxiv.org/pdf/2212.06870v1.pdf","comment":"CoRL 2022"},{"id":"http://arxiv.org/abs/2212.06868v1","updated":"2022-12-13T19:24:08Z","published":"2022-12-13T19:24:08Z","title":"Deep Image Style Transfer from Freeform Text","summary":"  This paper creates a novel method of deep neural style transfer by generating\nstyle images from freeform user text input. The language model and style\ntransfer model form a seamless pipeline that can create output images with\nsimilar losses and improved quality when compared to baseline style transfer\nmethods. The language model returns a closely matching image given a style text\nand description input, which is then passed to the style transfer model with an\ninput content image to create a final output. A proof-of-concept tool is also\ndeveloped to integrate the models and demonstrate the effectiveness of deep\nimage style transfer from freeform text.\n","authors":["Tejas Santanam","Mengyang Liu","Jiangyue Yu","Zhaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2212.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06858v1","updated":"2022-12-13T19:02:35Z","published":"2022-12-13T19:02:35Z","title":"LidarCLIP or: How I Learned to Talk to Point Clouds","summary":"  Research connecting text and images has recently seen several breakthroughs,\nwith models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection\nbetween text and other visual modalities, such as lidar data, has received less\nattention, prohibited by the lack of text-lidar datasets. In this work, we\npropose LidarCLIP, a mapping from automotive point clouds to a pre-existing\nCLIP embedding space. Using image-lidar pairs, we supervise a point cloud\nencoder with the image CLIP embeddings, effectively relating text and lidar\ndata with the image domain as an intermediary. We show the effectiveness of\nLidarCLIP by demonstrating that lidar-based retrieval is generally on par with\nimage-based retrieval, but with complementary strengths and weaknesses. By\ncombining image and lidar features, we improve upon both single-modality\nmethods and enable a targeted search for challenging detection scenarios under\nadverse sensor conditions. We also use LidarCLIP as a tool to investigate\nfundamental lidar capabilities through natural language. Finally, we leverage\nour compatibility with CLIP to explore a range of applications, such as point\ncloud captioning and lidar-to-image generation, without any additional\ntraining. We hope LidarCLIP can inspire future work to dive deeper into\nconnections between text and point cloud understanding. Code and trained models\navailable at https://github.com/atonderski/lidarclip.\n","authors":["Georg Hess","Adam Tonderski","Christoffer Petersson","Lennart Svensson","Kalle Åström"],"pdf_url":"https://arxiv.org/pdf/2212.06858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06826v1","updated":"2022-12-13T18:59:59Z","published":"2022-12-13T18:59:59Z","title":"Look Before You Match: Instance Understanding Matters in Video Object\n  Segmentation","summary":"  Exploring dense matching between the current frame and past frames for\nlong-range context modeling, memory-based methods have demonstrated impressive\nresults in video object segmentation (VOS) recently. Nevertheless, due to the\nlack of instance understanding ability, the above approaches are oftentimes\nbrittle to large appearance variations or viewpoint changes resulted from the\nmovement of objects and cameras. In this paper, we argue that instance\nunderstanding matters in VOS, and integrating it with memory-based matching can\nenjoy the synergy, which is intuitively sensible from the definition of VOS\ntask, \\ie, identifying and segmenting object instances within the video.\nTowards this goal, we present a two-branch network for VOS, where the\nquery-based instance segmentation (IS) branch delves into the instance details\nof the current frame and the VOS branch performs spatial-temporal matching with\nthe memory bank. We employ the well-learned object queries from IS branch to\ninject instance-specific information into the query key, with which the\ninstance-augmented matching is further performed. In addition, we introduce a\nmulti-path fusion block to effectively combine the memory readout with\nmulti-scale features from the instance segmentation decoder, which incorporates\nhigh-resolution instance-aware features to produce final segmentation results.\nOur method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6%\nand 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3%\nand 86.3%), outperforming alternative methods by clear margins.\n","authors":["Junke Wang","Dongdong Chen","Zuxuan Wu","Chong Luo","Chuanxin Tang","Xiyang Dai","Yucheng Zhao","Yujia Xie","Lu Yuan","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2212.06826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06822v1","updated":"2022-12-13T18:58:21Z","published":"2022-12-13T18:58:21Z","title":"Adversarial Attacks and Defences for Skin Cancer Classification","summary":"  There has been a concurrent significant improvement in the medical images\nused to facilitate diagnosis and the performance of machine learning techniques\nto perform tasks such as classification, detection, and segmentation in recent\nyears. As a result, a rapid increase in the usage of such systems can be\nobserved in the healthcare industry, for instance in the form of medical image\nclassification systems, where these models have achieved diagnostic parity with\nhuman physicians. One such application where this can be observed is in\ncomputer vision tasks such as the classification of skin lesions in\ndermatoscopic images. However, as stakeholders in the healthcare industry, such\nas insurance companies, continue to invest extensively in machine learning\ninfrastructure, it becomes increasingly important to understand the\nvulnerabilities in such systems. Due to the highly critical nature of the tasks\nbeing carried out by these machine learning models, it is necessary to analyze\ntechniques that could be used to take advantage of these vulnerabilities and\nmethods to defend against them. This paper explores common adversarial attack\ntechniques. The Fast Sign Gradient Method and Projected Descent Gradient are\nused against a Convolutional Neural Network trained to classify dermatoscopic\nimages of skin lesions. Following that, it also discusses one of the most\npopular adversarial defense techniques, adversarial training. The performance\nof the model that has been trained on adversarial examples is then tested\nagainst the previously mentioned attacks, and recommendations to improve neural\nnetworks robustness are thus provided based on the results of the experiment.\n","authors":["Vinay Jogani","Joy Purohit","Ishaan Shivhare","Samina Attari","Shraddha Surtkar"],"pdf_url":"https://arxiv.org/pdf/2212.06822v1.pdf","comment":"6 pages, 7 figures, 2 tables, 2nd International Conference for\n  Advancement in Technology (ICONAT 2023), Goa, India"},{"id":"http://arxiv.org/abs/2212.06820v1","updated":"2022-12-13T18:57:33Z","published":"2022-12-13T18:57:33Z","title":"Structured 3D Features for Reconstructing Relightable and Animatable\n  Avatars","summary":"  We introduce Structured 3D Features, a model based on a novel implicit 3D\nrepresentation that pools pixel-aligned image features onto dense 3D points\nsampled from a parametric, statistical human mesh surface. The 3D points have\nassociated semantics and can move freely in 3D space. This allows for optimal\ncoverage of the person of interest, beyond just the body shape, which in turn,\nadditionally helps modeling accessories, hair, and loose clothing. Owing to\nthis, we present a complete 3D transformer-based attention framework which,\ngiven a single image of a person in an unconstrained pose, generates an\nanimatable 3D reconstruction with albedo and illumination decomposition, as a\nresult of a single end-to-end model, trained semi-supervised, and with no\nadditional postprocessing. We show that our S3F model surpasses the previous\nstate-of-the-art on various tasks, including monocular 3D reconstruction, as\nwell as albedo and shading estimation. Moreover, we show that the proposed\nmethodology allows novel view synthesis, relighting, and re-posing the\nreconstruction, and can naturally be extended to handle multiple input images\n(e.g. different views of a person, or the same view, in different poses, in\nvideo). Finally, we demonstrate the editing capabilities of our model for 3D\nvirtual try-on applications.\n","authors":["Enric Corona","Mihai Zanfir","Thiemo Alldieck","Eduard Gabriel Bazavan","Andrei Zanfir","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2212.06820v1.pdf","comment":"Project page: https://enriccorona.github.io/s3f/ , Video:\n  https://www.youtube.com/watch?v=mcZGcQ6L-2s"},{"id":"http://arxiv.org/abs/2212.06817v1","updated":"2022-12-13T18:55:15Z","published":"2022-12-13T18:55:15Z","title":"RT-1: Robotics Transformer for Real-World Control at Scale","summary":"  By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer.github.io\n","authors":["Anthony Brohan","Noah Brown","Justice Carbajal","Yevgen Chebotar","Joseph Dabis","Chelsea Finn","Keerthana Gopalakrishnan","Karol Hausman","Alex Herzog","Jasmine Hsu","Julian Ibarz","Brian Ichter","Alex Irpan","Tomas Jackson","Sally Jesmonth","Nikhil J Joshi","Ryan Julian","Dmitry Kalashnikov","Yuheng Kuang","Isabel Leal","Kuang-Huei Lee","Sergey Levine","Yao Lu","Utsav Malla","Deeksha Manjunath","Igor Mordatch","Ofir Nachum","Carolina Parada","Jodilyn Peralta","Emily Perez","Karl Pertsch","Jornell Quiambao","Kanishka Rao","Michael Ryoo","Grecia Salazar","Pannag Sanketi","Kevin Sayed","Jaspiar Singh","Sumedh Sontakke","Austin Stone","Clayton Tan","Huong Tran","Vincent Vanhoucke","Steve Vega","Quan Vuong","Fei Xia","Ted Xiao","Peng Xu","Sichun Xu","Tianhe Yu","Brianna Zitkovich"],"pdf_url":"https://arxiv.org/pdf/2212.06817v1.pdf","comment":"See website at robotics-transformer.github.io"},{"id":"http://arxiv.org/abs/2212.06809v1","updated":"2022-12-13T18:47:57Z","published":"2022-12-13T18:47:57Z","title":"Real-Time Artificial Intelligence Assistance for Safe Laparoscopic\n  Cholecystectomy: Early-Stage Clinical Evaluation","summary":"  Artificial intelligence is set to be deployed in operating rooms to improve\nsurgical care. This early-stage clinical evaluation shows the feasibility of\nconcurrently attaining real-time, high-quality predictions from several deep\nneural networks for endoscopic video analysis deployed for assistance during\nthree laparoscopic cholecystectomies.\n","authors":["Pietro Mascagni","Deepak Alapatt","Alfonso Lapergola","Armine Vardazaryan","Jean-Paul Mazellier","Bernard Dallemagne","Didier Mutter","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2212.06809v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2212.06804v1","updated":"2022-12-13T18:36:29Z","published":"2022-12-13T18:36:29Z","title":"Can a face tell us anything about an NBA prospect? -- A Deep Learning\n  approach","summary":"  Statistical analysis and modeling is becoming increasingly popular for the\nworld's leading organizations, especially for professional NBA teams.\nSophisticated methods and models of sport talent evaluation have been created\nfor this purpose. In this research, we present a different perspective from the\ndominant tactic of statistical data analysis. Based on a strategy that NBA\nteams have followed in the past, hiring human professionals, we deploy image\nanalysis and Convolutional Neural Networks in an attempt to predict the career\ntrajectory of newly drafted players from each draft class. We created a\ndatabase consisting of about 1500 image data from players from every draft\nsince 1990. We then divided the players into five different quality classes\nbased on their expected NBA career. Next, we trained popular pre-trained image\nclassification models in our data and conducted a series of tests in an attempt\nto create models that give reliable predictions of the rookie players' careers.\nThe results of this study suggest that there is a potential correlation between\nfacial characteristics and athletic talent, worth of further investigation.\n","authors":["Andreas Gavros","Foteini Gavrou"],"pdf_url":"https://arxiv.org/pdf/2212.06804v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2204.06645v2","updated":"2022-12-13T18:26:16Z","published":"2022-04-13T21:43:28Z","title":"Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning","summary":"  In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a\nnonlinear dimensionality reduction technique that provides solutions to some\ndrawbacks in existing global nonlinear dimensionality reduction algorithms in\nimaging applications. Wassmap represents images via probability measures in\nWasserstein space, then uses pairwise Wasserstein distances between the\nassociated measures to produce a low-dimensional, approximately isometric\nembedding. We show that the algorithm is able to exactly recover parameters of\nsome image manifolds including those generated by translations or dilations of\na fixed generating measure. Additionally, we show that a discrete version of\nthe algorithm retrieves parameters from manifolds generated from discrete\nmeasures by providing a theoretical bridge to transfer recovery results from\nfunctional data to discrete data. Testing of the proposed algorithms on various\nimage data manifolds show that Wassmap yields good embeddings compared with\nother global and local techniques.\n","authors":["Keaton Hamm","Nick Henscheid","Shujie Kang"],"pdf_url":"https://arxiv.org/pdf/2204.06645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06795v1","updated":"2022-12-13T18:26:00Z","published":"2022-12-13T18:26:00Z","title":"GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group\n  Propagation","summary":"  We present the Group Propagation Vision Transformer (GPViT): a novel\nnonhierarchical (i.e. non-pyramidal) transformer model designed for general\nvisual recognition with high-resolution features. High-resolution features (or\ntokens) are a natural fit for tasks that involve perceiving fine-grained\ndetails such as detection and segmentation, but exchanging global information\nbetween these features is expensive in memory and computation because of the\nway self-attention scales. We provide a highly efficient alternative Group\nPropagation Block (GP Block) to exchange global information. In each GP Block,\nfeatures are first grouped together by a fixed number of learnable group\ntokens; we then perform Group Propagation where global information is exchanged\nbetween the grouped features; finally, global information in the updated\ngrouped features is returned back to the image features through a transformer\ndecoder. We evaluate GPViT on a variety of visual recognition tasks including\nimage classification, semantic segmentation, object detection, and instance\nsegmentation. Our method achieves significant performance gains over previous\nworks across all tasks, especially on tasks that require high-resolution\noutputs, for example, our GPViT-L3 outperforms Swin Transformer-B by 2.0 mIoU\non ADE20K semantic segmentation with only half as many parameters. Code and\npre-trained models are available at https://github.com/ChenhongyiYang/GPViT .\n","authors":["Chenhongyi Yang","Jiarui Xu","Shalini De Mello","Elliot J. Crowley","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.06795v1.pdf","comment":"Code: https://github.com/ChenhongyiYang/GPViT"},{"id":"http://arxiv.org/abs/2206.01467v2","updated":"2022-12-13T18:18:50Z","published":"2022-06-03T09:17:22Z","title":"The Importance of Image Interpretation: Patterns of Semantic\n  Misclassification in Real-World Adversarial Images","summary":"  Adversarial images are created with the intention of causing an image\nclassifier to produce a misclassification. In this paper, we propose that\nadversarial images should be evaluated based on semantic mismatch, rather than\nlabel mismatch, as used in current work. In other words, we propose that an\nimage of a \"mug\" would be considered adversarial if classified as \"turnip\", but\nnot as \"cup\", as current systems would assume. Our novel idea of taking\nsemantic misclassification into account in the evaluation of adversarial images\noffers two benefits. First, it is a more realistic conceptualization of what\nmakes an image adversarial, which is important in order to fully understand the\nimplications of adversarial images for security and privacy. Second, it makes\nit possible to evaluate the transferability of adversarial images to a\nreal-world classifier, without requiring the classifier's label set to have\nbeen available during the creation of the images. The paper carries out an\nevaluation of a transfer attack on a real-world image classifier that is made\npossible by our semantic misclassification approach. The attack reveals\npatterns in the semantics of adversarial misclassifications that could not be\ninvestigated using conventional label mismatch.\n","authors":["Zhengyu Zhao","Nga Dang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2206.01467v2.pdf","comment":"International Conference on Multimedia Modeling (MMM) 2023. Resources\n  are publicly available at\n  https://github.com/ZhengyuZhao/Targeted-Transfer/tree/main/human_eval"},{"id":"http://arxiv.org/abs/2211.12322v3","updated":"2022-12-13T18:07:53Z","published":"2022-11-22T15:13:47Z","title":"Computer Vision for Transit Travel Time Prediction: An End-to-End\n  Framework Using Roadside Urban Imagery","summary":"  Accurate travel time estimation is paramount for providing transit users with\nreliable schedules and dependable real-time information. This paper is the\nfirst to utilize roadside urban imagery for direct transit travel time\nprediction. We propose and evaluate an end-to-end framework integrating\ntraditional transit data sources with a roadside camera for automated roadside\nimage data acquisition, labeling, and model training to predict transit travel\ntimes across a segment of interest. First, we show how the GTFS real-time data\ncan be utilized as an efficient activation mechanism for a roadside camera unit\nmonitoring a segment of interest. Second, AVL data is utilized to generate\nground truth labels for the acquired images based on the observed transit\ntravel time percentiles across the camera-monitored segment during the time of\nimage acquisition. Finally, the generated labeled image dataset is used to\ntrain and thoroughly evaluate a Vision Transformer (ViT) model to predict a\ndiscrete transit travel time range (band). The results illustrate that the ViT\nmodel is able to learn image features and contents that best help it deduce the\nexpected travel time range with an average validation accuracy ranging between\n80%-85%. We assess the interpretability of the ViT model's predictions and\nshowcase how this discrete travel time band prediction can subsequently improve\ncontinuous transit travel time estimation. The workflow and results presented\nin this study provide an end-to-end, scalable, automated, and highly efficient\napproach for integrating traditional transit data sources and roadside imagery\nto improve the estimation of transit travel duration. This work also\ndemonstrates the value of incorporating real-time information from\ncomputer-vision sources, which are becoming increasingly accessible and can\nhave major implications for improving operations and passenger real-time\ninformation.\n","authors":["Awad Abdelhalim","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2211.12322v3.pdf","comment":"Final revised preprint"},{"id":"http://arxiv.org/abs/2212.06785v1","updated":"2022-12-13T17:59:20Z","published":"2022-12-13T17:59:20Z","title":"Learning 3D Representations from 2D Pre-trained Models via\n  Image-to-Point Masked Autoencoders","summary":"  Pre-training by numerous image data has become de-facto for robust 2D\nrepresentations. In contrast, due to the expensive data acquisition and\nannotation, a paucity of large-scale 3D datasets severely hinders the learning\nfor high-quality 3D features. In this paper, we propose an alternative to\nobtain superior 3D representations from 2D pre-trained models via\nImage-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised\npre-training, we leverage the well learned 2D knowledge to guide 3D masked\nautoencoding, which reconstructs the masked point tokens with an\nencoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D\nmodels to extract the multi-view visual features of the input point cloud, and\nthen conduct two types of image-to-point learning schemes on top. For one, we\nintroduce a 2D-guided masking strategy that maintains semantically important\npoint tokens to be visible for the encoder. Compared to random masking, the\nnetwork can better concentrate on significant 3D structures and recover the\nmasked tokens from key spatial cues. For another, we enforce these visible\ntokens to reconstruct the corresponding multi-view 2D features after the\ndecoder. This enables the network to effectively inherit high-level 2D\nsemantics learned from rich image data for discriminative 3D modeling. Aided by\nour image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning,\nachieves 93.4% accuracy for linear SVM on ModelNet40, competitive to the fully\ntrained results of existing methods. By further fine-tuning on on\nScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11%\naccuracy, +3.68% to the second-best, demonstrating superior transferable\ncapacity. Code will be available at https://github.com/ZrrSkywalker/I2P-MAE.\n","authors":["Renrui Zhang","Liuhui Wang","Yu Qiao","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2212.06785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06776v1","updated":"2022-12-13T17:51:32Z","published":"2022-12-13T17:51:32Z","title":"Unfolding Local Growth Rate Estimates for (Almost) Perfect Adversarial\n  Detection","summary":"  Convolutional neural networks (CNN) define the state-of-the-art solution on\nmany perceptual tasks. However, current CNN approaches largely remain\nvulnerable against adversarial perturbations of the input that have been\ncrafted specifically to fool the system while being quasi-imperceptible to the\nhuman eye. In recent years, various approaches have been proposed to defend\nCNNs against such attacks, for example by model hardening or by adding explicit\ndefence mechanisms. Thereby, a small \"detector\" is included in the network and\ntrained on the binary classification task of distinguishing genuine data from\ndata containing adversarial perturbations. In this work, we propose a simple\nand light-weight detector, which leverages recent findings on the relation\nbetween networks' local intrinsic dimensionality (LID) and adversarial attacks.\nBased on a re-interpretation of the LID measure and several simple adaptations,\nwe surpass the state-of-the-art on adversarial detection by a significant\nmargin and reach almost perfect results in terms of F1-score for several\nnetworks and datasets. Sources available at:\nhttps://github.com/adverML/multiLID\n","authors":["Peter Lorenz","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2212.06776v1.pdf","comment":"accepted at VISAPP23"},{"id":"http://arxiv.org/abs/2212.06756v1","updated":"2022-12-13T17:36:17Z","published":"2022-12-13T17:36:17Z","title":"Connectivity-constrained Interactive Panoptic Segmentation","summary":"  We address interactive panoptic annotation, where one segment all object and\nstuff regions in an image. We investigate two graph-based segmentation\nalgorithms that both enforce connectivity of each region, with a notable\nclass-aware Integer Linear Programming (ILP) formulation that ensures global\noptimum. Both algorithms can take RGB, or utilize the feature maps from any\nDCNN, whether trained on the target dataset or not, as input. We then propose\nan interactive, scribble-based annotation framework.\n","authors":["Ruobing Shen","Bo Tang","Andrea Lodi","Ismail Ben Ayed","Thomas Guthier"],"pdf_url":"https://arxiv.org/pdf/2212.06756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06735v1","updated":"2022-12-13T17:14:14Z","published":"2022-12-13T17:14:14Z","title":"POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image\n  and Time Series Classification","summary":"  The automated machine learning (AutoML) field has become increasingly\nrelevant in recent years. These algorithms can develop models without the need\nfor expert knowledge, facilitating the application of machine learning\ntechniques in the industry. Neural Architecture Search (NAS) exploits deep\nlearning techniques to autonomously produce neural network architectures whose\nresults rival the state-of-the-art models hand-crafted by AI experts. However,\nthis approach requires significant computational resources and hardware\ninvestments, making it less appealing for real-usage applications. This article\npresents the third version of Pareto-Optimal Progressive Neural Architecture\nSearch (POPNASv3), a new sequential model-based optimization NAS algorithm\ntargeting different hardware environments and multiple classification tasks.\nOur method is able to find competitive architectures within large search\nspaces, while keeping a flexible structure and data processing pipeline to\nadapt to different tasks. The algorithm employs Pareto optimality to reduce the\nnumber of architectures sampled during the search, drastically improving the\ntime efficiency without loss in accuracy. The experiments performed on images\nand time series classification datasets provide evidence that POPNASv3 can\nexplore a large set of assorted operators and converge to optimal architectures\nsuited for the type of data provided under different scenarios.\n","authors":["Andrea Falanti","Eugenio Lomurno","Danilo Ardagna","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2212.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.17398v3","updated":"2022-12-13T16:55:45Z","published":"2022-10-31T15:28:49Z","title":"Rethinking Generalization: The Impact of Annotation Style on Medical\n  Image Segmentation","summary":"  Generalization is an important attribute of machine learning models,\nparticularly for those that are to be deployed in a medical context, where\nunreliable predictions can have real world consequences. While the failure of\nmodels to generalize across datasets is typically attributed to a mismatch in\nthe data distributions, performance gaps are often a consequence of biases in\nthe 'ground-truth' label annotations. This is particularly important in the\ncontext of medical image segmentation of pathological structures (e.g.\nlesions), where the annotation process is much more subjective, and affected by\na number underlying factors, including the annotation protocol, rater\neducation/experience, and clinical aims, among others. In this paper, we show\nthat modeling annotation biases, rather than ignoring them, poses a promising\nway of accounting for differences in annotation style across datasets. To this\nend, we propose a generalized conditioning framework to (1) learn and account\nfor different annotation styles across multiple datasets using a single model,\n(2) identify similar annotation styles across different datasets in order to\npermit their effective aggregation, and (3) fine-tune a fully trained model to\na new annotation style with just a few samples. Next, we present an\nimage-conditioning approach to model annotation styles that correlate with\nspecific image features, potentially enabling detection biases to be more\neasily identified.\n","authors":["Brennan Nichyporuk","Jillian Cardinell","Justin Szeto","Raghav Mehta","Jean-Pierre R. Falet","Douglas L. Arnold","Sotirios A. Tsaftaris","Tal Arbel"],"pdf_url":"https://arxiv.org/pdf/2210.17398v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://www.melba-journal.org/papers/2022:029.html"},{"id":"http://arxiv.org/abs/2212.06727v1","updated":"2022-12-13T16:55:12Z","published":"2022-12-13T16:55:12Z","title":"What do Vision Transformers Learn? A Visual Exploration","summary":"  Vision transformers (ViTs) are quickly becoming the de-facto architecture for\ncomputer vision, yet we understand very little about why they work and what\nthey learn. While existing studies visually analyze the mechanisms of\nconvolutional neural networks, an analogous exploration of ViTs remains\nchallenging. In this paper, we first address the obstacles to performing\nvisualizations on ViTs. Assisted by these solutions, we observe that neurons in\nViTs trained with language model supervision (e.g., CLIP) are activated by\nsemantic concepts rather than visual features. We also explore the underlying\ndifferences between ViTs and CNNs, and we find that transformers detect image\nbackground features, just like their convolutional counterparts, but their\npredictions depend far less on high-frequency information. On the other hand,\nboth architecture types behave similarly in the way features progress from\nabstract patterns in early layers to concrete objects in late layers. In\naddition, we show that ViTs maintain spatial information in all layers except\nthe final layer. In contrast to previous works, we show that the last layer\nmost likely discards the spatial information and behaves as a learned global\npooling operation. Finally, we conduct large-scale visualizations on a wide\nrange of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to\nvalidate the effectiveness of our method.\n","authors":["Amin Ghiasi","Hamid Kazemi","Eitan Borgnia","Steven Reich","Manli Shu","Micah Goldblum","Andrew Gordon Wilson","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2212.06727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06726v1","updated":"2022-12-13T16:54:08Z","published":"2022-12-13T16:54:08Z","title":"Semantic Brain Decoding: from fMRI to conceptually similar image\n  reconstruction of visual stimuli","summary":"  Brain decoding is a field of computational neuroscience that uses measurable\nbrain activity to infer mental states or internal representations of perceptual\ninputs. Therefore, we propose a novel approach to brain decoding that also\nrelies on semantic and contextual similarity. We employ an fMRI dataset of\nnatural image vision and create a deep learning decoding pipeline inspired by\nthe existence of both bottom-up and top-down processes in human vision. We\ntrain a linear brain-to-feature model to map fMRI activity features to visual\nstimuli features, assuming that the brain projects visual information onto a\nspace that is homeomorphic to the latent space represented by the last\nconvolutional layer of a pretrained convolutional neural network, which\ntypically collects a variety of semantic features that summarize and highlight\nsimilarities and differences between concepts. These features are then\ncategorized in the latent space using a nearest-neighbor strategy, and the\nresults are used to condition a generative latent diffusion model to create\nnovel images. From fMRI data only, we produce reconstructions of visual stimuli\nthat match the original content very well on a semantic level, surpassing the\nstate of the art in previous literature. We evaluate our work and obtain good\nresults using a quantitative semantic metric (the Wu-Palmer similarity metric\nover the WordNet lexicon, which had an average value of 0.57) and perform a\nhuman evaluation experiment that resulted in correct evaluation, according to\nthe multiplicity of human criteria in evaluating image similarity, in over 80%\nof the test set.\n","authors":["Matteo Ferrante","Tommaso Boccato","Nicola Toschi"],"pdf_url":"https://arxiv.org/pdf/2212.06726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06717v1","updated":"2022-12-13T16:42:20Z","published":"2022-12-13T16:42:20Z","title":"A Machine Learning Enhanced Approach for Automated Sunquake Detection in\n  Acoustic Emission Maps","summary":"  Sunquakes are seismic emissions visible on the solar surface, associated with\nsome solar flares. Although discovered in 1998, they have only recently become\na more commonly detected phenomenon. Despite the availability of several manual\ndetection guidelines, to our knowledge, the astrophysical data produced for\nsunquakes is new to the field of Machine Learning. Detecting sunquakes is a\ndaunting task for human operators and this work aims to ease and, if possible,\nto improve their detection. Thus, we introduce a dataset constructed from\nacoustic egression-power maps of solar active regions obtained for Solar Cycles\n23 and 24 using the holography method. We then present a pedagogical approach\nto the application of machine learning representation methods for sunquake\ndetection using AutoEncoders, Contrastive Learning, Object Detection and\nrecurrent techniques, which we enhance by introducing several custom\ndomain-specific data augmentation transformations. We address the main\nchallenges of the automated sunquake detection task, namely the very high noise\npatterns in and outside the active region shadow and the extreme class\nimbalance given by the limited number of frames that present sunquake\nsignatures. With our trained models, we find temporal and spatial locations of\npeculiar acoustic emission and qualitatively associate them to eruptive and\nhigh energy emission. While noting that these models are still in a prototype\nstage and there is much room for improvement in metrics and bias levels, we\nhypothesize that their agreement on example use cases has the potential to\nenable detection of weak solar acoustic manifestations.\n","authors":["Vanessa Mercea","Alin Razvan Paraschiv","Daniela Adriana Lacatus","Anca Marginean","Diana Besliu-Ionescu"],"pdf_url":"https://arxiv.org/pdf/2212.06717v1.pdf","comment":"Solar Physics accepted for publication, 44 total pages, 9 appendix\n  pages, 21 figures, 6 tables"},{"id":"http://arxiv.org/abs/2212.06714v1","updated":"2022-12-13T16:35:35Z","published":"2022-12-13T16:35:35Z","title":"CNN-transformer mixed model for object detection","summary":"  Object detection, one of the three main tasks of computer vision, has been\nused in various applications. The main process is to use deep neural networks\nto extract the features of an image and then use the features to identify the\nclass and location of an object. Therefore, the main direction to improve the\naccuracy of object detection tasks is to improve the neural network to extract\nfeatures better. In this paper, I propose a convolutional module with a\ntransformer[1], which aims to improve the recognition accuracy of the model by\nfusing the detailed features extracted by CNN[2] with the global features\nextracted by a transformer and significantly reduce the computational effort of\nthe transformer module by deflating the feature mAP. The main execution steps\nare convolutional downsampling to reduce the feature map size, then\nself-attention calculation and upsampling, and finally concatenation with the\ninitial input. In the experimental part, after splicing the block to the end of\nYOLOv5n[3] and training 300 epochs on the coco dataset, the mAP improved by\n1.7% compared with the previous YOLOv5n, and the mAP curve did not show any\nsaturation phenomenon, so there is still potential for improvement. After 100\nrounds of training on the Pascal VOC dataset, the accuracy of the results\nreached 81%, which is 4.6 better than the faster RCNN[4] using resnet101[5] as\nthe backbone, but the number of parameters is less than one-twentieth of it.\n","authors":["Wenshuo Li"],"pdf_url":"https://arxiv.org/pdf/2212.06714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06710v1","updated":"2022-12-13T16:29:13Z","published":"2022-12-13T16:29:13Z","title":"TIER: Text-Image Entropy Regularization for CLIP-style models","summary":"  In this paper, we study the effect of a novel regularization scheme on\ncontrastive language-image pre-trained (CLIP) models. Our approach is based on\nthe observation that, in many domains, text tokens should only describe a small\nnumber of image regions and, likewise, each image region should correspond to\nonly a few text tokens. In CLIP-style models, this implies that text-token\nembeddings should have high similarity to only a small number of image-patch\nembeddings for a given image-text pair. We formalize this observation using a\nnovel regularization scheme that penalizes the entropy of the text-token to\nimage-patch similarity scores. We qualitatively and quantitatively demonstrate\nthat the proposed regularization scheme shrinks the text-token and image-patch\nsimilarity scores towards zero, thus achieving the desired effect. We\ndemonstrate the promise of our approach in an important medical context where\nthis underlying hypothesis naturally arises. Using our proposed approach, we\nachieve state of the art (SOTA) zero-shot performance on all tasks from the\nCheXpert chest x-ray dataset, outperforming an unregularized version of the\nmodel and several recently published self-supervised models.\n","authors":["Anil Palepu","Andrew L. Beam"],"pdf_url":"https://arxiv.org/pdf/2212.06710v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.11801v2","updated":"2022-12-13T16:19:01Z","published":"2022-11-21T19:09:52Z","title":"Self-Supervised Pre-training of 3D Point Cloud Networks with Image Data","summary":"  Reducing the quantity of annotations required for supervised training is\nvital when labels are scarce and costly. This reduction is especially important\nfor semantic segmentation tasks involving 3D datasets that are often\nsignificantly smaller and more challenging to annotate than their image-based\ncounterparts. Self-supervised pre-training on large unlabelled datasets is one\nway to reduce the amount of manual annotations needed. Previous work has\nfocused on pre-training with point cloud data exclusively; this approach often\nrequires two or more registered views. In the present work, we combine image\nand point cloud modalities, by first learning self-supervised image features\nand then using these features to train a 3D model. By incorporating image data,\nwhich is often included in many 3D datasets, our pre-training method only\nrequires a single scan of a scene. We demonstrate that our pre-training\napproach, despite using single scans, achieves comparable performance to other\nmulti-scan, point cloud-only methods.\n","authors":["Andrej Janda","Brandon Wagstaff","Edwin G. Ng","Jonathan Kelly"],"pdf_url":"https://arxiv.org/pdf/2211.11801v2.pdf","comment":"Accepted to the Conference on Robot Learning (CoRL'22) Workshop on\n  Pre-training Robot Learning, Auckland, New Zealand, December 14-18, 2022"},{"id":"http://arxiv.org/abs/2212.06701v1","updated":"2022-12-13T16:16:06Z","published":"2022-12-13T16:16:06Z","title":"A Novel Approach For Generating Customizable Light Field Datasets for\n  Machine Learning","summary":"  To train deep learning models, which often outperform traditional approaches,\nlarge datasets of a specified medium, e.g., images, are used in numerous areas.\nHowever, for light field-specific machine learning tasks, there is a lack of\nsuch available datasets. Therefore, we create our own light field datasets,\nwhich have great potential for a variety of applications due to the abundance\nof information in light fields compared to singular images. Using the Unity and\nC# frameworks, we develop a novel approach for generating large, scalable, and\nreproducible light field datasets based on customizable hardware configurations\nto accelerate light field deep learning research.\n","authors":["Julia Huang","Toure Smith","Aloukika Patro","Vidhi Chhabra"],"pdf_url":"https://arxiv.org/pdf/2212.06701v1.pdf","comment":"5 pages, 5 figures, accepted to and presented at MIT URTC Conference,\n  and will be published in IEEE proceedings"},{"id":"http://arxiv.org/abs/2212.06834v1","updated":"2022-12-13T16:12:45Z","published":"2022-12-13T16:12:45Z","title":"Deep Neural Networks integrating genomics and histopathological images\n  for predicting stages and survival time-to-event in colon cancer","summary":"  There exists unexplained diverse variation within the predefined colon cancer\nstages using only features either from genomics or histopathological whole\nslide images as prognostic factors. Unraveling this variation will bring about\nimproved in staging and treatment outcome, hence motivated by the advancement\nof Deep Neural Network libraries and different structures and factors within\nsome genomic dataset, we aggregate atypical patterns in histopathological\nimages with diverse carcinogenic expression from mRNA, miRNA and DNA\nMethylation as an integrative input source into an ensemble deep neural network\nfor colon cancer stages classification and samples stratification into low or\nhigh risk survival groups. The results of our Ensemble Deep Convolutional\nNeural Network model show an improved performance in stages classification on\nthe integrated dataset. The fused input features return Area under curve\nReceiver Operating Characteristic curve (AUC ROC) of 0.95 compared with AUC ROC\nof 0.71 and 0.68 obtained when only genomics and images features are used for\nthe stage's classification, respectively. Also, the extracted features were\nused to split the patients into low or high risk survival groups. Among the\n2548 fused features, 1695 features showed a statistically significant survival\nprobability differences between the two risk groups defined by the extracted\nfeatures.\n","authors":["Olalekan Ogundipe","Zeyneb Kurt","Wai Lok Woo"],"pdf_url":"https://arxiv.org/pdf/2212.06834v1.pdf","comment":"21 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2211.10227v3","updated":"2022-12-13T16:03:26Z","published":"2022-11-18T13:26:57Z","title":"Adversarial Detection by Approximation of Ensemble Boundary","summary":"  A spectral approximation of a Boolean function is proposed for approximating\nthe decision boundary of an ensemble of Deep Neural Networks (DNNs) solving\ntwo-class pattern recognition problems. The Walsh combination of relatively\nweak DNN classifiers is shown experimentally to be capable of detecting\nadversarial attacks. By observing the difference in Walsh coefficient\napproximation between clean and adversarial images, it appears that\ntransferability of attack may be used for detection. Approximating the decision\nboundary may also aid in understanding the learning and transferability\nproperties of DNNs. While the experiments here use images, the proposed\napproach of modelling two-class ensemble decision boundaries could in principle\nbe applied to any application area. Code for this paper implementing Walsh\nCoefficient Examples of approximating artificial Boolean functions can be found\nat https://doi.org/10.24433/CO.3695905.v1\n","authors":["T. Windeatt"],"pdf_url":"https://arxiv.org/pdf/2211.10227v3.pdf","comment":"6 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.06682v1","updated":"2022-12-13T15:58:25Z","published":"2022-12-13T15:58:25Z","title":"Towards Deeper and Better Multi-view Feature Fusion for 3D Semantic\n  Segmentation","summary":"  3D point clouds are rich in geometric structure information, while 2D images\ncontain important and continuous texture information. Combining 2D information\nto achieve better 3D semantic segmentation has become mainstream in 3D scene\nunderstanding. Albeit the success, it still remains elusive how to fuse and\nprocess the cross-dimensional features from these two distinct spaces. Existing\nstate-of-the-art usually exploit bidirectional projection methods to align the\ncross-dimensional features and realize both 2D & 3D semantic segmentation\ntasks. However, to enable bidirectional mapping, this framework often requires\na symmetrical 2D-3D network structure, thus limiting the network's flexibility.\nMeanwhile, such dual-task settings may distract the network easily and lead to\nover-fitting in the 3D segmentation task. As limited by the network's\ninflexibility, fused features can only pass through a decoder network, which\naffects model performance due to insufficient depth. To alleviate these\ndrawbacks, in this paper, we argue that despite its simplicity, projecting\nunidirectionally multi-view 2D deep semantic features into the 3D space aligned\nwith 3D deep semantic features could lead to better feature fusion. On the one\nhand, the unidirectional projection enforces our model focused more on the core\ntask, i.e., 3D segmentation; on the other hand, unlocking the bidirectional to\nunidirectional projection enables a deeper cross-domain semantic alignment and\nenjoys the flexibility to fuse better and complicated features from very\ndifferent spaces. In joint 2D-3D approaches, our proposed method achieves\nsuperior performance on the ScanNetv2 benchmark for 3D semantic segmentation.\n","authors":["Chaolong Yang","Yuyao Yan","Weiguang Zhao","Jianan Ye","Xi Yang","Amir Hussain","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2212.06682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06655v1","updated":"2022-12-13T15:37:53Z","published":"2022-12-13T15:37:53Z","title":"The Hateful Memes Challenge Next Move","summary":"  State-of-the-art image and text classification models, such as Convectional\nNeural Networks and Transformers, have long been able to classify their\nrespective unimodal reasoning satisfactorily with accuracy close to or\nexceeding human accuracy. However, images embedded with text, such as hateful\nmemes, are hard to classify using unimodal reasoning when difficult examples,\nsuch as benign confounders, are incorporated into the data set. We attempt to\ngenerate more labeled memes in addition to the Hateful Memes data set from\nFacebook AI, based on the framework of a winning team from the Hateful Meme\nChallenge. To increase the number of labeled memes, we explore semi-supervised\nlearning using pseudo-labels for newly introduced, unlabeled memes gathered\nfrom the Memotion Dataset 7K. We find that the semi-supervised learning task on\nunlabeled data required human intervention and filtering and that adding a\nlimited amount of new data yields no extra classification performance.\n","authors":["Weijun Jin","Lance Wilhelm"],"pdf_url":"https://arxiv.org/pdf/2212.06655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06643v1","updated":"2022-12-13T15:25:49Z","published":"2022-12-13T15:25:49Z","title":"Boosting Semi-Supervised Learning with Contrastive Complementary\n  Labeling","summary":"  Semi-supervised learning (SSL) has achieved great success in leveraging a\nlarge amount of unlabeled data to learn a promising classifier. A popular\napproach is pseudo-labeling that generates pseudo labels only for those\nunlabeled data with high-confidence predictions. As for the low-confidence\nones, existing methods often simply discard them because these unreliable\npseudo labels may mislead the model. Nevertheless, we highlight that these data\nwith low-confidence pseudo labels can be still beneficial to the training\nprocess. Specifically, although the class with the highest probability in the\nprediction is unreliable, we can assume that this sample is very unlikely to\nbelong to the classes with the lowest probabilities. In this way, these data\ncan be also very informative if we can effectively exploit these complementary\nlabels, i.e., the classes that a sample does not belong to. Inspired by this,\nwe propose a novel Contrastive Complementary Labeling (CCL) method that\nconstructs a large number of reliable negative pairs based on the complementary\nlabels and adopts contrastive learning to make use of all the unlabeled data.\nExtensive experiments demonstrate that CCL significantly improves the\nperformance on top of existing methods. More critically, our CCL is\nparticularly effective under the label-scarce settings. For example, we yield\nan improvement of 2.43% over FixMatch on CIFAR-10 only with 40 labeled data.\n","authors":["Qinyi Deng","Yong Guo","Zhibang Yang","Haolin Pan","Jian Chen"],"pdf_url":"https://arxiv.org/pdf/2212.06643v1.pdf","comment":"5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2212.06629v1","updated":"2022-12-13T15:01:22Z","published":"2022-12-13T15:01:22Z","title":"Aligning Visual and Lexical Semantics","summary":"  We discuss two kinds of semantics relevant to Computer Vision (CV) systems -\nVisual Semantics and Lexical Semantics. While visual semantics focus on how\nhumans build concepts when using vision to perceive a target reality, lexical\nsemantics focus on how humans build concepts of the same target reality through\nthe use of language. The lack of coincidence between visual and lexical\nsemantics, in turn, has a major impact on CV systems in the form of the\nSemantic Gap Problem (SGP). The paper, while extensively exemplifying the lack\nof coincidence as above, introduces a general, domain-agnostic methodology to\nenforce alignment between visual and lexical semantics.\n","authors":["Fausto Giunchiglia","Mayukh Bagchi","Xiaolei Diao"],"pdf_url":"https://arxiv.org/pdf/2212.06629v1.pdf","comment":"iConference 2023, Barcelona, March 27 - 29, 2023"},{"id":"http://arxiv.org/abs/2212.06626v1","updated":"2022-12-13T15:00:12Z","published":"2022-12-13T15:00:12Z","title":"DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo","summary":"  We propose a novel approach for deep learning-based Multi-View Stereo (MVS).\nFor each pixel in the reference image, our method leverages a deep architecture\nto search for the corresponding point in the source image directly along the\ncorresponding epipolar line. We denote our method DELS-MVS: Deep Epipolar Line\nSearch Multi-View Stereo. Previous works in deep MVS select a range of interest\nwithin the depth space, discretize it, and sample the epipolar line according\nto the resulting depth values: this can result in an uneven scanning of the\nepipolar line, hence of the image space. Instead, our method works directly on\nthe epipolar line: this guarantees an even scanning of the image space and\navoids both the need to select a depth range of interest, which is often not\nknown a priori and can vary dramatically from scene to scene, and the need for\na suitable discretization of the depth space. In fact, our search is iterative,\nwhich avoids the building of a cost volume, costly both to store and to\nprocess. Finally, our method performs a robust geometry-aware fusion of the\nestimated depth maps, leveraging a confidence predicted alongside each depth.\nWe test DELS-MVS on the ETH3D, Tanks and Temples and DTU benchmarks and achieve\ncompetitive results with respect to state-of-the-art approaches.\n","authors":["Christian Sormann","Emanuele Santellani","Mattia Rossi","Andreas Kuhn","Friedrich Fraundorfer"],"pdf_url":"https://arxiv.org/pdf/2212.06626v1.pdf","comment":"accepted at WACV 2023"},{"id":"http://arxiv.org/abs/2105.10379v2","updated":"2022-12-13T14:50:53Z","published":"2021-05-21T14:41:31Z","title":"3D Human Pose Regression using Graph Convolutional Network","summary":"  3D human pose estimation is a difficult task, due to challenges such as\noccluded body parts and ambiguous poses. Graph convolutional networks encode\nthe structural information of the human skeleton in the form of an adjacency\nmatrix, which is beneficial for better pose prediction. We propose one such\ngraph convolutional network named PoseGraphNet for 3D human pose regression\nfrom 2D poses. Our network uses an adaptive adjacency matrix and kernels\nspecific to neighbor groups. We evaluate our model on the Human3.6M dataset\nwhich is a standard dataset for 3D pose estimation. Our model's performance is\nclose to the state-of-the-art, but with much fewer parameters. The model learns\ninteresting adjacency relations between joints that have no physical\nconnections, but are behaviorally similar.\n","authors":["Soubarna Banik","Alejandro Mendoza Gracia","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2105.10379v2.pdf","comment":"Paper accepted in IEEE ICIP 2021, DOI will be updated once published"},{"id":"http://arxiv.org/abs/2212.06595v1","updated":"2022-12-13T14:14:48Z","published":"2022-12-13T14:14:48Z","title":"OAMixer: Object-aware Mixing Layer for Vision Transformers","summary":"  Patch-based models, e.g., Vision Transformers (ViTs) and Mixers, have shown\nimpressive results on various visual recognition tasks, alternating classic\nconvolutional networks. While the initial patch-based models (ViTs) treated all\npatches equally, recent studies reveal that incorporating inductive bias like\nspatiality benefits the representations. However, most prior works solely\nfocused on the location of patches, overlooking the scene structure of images.\nThus, we aim to further guide the interaction of patches using the object\ninformation. Specifically, we propose OAMixer (object-aware mixing layer),\nwhich calibrates the patch mixing layers of patch-based models based on the\nobject labels. Here, we obtain the object labels in unsupervised or\nweakly-supervised manners, i.e., no additional human-annotating cost is\nnecessary. Using the object labels, OAMixer computes a reweighting mask with a\nlearnable scale parameter that intensifies the interaction of patches\ncontaining similar objects and applies the mask to the patch mixing layers. By\nlearning an object-centric representation, we demonstrate that OAMixer improves\nthe classification accuracy and background robustness of various patch-based\nmodels, including ViTs, MLP-Mixers, and ConvMixers. Moreover, we show that\nOAMixer enhances various downstream tasks, including large-scale\nclassification, self-supervised learning, and multi-object recognition,\nverifying the generic applicability of OAMixer\n","authors":["Hyunwoo Kang","Sangwoo Mo","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2212.06595v1.pdf","comment":"CVPR Transformers for Vision Workshop 2022. First two authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2212.06593v1","updated":"2022-12-13T14:09:32Z","published":"2022-12-13T14:09:32Z","title":"FastMIM: Expediting Masked Image Modeling Pre-training for Vision","summary":"  The combination of transformers and masked image modeling (MIM) pre-training\nframework has shown great potential in various vision tasks. However, the\npre-training computational budget is too heavy and withholds the MIM from\nbecoming a practical training paradigm. This paper presents FastMIM, a simple\nand generic framework for expediting masked image modeling with the following\ntwo steps: (i) pre-training vision backbones with low-resolution input images;\nand (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead\nof original RGB values of the input images. In addition, we propose FastMIM-P\nto progressively enlarge the input resolution during pre-training stage to\nfurther enhance the transfer results of models with high capacity. We point out\nthat: (i) a wide range of input resolutions in pre-training phase can lead to\nsimilar performances in fine-tuning phase and downstream tasks such as\ndetection and segmentation; (ii) the shallow layers of encoder are more\nimportant during pre-training and discarding last several layers can speed up\nthe training stage with no harm to fine-tuning performance; (iii) the decoder\nshould match the size of selected network; and (iv) HOG is more stable than RGB\nvalues when resolution transfers;. Equipped with FastMIM, all kinds of vision\nbackbones can be pre-trained in an efficient way. For example, we can achieve\n83.8%/84.1% top-1 accuracy on ImageNet-1K with ViT-B/Swin-B as backbones.\nCompared to previous relevant approaches, we can achieve comparable or better\ntop-1 accuracy while accelerate the training procedure by $\\sim$5$\\times$. Code\ncan be found in https://github.com/ggjy/FastMIM.pytorch.\n","authors":["Jianyuan Guo","Kai Han","Han Wu","Yehui Tang","Yunhe Wang","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2212.06593v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2112.08643v3","updated":"2022-12-13T14:08:55Z","published":"2021-12-16T05:49:51Z","title":"TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) tackles the novel class recognition problem by\ntransferring semantic knowledge from seen classes to unseen ones. Existing\nattention-based models have struggled to learn inferior region features in a\nsingle image by solely using unidirectional attention, which ignore the\ntransferability and discriminative attribute localization of visual features.\nIn this paper, we propose a cross attribute-guided Transformer network, termed\nTransZero++, to refine visual features and learn accurate attribute\nlocalization for semantic-augmented visual embedding representations in ZSL.\nTransZero++ consists of an attribute$\\rightarrow$visual Transformer sub-net\n(AVT) and a visual$\\rightarrow$attribute Transformer sub-net (VAT).\nSpecifically, AVT first takes a feature augmentation encoder to alleviate the\ncross-dataset problem, and improves the transferability of visual features by\nreducing the entangled relative geometry relationships among region features.\nThen, an attribute$\\rightarrow$visual decoder is employed to localize the image\nregions most relevant to each attribute in a given image for attribute-based\nvisual feature representations. Analogously, VAT uses the similar feature\naugmentation encoder to refine the visual features, which are further applied\nin visual$\\rightarrow$attribute decoder to learn visual-based attribute\nfeatures. By further introducing semantical collaborative losses, the two\nattribute-guided transformers teach each other to learn semantic-augmented\nvisual embeddings via semantical collaborative learning. Extensive experiments\nshow that TransZero++ achieves the new state-of-the-art results on three\nchallenging ZSL benchmarks. The codes are available at:\n\\url{https://github.com/shiming-chen/TransZero_pp}.\n","authors":["Shiming Chen","Ziming Hong","Wenjin Hou","Guo-Sen Xie","Yibing Song","Jian Zhao","Xinge You","Shuicheng Yan","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2112.08643v3.pdf","comment":"This is an extention of AAAI'22 paper (TransZero). Accepted to TPAMI.\n  arXiv admin note: substantial text overlap with arXiv:2112.01683"},{"id":"http://arxiv.org/abs/2212.01737v2","updated":"2022-12-13T13:54:49Z","published":"2022-12-04T04:03:34Z","title":"RLogist: Fast Observation Strategy on Whole-slide Images with Deep\n  Reinforcement Learning","summary":"  Whole-slide images (WSI) in computational pathology have high resolution with\ngigapixel size, but are generally with sparse regions of interest, which leads\nto weak diagnostic relevance and data inefficiency for each area in the slide.\nMost of the existing methods rely on a multiple instance learning framework\nthat requires densely sampling local patches at high magnification. The\nlimitation is evident in the application stage as the heavy computation for\nextracting patch-level features is inevitable. In this paper, we develop\nRLogist, a benchmarking deep reinforcement learning (DRL) method for fast\nobservation strategy on WSIs. Imitating the diagnostic logic of human\npathologists, our RL agent learns how to find regions of observation value and\nobtain representative features across multiple resolution levels, without\nhaving to analyze each part of the WSI at the high magnification. We benchmark\nour method on two whole-slide level classification tasks, including detection\nof metastases in WSIs of lymph node sections, and subtyping of lung cancer.\nExperimental results demonstrate that RLogist achieves competitive\nclassification performance compared to typical multiple instance learning\nalgorithms, while having a significantly short observation path. In addition,\nthe observation path given by RLogist provides good decision-making\ninterpretability, and its ability of reading path navigation can potentially be\nused by pathologists for educational/assistive purposes. Our code is available\nat: \\url{https://github.com/tencent-ailab/RLogist}.\n","authors":["Boxuan Zhao","Jun Zhang","Deheng Ye","Jian Cao","Xiao Han","Qiang Fu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2212.01737v2.pdf","comment":"accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2212.06556v1","updated":"2022-12-13T13:15:20Z","published":"2022-12-13T13:15:20Z","title":"Localized Latent Updates for Fine-Tuning Vision-Language Models","summary":"  Although massive pre-trained vision-language models like CLIP show impressive\ngeneralization capabilities for many tasks, still it often remains necessary to\nfine-tune them for improved performance on specific datasets. When doing so, it\nis desirable that updating the model is fast and that the model does not lose\nits capabilities on data outside of the dataset, as is often the case with\nclassical fine-tuning approaches. In this work we suggest a lightweight\nadapter, that only updates the models predictions close to seen datapoints. We\ndemonstrate the effectiveness and speed of this relatively simple approach in\nthe context of few-shot learning, where our results both on classes seen and\nunseen during training are comparable with or improve on the state of the art.\n","authors":["Moritz Ibing","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2212.06556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06550v1","updated":"2022-12-13T13:06:21Z","published":"2022-12-13T13:06:21Z","title":"Body Segmentation Using Multi-task Learning","summary":"  Body segmentation is an important step in many computer vision problems\ninvolving human images and one of the key components that affects the\nperformance of all downstream tasks. Several prior works have approached this\nproblem using a multi-task model that exploits correlations between different\ntasks to improve segmentation performance. Based on the success of such\nsolutions, we present in this paper a novel multi-task model for human\nsegmentation/parsing that involves three tasks, i.e., (i) keypoint-based\nskeleton estimation, (ii) dense pose prediction, and (iii) human-body\nsegmentation. The main idea behind the proposed Segmentation--Pose--DensePose\nmodel (or SPD for short) is to learn a better segmentation model by sharing\nknowledge across different, yet related tasks. SPD is based on a shared deep\nneural network backbone that branches off into three task-specific model heads\nand is learned using a multi-task optimization objective. The performance of\nthe model is analysed through rigorous experiments on the LIP and ATR datasets\nand in comparison to a recent (state-of-the-art) multi-task body-segmentation\nmodel. Comprehensive ablation studies are also presented. Our experimental\nresults show that the proposed multi-task (segmentation) model is highly\ncompetitive and that the introduction of additional tasks contributes towards a\nhigher overall segmentation performance.\n","authors":["Julijan Jug","Ajda Lampe","Vitomir Štruc","Peter Peer"],"pdf_url":"https://arxiv.org/pdf/2212.06550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06536v1","updated":"2022-12-13T12:40:28Z","published":"2022-12-13T12:40:28Z","title":"On Mini-Batch Training with Varying Length Time Series","summary":"  In real-world time series recognition applications, it is possible to have\ndata with varying length patterns. However, when using artificial neural\nnetworks (ANN), it is standard practice to use fixed-sized mini-batches. To do\nthis, time series data with varying lengths are typically normalized so that\nall the patterns are the same length. Normally, this is done using zero padding\nor truncation without much consideration. We propose a novel method of\nnormalizing the lengths of the time series in a dataset by exploiting the\ndynamic matching ability of Dynamic Time Warping (DTW). In this way, the time\nseries lengths in a dataset can be set to a fixed size while maintaining\nfeatures typical to the dataset. In the experiments, all 11 datasets with\nvarying length time series from the 2018 UCR Time Series Archive are used. We\nevaluate the proposed method by comparing it with 18 other length normalization\nmethods on a Convolutional Neural Network (CNN), a Long-Short Term Memory\nnetwork (LSTM), and a Bidirectional LSTM (BLSTM).\n","authors":["Brian Kenji Iwana"],"pdf_url":"https://arxiv.org/pdf/2212.06536v1.pdf","comment":"Accepted to ICASSP 2022"},{"id":"http://arxiv.org/abs/2212.06524v1","updated":"2022-12-13T12:17:13Z","published":"2022-12-13T12:17:13Z","title":"SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse\n  Spatial-Temporal Guidance","summary":"  Real-time monocular 3D reconstruction is a challenging problem that remains\nunsolved. Although recent end-to-end methods have demonstrated promising\nresults, tiny structures and geometric boundaries are hardly captured due to\ntheir insufficient supervision neglecting spatial details and oversimplified\nfeature fusion ignoring temporal cues. To address the problems, we propose an\nend-to-end 3D reconstruction network SST, which utilizes Sparse estimated\npoints from visual SLAM system as additional Spatial guidance and fuses\nTemporal features via a novel cross-modal attention mechanism, achieving more\ndetailed reconstruction results. We propose a Local Spatial-Temporal Fusion\nmodule to exploit more informative spatial-temporal cues from multi-view color\ninformation and sparse priors, as well a Global Spatial-Temporal Fusion module\nto refine the local TSDF volumes with the world-frame model from coarse to\nfine. Extensive experiments on ScanNet and 7-Scenes demonstrate that SST\noutperforms all state-of-the-art competitors, whilst keeping a high inference\nspeed at 59 FPS, enabling real-world applications with real-time requirements.\n","authors":["Chenyangguang Zhang","Zhiqiang Lou","Yan Di","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2212.06524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05867v2","updated":"2022-12-13T12:15:17Z","published":"2022-12-12T13:10:19Z","title":"ALSO: Automotive Lidar Self-supervision by Occupancy estimation","summary":"  We propose a new self-supervised method for pre-training the backbone of deep\nperception models operating on point clouds. The core idea is to train the\nmodel on a pretext task which is the reconstruction of the surface on which the\n3D points are sampled, and to use the underlying latent vectors as input to the\nperception head. The intuition is that if the network is able to reconstruct\nthe scene surface, given only sparse input points, then it probably also\ncaptures some fragments of semantic information, that can be used to boost an\nactual perception task. This principle has a very simple formulation, which\nmakes it both easy to implement and widely applicable to a large range of 3D\nsensors and deep networks performing semantic segmentation or object detection.\nIn fact, it supports a single-stream pipeline, as opposed to most contrastive\nlearning approaches, allowing training on limited resources. We conducted\nextensive experiments on various autonomous driving datasets, involving very\ndifferent kinds of lidars, for both semantic segmentation and object detection.\nThe results show the effectiveness of our method to learn useful\nrepresentations without any annotation, compared to existing approaches. Code\nis available at https://github.com/valeoai/ALSO\n","authors":["Alexandre Boulch","Corentin Sautier","Björn Michele","Gilles Puy","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2212.05867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06516v1","updated":"2022-12-13T12:02:21Z","published":"2022-12-13T12:02:21Z","title":"Overview of The MediaEval 2022 Predicting Video Memorability Task","summary":"  This paper describes the 5th edition of the Predicting Video Memorability\nTask as part of MediaEval2022. This year we have reorganised and simplified the\ntask in order to lubricate a greater depth of inquiry. Similar to last year,\ntwo datasets are provided in order to facilitate generalisation, however, this\nyear we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem\ndataset in order to remedy underlying data quality issues, and to prioritise\nshort-term memorability prediction by elevating the Memento10k dataset as the\nprimary dataset. Additionally, a fully fledged electroencephalography\n(EEG)-based prediction sub-task is introduced. In this paper, we outline the\ncore facets of the task and its constituent sub-tasks; describing the datasets,\nevaluation metrics, and requirements for participant submissions.\n","authors":["Lorin Sweeney","Mihai Gabriel Constantin","Claire-Hélène Demarty","Camilo Fosco","Alba G. Seco de Herrera","Sebastian Halder","Graham Healy","Bogdan Ionescu","Ana Matran-Fernandez","Alan F. Smeaton","Mushfika Sultana"],"pdf_url":"https://arxiv.org/pdf/2212.06516v1.pdf","comment":"6 pages. In: MediaEval Multimedia Benchmark Workshop Working Notes,\n  2022"},{"id":"http://arxiv.org/abs/2212.06515v1","updated":"2022-12-13T12:02:05Z","published":"2022-12-13T12:02:05Z","title":"AdvMIL: Adversarial Multiple Instance Learning for the Survival Analysis\n  on Whole-Slide Images","summary":"  The survival analysis on histological whole-slide images (WSIs) is one of the\nmost important means to estimate patient prognosis. Although many\nweakly-supervised deep learning models have been developed for gigapixel WSIs,\ntheir potential is generally restricted by classical survival analysis rules\nand fully-supervision requirements. As a result, these models provide patients\nonly with a completely-certain point estimation of time-to-event, and they\ncould only learn from the well-annotated WSI data currently at a small scale.\nTo tackle these problems, we propose a novel adversarial multiple instance\nlearning (AdvMIL) framework. This framework is based on adversarial\ntime-to-event modeling, and it integrates the multiple instance learning (MIL)\nthat is much necessary for WSI representation learning. It is a plug-and-play\none, so that most existing WSI-based models with embedding-level MIL networks\ncan be easily upgraded by applying this framework, gaining the improved ability\nof survival distribution estimation and semi-supervised learning. Our extensive\nexperiments show that AdvMIL could not only bring performance improvement to\nmainstream WSI models at a relatively low computational cost, but also enable\nthese models to learn from unlabeled data with semi-supervised learning. Our\nAdvMIL framework could promote the research of time-to-event modeling in\ncomputational pathology with its novel paradigm of adversarial MIL.\n","authors":["Pei Liu","Luping Ji","Feng Ye","Bo Fu"],"pdf_url":"https://arxiv.org/pdf/2212.06515v1.pdf","comment":"13 pages, 10 figures, 8 tables"},{"id":"http://arxiv.org/abs/2212.06512v1","updated":"2022-12-13T11:52:33Z","published":"2022-12-13T11:52:33Z","title":"DifFace: Blind Face Restoration with Diffused Error Contraction","summary":"  While deep learning-based methods for blind face restoration have achieved\nunprecedented success, they still suffer from two major limitations. First,\nmost of them deteriorate when facing complex degradations out of their training\ndata. Second, these methods require multiple constraints, e.g., fidelity,\nperceptual, and adversarial losses, which require laborious hyper-parameter\ntuning to stabilize and balance their influences. In this work, we propose a\nnovel method named DifFace that is capable of coping with unseen and complex\ndegradations more gracefully without complicated loss designs. The key of our\nmethod is to establish a posterior distribution from the observed low-quality\n(LQ) image to its high-quality (HQ) counterpart. In particular, we design a\ntransition distribution from the LQ image to the intermediate state of a\npre-trained diffusion model and then gradually transmit from this intermediate\nstate to the HQ target by recursively applying a pre-trained diffusion model.\nThe transition distribution only relies on a restoration backbone that is\ntrained with $L_2$ loss on some synthetic data, which favorably avoids the\ncumbersome training process in existing methods. Moreover, the transition\ndistribution can contract the error of the restoration backbone and thus makes\nour method more robust to unknown degradations. Comprehensive experiments show\nthat DifFace is superior to current state-of-the-art methods, especially in\ncases with severe degradations. Our code and model are available at\nhttps://github.com/zsyOAOA/DifFace.\n","authors":["Zongsheng Yue","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2212.06512v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2212.06833v1","updated":"2022-12-13T11:51:03Z","published":"2022-12-13T11:51:03Z","title":"3rd Continual Learning Workshop Challenge on Egocentric Category and\n  Instance Level Object Understanding","summary":"  Continual Learning, also known as Lifelong or Incremental Learning, has\nrecently gained renewed interest among the Artificial Intelligence research\ncommunity. Recent research efforts have quickly led to the design of novel\nalgorithms able to reduce the impact of the catastrophic forgetting phenomenon\nin deep neural networks. Due to this surge of interest in the field, many\ncompetitions have been held in recent years, as they are an excellent\nopportunity to stimulate research in promising directions. This paper\nsummarizes the ideas, design choices, rules, and results of the challenge held\nat the 3rd Continual Learning in Computer Vision (CLVision) Workshop at CVPR\n2022. The focus of this competition is the complex continual object detection\ntask, which is still underexplored in literature compared to classification\ntasks. The challenge is based on the challenge version of the novel EgoObjects\ndataset, a large-scale egocentric object dataset explicitly designed to\nbenchmark continual learning algorithms for egocentric category-/instance-level\nobject understanding, which covers more than 1k unique main objects and 250+\ncategories in around 100k video frames.\n","authors":["Lorenzo Pellegrini","Chenchen Zhu","Fanyi Xiao","Zhicheng Yan","Antonio Carta","Matthias De Lange","Vincenzo Lomonaco","Roshan Sumbaly","Pau Rodriguez","David Vazquez"],"pdf_url":"https://arxiv.org/pdf/2212.06833v1.pdf","comment":"21 pages, 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.06506v1","updated":"2022-12-13T11:42:23Z","published":"2022-12-13T11:42:23Z","title":"Solving Sample-Level Out-of-Distribution Detection on 3D Medical Images","summary":"  Deep Learning (DL) models tend to perform poorly when the data comes from a\ndistribution different from the training one. In critical applications such as\nmedical imaging, out-of-distribution (OOD) detection helps to identify such\ndata samples, increasing the model's reliability. Recent works have developed\nDL-based OOD detection that achieves promising results on 2D medical images.\nHowever, scaling most of these approaches on 3D images is computationally\nintractable. Furthermore, the current 3D solutions struggle to achieve\nacceptable results in detecting even synthetic OOD samples. Such limited\nperformance might indicate that DL often inefficiently embeds large volumetric\nimages. We argue that using the intensity histogram of the original CT or MRI\nscan as embedding is descriptive enough to run OOD detection. Therefore, we\npropose a histogram-based method that requires no DL and achieves almost\nperfect results in this domain. Our proposal is supported two-fold. We evaluate\nthe performance on the publicly available datasets, where our method scores 1.0\nAUROC in most setups. And we score second in the Medical Out-of-Distribution\nchallenge without fine-tuning and exploiting task-specific knowledge. Carefully\ndiscussing the limitations, we conclude that our method solves the sample-level\nOOD detection on 3D medical images in the current setting.\n","authors":["Daria Frolova","Anton Vasiliuk","Mikhail Belyaev","Boris Shirokikh"],"pdf_url":"https://arxiv.org/pdf/2212.06506v1.pdf","comment":"20 pages, 3 figures, submitted to Computerized Medical Imaging and\n  Graphics"},{"id":"http://arxiv.org/abs/2212.06493v1","updated":"2022-12-13T11:18:08Z","published":"2022-12-13T11:18:08Z","title":"Pixel is All You Need: Adversarial Trajectory-Ensemble Active Learning\n  for Salient Object Detection","summary":"  Although weakly-supervised techniques can reduce the labeling effort, it is\nunclear whether a saliency model trained with weakly-supervised data (e.g.,\npoint annotation) can achieve the equivalent performance of its\nfully-supervised version. This paper attempts to answer this unexplored\nquestion by proving a hypothesis: there is a point-labeled dataset where\nsaliency models trained on it can achieve equivalent performance when trained\non the densely annotated dataset. To prove this conjecture, we proposed a novel\nyet effective adversarial trajectory-ensemble active learning (ATAL). Our\ncontributions are three-fold: 1) Our proposed adversarial attack triggering\nuncertainty can conquer the overconfidence of existing active learning methods\nand accurately locate these uncertain pixels. {2)} Our proposed\ntrajectory-ensemble uncertainty estimation method maintains the advantages of\nthe ensemble networks while significantly reducing the computational cost. {3)}\nOur proposed relationship-aware diversity sampling algorithm can conquer\noversampling while boosting performance. Experimental results show that our\nATAL can find such a point-labeled dataset, where a saliency model trained on\nit obtained $97\\%$ -- $99\\%$ performance of its fully-supervised version with\nonly ten annotated points per image.\n","authors":["Zhenyu Wu","Lin Wang","Wei Wang","Qing Xia","Chenglizhao Chen","Aimin Hao","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2212.06493v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2212.06486v1","updated":"2022-12-13T11:13:59Z","published":"2022-12-13T11:13:59Z","title":"Semantics-Consistent Feature Search for Self-Supervised Visual\n  Representation Learning","summary":"  In contrastive self-supervised learning, the common way to learn\ndiscriminative representation is to pull different augmented \"views\" of the\nsame image closer while pushing all other images further apart, which has been\nproven to be effective. However, it is unavoidable to construct undesirable\nviews containing different semantic concepts during the augmentation procedure.\nIt would damage the semantic consistency of representation to pull these\naugmentations closer in the feature space indiscriminately. In this study, we\nintroduce feature-level augmentation and propose a novel semantics-consistent\nfeature search (SCFS) method to mitigate this negative effect. The main idea of\nSCFS is to adaptively search semantics-consistent features to enhance the\ncontrast between semantics-consistent regions in different augmentations. Thus,\nthe trained model can learn to focus on meaningful object regions, improving\nthe semantic representation ability. Extensive experiments conducted on\ndifferent datasets and tasks demonstrate that SCFS effectively improves the\nperformance of self-supervised learning and achieves state-of-the-art\nperformance on different downstream tasks.\n","authors":["Kaiyou Song","Shan Zhang","Zihao An","Zimeng Luo","Tong Wang","Jin Xie"],"pdf_url":"https://arxiv.org/pdf/2212.06486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09879v2","updated":"2022-12-13T11:01:51Z","published":"2022-10-18T14:13:20Z","title":"Unsupervised visualization of image datasets using contrastive learning","summary":"  Visualization methods based on the nearest neighbor graph, such as t-SNE or\nUMAP, are widely used for visualizing high-dimensional data. Yet, these\napproaches only produce meaningful results if the nearest neighbors themselves\nare meaningful. For images represented in pixel space this is not the case, as\ndistances in pixel space are often not capturing our sense of similarity and\ntherefore neighbors are not semantically close. This problem can be\ncircumvented by self-supervised approaches based on contrastive learning, such\nas SimCLR, relying on data augmentation to generate implicit neighbors, but\nthese methods do not produce two-dimensional embeddings suitable for\nvisualization. Here, we present a new method, called t-SimCNE, for unsupervised\nvisualization of image data. T-SimCNE combines ideas from contrastive learning\nand neighbor embeddings, and trains a parametric mapping from the\nhigh-dimensional pixel space into two dimensions. We show that the resulting 2D\nembeddings achieve classification accuracy comparable to the state-of-the-art\nhigh-dimensional SimCLR representations, thus faithfully capturing semantic\nrelationships. Using t-SimCNE, we obtain informative visualizations of the\nCIFAR-10 and CIFAR-100 datasets, showing rich cluster structure and\nhighlighting artifacts and outliers.\n","authors":["Jan Niklas Böhm","Philipp Berens","Dmitry Kobak"],"pdf_url":"https://arxiv.org/pdf/2210.09879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05719v2","updated":"2022-12-13T10:43:04Z","published":"2022-12-12T05:52:26Z","title":"Tensor Factorization via Transformed Tensor-Tensor Product for Image\n  Alignment","summary":"  In this paper, we study the problem of a batch of linearly correlated image\nalignment, where the observed images are deformed by some unknown domain\ntransformations, and corrupted by additive Gaussian noise and sparse noise\nsimultaneously. By stacking these images as the frontal slices of a third-order\ntensor, we propose to utilize the tensor factorization method via transformed\ntensor-tensor product to explore the low-rankness of the underlying tensor,\nwhich is factorized into the product of two smaller tensors via transformed\ntensor-tensor product under any unitary transformation. The main advantage of\ntransformed tensor-tensor product is that its computational complexity is lower\ncompared with the existing literature based on transformed tensor nuclear norm.\nMoreover, the tensor $\\ell_p$ $(0<p<1)$ norm is employed to characterize the\nsparsity of sparse noise and the tensor Frobenius norm is adopted to model\nadditive Gaussian noise. A generalized Gauss-Newton algorithm is designed to\nsolve the resulting model by linearizing the domain transformations and a\nproximal Gauss-Seidel algorithm is developed to solve the corresponding\nsubproblem. Furthermore, the convergence of the proximal Gauss-Seidel algorithm\nis established, whose convergence rate is also analyzed based on the\nKurdyka-$\\L$ojasiewicz property. Extensive numerical experiments on real-world\nimage datasets are carried out to demonstrate the superior performance of the\nproposed method as compared to several state-of-the-art methods in both\naccuracy and computational time.\n","authors":["Sijia Xia","Duo Qiu","Xiongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.05719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12295v3","updated":"2022-12-13T10:32:22Z","published":"2022-02-24T18:51:19Z","title":"Factorizer: A Scalable Interpretable Approach to Context Modeling for\n  Medical Image Segmentation","summary":"  Convolutional Neural Networks (CNNs) with U-shaped architectures have\ndominated medical image segmentation, which is crucial for various clinical\npurposes. However, the inherent locality of convolution makes CNNs fail to\nfully exploit global context, essential for better recognition of some\nstructures, e.g., brain lesions. Transformers have recently proven promising\nperformance on vision tasks, including semantic segmentation, mainly due to\ntheir capability of modeling long-range dependencies. Nevertheless, the\nquadratic complexity of attention makes existing Transformer-based models use\nself-attention layers only after somehow reducing the image resolution, which\nlimits the ability to capture global contexts present at higher resolutions.\nTherefore, this work introduces a family of models, dubbed Factorizer, which\nleverages the power of low-rank matrix factorization for constructing an\nend-to-end segmentation model. Specifically, we propose a linearly scalable\napproach to context modeling, formulating Nonnegative Matrix Factorization\n(NMF) as a differentiable layer integrated into a U-shaped architecture. The\nshifted window technique is also utilized in combination with NMF to\neffectively aggregate local information. Factorizers compete favorably with\nCNNs and Transformers in terms of accuracy, scalability, and interpretability,\nachieving state-of-the-art results on the BraTS dataset for brain tumor\nsegmentation and ISLES'22 dataset for stroke lesion segmentation. Highly\nmeaningful NMF components give an additional interpretability advantage to\nFactorizers over CNNs and Transformers. Moreover, our ablation studies reveal a\ndistinctive feature of Factorizers that enables a significant speed-up in\ninference for a trained Factorizer without any extra steps and without\nsacrificing much accuracy. The code and models are publicly available at\nhttps://github.com/pashtari/factorizer.\n","authors":["Pooya Ashtari","Diana M. Sima","Lieven De Lathauwer","Dominique Sappey-Marinier","Frederik Maes","Sabine Van Huffel"],"pdf_url":"https://arxiv.org/pdf/2202.12295v3.pdf","comment":"31 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2208.09787v2","updated":"2022-12-13T10:30:06Z","published":"2022-08-21T03:07:36Z","title":"RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking","summary":"  RGB-D object tracking has attracted considerable attention recently,\nachieving promising performance thanks to the symbiosis between visual and\ndepth channels. However, given a limited amount of annotated RGB-D tracking\ndata, most state-of-the-art RGB-D trackers are simple extensions of\nhigh-performance RGB-only trackers, without fully exploiting the underlying\npotential of the depth channel in the offline training stage. To address the\ndataset deficiency issue, a new RGB-D dataset named RGBD1K is released in this\npaper. The RGBD1K contains 1,050 sequences with about 2.5M frames in total. To\ndemonstrate the benefits of training on a larger RGB-D data set in general, and\nRGBD1K in particular, we develop a transformer-based RGB-D tracker, named SPT,\nas a baseline for future visual object tracking studies using the new dataset.\nThe results, of extensive experiments using the SPT tracker emonstrate the\npotential of the RGBD1K dataset to improve the performance of RGB-D tracking,\ninspiring future developments of effective tracker designs. The dataset and\ncodes will be available on the project homepage:\nhttps://github.com/xuefeng-zhu5/RGBD1K.\n","authors":["Xue-Feng Zhu","Tianyang Xu","Zhangyong Tang","Zucheng Wu","Haodong Liu","Xiao Yang","Xiao-Jun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2208.09787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06461v1","updated":"2022-12-13T10:21:15Z","published":"2022-12-13T10:21:15Z","title":"A Statistical Model for Predicting Generalization in Few-Shot\n  Classification","summary":"  The estimation of the generalization error of classifiers often relies on a\nvalidation set. Such a set is hardly available in few-shot learning scenarios,\na highly disregarded shortcoming in the field. In these scenarios, it is common\nto rely on features extracted from pre-trained neural networks combined with\ndistance-based classifiers such as nearest class mean. In this work, we\nintroduce a Gaussian model of the feature distribution. By estimating the\nparameters of this model, we are able to predict the generalization error on\nnew classification tasks with few samples. We observe that accurate distance\nestimates between class-conditional densities are the key to accurate estimates\nof the generalization performance. Therefore, we propose an unbiased estimator\nfor these distances and integrate it in our numerical analysis. We show that\nour approach outperforms alternatives such as the leave-one-out\ncross-validation strategy in few-shot settings.\n","authors":["Yassir Bendou","Vincent Gripon","Bastien Pasdeloup","Lukas Mauch","Stefan Uhlich","Fabien Cardinaux","Ghouthi Boukli Hacene","Javier Alonso Garcia"],"pdf_url":"https://arxiv.org/pdf/2212.06461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.03364v3","updated":"2022-12-13T10:20:14Z","published":"2022-11-07T08:37:48Z","title":"Medical Diffusion -- Denoising Diffusion Probabilistic Models for 3D\n  Medical Image Generation","summary":"  Recent advances in computer vision have shown promising results in image\ngeneration. Diffusion probabilistic models in particular have generated\nrealistic images from textual input, as demonstrated by DALL-E 2, Imagen and\nStable Diffusion. However, their use in medicine, where image data typically\ncomprises three-dimensional volumes, has not been systematically evaluated.\nSynthetic images may play a crucial role in privacy preserving artificial\nintelligence and can also be used to augment small datasets. Here we show that\ndiffusion probabilistic models can synthesize high quality medical imaging\ndata, which we show for Magnetic Resonance Images (MRI) and Computed Tomography\n(CT) images. We provide quantitative measurements of their performance through\na reader study with two medical experts who rated the quality of the\nsynthesized images in three categories: Realistic image appearance, anatomical\ncorrectness and consistency between slices. Furthermore, we demonstrate that\nsynthetic images can be used in a self-supervised pre-training and improve the\nperformance of breast segmentation models when data is scarce (dice score 0.91\nvs. 0.95 without vs. with synthetic data).\n","authors":["Firas Khader","Gustav Mueller-Franzes","Soroosh Tayebi Arasteh","Tianyu Han","Christoph Haarburger","Maximilian Schulze-Hagen","Philipp Schad","Sandy Engelhardt","Bettina Baessler","Sebastian Foersch","Johannes Stegmaier","Christiane Kuhl","Sven Nebelung","Jakob Nikolas Kather","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2211.03364v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.12273v4","updated":"2022-12-13T10:06:59Z","published":"2022-03-23T08:40:42Z","title":"DAN: a Segmentation-free Document Attention Network for Handwritten\n  Document Recognition","summary":"  Unconstrained handwritten text recognition is a challenging computer vision\ntask. It is traditionally handled by a two-step approach, combining line\nsegmentation followed by text line recognition. For the first time, we propose\nan end-to-end segmentation-free architecture for the task of handwritten\ndocument recognition: the Document Attention Network. In addition to text\nrecognition, the model is trained to label text parts using begin and end tags\nin an XML-like fashion. This model is made up of an FCN encoder for feature\nextraction and a stack of transformer decoder layers for a recurrent\ntoken-by-token prediction process. It takes whole text documents as input and\nsequentially outputs characters, as well as logical layout tokens. Contrary to\nthe existing segmentation-based approaches, the model is trained without using\nany segmentation label. We achieve competitive results on the READ 2016 dataset\nat page level, as well as double-page level with a CER of 3.43% and 3.70%,\nrespectively. We also provide results for the RIMES 2009 dataset at page level,\nreaching 4.54% of CER.\n  We provide all source code and pre-trained model weights at\nhttps://github.com/FactoDeepLearning/DAN.\n","authors":["Denis Coquenet","Clément Chatelain","Thierry Paquet"],"pdf_url":"https://arxiv.org/pdf/2203.12273v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06458v1","updated":"2022-12-13T10:04:01Z","published":"2022-12-13T10:04:01Z","title":"HS-Diffusion: Learning a Semantic-Guided Diffusion Model for Head\n  Swapping","summary":"  Image-based head swapping task aims to stitch a source head to another source\nbody flawlessly. This seldom-studied task faces two major challenges: 1)\nPreserving the head and body from various sources while generating a seamless\ntransition region. 2) No paired head swapping dataset and benchmark so far. In\nthis paper, we propose an image-based head swapping framework (HS-Diffusion)\nwhich consists of a semantic-guided latent diffusion model (SG-LDM) and a\nsemantic layout generator. We blend the semantic layouts of source head and\nsource body, and then inpaint the transition region by the semantic layout\ngenerator, achieving a coarse-grained head swapping. SG-LDM can further\nimplement fine-grained head swapping with the blended layout as condition by a\nprogressive fusion process, while preserving source head and source body with\nhigh-quality reconstruction. To this end, we design a head-cover augmentation\nstrategy for training and a neck alignment trick for geometric realism.\nImportantly, we construct a new image-based head swapping benchmark and propose\ntwo tailor-designed metrics (Mask-FID and Focal-FID). Extensive experiments\ndemonstrate the superiority of our framework. The code will be available:\nhttps://github.com/qinghew/HS-Diffusion.\n","authors":["Qinghe Wang","Lijie Liu","Miao Hua","Qian He","Pengfei Zhu","Bing Cao","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2212.06458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12933v3","updated":"2022-12-13T09:49:03Z","published":"2022-11-23T13:08:58Z","title":"Join the High Accuracy Club on ImageNet with A Binary Neural Network\n  Ticket","summary":"  Binary neural networks are the extreme case of network quantization, which\nhas long been thought of as a potential edge machine learning solution.\nHowever, the significant accuracy gap to the full-precision counterparts\nrestricts their creative potential for mobile applications. In this work, we\nrevisit the potential of binary neural networks and focus on a compelling but\nunanswered problem: how can a binary neural network achieve the crucial\naccuracy level (e.g., 80%) on ILSVRC-2012 ImageNet? We achieve this goal by\nenhancing the optimization process from three complementary perspectives: (1)\nWe design a novel binary architecture BNext based on a comprehensive study of\nbinary architectures and their optimization process. (2) We propose a novel\nknowledge-distillation technique to alleviate the counter-intuitive overfitting\nproblem observed when attempting to train extremely accurate binary models. (3)\nWe analyze the data augmentation pipeline for binary networks and modernize it\nwith up-to-date techniques from full-precision models. The evaluation results\non ImageNet show that BNext, for the first time, pushes the binary model\naccuracy boundary to 80.57% and significantly outperforms all the existing\nbinary networks. Code and trained models are available at:\nhttps://github.com/hpi-xnor/BNext.git.\n","authors":["Nianhui Guo","Joseph Bethge","Christoph Meinel","Haojin Yang"],"pdf_url":"https://arxiv.org/pdf/2211.12933v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.06933v1","updated":"2022-12-13T23:06:20Z","published":"2022-12-13T23:06:20Z","title":"Paraphrase Identification with Deep Learning: A Review of Datasets and\n  Methods","summary":"  The rapid advancement of AI technology has made text generation tools like\nGPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can\npose serious threat to the credibility of various forms of media if these\ntechnologies are used for plagiarism, including scientific literature and news\nsources. Despite the development of automated methods for paraphrase\nidentification, detecting this type of plagiarism remains a challenge due to\nthe disparate nature of the datasets on which these methods are trained. In\nthis study, we review traditional and current approaches to paraphrase\nidentification and propose a refined typology of paraphrases. We also\ninvestigate how this typology is represented in popular datasets and how\nunder-representation of certain types of paraphrases impacts detection\ncapabilities. Finally, we outline new directions for future research and\ndatasets in the pursuit of more effective paraphrase detection using AI.\n","authors":["Chao Zhou","Cheng Qiu","Daniel E. Acuna"],"pdf_url":"https://arxiv.org/pdf/2212.06933v1.pdf","comment":"36 pages, 2 figures, 6 tables, 173 references"},{"id":"http://arxiv.org/abs/2112.11800v3","updated":"2022-12-13T19:03:38Z","published":"2021-12-22T11:15:49Z","title":"STEREO: Scientific Text Reuse in Open Access Publications","summary":"  We present the Webis-STEREO-21 dataset, a massive collection of Scientific\nText Reuse in Open-access publications. It contains more than 91 million cases\nof reused text passages found in 4.2 million unique open-access publications.\nFeaturing a high coverage of scientific disciplines and varieties of reuse, as\nwell as comprehensive metadata to contextualize each case, our dataset\naddresses the most salient shortcomings of previous ones on scientific writing.\nWebis-STEREO-21 allows for tackling a wide range of research questions from\ndifferent scientific backgrounds, facilitating both qualitative and\nquantitative analysis of the phenomenon as well as a first-time grounding on\nthe base rate of text reuse in scientific publications.\n","authors":["Lukas Gienapp","Wolfgang Kircheis","Bjarne Sievers","Benno Stein","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2112.11800v3.pdf","comment":"14 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.06750v1","updated":"2022-12-13T17:32:44Z","published":"2022-12-13T17:32:44Z","title":"FairRoad: Achieving Fairness for Recommender Systems with Optimized\n  Antidote Data","summary":"  Today, recommender systems have played an increasingly important role in\nshaping our experiences of digital environments and social interactions.\nHowever, as recommender systems become ubiquitous in our society, recent years\nhave also witnessed significant fairness concerns for recommender systems.\nSpecifically, studies have shown that recommender systems may inherit or even\namplify biases from historical data, and as a result, provide unfair\nrecommendations. To address fairness risks in recommender systems, most of the\nprevious approaches to date are focused on modifying either the existing\ntraining data samples or the deployed recommender algorithms, but unfortunately\nwith limited degrees of success. In this paper, we propose a new approach\ncalled fair recommendation with optimized antidote data (FairRoad), which aims\nto improve the fairness performances of recommender systems through the\nconstruction of a small and carefully crafted antidote dataset. Toward this\nend, we formulate our antidote data generation task as a mathematical\noptimization problem, which minimizes the unfairness of the targeted\nrecommender systems while not disrupting the deployed recommendation\nalgorithms. Extensive experiments show that our proposed antidote data\ngeneration algorithm significantly improve the fairness of recommender systems\nwith a small amounts of antidote data.\n","authors":["Minghong Fang","Jia Liu","Michinari Momma","Yi Sun"],"pdf_url":"https://arxiv.org/pdf/2212.06750v1.pdf","comment":"Accepted by SACMAT 2022"},{"id":"http://arxiv.org/abs/2212.06679v1","updated":"2022-12-13T15:57:32Z","published":"2022-12-13T15:57:32Z","title":"Predicting Knowledge Gain for MOOC Video Consumption","summary":"  Informal learning on the Web using search engines as well as more structured\nlearning on MOOC platforms have become very popular in recent years. As a\nresult of the vast amount of available learning resources, intelligent\nretrieval and recommendation methods are indispensable -- this is true also for\nMOOC videos. However, the automatic assessment of this content with regard to\npredicting (potential) knowledge gain has not been addressed by previous work\nyet. In this paper, we investigate whether we can predict learning success\nafter MOOC video consumption using 1) multimodal features covering slide and\nspeech content, and 2) a wide range of text-based features describing the\ncontent of the video. In a comprehensive experimental setting, we test four\ndifferent classifiers and various feature subset combinations. We conduct a\ndetailed feature importance analysis to gain insights in which modality\nbenefits knowledge gain prediction the most.\n","authors":["Christian Otto","Markos Stamatakis","Anett Hoppe","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2212.06679v1.pdf","comment":"13 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2212.03760v2","updated":"2022-12-13T14:14:53Z","published":"2022-12-07T16:31:14Z","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching\n  Task-specific and Task-agnostic Representation Learning","summary":"  Recent studies have proposed unified user modeling frameworks that leverage\nuser behavior data from various applications. Many of them benefit from\nutilizing users' behavior sequences as plain texts, representing rich\ninformation in any domain or system without losing generality. Hence, a\nquestion arises: Can language modeling for user history corpus help improve\nrecommender systems? While its versatile usability has been widely investigated\nin many domains, its applications to recommender systems still remain\nunderexplored. We show that language modeling applied directly to task-specific\nuser histories achieves excellent results on diverse recommendation tasks.\nAlso, leveraging additional task-agnostic user histories delivers significant\nperformance benefits. We further demonstrate that our approach can provide\npromising transfer learning capabilities for a broad spectrum of real-world\nrecommender systems, even on unseen domains and services.\n","authors":["Kyuyong Shin","Hanock Kwak","Wonjae Kim","Jisu Jeong","Seungjae Jung","Kyung-Min Kim","Jung-Woo Ha","Sang-Woo Lee"],"pdf_url":"https://arxiv.org/pdf/2212.03760v2.pdf","comment":"14 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2212.06560v1","updated":"2022-12-13T13:29:47Z","published":"2022-12-13T13:29:47Z","title":"Exploring Fake News Detection with Heterogeneous Social Media Context\n  Graphs","summary":"  Fake news detection has become a research area that goes way beyond a purely\nacademic interest as it has direct implications on our society as a whole.\nRecent advances have primarily focused on textbased approaches. However, it has\nbecome clear that to be effective one needs to incorporate additional,\ncontextual information such as spreading behaviour of news articles and user\ninteraction patterns on social media. We propose to construct heterogeneous\nsocial context graphs around news articles and reformulate the problem as a\ngraph classification task. Exploring the incorporation of different types of\ninformation (to get an idea as to what level of social context is most\neffective) and using different graph neural network architectures indicates\nthat this approach is highly effective with robust results on a common\nbenchmark dataset.\n","authors":["Gregor Donabauer","Udo Kruschwitz"],"pdf_url":"https://arxiv.org/pdf/2212.06560v1.pdf","comment":"Preprint accepted at the 45th European Conference on Information\n  Retrieval (ECIR 2023)"},{"id":"http://arxiv.org/abs/2212.06552v1","updated":"2022-12-13T13:08:03Z","published":"2022-12-13T13:08:03Z","title":"Domain Adaptation for Dense Retrieval through Self-Supervision by\n  Pseudo-Relevance Labeling","summary":"  Although neural information retrieval has witnessed great improvements,\nrecent works showed that the generalization ability of dense retrieval models\non target domains with different distributions is limited, which contrasts with\nthe results obtained with interaction-based models. To address this issue,\nresearchers have resorted to adversarial learning and query generation\napproaches; both approaches nevertheless resulted in limited improvements. In\nthis paper, we propose to use a self-supervision approach in which\npseudo-relevance labels are automatically generated on the target domain. To do\nso, we first use the standard BM25 model on the target domain to obtain a first\nranking of documents, and then use the interaction-based model T53B to re-rank\ntop documents. We further combine this approach with knowledge distillation\nrelying on an interaction-based teacher model trained on the source domain. Our\nexperiments reveal that pseudo-relevance labeling using T53B and the MiniLM\nteacher performs on average better than other approaches and helps improve the\nstate-of-the-art query generation approach GPL when it is fine-tuned on the\npseudo-relevance labeled data.\n","authors":["Minghan Li","Eric Gaussier"],"pdf_url":"https://arxiv.org/pdf/2212.06552v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2212.06543v1","updated":"2022-12-13T12:56:55Z","published":"2022-12-13T12:56:55Z","title":"Modelling Stance Detection as Textual Entailment Recognition and\n  Leveraging Measurement Knowledge from Social Sciences","summary":"  Stance detection (SD) can be considered a special case of textual entailment\nrecognition (TER), a generic natural language task. Modelling SD as TER may\noffer benefits like more training data and a more general learning scheme. In\nthis paper, we present an initial empirical analysis of this approach. We apply\nit to a difficult but relevant test case where no existing labelled SD dataset\nis available, because this is where modelling SD as TER may be especially\nhelpful. We also leverage measurement knowledge from social sciences to improve\nmodel performance. We discuss our findings and suggest future research\ndirections.\n","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri"],"pdf_url":"https://arxiv.org/pdf/2212.06543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06540v1","updated":"2022-12-13T12:51:14Z","published":"2022-12-13T12:51:14Z","title":"Automatic ESG Assessment of Companies by Mining and Evaluating Media\n  Coverage Data: NLP Approach and Tool","summary":"  Context: Sustainable corporate behavior is increasingly valued by society and\nimpacts corporate reputation and customer trust. Hence, companies regularly\npublish sustainability reports to shed light on their impact on environmental,\nsocial, and governance (ESG) factors. Problem: Sustainability reports are\nwritten by companies themselves and are therefore considered a\ncompany-controlled source. Contrary, studies reveal that non-corporate channels\n(e.g., media coverage) represent the main driver for ESG transparency. However,\nanalysing media coverage regarding ESG factors is challenging since (1) the\namount of published news articles grows daily, (2) media coverage data does not\nnecessarily deal with an ESG-relevant topic, meaning that it must be carefully\nfiltered, and (3) the majority of media coverage data is unstructured. Research\nGoal: We aim to extract ESG-relevant information from textual media reactions\nautomatically to calculate an ESG score for a given company. Our goal is to\nreduce the cost of ESG data collection and make ESG information available to\nthe general public. Contribution: Our contributions are three-fold: First, we\npublish a corpus of 432,411 news headlines annotated as being environmental-,\ngovernance-, social-related, or ESG-irrelevant. Second, we present our\ntool-supported approach called ESG-Miner capable of analyzing and evaluating\nheadlines on corporate ESG-performance automatically. Third, we demonstrate the\nfeasibility of our approach in an experiment and apply the ESG-Miner on 3000\nmanually labeled headlines. Our approach processes 96.7 % of the headlines\ncorrectly and shows a great performance in detecting environmental-related\nheadlines along with their correct sentiment. We encourage fellow researchers\nand practitioners to use the ESG-Miner at https://www.esg-miner.com.\n","authors":["Jannik Fischbach","Max Adam","Victor Dzhagatspanyan","Daniel Mendez","Julian Frattini","Oleksandr Kosenkov","Parisa Elahidoost"],"pdf_url":"https://arxiv.org/pdf/2212.06540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05767v2","updated":"2022-12-13T10:35:12Z","published":"2022-12-12T08:40:04Z","title":"Reasoning over Different Types of Knowledge Graphs: Static, Temporal and\n  Multi-Modal","summary":"  Knowledge graph reasoning (KGR), aiming to deduce new facts from existing\nfacts based on mined logic rules underlying knowledge graphs (KGs), has become\na fast-growing research direction. It has been proven to significantly benefit\nthe usage of KGs in many AI applications, such as question answering and\nrecommendation systems, etc. According to the graph types, the existing KGR\nmodels can be roughly divided into three categories, i.e., static models,\ntemporal models, and multi-modal models. The early works in this domain mainly\nfocus on static KGR and tend to directly apply general knowledge graph\nembedding models to the reasoning task. However, these models are not suitable\nfor more complex but practical tasks, such as inductive static KGR, temporal\nKGR, and multi-modal KGR. To this end, multiple works have been developed\nrecently, but no survey papers and open-source repositories comprehensively\nsummarize and discuss models in this important direction. To fill the gap, we\nconduct a survey for knowledge graph reasoning tracing from static to temporal\nand then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR\nmodels, and typical datasets are introduced and discussed consequently.\nMoreover, we discuss the challenges and potential opportunities. The\ncorresponding open-source repository is shared on GitHub:\nhttps://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.\n","authors":["Ke Liang","Lingyuan Meng","Meng Liu","Yue Liu","Wenxuan Tu","Siwei Wang","Sihang Zhou","Xinwang Liu","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2212.05767v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2212.05995v2","updated":"2022-12-13T09:38:07Z","published":"2022-12-12T15:55:57Z","title":"Multivariate Powered Dirichlet Hawkes Process","summary":"  The publication time of a document carries a relevant information about its\nsemantic content. The Dirichlet-Hawkes process has been proposed to jointly\nmodel textual information and publication dynamics. This approach has been used\nwith success in several recent works, and extended to tackle specific\nchallenging problems --typically for short texts or entangled publication\ndynamics. However, the prior in its current form does not allow for complex\npublication dynamics. In particular, inferred topics are independent from each\nother --a publication about finance is assumed to have no influence on\npublications about politics, for instance.\n  In this work, we develop the Multivariate Powered Dirichlet-Hawkes Process\n(MPDHP), that alleviates this assumption. Publications about various topics can\nnow influence each other. We detail and overcome the technical challenges that\narise from considering interacting topics. We conduct a systematic evaluation\nof MPDHP on a range of synthetic datasets to define its application domain and\nlimitations. Finally, we develop a use case of the MPDHP on Reddit data. At the\nend of this article, the interested reader will know how and when to use MPDHP,\nand when not to.\n","authors":["Gaël Poux-Médard","Julien Velcin","Sabine Loudcher"],"pdf_url":"https://arxiv.org/pdf/2212.05995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.00216v2","updated":"2022-12-13T02:34:56Z","published":"2022-04-01T05:38:53Z","title":"Sequential Recommendation with Causal Behavior Discovery","summary":"  The key of sequential recommendation lies in the accurate item correlation\nmodeling. Previous models infer such information based on item co-occurrences,\nwhich may fail to capture the real causal relations, and impact the\nrecommendation performance and explainability. In this paper, we equip\nsequential recommendation with a novel causal discovery module to capture\ncausalities among user behaviors. Our general idea is firstly assuming a causal\ngraph underlying item correlations, and then we learn the causal graph jointly\nwith the sequential recommender model by fitting the real user behavior data.\nMore specifically, in order to satisfy the causality requirement, the causal\ngraph is regularized by a differentiable directed acyclic constraint.\nConsidering that the number of items in recommender systems can be very large,\nwe represent different items with a unified set of latent clusters, and the\ncausal graph is defined on the cluster level, which enhances the model\nscalability and robustness. In addition, we provide theoretical analysis on the\nidentifiability of the learned causal graph. To the best of our knowledge, this\npaper makes a first step towards combining sequential recommendation with\ncausal discovery. For evaluating the recommendation performance, we implement\nour framework with different neural sequential architectures, and compare them\nwith many state-of-the-art methods based on real-world datasets. Empirical\nstudies manifest that our model can on average improve the performance by about\n7% and 11% on f1 and NDCG, respectively. To evaluate the model explainability,\nwe build a new dataset with human labeled explanations for both quantitative\nand qualitative analysis.\n","authors":["Zhenlei Wang","Xu Chen","Rui Zhou","Quanyu Dai","Zhenhua Dong","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2204.00216v2.pdf","comment":"Accepted by ICDE 2023"},{"id":"http://arxiv.org/abs/2208.13713v2","updated":"2022-12-13T02:30:52Z","published":"2022-08-29T16:49:24Z","title":"Online Bidding Algorithms for Return-on-Spend Constrained Advertisers","summary":"  Online advertising has recently grown into a highly competitive and complex\nmulti-billion-dollar industry, with advertisers bidding for ad slots at large\nscales and high frequencies. This has resulted in a growing need for efficient\n\"auto-bidding\" algorithms that determine the bids for incoming queries to\nmaximize advertisers' targets subject to their specified constraints. This work\nexplores efficient online algorithms for a single value-maximizing advertiser\nunder an increasingly popular constraint: Return-on-Spend (RoS). We quantify\nefficiency in terms of regret relative to the optimal algorithm, which knows\nall queries a priori.\n  We contribute a simple online algorithm that achieves near-optimal regret in\nexpectation while always respecting the specified RoS constraint when the input\nsequence of queries are i.i.d. samples from some distribution. We also\nintegrate our results with the previous work of Balseiro, Lu, and Mirrokni\n[BLM20] to achieve near-optimal regret while respecting both RoS and fixed\nbudget constraints.\n  Our algorithm follows the primal-dual framework and uses online mirror\ndescent (OMD) for the dual updates. However, we need to use a non-canonical\nsetup of OMD, and therefore the classic low-regret guarantee of OMD, which is\nfor the adversarial setting in online learning, no longer holds. Nonetheless,\nin our case and more generally where low-regret dynamics are applied in\nalgorithm design, the gradients encountered by OMD can be far from adversarial\nbut influenced by our algorithmic choices. We exploit this key insight to show\nour OMD setup achieves low regret in the realm of our algorithm.\n","authors":["Zhe Feng","Swati Padmanabhan","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2208.13713v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2204.05798v2","updated":"2022-12-13T23:32:39Z","published":"2022-04-12T13:32:31Z","title":"Hypercomplex Neural Architectures for Multi-View Breast Cancer\n  Classification","summary":"  Traditionally, deep learning methods for breast cancer classification perform\na single-view analysis. However, radiologists simultaneously analyze all four\nviews that compose a mammography exam, owing to the correlations contained in\nmammography views, which present crucial information for identifying tumors. In\nlight of this, some studies have started to propose multi-view methods.\nNevertheless, in such existing architectures, mammogram views are processed as\nindependent images by separate convolutional branches, thus losing correlations\namong them. To overcome such limitations, in this paper we propose a novel\napproach for multi-view breast cancer classification based on parameterized\nhypercomplex neural networks. Thanks to hypercomplex algebra properties, our\nnetworks are able to model, and thus leverage, existing correlations between\nthe different views that comprise a mammogram, thus mimicking the reading\nprocess performed by clinicians. The proposed methods are able to handle the\ninformation of a patient altogether without breaking the multi-view nature of\nthe exam. We define architectures designed to process two-view exams, namely\nPHResNets, and four-view exams, i.e., PHYSEnet and PHYBOnet. Through an\nextensive experimental evaluation conducted with publicly available datasets,\nwe demonstrate that our proposed models clearly outperform real-valued\ncounterparts and also state-of-the-art methods, proving that breast cancer\nclassification benefits from the proposed multi-view architectures. We also\nassess the method's robustness beyond mammogram analysis by considering\ndifferent benchmarks, as well as a finer-scaled task such as segmentation. Full\ncode and pretrained models for complete reproducibility of our experiments are\nfreely available at: https://github.com/ispamm/PHBreast.\n","authors":["Eleonora Lopez","Eleonora Grassucci","Martina Valleriani","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2204.05798v2.pdf","comment":"This paper has been submitted to IEEE Transactions on Neural Networks\n  and Learning Systems"},{"id":"http://arxiv.org/abs/2207.11640v2","updated":"2022-12-13T23:26:05Z","published":"2022-07-24T02:38:54Z","title":"Reliable amortized variational inference with physics-based latent\n  distribution correction","summary":"  Bayesian inference for high-dimensional inverse problems is computationally\ncostly and requires selecting a suitable prior distribution. Amortized\nvariational inference addresses these challenges via a neural network that acts\nas a surrogate conditional distribution, matching the posterior distribution\nnot only for one instance of data, but a distribution of data pertaining to a\nspecific inverse problem. During inference, the neural network -- in our case a\nconditional normalizing flow -- provides posterior samples with virtually no\ncost. However, the accuracy of Amortized variational inference relies on the\navailability of high-fidelity training data, which seldom exists in geophysical\ninverse problems due to the Earth's heterogeneity. In addition, the network is\nprone to errors if evaluated over out-of-distribution data. As such, we propose\nto increases the resilience of amortized variational inference in presence of\nmoderate data distribution shifts. We achieve this via a correction to the\nlatent distribution that improves the posterior distribution approximation for\nthe data at hand. The correction involves relaxing the standard Gaussian\nassumption on the latent distribution and parameterizing it via a Gaussian\ndistribution with an unknown mean and (diagonal) covariance. These unknowns are\nthen estimated by minimizing the Kullback-Leibler divergence between the\ncorrected and (physics-based) true posterior distributions. While generic and\napplicable to other inverse problems, by means of a linearized seismic imaging\nexample, we show that our correction step improves the robustness of amortized\nvariational inference with respect to changes in number of seismic sources,\nnoise variance, and shifts in the prior distribution. This approach provides a\nseismic image with limited artifacts and an assessment of its uncertainty with\napproximately the same cost as five reverse-time migrations.\n","authors":["Ali Siahkoohi","Gabrio Rizzuti","Rafael Orozco","Felix J. Herrmann"],"pdf_url":"https://arxiv.org/pdf/2207.11640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.13419v7","updated":"2022-12-13T23:18:51Z","published":"2021-09-28T01:20:08Z","title":"The Role of Lookahead and Approximate Policy Evaluation in Reinforcement\n  Learning with Linear Value Function Approximation","summary":"  Function approximation is widely used in reinforcement learning to handle the\ncomputational difficulties associated with very large state spaces. However,\nfunction approximation introduces errors which may lead to instabilities when\nusing approximate dynamic programming techniques to obtain the optimal policy.\nTherefore, techniques such as lookahead for policy improvement and m-step\nrollout for policy evaluation are used in practice to improve the performance\nof approximate dynamic programming with function approximation. We\nquantitatively characterize, for the first time, the impact of lookahead and\nm-step rollout on the performance of approximate dynamic programming (DP) with\nfunction approximation: (i) without a sufficient combination of lookahead and\nm-step rollout, approximate DP may not converge, (ii) both lookahead and m-step\nrollout improve the convergence rate of approximate DP, and (iii) lookahead\nhelps mitigate the effect of function approximation and the discount factor on\nthe asymptotic performance of the algorithm. Our results are presented for two\napproximate DP methods: one which uses least-squares regression to perform\nfunction approximation and another which performs several steps of gradient\ndescent of the least-squares objective in each iteration.\n","authors":["Anna Winnicki","Joseph Lubars","Michael Livesay","R. Srikant"],"pdf_url":"https://arxiv.org/pdf/2109.13419v7.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2103.06671v6","updated":"2022-12-13T23:12:15Z","published":"2021-03-11T14:01:14Z","title":"Sample Complexity of Offline Reinforcement Learning with Deep ReLU\n  Networks","summary":"  Offline reinforcement learning (RL) leverages previously collected data for\npolicy optimization without any further active exploration. Despite the recent\ninterest in this problem, its theoretical results in neural network function\napproximation settings remain elusive. In this paper, we study the statistical\ntheory of offline RL with deep ReLU network function approximation. In\nparticular, we establish the sample complexity of $n = \\tilde{\\mathcal{O}}(\nH^{4 + 4 \\frac{d}{\\alpha}} \\kappa_{\\mu}^{1 + \\frac{d}{\\alpha}} \\epsilon^{-2 -\n2\\frac{d}{\\alpha}} )$ for offline RL with deep ReLU networks, where\n$\\kappa_{\\mu}$ is a measure of distributional shift, {$H = (1-\\gamma)^{-1}$ is\nthe effective horizon length}, $d$ is the dimension of the state-action space,\n$\\alpha$ is a (possibly fractional) smoothness parameter of the underlying\nMarkov decision process (MDP), and $\\epsilon$ is a user-specified error.\nNotably, our sample complexity holds under two novel considerations: the Besov\ndynamic closure and the correlated structure. While the Besov dynamic closure\nsubsumes the dynamic conditions for offline RL in the prior works, the\ncorrelated structure renders the prior works of offline RL with general/neural\nnetwork function approximation improper or inefficient {in long (effective)\nhorizon problems}. To the best of our knowledge, this is the first theoretical\ncharacterization of the sample complexity of offline RL with deep neural\nnetwork function approximation under the general Besov regularity condition\nthat goes beyond {the linearity regime} in the traditional Reproducing Hilbert\nkernel spaces and Neural Tangent Kernels.\n","authors":["Thanh Nguyen-Tang","Sunil Gupta","Hung Tran-The","Svetha Venkatesh"],"pdf_url":"https://arxiv.org/pdf/2103.06671v6.pdf","comment":"https://openreview.net/forum?id=LdEm0umNcv"},{"id":"http://arxiv.org/abs/2212.06929v1","updated":"2022-12-13T22:54:24Z","published":"2022-12-13T22:54:24Z","title":"Generating extreme quantum scattering in graphene with machine learning","summary":"  Graphene quantum dots provide a platform for manipulating electron behaviors\nin two-dimensional (2D) Dirac materials. Most previous works were of the\n\"forward\" type in that the objective was to solve various confinement,\ntransport and scattering problems with given structures that can be generated\nby, e.g., applying an external electrical field. There are applications such as\ncloaking or superscattering where the challenging problem of inverse design\nneeds to be solved: finding a quantum-dot structure according to certain\ndesired functional characteristics. A brute-force search of the system\nconfiguration based directly on the solutions of the Dirac equation is\ncomputational infeasible. We articulate a machine-learning approach to\naddressing the inverse-design problem where artificial neural networks subject\nto physical constraints are exploited to replace the rigorous Dirac equation\nsolver. In particular, we focus on the problem of designing a quantum dot\nstructure to generate both cloaking and superscattering in terms of the\nscattering efficiency as a function of the energy. We construct a physical loss\nfunction that enables accurate prediction of the scattering characteristics. We\ndemonstrate that, in the regime of Klein tunneling, the scattering efficiency\ncan be designed to vary over two orders of magnitudes, allowing any scattering\ncurve to be generated from a proper combination of the gate potentials. Our\nphysics-based machine-learning approach can be a powerful design tool for 2D\nDirac material-based electronics.\n","authors":["Chen-Di Han","Ying-Cheng Lai"],"pdf_url":"https://arxiv.org/pdf/2212.06929v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2212.06925v1","updated":"2022-12-13T22:42:00Z","published":"2022-12-13T22:42:00Z","title":"On the Relationship Between Explanation and Prediction: A Causal View","summary":"  Explainability has become a central requirement for the development,\ndeployment, and adoption of machine learning (ML) models and we are yet to\nunderstand what explanation methods can and cannot do. Several factors such as\ndata, model prediction, hyperparameters used in training the model, and random\ninitialization can all influence downstream explanations. While previous work\nempirically hinted that explanations (E) may have little relationship with the\nprediction (Y), there is a lack of conclusive study to quantify this\nrelationship. Our work borrows tools from causal inference to systematically\nassay this relationship. More specifically, we measure the relationship between\nE and Y by measuring the treatment effect when intervening on their causal\nancestors (hyperparameters) (inputs to generate saliency-based Es or Ys). We\ndiscover that Y's relative direct influence on E follows an odd pattern; the\ninfluence is higher in the lowest-performing models than in mid-performing\nmodels, and it then decreases in the top-performing models. We believe our work\nis a promising first step towards providing better guidance for practitioners\nwho can make more informed decisions in utilizing these explanations by knowing\nwhat factors are at play and how they relate to their end task.\n","authors":["Amir-Hossein Karimi","Krikamol Muandet","Simon Kornblith","Bernhard Schölkopf","Been Kim"],"pdf_url":"https://arxiv.org/pdf/2212.06925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06921v1","updated":"2022-12-13T22:29:14Z","published":"2022-12-13T22:29:14Z","title":"Losses over Labels: Weakly Supervised Learning via Direct Loss\n  Construction","summary":"  Owing to the prohibitive costs of generating large amounts of labeled data,\nprogrammatic weak supervision is a growing paradigm within machine learning. In\nthis setting, users design heuristics that provide noisy labels for subsets of\nthe data. These weak labels are combined (typically via a graphical model) to\nform pseudolabels, which are then used to train a downstream model. In this\nwork, we question a foundational premise of the typical weakly supervised\nlearning pipeline: given that the heuristic provides all ``label\" information,\nwhy do we need to generate pseudolabels at all? Instead, we propose to directly\ntransform the heuristics themselves into corresponding loss functions that\npenalize differences between our model and the heuristic. By constructing\nlosses directly from the heuristics, we can incorporate more information than\nis used in the standard weakly supervised pipeline, such as how the heuristics\nmake their decisions, which explicitly informs feature selection during\ntraining. We call our method Losses over Labels (LoL) as it creates losses\ndirectly from heuristics without going through the intermediate step of a\nlabel. We show that LoL improves upon existing weak supervision methods on\nseveral benchmark text and image classification tasks and further demonstrate\nthat incorporating gradient information leads to better performance on almost\nevery task.\n","authors":["Dylan Sam","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2212.06921v1.pdf","comment":"13 pages, 3 figures, To be published in AAAI 2023"},{"id":"http://arxiv.org/abs/2211.10943v2","updated":"2022-12-13T22:19:14Z","published":"2022-11-20T10:49:22Z","title":"Scalable Collaborative Learning via Representation Sharing","summary":"  Privacy-preserving machine learning has become a key conundrum for\nmulti-party artificial intelligence. Federated learning (FL) and Split Learning\n(SL) are two frameworks that enable collaborative learning while keeping the\ndata private (on device). In FL, each data holder trains a model locally and\nreleases it to a central server for aggregation. In SL, the clients must\nrelease individual cut-layer activations (smashed data) to the server and wait\nfor its response (during both inference and back propagation). While relevant\nin several settings, both of these schemes have a high communication cost, rely\non server-level computation algorithms and do not allow for tunable levels of\ncollaboration. In this work, we present a novel approach for privacy-preserving\nmachine learning, where the clients collaborate via online knowledge\ndistillation using a contrastive loss (contrastive w.r.t. the labels). The goal\nis to ensure that the participants learn similar features on similar classes\nwithout sharing their input data. To do so, each client releases averaged last\nhidden layer activations of similar labels to a central server that only acts\nas a relay (i.e., is not involved in the training or aggregation of the\nmodels). Then, the clients download these last layer activations (feature\nrepresentations) of the ensemble of users and distill their knowledge in their\npersonal model using a contrastive objective. For cross-device applications\n(i.e., small local datasets and limited computational capacity), this approach\nincreases the utility of the models compared to independent learning and other\nfederated knowledge distillation (FD) schemes, is communication efficient and\nis scalable with the number of clients. We prove theoretically that our\nframework is well-posed, and we benchmark its performance against standard FD\nand FL on various datasets using different model architectures.\n","authors":["Frédéric Berdoz","Abhishek Singh","Martin Jaggi","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2211.10943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14396v2","updated":"2022-12-13T21:34:54Z","published":"2022-10-26T00:23:36Z","title":"FeDXL: Provable Federated Learning for Deep X-Risk Optimization","summary":"  In this paper, we tackle a novel federated learning (FL) problem for\noptimizing a family of X-risks, to which no existing FL algorithms are\napplicable. In particular, the objective has the form of $\\mathbb E_{z\\sim S_1}\nf(\\mathbb E_{z'\\sim S_2} \\ell(w; z, z'))$, where two sets of data $S_1, S_2$\nare distributed over multiple machines, $\\ell(\\cdot)$ is a pairwise loss that\nonly depends on the prediction outputs of the input data pairs $(z, z')$, and\n$f(\\cdot)$ is possibly a non-linear non-convex function. This problem has\nimportant applications in machine learning, e.g., AUROC maximization with a\npairwise loss, and partial AUROC maximization with a compositional loss. The\nchallenges for designing an FL algorithm lie in the non-decomposability of the\nobjective over multiple machines and the interdependency between different\nmachines. To address the challenges, we propose an active-passive decomposition\nframework that decouples the gradient's components with two types, namely\nactive parts and passive parts, where the active parts depend on local data\nthat are computed with the local model and the passive parts depend on other\nmachines that are communicated/computed based on historical models and samples.\nUnder this framework, we develop two provable FL algorithms (FeDXL) for\nhandling linear and nonlinear $f$, respectively, based on federated averaging\nand merging. We develop a novel theoretical analysis to combat the latency of\nthe passive parts and the interdependency between the local model parameters\nand the involved data for computing local gradient estimators. We establish\nboth iteration and communication complexities and show that using the\nhistorical samples and models for computing the passive parts do not degrade\nthe complexities. We conduct empirical studies of FeDXL for deep AUROC and\npartial AUROC maximization, and demonstrate their performance compared with\nseveral baselines.\n","authors":["Zhishuai Guo","Rong Jin","Jiebo Luo","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2210.14396v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06908v1","updated":"2022-12-13T21:21:07Z","published":"2022-12-13T21:21:07Z","title":"Enabling the Wireless Metaverse via Semantic Multiverse Communication","summary":"  Metaverse over wireless networks is an emerging use case of the sixth\ngeneration (6G) wireless systems, posing unprecedented challenges in terms of\nits multi-modal data transmissions with stringent latency and reliability\nrequirements. Towards enabling this wireless metaverse, in this article we\npropose a novel semantic communication (SC) framework by decomposing the\nmetaverse into human/machine agent-specific semantic multiverses (SMs). An SM\nstored at each agent comprises a semantic encoder and a generator, leveraging\nrecent advances in generative artificial intelligence (AI). To improve\ncommunication efficiency, the encoder learns the semantic representations (SRs)\nof multi-modal data, while the generator learns how to manipulate them for\nlocally rendering scenes and interactions in the metaverse. Since these learned\nSMs are biased towards local environments, their success hinges on\nsynchronizing heterogeneous SMs in the background while communicating SRs in\nthe foreground, turning the wireless metaverse problem into the problem of\nsemantic multiverse communication (SMC). Based on this SMC architecture, we\npropose several promising algorithmic and analytic tools for modeling and\ndesigning SMC, ranging from distributed learning and multi-agent reinforcement\nlearning (MARL) to signaling games and symbolic AI.\n","authors":["Jihong Park","Jinho Choi","Seong-Lyun Kim","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2212.06908v1.pdf","comment":"7 pages, 6 figures, submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2212.06898v1","updated":"2022-12-13T20:51:03Z","published":"2022-12-13T20:51:03Z","title":"Bridging Graph Position Encodings for Transformers with Weighted\n  Graph-Walking Automata","summary":"  A current goal in the graph neural network literature is to enable\ntransformers to operate on graph-structured data, given their success on\nlanguage and vision tasks. Since the transformer's original sinusoidal\npositional encodings (PEs) are not applicable to graphs, recent work has\nfocused on developing graph PEs, rooted in spectral graph theory or various\nspatial features of a graph. In this work, we introduce a new graph PE, Graph\nAutomaton PE (GAPE), based on weighted graph-walking automata (a novel\nextension of graph-walking automata). We compare the performance of GAPE with\nother PE schemes on both machine translation and graph-structured tasks, and we\nshow that it generalizes several other PEs. An additional contribution of this\nstudy is a theoretical and controlled experimental comparison of many recent\nPEs in graph transformers, independent of the use of edge features.\n","authors":["Patrick Soga","David Chiang"],"pdf_url":"https://arxiv.org/pdf/2212.06898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06896v1","updated":"2022-12-13T20:48:06Z","published":"2022-12-13T20:48:06Z","title":"In-Season Crop Progress in Unsurveyed Regions using Networks Trained on\n  Synthetic Data","summary":"  Many commodity crops have growth stages during which they are particularly\nvulnerable to stress-induced yield loss. In-season crop progress information is\nuseful for quantifying crop risk, and satellite remote sensing (RS) can be used\nto track progress at regional scales. At present, all existing RS-based crop\nprogress estimation (CPE) methods which target crop-specific stages rely on\nground truth data for training/calibration. This reliance on ground survey data\nconfines CPE methods to surveyed regions, limiting their utility. In this\nstudy, a new method is developed for conducting RS-based in-season CPE in\nunsurveyed regions by combining data from surveyed regions with synthetic crop\nprogress data generated for an unsurveyed region. Corn-growing zones in\nArgentina were used as surrogate 'unsurveyed' regions. Existing weather\ngeneration, crop growth, and optical radiative transfer models were linked to\nproduce synthetic weather, crop progress, and canopy reflectance data. A neural\nnetwork (NN) method based upon bi-directional Long Short-Term Memory was\ntrained separately on surveyed data, synthetic data, and two different\ncombinations of surveyed and synthetic data. A stopping criterion was developed\nwhich uses the weighted divergence of surveyed and synthetic data validation\nloss. Net F1 scores across all crop progress stages increased by 8.7% when\ntrained on a combination of surveyed region and synthetic data, and overall\nperformance was only 21% lower than when the NN was trained on surveyed data\nand applied in the US Midwest. Performance gain from synthetic data was\ngreatest in zones with dual planting windows, while the inclusion of surveyed\nregion data from the US Midwest helped mitigate NN sensitivity to noise in NDVI\ndata. Overall results suggest in-season CPE in other unsurveyed regions may be\npossible with increased quantity and variety of synthetic crop progress data.\n","authors":["George Worrall","Jasmeet Judge"],"pdf_url":"https://arxiv.org/pdf/2212.06896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06891v1","updated":"2022-12-13T20:33:54Z","published":"2022-12-13T20:33:54Z","title":"Interactive Learning with Pricing for Optimal and Stable Allocations in\n  Markets","summary":"  Large-scale online recommendation systems must facilitate the allocation of a\nlimited number of items among competing users while learning their preferences\nfrom user feedback. As a principled way of incorporating market constraints and\nuser incentives in the design, we consider our objectives to be two-fold:\nmaximal social welfare with minimal instability. To maximize social welfare,\nour proposed framework enhances the quality of recommendations by exploring\nallocations that optimistically maximize the rewards. To minimize instability,\na measure of users' incentives to deviate from recommended allocations, the\nalgorithm prices the items based on a scheme derived from the Walrasian\nequilibria. Though it is known that these equilibria yield stable prices for\nmarkets with known user preferences, our approach accounts for the inherent\nuncertainty in the preferences and further ensures that the users accept their\nrecommendations under offered prices. To the best of our knowledge, our\napproach is the first to integrate techniques from combinatorial bandits,\noptimal resource allocation, and collaborative filtering to obtain an algorithm\nthat achieves sub-linear social welfare regret as well as sub-linear\ninstability. Empirical studies on synthetic and real-world data also\ndemonstrate the efficacy of our strategy compared to approaches that do not\nfully incorporate all these aspects.\n","authors":["Yigit Efe Erginbas","Soham Phade","Kannan Ramchandran"],"pdf_url":"https://arxiv.org/pdf/2212.06891v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2207.04143"},{"id":"http://arxiv.org/abs/2212.06874v1","updated":"2022-12-13T19:38:39Z","published":"2022-12-13T19:38:39Z","title":"Statistical Safety and Robustness Guarantees for Feedback Motion\n  Planning of Unknown Underactuated Stochastic Systems","summary":"  We present a method for providing statistical guarantees on runtime safety\nand goal reachability for integrated planning and control of a class of systems\nwith unknown nonlinear stochastic underactuated dynamics. Specifically, given a\ndynamics dataset, our method jointly learns a mean dynamics model, a\nspatially-varying disturbance bound that captures the effect of noise and model\nmismatch, and a feedback controller based on contraction theory that stabilizes\nthe learned dynamics. We propose a sampling-based planner that uses the mean\ndynamics model and simultaneously bounds the closed-loop tracking error via a\nlearned disturbance bound. We employ techniques from Extreme Value Theory (EVT)\nto estimate, to a specified level of confidence, several constants which\ncharacterize the learned components and govern the size of the tracking error\nbound. This ensures plans are guaranteed to be safely tracked at runtime. We\nvalidate that our guarantees translate to empirical safety in simulation on a\n10D quadrotor, and in the real world on a physical CrazyFlie quadrotor and\nClearpath Jackal robot, whereas baselines that ignore the model error and\nstochasticity are unsafe.\n","authors":["Craig Knuth","Glen Chou","Jamie Reese","Joe Moore"],"pdf_url":"https://arxiv.org/pdf/2212.06874v1.pdf","comment":"Submitted to ICRA 2023"},{"id":"http://arxiv.org/abs/2212.06868v1","updated":"2022-12-13T19:24:08Z","published":"2022-12-13T19:24:08Z","title":"Deep Image Style Transfer from Freeform Text","summary":"  This paper creates a novel method of deep neural style transfer by generating\nstyle images from freeform user text input. The language model and style\ntransfer model form a seamless pipeline that can create output images with\nsimilar losses and improved quality when compared to baseline style transfer\nmethods. The language model returns a closely matching image given a style text\nand description input, which is then passed to the style transfer model with an\ninput content image to create a final output. A proof-of-concept tool is also\ndeveloped to integrate the models and demonstrate the effectiveness of deep\nimage style transfer from freeform text.\n","authors":["Tejas Santanam","Mengyang Liu","Jiangyue Yu","Zhaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2212.06868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07177v3","updated":"2022-12-13T19:18:27Z","published":"2022-04-14T18:07:50Z","title":"Active Learning for Regression by Inverse Distance Weighting","summary":"  This paper proposes an active learning (AL) algorithm to solve regression\nproblems based on inverse-distance weighting functions for selecting the\nfeature vectors to query. The algorithm has the following features: (i)\nsupports both pool-based and population-based sampling; (ii) is not tailored to\na particular class of predictors; (iii) can handle known and unknown\nconstraints on the queryable feature vectors; and (iv) can run either\nsequentially, or in batch mode, depending on how often the predictor is\nretrained. The potentials of the method are shown in numerical tests on\nillustrative synthetic problems and real-world datasets. An implementation of\nthe algorithm, which we call IDEAL (Inverse-Distance based Exploration for\nActive Learning), is available at http://cse.lab.imtlucca.it/~bemporad/ideal.\n","authors":["Alberto Bemporad"],"pdf_url":"https://arxiv.org/pdf/2204.07177v3.pdf","comment":"26 pages, 11 figures. Submitted for publication"},{"id":"http://arxiv.org/abs/2208.07448v3","updated":"2022-12-13T19:15:05Z","published":"2022-08-15T21:56:30Z","title":"Self-Supervised Learning for Anomalous Channel Detection in EEG Graphs:\n  Application to Seizure Analysis","summary":"  Electroencephalogram (EEG) signals are effective tools towards seizure\nanalysis where one of the most important challenges is accurate detection of\nseizure events and brain regions in which seizure happens or initiates.\nHowever, all existing machine learning-based algorithms for seizure analysis\nrequire access to the labeled seizure data while acquiring labeled data is very\nlabor intensive, expensive, as well as clinicians dependent given the\nsubjective nature of the visual qualitative interpretation of EEG signals. In\nthis paper, we propose to detect seizure channels and clips in a\nself-supervised manner where no access to the seizure data is needed. The\nproposed method considers local structural and contextual information embedded\nin EEG graphs by employing positive and negative sub-graphs. We train our\nmethod through minimizing contrastive and generative losses. The employ of\nlocal EEG sub-graphs makes the algorithm an appropriate choice when accessing\nto the all EEG channels is impossible due to complications such as skull\nfractures. We conduct an extensive set of experiments on the largest seizure\ndataset and demonstrate that our proposed framework outperforms the\nstate-of-the-art methods in the EEG-based seizure study. The proposed method is\nthe only study that requires no access to the seizure data in its training\nphase, yet establishes a new state-of-the-art to the field, and outperforms all\nrelated supervised methods.\n","authors":["Thi Kieu Khanh Ho","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2208.07448v3.pdf","comment":"Accepted at AAAI-23"},{"id":"http://arxiv.org/abs/2212.06858v1","updated":"2022-12-13T19:02:35Z","published":"2022-12-13T19:02:35Z","title":"LidarCLIP or: How I Learned to Talk to Point Clouds","summary":"  Research connecting text and images has recently seen several breakthroughs,\nwith models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection\nbetween text and other visual modalities, such as lidar data, has received less\nattention, prohibited by the lack of text-lidar datasets. In this work, we\npropose LidarCLIP, a mapping from automotive point clouds to a pre-existing\nCLIP embedding space. Using image-lidar pairs, we supervise a point cloud\nencoder with the image CLIP embeddings, effectively relating text and lidar\ndata with the image domain as an intermediary. We show the effectiveness of\nLidarCLIP by demonstrating that lidar-based retrieval is generally on par with\nimage-based retrieval, but with complementary strengths and weaknesses. By\ncombining image and lidar features, we improve upon both single-modality\nmethods and enable a targeted search for challenging detection scenarios under\nadverse sensor conditions. We also use LidarCLIP as a tool to investigate\nfundamental lidar capabilities through natural language. Finally, we leverage\nour compatibility with CLIP to explore a range of applications, such as point\ncloud captioning and lidar-to-image generation, without any additional\ntraining. We hope LidarCLIP can inspire future work to dive deeper into\nconnections between text and point cloud understanding. Code and trained models\navailable at https://github.com/atonderski/lidarclip.\n","authors":["Georg Hess","Adam Tonderski","Christoffer Petersson","Lennart Svensson","Kalle Åström"],"pdf_url":"https://arxiv.org/pdf/2212.06858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00682v2","updated":"2022-12-13T19:00:31Z","published":"2022-12-01T17:38:01Z","title":"Shining light on data: Geometric data analysis through quantum dynamics","summary":"  Experimental sciences have come to depend heavily on our ability to organize\nand interpret high-dimensional datasets. Natural laws, conservation principles,\nand inter-dependencies among observed variables yield geometric structure, with\nfewer degrees of freedom, on the dataset. We introduce the frameworks of\nsemiclassical and microlocal analysis to data analysis and develop a novel, yet\nnatural uncertainty principle for extracting fine-scale features of this\ngeometric structure in data, crucially dependent on data-driven approximations\nto quantum mechanical processes underlying geometric optics. This leads to the\nfirst tractable algorithm for approximation of wave dynamics and geodesics on\ndata manifolds with rigorous probabilistic convergence rates under the manifold\nhypothesis. We demonstrate our algorithm on real-world datasets, including an\nanalysis of population mobility information during the COVID-19 pandemic to\nachieve four-fold improvement in dimensionality reduction over existing\nstate-of-the-art and reveal anomalous behavior exhibited by less than 1.2% of\nthe entire dataset. Our work initiates the study of data-driven quantum\ndynamics for analyzing datasets, and we outline several future directions for\nresearch.\n","authors":["Akshat Kumar","Mohan Sarovar"],"pdf_url":"https://arxiv.org/pdf/2212.00682v2.pdf","comment":"Supplementary Material has high overlap with arXiv:2112.11161 by the\n  same authors. v2 reorganizes presentation of results in paper"},{"id":"http://arxiv.org/abs/2212.06822v1","updated":"2022-12-13T18:58:21Z","published":"2022-12-13T18:58:21Z","title":"Adversarial Attacks and Defences for Skin Cancer Classification","summary":"  There has been a concurrent significant improvement in the medical images\nused to facilitate diagnosis and the performance of machine learning techniques\nto perform tasks such as classification, detection, and segmentation in recent\nyears. As a result, a rapid increase in the usage of such systems can be\nobserved in the healthcare industry, for instance in the form of medical image\nclassification systems, where these models have achieved diagnostic parity with\nhuman physicians. One such application where this can be observed is in\ncomputer vision tasks such as the classification of skin lesions in\ndermatoscopic images. However, as stakeholders in the healthcare industry, such\nas insurance companies, continue to invest extensively in machine learning\ninfrastructure, it becomes increasingly important to understand the\nvulnerabilities in such systems. Due to the highly critical nature of the tasks\nbeing carried out by these machine learning models, it is necessary to analyze\ntechniques that could be used to take advantage of these vulnerabilities and\nmethods to defend against them. This paper explores common adversarial attack\ntechniques. The Fast Sign Gradient Method and Projected Descent Gradient are\nused against a Convolutional Neural Network trained to classify dermatoscopic\nimages of skin lesions. Following that, it also discusses one of the most\npopular adversarial defense techniques, adversarial training. The performance\nof the model that has been trained on adversarial examples is then tested\nagainst the previously mentioned attacks, and recommendations to improve neural\nnetworks robustness are thus provided based on the results of the experiment.\n","authors":["Vinay Jogani","Joy Purohit","Ishaan Shivhare","Samina Attari","Shraddha Surtkar"],"pdf_url":"https://arxiv.org/pdf/2212.06822v1.pdf","comment":"6 pages, 7 figures, 2 tables, 2nd International Conference for\n  Advancement in Technology (ICONAT 2023), Goa, India"},{"id":"http://arxiv.org/abs/2212.06817v1","updated":"2022-12-13T18:55:15Z","published":"2022-12-13T18:55:15Z","title":"RT-1: Robotics Transformer for Real-World Control at Scale","summary":"  By transferring knowledge from large, diverse, task-agnostic datasets, modern\nmachine learning models can solve specific downstream tasks either zero-shot or\nwith small task-specific datasets to a high level of performance. While this\ncapability has been demonstrated in other fields such as computer vision,\nnatural language processing or speech recognition, it remains to be shown in\nrobotics, where the generalization capabilities of the models are particularly\ncritical due to the difficulty of collecting real-world robotic data. We argue\nthat one of the keys to the success of such general robotic models lies with\nopen-ended task-agnostic training, combined with high-capacity architectures\nthat can absorb all of the diverse, robotic data. In this paper, we present a\nmodel class, dubbed Robotics Transformer, that exhibits promising scalable\nmodel properties. We verify our conclusions in a study of different model\nclasses and their ability to generalize as a function of the data size, model\nsize, and data diversity based on a large-scale data collection on real robots\nperforming real-world tasks. The project's website and videos can be found at\nrobotics-transformer.github.io\n","authors":["Anthony Brohan","Noah Brown","Justice Carbajal","Yevgen Chebotar","Joseph Dabis","Chelsea Finn","Keerthana Gopalakrishnan","Karol Hausman","Alex Herzog","Jasmine Hsu","Julian Ibarz","Brian Ichter","Alex Irpan","Tomas Jackson","Sally Jesmonth","Nikhil J Joshi","Ryan Julian","Dmitry Kalashnikov","Yuheng Kuang","Isabel Leal","Kuang-Huei Lee","Sergey Levine","Yao Lu","Utsav Malla","Deeksha Manjunath","Igor Mordatch","Ofir Nachum","Carolina Parada","Jodilyn Peralta","Emily Perez","Karl Pertsch","Jornell Quiambao","Kanishka Rao","Michael Ryoo","Grecia Salazar","Pannag Sanketi","Kevin Sayed","Jaspiar Singh","Sumedh Sontakke","Austin Stone","Clayton Tan","Huong Tran","Vincent Vanhoucke","Steve Vega","Quan Vuong","Fei Xia","Ted Xiao","Peng Xu","Sichun Xu","Tianhe Yu","Brianna Zitkovich"],"pdf_url":"https://arxiv.org/pdf/2212.06817v1.pdf","comment":"See website at robotics-transformer.github.io"},{"id":"http://arxiv.org/abs/2212.06836v1","updated":"2022-12-13T18:45:00Z","published":"2022-12-13T18:45:00Z","title":"Towards Efficient and Domain-Agnostic Evasion Attack with\n  High-dimensional Categorical Inputs","summary":"  Our work targets at searching feasible adversarial perturbation to attack a\nclassifier with high-dimensional categorical inputs in a domain-agnostic\nsetting. This is intrinsically an NP-hard knapsack problem where the\nexploration space becomes explosively larger as the feature dimension\nincreases. Without the help of domain knowledge, solving this problem via\nheuristic method, such as Branch-and-Bound, suffers from exponential\ncomplexity, yet can bring arbitrarily bad attack results. We address the\nchallenge via the lens of multi-armed bandit based combinatorial search. Our\nproposed method, namely FEAT, treats modifying each categorical feature as\npulling an arm in multi-armed bandit programming. Our objective is to achieve\nhighly efficient and effective attack using an Orthogonal Matching Pursuit\n(OMP)-enhanced Upper Confidence Bound (UCB) exploration strategy. Our\ntheoretical analysis bounding the regret gap of FEAT guarantees its practical\nattack performance. In empirical analysis, we compare FEAT with other\nstate-of-the-art domain-agnostic attack methods over various real-world\ncategorical data sets of different applications. Substantial experimental\nobservations confirm the expected efficiency and attack effectiveness of FEAT\napplied in different application scenarios. Our work further hints the\napplicability of FEAT for assessing the adversarial vulnerability of\nclassification systems with high-dimensional categorical inputs.\n","authors":["Hongyan Bao","Yufei Han","Yujun Zhou","Xin Gao","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.06836v1.pdf","comment":"AAAI 2023"},{"id":"http://arxiv.org/abs/2212.06803v1","updated":"2022-12-13T18:36:19Z","published":"2022-12-13T18:36:19Z","title":"Fair Infinitesimal Jackknife: Mitigating the Influence of Biased\n  Training Data Points Without Refitting","summary":"  In consequential decision-making applications, mitigating unwanted biases in\nmachine learning models that yield systematic disadvantage to members of groups\ndelineated by sensitive attributes such as race and gender is one key\nintervention to strive for equity. Focusing on demographic parity and equality\nof opportunity, in this paper we propose an algorithm that improves the\nfairness of a pre-trained classifier by simply dropping carefully selected\ntraining data points. We select instances based on their influence on the\nfairness metric of interest, computed using an infinitesimal jackknife-based\napproach. The dropping of training points is done in principle, but in practice\ndoes not require the model to be refit. Crucially, we find that such an\nintervention does not substantially reduce the predictive performance of the\nmodel but drastically improves the fairness metric. Through careful\nexperiments, we evaluate the effectiveness of the proposed approach on diverse\ntasks and find that it consistently improves upon existing alternatives.\n","authors":["Prasanna Sattigeri","Soumya Ghosh","Inkit Padhi","Pierre Dognin","Kush R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2212.06803v1.pdf","comment":"Accepted at Neurips 2022"},{"id":"http://arxiv.org/abs/2212.06797v1","updated":"2022-12-13T18:29:03Z","published":"2022-12-13T18:29:03Z","title":"AutoPV: Automated photovoltaic forecasts with limited information using\n  an ensemble of pre-trained models","summary":"  Accurate PhotoVoltaic (PV) power generation forecasting is vital for the\nefficient operation of Smart Grids. The automated design of such accurate\nforecasting models for individual PV plants includes two challenges: First,\ninformation about the PV mounting configuration (i.e. inclination and azimuth\nangles) is often missing. Second, for new PV plants, the amount of historical\ndata available to train a forecasting model is limited (cold-start problem). We\naddress these two challenges by proposing a new method for day-ahead PV power\ngeneration forecasts called AutoPV. AutoPV is a weighted ensemble of\nforecasting models that represent different PV mounting configurations. This\nrepresentation is achieved by pre-training each forecasting model on a separate\nPV plant and by scaling the model's output with the peak power rating of the\ncorresponding PV plant. To tackle the cold-start problem, we initially weight\neach forecasting model in the ensemble equally. To tackle the problem of\nmissing information about the PV mounting configuration, we use new data that\nbecome available during operation to adapt the ensemble weights to minimize the\nforecasting error. AutoPV is advantageous as the unknown PV mounting\nconfiguration is implicitly reflected in the ensemble weights, and only the PV\nplant's peak power rating is required to re-scale the ensemble's output. AutoPV\nalso allows to represent PV plants with panels distributed on different roofs\nwith varying alignments, as these mounting configurations can be reflected\nproportionally in the weighting. Additionally, the required computing memory is\ndecoupled when scaling AutoPV to hundreds of PV plants, which is beneficial in\nSmart Grids with limited computing capabilities. For a real-world data set with\n11 PV plants, the accuracy of AutoPV is comparable to a model trained on two\nyears of data and outperforms an incrementally trained model.\n","authors":["Stefan Meisenbacher","Benedikt Heidrich","Tim Martin","Ralf Mikut","Veit Hagenmeyer"],"pdf_url":"https://arxiv.org/pdf/2212.06797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.06645v2","updated":"2022-12-13T18:26:16Z","published":"2022-04-13T21:43:28Z","title":"Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning","summary":"  In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a\nnonlinear dimensionality reduction technique that provides solutions to some\ndrawbacks in existing global nonlinear dimensionality reduction algorithms in\nimaging applications. Wassmap represents images via probability measures in\nWasserstein space, then uses pairwise Wasserstein distances between the\nassociated measures to produce a low-dimensional, approximately isometric\nembedding. We show that the algorithm is able to exactly recover parameters of\nsome image manifolds including those generated by translations or dilations of\na fixed generating measure. Additionally, we show that a discrete version of\nthe algorithm retrieves parameters from manifolds generated from discrete\nmeasures by providing a theoretical bridge to transfer recovery results from\nfunctional data to discrete data. Testing of the proposed algorithms on various\nimage data manifolds show that Wassmap yields good embeddings compared with\nother global and local techniques.\n","authors":["Keaton Hamm","Nick Henscheid","Shujie Kang"],"pdf_url":"https://arxiv.org/pdf/2204.06645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06795v1","updated":"2022-12-13T18:26:00Z","published":"2022-12-13T18:26:00Z","title":"GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group\n  Propagation","summary":"  We present the Group Propagation Vision Transformer (GPViT): a novel\nnonhierarchical (i.e. non-pyramidal) transformer model designed for general\nvisual recognition with high-resolution features. High-resolution features (or\ntokens) are a natural fit for tasks that involve perceiving fine-grained\ndetails such as detection and segmentation, but exchanging global information\nbetween these features is expensive in memory and computation because of the\nway self-attention scales. We provide a highly efficient alternative Group\nPropagation Block (GP Block) to exchange global information. In each GP Block,\nfeatures are first grouped together by a fixed number of learnable group\ntokens; we then perform Group Propagation where global information is exchanged\nbetween the grouped features; finally, global information in the updated\ngrouped features is returned back to the image features through a transformer\ndecoder. We evaluate GPViT on a variety of visual recognition tasks including\nimage classification, semantic segmentation, object detection, and instance\nsegmentation. Our method achieves significant performance gains over previous\nworks across all tasks, especially on tasks that require high-resolution\noutputs, for example, our GPViT-L3 outperforms Swin Transformer-B by 2.0 mIoU\non ADE20K semantic segmentation with only half as many parameters. Code and\npre-trained models are available at https://github.com/ChenhongyiYang/GPViT .\n","authors":["Chenhongyi Yang","Jiarui Xu","Shalini De Mello","Elliot J. Crowley","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2212.06795v1.pdf","comment":"Code: https://github.com/ChenhongyiYang/GPViT"},{"id":"http://arxiv.org/abs/2206.01467v2","updated":"2022-12-13T18:18:50Z","published":"2022-06-03T09:17:22Z","title":"The Importance of Image Interpretation: Patterns of Semantic\n  Misclassification in Real-World Adversarial Images","summary":"  Adversarial images are created with the intention of causing an image\nclassifier to produce a misclassification. In this paper, we propose that\nadversarial images should be evaluated based on semantic mismatch, rather than\nlabel mismatch, as used in current work. In other words, we propose that an\nimage of a \"mug\" would be considered adversarial if classified as \"turnip\", but\nnot as \"cup\", as current systems would assume. Our novel idea of taking\nsemantic misclassification into account in the evaluation of adversarial images\noffers two benefits. First, it is a more realistic conceptualization of what\nmakes an image adversarial, which is important in order to fully understand the\nimplications of adversarial images for security and privacy. Second, it makes\nit possible to evaluate the transferability of adversarial images to a\nreal-world classifier, without requiring the classifier's label set to have\nbeen available during the creation of the images. The paper carries out an\nevaluation of a transfer attack on a real-world image classifier that is made\npossible by our semantic misclassification approach. The attack reveals\npatterns in the semantics of adversarial misclassifications that could not be\ninvestigated using conventional label mismatch.\n","authors":["Zhengyu Zhao","Nga Dang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2206.01467v2.pdf","comment":"International Conference on Multimedia Modeling (MMM) 2023. Resources\n  are publicly available at\n  https://github.com/ZhengyuZhao/Targeted-Transfer/tree/main/human_eval"},{"id":"http://arxiv.org/abs/2212.01133v3","updated":"2022-12-13T18:12:28Z","published":"2022-12-02T12:26:00Z","title":"Ripple: Concept-Based Interpretation for Raw Time Series Models in\n  Education","summary":"  Time series is the most prevalent form of input data for educational\nprediction tasks. The vast majority of research using time series data focuses\non hand-crafted features, designed by experts for predictive performance and\ninterpretability. However, extracting these features is labor-intensive for\nhumans and computers. In this paper, we propose an approach that utilizes\nirregular multivariate time series modeling with graph neural networks to\nachieve comparable or better accuracy with raw time series clickstreams in\ncomparison to hand-crafted features. Furthermore, we extend concept activation\nvectors for interpretability in raw time series models. We analyze these\nadvances in the education domain, addressing the task of early student\nperformance prediction for downstream targeted interventions and instructional\nsupport. Our experimental analysis on 23 MOOCs with millions of combined\ninteractions over six behavioral dimensions show that models designed with our\napproach can (i) beat state-of-the-art educational time series baselines with\nno feature extraction and (ii) provide interpretable insights for personalized\ninterventions. Source code: https://github.com/epfl-ml4ed/ripple/.\n","authors":["Mohammad Asadi","Vinitra Swamy","Jibril Frej","Julien Vignoud","Mirko Marras","Tanja Käser"],"pdf_url":"https://arxiv.org/pdf/2212.01133v3.pdf","comment":"Accepted as a full paper at AAAI 2023: 37th AAAI Conference on\n  Artificial Intelligence (EAAI: AI for Education Special Track), 7-14 of\n  February 2023, Washington DC, USA"},{"id":"http://arxiv.org/abs/2204.10233v2","updated":"2022-12-13T18:00:46Z","published":"2022-04-21T16:12:19Z","title":"A Sandbox Tool to Bias(Stress)-Test Fairness Algorithms","summary":"  Motivated by the growing importance of reducing unfairness in ML predictions,\nFair-ML researchers have presented an extensive suite of algorithmic\n'fairness-enhancing' remedies. Most existing algorithms, however, are agnostic\nto the sources of the observed unfairness. As a result, the literature\ncurrently lacks guiding frameworks to specify conditions under which each\nalgorithmic intervention can potentially alleviate the underpinning cause of\nunfairness. To close this gap, we scrutinize the underlying biases (e.g., in\nthe training data or design choices) that cause observational unfairness. We\npresent the conceptual idea and a first implementation of a bias-injection\nsandbox tool to investigate fairness consequences of various biases and assess\nthe effectiveness of algorithmic remedies in the presence of specific types of\nbias. We call this process the bias(stress)-testing of algorithmic\ninterventions. Unlike existing toolkits, ours provides a controlled environment\nto counterfactually inject biases in the ML pipeline. This stylized setup\noffers the distinct capability of testing fairness interventions beyond\nobservational data and against an unbiased benchmark. In particular, we can\ntest whether a given remedy can alleviate the injected bias by comparing the\npredictions resulting after the intervention in the biased setting with true\nlabels in the unbiased regime-that is, before any bias injection. We illustrate\nthe utility of our toolkit via a proof-of-concept case study on synthetic data.\nOur empirical analysis showcases the type of insights that can be obtained\nthrough our simulations.\n","authors":["Nil-Jana Akpinar","Manish Nagireddy","Logan Stapleton","Hao-Fei Cheng","Haiyi Zhu","Steven Wu","Hoda Heidari"],"pdf_url":"https://arxiv.org/pdf/2204.10233v2.pdf","comment":"Appeared as a poster at the second ACM conference on Equity and\n  Access in Algorithms, Mechanisms, and Optimization (EAAMO'22)"},{"id":"http://arxiv.org/abs/2212.06759v1","updated":"2022-12-13T17:41:58Z","published":"2022-12-13T17:41:58Z","title":"Learning Robotic Navigation from Experience: Principles, Methods, and\n  Recent Results","summary":"  Navigation is one of the most heavily studied problems in robotics, and is\nconventionally approached as a geometric mapping and planning problem. However,\nreal-world navigation presents a complex set of physical challenges that defies\nsimple geometric abstractions. Machine learning offers a promising way to go\nbeyond geometry and conventional planning, allowing for navigational systems\nthat make decisions based on actual prior experience. Such systems can reason\nabout traversability in ways that go beyond geometry, accounting for the\nphysical outcomes of their actions and exploiting patterns in real-world\nenvironments. They can also improve as more data is collected, potentially\nproviding a powerful network effect. In this article, we present a general\ntoolkit for experiential learning of robotic navigation skills that unifies\nseveral recent approaches, describe the underlying design principles, summarize\nexperimental results from several of our recent papers, and discuss open\nproblems and directions for future work.\n","authors":["Sergey Levine","Dhruv Shah"],"pdf_url":"https://arxiv.org/pdf/2212.06759v1.pdf","comment":"Final print version is here:\n  https://royalsocietypublishing.org/doi/10.1098/rstb.2021.0447"},{"id":"http://arxiv.org/abs/2212.06757v1","updated":"2022-12-13T17:39:18Z","published":"2022-12-13T17:39:18Z","title":"Gradient flow in the gaussian covariate model: exact solution of\n  learning curves and multiple descent structures","summary":"  A recent line of work has shown remarkable behaviors of the generalization\nerror curves in simple learning models. Even the least-squares regression has\nshown atypical features such as the model-wise double descent, and further\nworks have observed triple or multiple descents. Another important\ncharacteristic are the epoch-wise descent structures which emerge during\ntraining. The observations of model-wise and epoch-wise descents have been\nanalytically derived in limited theoretical settings (such as the random\nfeature model) and are otherwise experimental. In this work, we provide a full\nand unified analysis of the whole time-evolution of the generalization curve,\nin the asymptotic large-dimensional regime and under gradient-flow, within a\nwider theoretical setting stemming from a gaussian covariate model. In\nparticular, we cover most cases already disparately observed in the literature,\nand also provide examples of the existence of multiple descent structures as a\nfunction of a model parameter or time. Furthermore, we show that our\ntheoretical predictions adequately match the learning curves obtained by\ngradient descent over realistic datasets. Technically we compute averages of\nrational expressions involving random matrices using recent developments in\nrandom matrix theory based on \"linear pencils\". Another contribution, which is\nalso of independent interest in random matrix theory, is a new derivation of\nrelated fixed point equations (and an extension there-off) using Dyson brownian\nmotions.\n","authors":["Antione Bodin","Nicolas Macris"],"pdf_url":"https://arxiv.org/pdf/2212.06757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06751v1","updated":"2022-12-13T17:33:02Z","published":"2022-12-13T17:33:02Z","title":"Multi-objective Tree-structured Parzen Estimator Meets Meta-learning","summary":"  Hyperparameter optimization (HPO) is essential for the better performance of\ndeep learning, and practitioners often need to consider the trade-off between\nmultiple metrics, such as error rate, latency, memory requirements, robustness,\nand algorithmic fairness. Due to this demand and the heavy computation of deep\nlearning, the acceleration of multi-objective (MO) optimization becomes ever\nmore important. Although meta-learning has been extensively studied to speedup\nHPO, existing methods are not applicable to the MO tree-structured parzen\nestimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we\nextend TPE's acquisition function to the meta-learning setting, using a task\nsimilarity defined by the overlap in promising domains of each task. In a\ncomprehensive set of experiments, we demonstrate that our method accelerates\nMO-TPE on tabular HPO benchmarks and yields state-of-the-art performance. Our\nmethod was also validated externally by winning the AutoML 2022 competition on\n\"Multiobjective Hyperparameter Optimization for Transformers\".\n","authors":["Shuhei Watanabe","Noow Awad","Masaki Onishi","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2212.06751v1.pdf","comment":"Meta-learning workshop on NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.06750v1","updated":"2022-12-13T17:32:44Z","published":"2022-12-13T17:32:44Z","title":"FairRoad: Achieving Fairness for Recommender Systems with Optimized\n  Antidote Data","summary":"  Today, recommender systems have played an increasingly important role in\nshaping our experiences of digital environments and social interactions.\nHowever, as recommender systems become ubiquitous in our society, recent years\nhave also witnessed significant fairness concerns for recommender systems.\nSpecifically, studies have shown that recommender systems may inherit or even\namplify biases from historical data, and as a result, provide unfair\nrecommendations. To address fairness risks in recommender systems, most of the\nprevious approaches to date are focused on modifying either the existing\ntraining data samples or the deployed recommender algorithms, but unfortunately\nwith limited degrees of success. In this paper, we propose a new approach\ncalled fair recommendation with optimized antidote data (FairRoad), which aims\nto improve the fairness performances of recommender systems through the\nconstruction of a small and carefully crafted antidote dataset. Toward this\nend, we formulate our antidote data generation task as a mathematical\noptimization problem, which minimizes the unfairness of the targeted\nrecommender systems while not disrupting the deployed recommendation\nalgorithms. Extensive experiments show that our proposed antidote data\ngeneration algorithm significantly improve the fairness of recommender systems\nwith a small amounts of antidote data.\n","authors":["Minghong Fang","Jia Liu","Michinari Momma","Yi Sun"],"pdf_url":"https://arxiv.org/pdf/2212.06750v1.pdf","comment":"Accepted by SACMAT 2022"},{"id":"http://arxiv.org/abs/2210.12933v2","updated":"2022-12-13T17:23:25Z","published":"2022-10-24T03:22:20Z","title":"Multi-Agent Path Finding via Tree LSTM","summary":"  In recent years, Multi-Agent Path Finding (MAPF) has attracted attention from\nthe fields of both Operations Research (OR) and Reinforcement Learning (RL).\nHowever, in the 2021 Flatland3 Challenge, a competition on MAPF, the best RL\nmethod scored only 27.9, far less than the best OR method. This paper proposes\na new RL solution to Flatland3 Challenge, which scores 125.3, several times\nhigher than the best RL solution before. We creatively apply a novel network\narchitecture, TreeLSTM, to MAPF in our solution. Together with several other RL\ntechniques, including reward shaping, multiple-phase training, and centralized\ncontrol, our solution is comparable to the top 2-3 OR methods.\n","authors":["Yuhao Jiang","Kunjie Zhang","Qimai Li","Jiaxin Chen","Xiaolong Zhu"],"pdf_url":"https://arxiv.org/pdf/2210.12933v2.pdf","comment":"Appear in AAAI23-MAPF"},{"id":"http://arxiv.org/abs/2212.06742v1","updated":"2022-12-13T17:21:44Z","published":"2022-12-13T17:21:44Z","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for\n  Programming Languages","summary":"  Software engineers working with the same programming language (PL) may speak\ndifferent natural languages (NLs) and vice versa, erecting huge barriers to\ncommunication and working efficiency. Recent studies have demonstrated the\neffectiveness of generative pre-training in computer programs, yet they are\nalways English-centric. In this work, we step towards bridging the gap between\nmultilingual NLs and multilingual PLs for large language models (LLMs). We\nrelease ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs.\nWe employ two methods for universal cross-lingual pre-training: span-corruption\nlanguage modeling that learns patterns from monolingual NL or PL; and\npivot-based translation language modeling that relies on parallel data of many\nNLs and PLs. Extensive results show that ERNIE-Code outperforms previous\nmultilingual LLMs for PL or NL across a wide range of end tasks of code\nintelligence, including multilingual code-to-text, text-to-code, code-to-code,\nand text-to-text generation. We further show its advantage of zero-shot\nprompting on multilingual code summarization and text-to-text translation. We\nwill make our code and pre-trained models publicly available.\n","authors":["Yekun Chai","Shuohuan Wang","Chao Pang","Yu Sun","Hao Tian","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2212.06742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06735v1","updated":"2022-12-13T17:14:14Z","published":"2022-12-13T17:14:14Z","title":"POPNASv3: a Pareto-Optimal Neural Architecture Search Solution for Image\n  and Time Series Classification","summary":"  The automated machine learning (AutoML) field has become increasingly\nrelevant in recent years. These algorithms can develop models without the need\nfor expert knowledge, facilitating the application of machine learning\ntechniques in the industry. Neural Architecture Search (NAS) exploits deep\nlearning techniques to autonomously produce neural network architectures whose\nresults rival the state-of-the-art models hand-crafted by AI experts. However,\nthis approach requires significant computational resources and hardware\ninvestments, making it less appealing for real-usage applications. This article\npresents the third version of Pareto-Optimal Progressive Neural Architecture\nSearch (POPNASv3), a new sequential model-based optimization NAS algorithm\ntargeting different hardware environments and multiple classification tasks.\nOur method is able to find competitive architectures within large search\nspaces, while keeping a flexible structure and data processing pipeline to\nadapt to different tasks. The algorithm employs Pareto optimality to reduce the\nnumber of architectures sampled during the search, drastically improving the\ntime efficiency without loss in accuracy. The experiments performed on images\nand time series classification datasets provide evidence that POPNASv3 can\nexplore a large set of assorted operators and converge to optimal architectures\nsuited for the type of data provided under different scenarios.\n","authors":["Andrea Falanti","Eugenio Lomurno","Danilo Ardagna","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2212.06735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.07390v2","updated":"2022-12-13T17:12:02Z","published":"2022-05-15T22:15:21Z","title":"Learning Representations for New Sound Classes With Continual\n  Self-Supervised Learning","summary":"  In this paper, we work on a sound recognition system that continually\nincorporates new sound classes. Our main goal is to develop a framework where\nthe model can be updated without relying on labeled data. For this purpose, we\npropose adopting representation learning, where an encoder is trained using\nunlabeled data. This learning framework enables the study and implementation of\na practically relevant use case where only a small amount of the labels is\navailable in a continual learning context. We also make the empirical\nobservation that a similarity-based representation learning method within this\nframework is robust to forgetting even if no explicit mechanism against\nforgetting is employed. We show that this approach obtains similar performance\ncompared to several distillation-based continual learning methods when employed\non self-supervised representation learning methods.\n","authors":["Zhepei Wang","Cem Subakan","Xilin Jiang","Junkai Wu","Efthymios Tzinis","Mirco Ravanelli","Paris Smaragdis"],"pdf_url":"https://arxiv.org/pdf/2205.07390v2.pdf","comment":"Accepted to IEEE Signal Processing Letters"},{"id":"http://arxiv.org/abs/2212.00781v2","updated":"2022-12-13T17:04:30Z","published":"2022-12-01T18:58:26Z","title":"Second-order optimization with lazy Hessians","summary":"  We analyze Newton's method with lazy Hessian updates for solving general\npossibly non-convex optimization problems. We propose to reuse a previously\nseen Hessian for several iterations while computing new gradients at each step\nof the method. This significantly reduces the overall arithmetical complexity\nof second-order optimization schemes. By using the cubic regularization\ntechnique, we establish fast global convergence of our method to a second-order\nstationary point, while the Hessian does not need to be updated each iteration.\nFor convex problems, we justify global and local superlinear rates for lazy\nNewton steps with quadratic regularization, which is easier to compute. The\noptimal frequency for updating the Hessian is once every $d$ iterations, where\n$d$ is the dimension of the problem. This provably improves the total\narithmetical complexity of second-order algorithms by a factor $\\sqrt{d}$.\n","authors":["Nikita Doikov","El Mahdi Chayti","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2212.00781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.14352v3","updated":"2022-12-13T17:01:31Z","published":"2022-03-27T17:21:36Z","title":"Physics Guided Deep Learning for Generative Design of Crystal Materials\n  with Symmetry Constraints","summary":"  Discovering new materials is a challenging task in materials science crucial\nto the progress of human society. Conventional approaches based on experiments\nand simulations are labor-intensive or costly with success heavily depending on\nexperts' heuristic knowledge. Here, we propose a deep learning based Physics\nGuided Crystal Generative Model (PGCGM) for efficient crystal material design\nwith high structural diversity and symmetry. Our model increases the generation\nvalidity by more than 700\\% compared to FTCP, one of the latest structure\ngenerators and by more than 45\\% compared to our previous CubicGAN model.\nDensity Functional Theory (DFT) calculations are used to validate the generated\nstructures with 1,869 materials out of 2,000 are successfully optimized and\ndeposited into the Carolina Materials Database \\url{www.carolinamatdb.org}, of\nwhich 39.6\\% have negative formation energy and 5.3\\% have energy-above-hull\nless than 0.25 eV/atom, indicating their thermodynamic stability and potential\nsynthesizability.\n","authors":["Yong Zhao","Edirisuriya M. Dilanga Siriwardane","Zhenyao Wu","Nihang Fu","Mohammed Al-Fahdi","Ming Hu","Jianjun Hu"],"pdf_url":"https://arxiv.org/pdf/2203.14352v3.pdf","comment":"18 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2212.06717v1","updated":"2022-12-13T16:42:20Z","published":"2022-12-13T16:42:20Z","title":"A Machine Learning Enhanced Approach for Automated Sunquake Detection in\n  Acoustic Emission Maps","summary":"  Sunquakes are seismic emissions visible on the solar surface, associated with\nsome solar flares. Although discovered in 1998, they have only recently become\na more commonly detected phenomenon. Despite the availability of several manual\ndetection guidelines, to our knowledge, the astrophysical data produced for\nsunquakes is new to the field of Machine Learning. Detecting sunquakes is a\ndaunting task for human operators and this work aims to ease and, if possible,\nto improve their detection. Thus, we introduce a dataset constructed from\nacoustic egression-power maps of solar active regions obtained for Solar Cycles\n23 and 24 using the holography method. We then present a pedagogical approach\nto the application of machine learning representation methods for sunquake\ndetection using AutoEncoders, Contrastive Learning, Object Detection and\nrecurrent techniques, which we enhance by introducing several custom\ndomain-specific data augmentation transformations. We address the main\nchallenges of the automated sunquake detection task, namely the very high noise\npatterns in and outside the active region shadow and the extreme class\nimbalance given by the limited number of frames that present sunquake\nsignatures. With our trained models, we find temporal and spatial locations of\npeculiar acoustic emission and qualitatively associate them to eruptive and\nhigh energy emission. While noting that these models are still in a prototype\nstage and there is much room for improvement in metrics and bias levels, we\nhypothesize that their agreement on example use cases has the potential to\nenable detection of weak solar acoustic manifestations.\n","authors":["Vanessa Mercea","Alin Razvan Paraschiv","Daniela Adriana Lacatus","Anca Marginean","Diana Besliu-Ionescu"],"pdf_url":"https://arxiv.org/pdf/2212.06717v1.pdf","comment":"Solar Physics accepted for publication, 44 total pages, 9 appendix\n  pages, 21 figures, 6 tables"},{"id":"http://arxiv.org/abs/2110.04903v4","updated":"2022-12-13T16:39:51Z","published":"2021-10-10T20:55:34Z","title":"Normative Modeling using Multimodal Variational Autoencoders to Identify\n  Abnormal Brain Structural Patterns in Alzheimer Disease","summary":"  Normative modelling is an emerging method for understanding the underlying\nheterogeneity within brain disorders like Alzheimer Disease (AD) by quantifying\nhow each patient deviates from the expected normative pattern that has been\nlearned from a healthy control distribution. Since AD is a multifactorial\ndisease with more than one biological pathways, multimodal magnetic resonance\nimaging (MRI) neuroimaging data can provide complementary information about the\ndisease heterogeneity. However, existing deep learning based normative models\non multimodal MRI data use unimodal autoencoders with a single encoder and\ndecoder that may fail to capture the relationship between brain measurements\nextracted from different MRI modalities. In this work, we propose multi-modal\nvariational autoencoder (mmVAE) based normative modelling framework that can\ncapture the joint distribution between different modalities to identify\nabnormal brain structural patterns in AD. Our multi-modal framework takes as\ninput Freesurfer processed brain region volumes from T1-weighted (cortical and\nsubcortical) and T2-weighed (hippocampal) scans of cognitively normal\nparticipants to learn the morphological characteristics of the healthy brain.\nThe estimated normative model is then applied on Alzheimer Disease (AD)\npatients to quantify the deviation in brain volumes and identify the abnormal\nbrain structural patterns due to the effect of the different AD stages. Our\nexperimental results show that modeling joint distribution between the multiple\nMRI modalities generates deviation maps that are more sensitive to disease\nstaging within AD, have a better correlation with patient cognition and result\nin higher number of brain regions with statistically significant deviations\ncompared to a unimodal baseline model with all modalities concatenated as a\nsingle input.\n","authors":["Sayantan Kumar","Philip Payne","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2110.04903v4.pdf","comment":"Medical Imaging Meets NeurIPS workshop in NeurIPS 2022"},{"id":"http://arxiv.org/abs/2208.08056v3","updated":"2022-12-13T16:36:45Z","published":"2022-08-17T04:01:29Z","title":"Sampling Through the Lens of Sequential Decision Making","summary":"  Sampling is ubiquitous in machine learning methodologies. Due to the growth\nof large datasets and model complexity, we want to learn and adapt the sampling\nprocess while training a representation. Towards achieving this grand goal, a\nvariety of sampling techniques have been proposed. However, most of them either\nuse a fixed sampling scheme or adjust the sampling scheme based on simple\nheuristics. They cannot choose the best sample for model training in different\nstages. Inspired by \"Think, Fast and Slow\" (System 1 and System 2) in cognitive\nscience, we propose a reward-guided sampling strategy called Adaptive Sample\nwith Reward (ASR) to tackle this challenge. To the best of our knowledge, this\nis the first work utilizing reinforcement learning (RL) to address the sampling\nproblem in representation learning. Our approach optimally adjusts the sampling\nprocess to achieve optimal performance. We explore geographical relationships\namong samples by distance-based sampling to maximize overall cumulative reward.\nWe apply ASR to the long-standing sampling problems in similarity-based loss\nfunctions. Empirical results in information retrieval and clustering\ndemonstrate ASR's superb performance across different datasets. We also discuss\nan engrossing phenomenon which we name as \"ASR gravity well\" in experiments.\n","authors":["Jason Xiaotian Dou","Alvin Qingkai Pan","Runxue Bao","Haiyi Harry Mao","Lei Luo","Zhi-Hong Mao"],"pdf_url":"https://arxiv.org/pdf/2208.08056v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06710v1","updated":"2022-12-13T16:29:13Z","published":"2022-12-13T16:29:13Z","title":"TIER: Text-Image Entropy Regularization for CLIP-style models","summary":"  In this paper, we study the effect of a novel regularization scheme on\ncontrastive language-image pre-trained (CLIP) models. Our approach is based on\nthe observation that, in many domains, text tokens should only describe a small\nnumber of image regions and, likewise, each image region should correspond to\nonly a few text tokens. In CLIP-style models, this implies that text-token\nembeddings should have high similarity to only a small number of image-patch\nembeddings for a given image-text pair. We formalize this observation using a\nnovel regularization scheme that penalizes the entropy of the text-token to\nimage-patch similarity scores. We qualitatively and quantitatively demonstrate\nthat the proposed regularization scheme shrinks the text-token and image-patch\nsimilarity scores towards zero, thus achieving the desired effect. We\ndemonstrate the promise of our approach in an important medical context where\nthis underlying hypothesis naturally arises. Using our proposed approach, we\nachieve state of the art (SOTA) zero-shot performance on all tasks from the\nCheXpert chest x-ray dataset, outperforming an unregularized version of the\nmodel and several recently published self-supervised models.\n","authors":["Anil Palepu","Andrew L. Beam"],"pdf_url":"https://arxiv.org/pdf/2212.06710v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.06834v1","updated":"2022-12-13T16:12:45Z","published":"2022-12-13T16:12:45Z","title":"Deep Neural Networks integrating genomics and histopathological images\n  for predicting stages and survival time-to-event in colon cancer","summary":"  There exists unexplained diverse variation within the predefined colon cancer\nstages using only features either from genomics or histopathological whole\nslide images as prognostic factors. Unraveling this variation will bring about\nimproved in staging and treatment outcome, hence motivated by the advancement\nof Deep Neural Network libraries and different structures and factors within\nsome genomic dataset, we aggregate atypical patterns in histopathological\nimages with diverse carcinogenic expression from mRNA, miRNA and DNA\nMethylation as an integrative input source into an ensemble deep neural network\nfor colon cancer stages classification and samples stratification into low or\nhigh risk survival groups. The results of our Ensemble Deep Convolutional\nNeural Network model show an improved performance in stages classification on\nthe integrated dataset. The fused input features return Area under curve\nReceiver Operating Characteristic curve (AUC ROC) of 0.95 compared with AUC ROC\nof 0.71 and 0.68 obtained when only genomics and images features are used for\nthe stage's classification, respectively. Also, the extracted features were\nused to split the patients into low or high risk survival groups. Among the\n2548 fused features, 1695 features showed a statistically significant survival\nprobability differences between the two risk groups defined by the extracted\nfeatures.\n","authors":["Olalekan Ogundipe","Zeyneb Kurt","Wai Lok Woo"],"pdf_url":"https://arxiv.org/pdf/2212.06834v1.pdf","comment":"21 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.06691v1","updated":"2022-12-13T16:04:16Z","published":"2022-12-13T16:04:16Z","title":"Quantum Clustering with k-Means: a Hybrid Approach","summary":"  Quantum computing is a promising paradigm based on quantum theory for\nperforming fast computations. Quantum algorithms are expected to surpass their\nclassical counterparts in terms of computational complexity for certain tasks,\nincluding machine learning. In this paper, we design, implement, and evaluate\nthree hybrid quantum k-Means algorithms, exploiting different degree of\nparallelism. Indeed, each algorithm incrementally leverages quantum parallelism\nto reduce the complexity of the cluster assignment step up to a constant cost.\nIn particular, we exploit quantum phenomena to speed up the computation of\ndistances. The core idea is that the computation of distances between records\nand centroids can be executed simultaneously, thus saving time, especially for\nbig datasets. We show that our hybrid quantum k-Means algorithms can be more\nefficient than the classical version, still obtaining comparable clustering\nresults.\n","authors":["Alessandro Poggiali","Alessandro Berti","Anna Bernasconi","Gianna Del Corso","Riccardo Guidotti"],"pdf_url":"https://arxiv.org/pdf/2212.06691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10227v3","updated":"2022-12-13T16:03:26Z","published":"2022-11-18T13:26:57Z","title":"Adversarial Detection by Approximation of Ensemble Boundary","summary":"  A spectral approximation of a Boolean function is proposed for approximating\nthe decision boundary of an ensemble of Deep Neural Networks (DNNs) solving\ntwo-class pattern recognition problems. The Walsh combination of relatively\nweak DNN classifiers is shown experimentally to be capable of detecting\nadversarial attacks. By observing the difference in Walsh coefficient\napproximation between clean and adversarial images, it appears that\ntransferability of attack may be used for detection. Approximating the decision\nboundary may also aid in understanding the learning and transferability\nproperties of DNNs. While the experiments here use images, the proposed\napproach of modelling two-class ensemble decision boundaries could in principle\nbe applied to any application area. Code for this paper implementing Walsh\nCoefficient Examples of approximating artificial Boolean functions can be found\nat https://doi.org/10.24433/CO.3695905.v1\n","authors":["T. Windeatt"],"pdf_url":"https://arxiv.org/pdf/2211.10227v3.pdf","comment":"6 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.06655v1","updated":"2022-12-13T15:37:53Z","published":"2022-12-13T15:37:53Z","title":"The Hateful Memes Challenge Next Move","summary":"  State-of-the-art image and text classification models, such as Convectional\nNeural Networks and Transformers, have long been able to classify their\nrespective unimodal reasoning satisfactorily with accuracy close to or\nexceeding human accuracy. However, images embedded with text, such as hateful\nmemes, are hard to classify using unimodal reasoning when difficult examples,\nsuch as benign confounders, are incorporated into the data set. We attempt to\ngenerate more labeled memes in addition to the Hateful Memes data set from\nFacebook AI, based on the framework of a winning team from the Hateful Meme\nChallenge. To increase the number of labeled memes, we explore semi-supervised\nlearning using pseudo-labels for newly introduced, unlabeled memes gathered\nfrom the Memotion Dataset 7K. We find that the semi-supervised learning task on\nunlabeled data required human intervention and filtering and that adding a\nlimited amount of new data yields no extra classification performance.\n","authors":["Weijun Jin","Lance Wilhelm"],"pdf_url":"https://arxiv.org/pdf/2212.06655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.10570v3","updated":"2022-12-13T15:32:46Z","published":"2021-10-20T14:00:02Z","title":"Behavioral Experiments for Understanding Catastrophic Forgetting","summary":"  In this paper we explore whether the fundamental tool of experimental\npsychology, the behavioral experiment, has the power to generate insight not\nonly into humans and animals, but artificial systems too. We apply the\ntechniques of experimental psychology to investigating catastrophic forgetting\nin neural networks. We present a series of controlled experiments with\ntwo-layer ReLU networks, and exploratory results revealing a new understanding\nof the behavior of catastrophic forgetting. Alongside our empirical findings,\nwe demonstrate an alternative, behavior-first approach to investigating neural\nnetwork phenomena.\n","authors":["Samuel J. Bell","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2110.10570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.10060v2","updated":"2022-12-13T15:28:19Z","published":"2022-05-20T10:10:32Z","title":"The Unreasonable Effectiveness of Deep Evidential Regression","summary":"  There is a significant need for principled uncertainty reasoning in machine\nlearning systems as they are increasingly deployed in safety-critical domains.\nA new approach with uncertainty-aware regression-based neural networks (NNs),\nbased on learning evidential distributions for aleatoric and epistemic\nuncertainties, shows promise over traditional deterministic methods and typical\nBayesian NNs, notably with the capabilities to disentangle aleatoric and\nepistemic uncertainties. Despite some empirical success of Deep Evidential\nRegression (DER), there are important gaps in the mathematical foundation that\nraise the question of why the proposed technique seemingly works. We detail the\ntheoretical shortcomings and analyze the performance on synthetic and\nreal-world data sets, showing that Deep Evidential Regression is a heuristic\nrather than an exact uncertainty quantification. We go on to propose\ncorrections and redefinitions of how aleatoric and epistemic uncertainties\nshould be extracted from NNs.\n","authors":["Nis Meinert","Jakob Gawlikowski","Alexander Lavin"],"pdf_url":"https://arxiv.org/pdf/2205.10060v2.pdf","comment":"11 pages, 25 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2212.02339v2","updated":"2022-12-13T12:28:21Z","published":"2022-12-05T15:15:10Z","title":"DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking","summary":"  Audio watermarking is widely used for leaking source tracing. The robustness\nof the watermark determines the traceability of the algorithm. With the\ndevelopment of digital technology, audio re-recording (AR) has become an\nefficient and covert means to steal secrets. AR process could drastically\ndestroy the watermark signal while preserving the original information. This\nputs forward a new requirement for audio watermarking at this stage, that is,\nto be robust to AR distortions. Unfortunately, none of the existing algorithms\ncan effectively resist AR attacks due to the complexity of the AR process. To\naddress this limitation, this paper proposes DeAR, a deep-learning-based audio\nre-recording resistant watermarking. Inspired by DNN-based image watermarking,\nwe pioneer a deep learning framework for audio carriers, based on which the\nwatermark signal can be effectively embedded and extracted. Meanwhile, in order\nto resist the AR attack, we delicately analyze the distortions that occurred in\nthe AR process and design the corresponding distortion layer to cooperate with\nthe proposed watermarking framework. Extensive experiments show that the\nproposed algorithm can resist not only common electronic channel distortions\nbut also AR distortions. Under the premise of high-quality embedding\n(SNR=25.86dB), in the case of a common re-recording distance (20cm), the\nalgorithm can effectively achieve an average bit recovery accuracy of 98.55%.\n","authors":["Chang Liu","Jie Zhang","Han Fang","Zehua Ma","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2212.02339v2.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2212.06516v1","updated":"2022-12-13T12:02:21Z","published":"2022-12-13T12:02:21Z","title":"Overview of The MediaEval 2022 Predicting Video Memorability Task","summary":"  This paper describes the 5th edition of the Predicting Video Memorability\nTask as part of MediaEval2022. This year we have reorganised and simplified the\ntask in order to lubricate a greater depth of inquiry. Similar to last year,\ntwo datasets are provided in order to facilitate generalisation, however, this\nyear we have replaced the TRECVid2019 Video-to-Text dataset with the VideoMem\ndataset in order to remedy underlying data quality issues, and to prioritise\nshort-term memorability prediction by elevating the Memento10k dataset as the\nprimary dataset. Additionally, a fully fledged electroencephalography\n(EEG)-based prediction sub-task is introduced. In this paper, we outline the\ncore facets of the task and its constituent sub-tasks; describing the datasets,\nevaluation metrics, and requirements for participant submissions.\n","authors":["Lorin Sweeney","Mihai Gabriel Constantin","Claire-Hélène Demarty","Camilo Fosco","Alba G. Seco de Herrera","Sebastian Halder","Graham Healy","Bogdan Ionescu","Ana Matran-Fernandez","Alan F. Smeaton","Mushfika Sultana"],"pdf_url":"https://arxiv.org/pdf/2212.06516v1.pdf","comment":"6 pages. In: MediaEval Multimedia Benchmark Workshop Working Notes,\n  2022"},{"id":"http://arxiv.org/abs/2203.01311v3","updated":"2022-12-13T05:37:14Z","published":"2022-03-02T18:56:20Z","title":"HighMMT: Quantifying Modality & Interaction Heterogeneity for\n  High-Modality Representation Learning","summary":"  Many real-world problems are inherently multimodal, from the communicative\nmodalities humans use to express social and emotional states to the force,\nproprioception, and visual sensors ubiquitous on robots. While there has been\nan explosion of interest in multimodal representation learning, these methods\nare still largely focused on a small set of modalities, primarily in the\nlanguage, vision, and audio space. In order to accelerate generalization\ntowards diverse and understudied modalities, this paper studies efficient\nrepresentation learning for high-modality scenarios. Since adding new models\nfor every new modality or task becomes prohibitively expensive, a critical\ntechnical challenge is heterogeneity quantification: how can we measure which\nmodalities encode similar information and interactions in order to permit\nparameter sharing with previous modalities? We propose two new\ninformation-theoretic metrics for heterogeneity quantification: (1) modality\nheterogeneity studies how similar 2 modalities $\\{X_1,X_2\\}$ are by measuring\nhow much information can be transferred from $X_1$ to $X_2$, while (2)\ninteraction heterogeneity studies how similarly pairs of modalities\n$\\{X_1,X_2\\}, \\{X_3,X_4\\}$ interact by measuring how much interaction\ninformation can be transferred from $\\{X_1,X_2\\}$ to $\\{X_3,X_4\\}$. We show the\nimportance of these proposed metrics in high-modality scenarios as a way to\nautomatically prioritize the fusion of modalities that contain unique\ninformation or interactions. The result is a single model, HighMMT, that scales\nup to $10$ modalities and $15$ tasks from $5$ different research areas. Not\nonly does HighMMT outperform prior methods on the tradeoff between performance\nand efficiency, it also demonstrates a crucial scaling behavior: performance\ncontinues to improve with each modality added, and transfers to entirely new\nmodalities and tasks during fine-tuning.\n","authors":["Paul Pu Liang","Yiwei Lyu","Xiang Fan","Jeffrey Tsaw","Yudong Liu","Shentong Mo","Dani Yogatama","Louis-Philippe Morency","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2203.01311v3.pdf","comment":"Code available at https://github.com/pliang279/HighMMT"}]},"2022-12-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.06267v1","updated":"2022-12-12T22:08:45Z","published":"2022-12-12T22:08:45Z","title":"Mortality Prediction Models with Clinical Notes Using Sparse Attention\n  at the Word and Sentence Levels","summary":"  Intensive Care in-hospital mortality prediction has various clinical\napplications. Neural prediction models, especially when capitalising on\nclinical notes, have been put forward as improvement on currently existing\nmodels. However, to be acceptable these models should be performant and\ntransparent. This work studies different attention mechanisms for clinical\nneural prediction models in terms of their discrimination and calibration.\nSpecifically, we investigate sparse attention as an alternative to dense\nattention weights in the task of in-hospital mortality prediction from clinical\nnotes. We evaluate the attention mechanisms based on: i) local self-attention\nover words in a sentence, and ii) global self-attention with a transformer\narchitecture across sentences. We demonstrate that the sparse mechanism\napproach outperforms the dense one for the local self-attention in terms of\npredictive performance with a publicly available dataset, and puts higher\nattention to prespecified relevant directive words. The performance at the\nsentence level, however, deteriorates as sentences including the influential\ndirective words tend to be dropped all together.\n","authors":["Miguel Rios","Ameen Abu-Hanna"],"pdf_url":"https://arxiv.org/pdf/2212.06267v1.pdf","comment":"Technical Reports at the Department of Medical Informatics, Amsterdam\n  UMC, 2021. https://kik.amc.nl/KIK/reports/TR2021-01.pdf"},{"id":"http://arxiv.org/abs/2211.09536v2","updated":"2022-12-12T20:14:38Z","published":"2022-11-17T13:59:34Z","title":"Towards Building Text-To-Speech Systems for the Next Billion Users","summary":"  Deep learning based text-to-speech (TTS) systems have been evolving rapidly\nwith advances in model architectures, training methodologies, and\ngeneralization across speakers and languages. However, these advances have not\nbeen thoroughly investigated for Indian language speech synthesis. Such\ninvestigation is computationally expensive given the number and diversity of\nIndian languages, relatively lower resource availability, and the diverse set\nof advances in neural TTS that remain untested. In this paper, we evaluate the\nchoice of acoustic models, vocoders, supplementary loss functions, training\nschedules, and speaker and language diversity for Dravidian and Indo-Aryan\nlanguages. Based on this, we identify monolingual models with FastPitch and\nHiFi-GAN V1, trained jointly on male and female speakers to perform the best.\nWith this setup, we train and evaluate TTS models for 13 languages and find our\nmodels to significantly improve upon existing models in all languages as\nmeasured by mean opinion scores. We open-source all models on the Bhashini\nplatform.\n","authors":["Gokul Karthik Kumar","Praveen S V","Pratyush Kumar","Mitesh M. Khapra","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2211.09536v2.pdf","comment":"Under review at ICASSP 2023. Gokul and Praveen contributed equally"},{"id":"http://arxiv.org/abs/2112.14569v2","updated":"2022-12-12T19:42:06Z","published":"2021-12-29T14:22:42Z","title":"Fine-Tuning Transformers: Vocabulary Transfer","summary":"  Transformers are responsible for the vast majority of recent advances in\nnatural language processing. The majority of practical natural language\nprocessing applications of these models are typically enabled through transfer\nlearning. This paper studies if corpus-specific tokenization used for\nfine-tuning improves the resulting performance of the model. Through a series\nof experiments, we demonstrate that such tokenization combined with the\ninitialization and fine-tuning strategy for the vocabulary tokens speeds up the\ntransfer and boosts the performance of the fine-tuned model. We call this\naspect of transfer facilitation vocabulary transfer.\n","authors":["Vladislav Mosin","Igor Samenko","Alexey Tikhonov","Borislav Kozlovskii","Ivan P. Yamshchikov"],"pdf_url":"https://arxiv.org/pdf/2112.14569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06121v1","updated":"2022-12-12T18:50:03Z","published":"2022-12-12T18:50:03Z","title":"In Defense of Cross-Encoders for Zero-Shot Retrieval","summary":"  Bi-encoders and cross-encoders are widely used in many state-of-the-art\nretrieval pipelines. In this work we study the generalization ability of these\ntwo types of architectures on a wide range of parameter count on both in-domain\nand out-of-domain scenarios. We find that the number of parameters and early\nquery-document interactions of cross-encoders play a significant role in the\ngeneralization ability of retrieval models. Our experiments show that\nincreasing model size results in marginal gains on in-domain test sets, but\nmuch larger gains in new domains never seen during fine-tuning. Furthermore, we\nshow that cross-encoders largely outperform bi-encoders of similar size in\nseveral tasks. In the BEIR benchmark, our largest cross-encoder surpasses a\nstate-of-the-art bi-encoder by more than 4 average points. Finally, we show\nthat using bi-encoders as first-stage retrievers provides no gains in\ncomparison to a simpler retriever such as BM25 on out-of-domain tasks. The code\nis available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n","authors":["Guilherme Rosa","Luiz Bonifacio","Vitor Jeronymo","Hugo Abonizio","Marzieh Fadaee","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2212.06121v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.02873"},{"id":"http://arxiv.org/abs/2212.06094v1","updated":"2022-12-12T18:09:09Z","published":"2022-12-12T18:09:09Z","title":"Prompting Is Programming: A Query Language For Large Language Models","summary":"  Large language models have demonstrated outstanding performance on a wide\nrange of tasks such as question answering and code generation. On a high level,\ngiven an input, a language model can be used to automatically complete the\nsequence in a statistically-likely way. Based on this, users prompt these\nmodels with language instructions or examples, to implement a variety of\ndownstream tasks. Advanced prompting methods can even imply interaction between\nthe language model, a user, and external tools such as calculators. However, to\nobtain state-of-the-art performance or adapt language models for specific\ntasks, complex task- and model-specific programs have to be implemented, which\nmay still require ad-hoc interaction.\n  Based on this, we present the novel idea of Language Model Programming (LMP).\nLMP generalizes language model prompting from pure text prompts to an intuitive\ncombination of text prompting and scripting. Additionally, LMP allows\nconstraints to be specified over the language model output. This enables easy\nadaption to many tasks, while abstracting language model internals and\nproviding high-level semantics. To enable LMP, we implement LMQL (short for\nLanguage Model Query Language), which leverages the constraints and control\nflow from an LMP prompt to generate an efficient inference procedure that\nminimizes the number of expensive calls to the underlying language model. We\nshow that LMQL can capture a wide range of state-of-the-art prompting methods\nin an intuitive way, especially facilitating interactive flows that are\nchallenging to implement with existing high-level APIs. Our evaluation shows\nthat we retain or increase the accuracy on several downstream tasks, while also\nsignificantly reducing the required amount of computation or cost in the case\nof pay-to-use APIs (13-85% cost savings).\n","authors":["Luca Beurer-Kellner","Marc Fischer","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2212.06094v1.pdf","comment":"21 pages + Appendix"},{"id":"http://arxiv.org/abs/2206.02873v5","updated":"2022-12-12T17:45:58Z","published":"2022-06-06T19:56:14Z","title":"No Parameter Left Behind: How Distillation and Model Size Affect\n  Zero-Shot Retrieval","summary":"  Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n","authors":["Guilherme Moraes Rosa","Luiz Bonifacio","Vitor Jeronymo","Hugo Abonizio","Marzieh Fadaee","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2206.02873v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02595v3","updated":"2022-12-12T16:52:03Z","published":"2022-10-05T23:01:36Z","title":"Exploration of A Self-Supervised Speech Model: A Study on Emotional\n  Corpora","summary":"  Self-supervised speech models have grown fast during the past few years and\nhave proven feasible for use in various downstream tasks. Some recent work has\nstarted to look at the characteristics of these models, yet many concerns have\nnot been fully addressed. In this work, we conduct a study on emotional corpora\nto explore a popular self-supervised model -- wav2vec 2.0. Via a set of\nquantitative analysis, we mainly demonstrate that: 1) wav2vec 2.0 appears to\ndiscard paralinguistic information that is less useful for word recognition\npurposes; 2) for emotion recognition, representations from the middle layer\nalone perform as well as those derived from layer averaging, while the final\nlayer results in the worst performance in some cases; 3) current\nself-supervised models may not be the optimal solution for downstream tasks\nthat make use of non-lexical features. Our work provides novel findings that\nwill aid future research in this area and theoretical basis for the use of\nexisting models.\n","authors":["Yuanchao Li","Yumnah Mohamied","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2210.02595v3.pdf","comment":"Accepted to SLT 2022"},{"id":"http://arxiv.org/abs/2212.06002v1","updated":"2022-12-12T16:03:38Z","published":"2022-12-12T16:03:38Z","title":"Effective Seed-Guided Topic Discovery by Integrating Multiple Types of\n  Contexts","summary":"  Instead of mining coherent topics from a given text corpus in a completely\nunsupervised manner, seed-guided topic discovery methods leverage user-provided\nseed words to extract distinctive and coherent topics so that the mined topics\ncan better cater to the user's interest. To model the semantic correlation\nbetween words and seeds for discovering topic-indicative terms, existing\nseed-guided approaches utilize different types of context signals, such as\ndocument-level word co-occurrences, sliding window-based local contexts, and\ngeneric linguistic knowledge brought by pre-trained language models. In this\nwork, we analyze and show empirically that each type of context information has\nits value and limitation in modeling word semantics under seed guidance, but\ncombining three types of contexts (i.e., word embeddings learned from local\ncontexts, pre-trained language model representations obtained from\ngeneral-domain training, and topic-indicative sentences retrieved based on seed\ninformation) allows them to complement each other for discovering quality\ntopics. We propose an iterative framework, SeedTopicMine, which jointly learns\nfrom the three types of contexts and gradually fuses their context signals via\nan ensemble ranking process. Under various sets of seeds and on multiple\ndatasets, SeedTopicMine consistently yields more coherent and accurate topics\nthan existing seed-guided topic discovery approaches.\n","authors":["Yu Zhang","Yunyi Zhang","Martin Michalski","Yucheng Jiang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2212.06002v1.pdf","comment":"9 pages; Accepted to WSDM 2023"},{"id":"http://arxiv.org/abs/2212.05998v1","updated":"2022-12-12T16:00:20Z","published":"2022-12-12T16:00:20Z","title":"Continuation KD: Improved Knowledge Distillation through the Lens of\n  Continuation Optimization","summary":"  Knowledge Distillation (KD) has been extensively used for natural language\nunderstanding (NLU) tasks to improve a small model's (a student) generalization\nby transferring the knowledge from a larger model (a teacher). Although KD\nmethods achieve state-of-the-art performance in numerous settings, they suffer\nfrom several problems limiting their performance. It is shown in the literature\nthat the capacity gap between the teacher and the student networks can make KD\nineffective. Additionally, existing KD techniques do not mitigate the noise in\nthe teacher's output: modeling the noisy behaviour of the teacher can distract\nthe student from learning more useful features. We propose a new KD method that\naddresses these problems and facilitates the training compared to previous\ntechniques. Inspired by continuation optimization, we design a training\nprocedure that optimizes the highly non-convex KD objective by starting with\nthe smoothed version of this objective and making it more complex as the\ntraining proceeds. Our method (Continuation-KD) achieves state-of-the-art\nperformance across various compact architectures on NLU (GLUE benchmark) and\ncomputer vision tasks (CIFAR-10 and CIFAR-100).\n","authors":["Aref Jafari","Ivan Kobyzev","Mehdi Rezagholizadeh","Pascal Poupart","Ali Ghodsi"],"pdf_url":"https://arxiv.org/pdf/2212.05998v1.pdf","comment":"Published at EMNLP 2022 (Findings)"},{"id":"http://arxiv.org/abs/2212.05982v1","updated":"2022-12-12T15:40:30Z","published":"2022-12-12T15:40:30Z","title":"Real-World Compositional Generalization with Disentangled\n  Sequence-to-Sequence Learning","summary":"  Compositional generalization is a basic mechanism in human language learning,\nwhich current neural networks struggle with. A recently proposed Disentangled\nsequence-to-sequence model (Dangle) shows promising generalization capability\nby learning specialized encodings for each decoding step. We introduce two key\nmodifications to this model which encourage more disentangled representations\nand improve its compute and memory efficiency, allowing us to tackle\ncompositional generalization in a more realistic setting. Specifically, instead\nof adaptively re-encoding source keys and values at each time step, we\ndisentangle their representations and only re-encode keys periodically, at some\ninterval. Our new architecture leads to better generalization performance\nacross existing tasks and datasets, and a new machine translation benchmark\nwhich we create by detecting naturally occurring compositional patterns in\nrelation to a training set. We show this methodology better emulates real-world\nrequirements than artificial challenges.\n","authors":["Hao Zheng","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2212.05982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05974v1","updated":"2022-12-12T15:29:48Z","published":"2022-12-12T15:29:48Z","title":"Federated NLP in Few-shot Scenarios","summary":"  Natural language processing (NLP) sees rich mobile applications. To support\nvarious language understanding tasks, a foundation NLP model is often\nfine-tuned in a federated, privacy-preserving setting (FL). This process\ncurrently relies on at least hundreds of thousands of labeled training samples\nfrom mobile clients; yet mobile users often lack willingness or knowledge to\nlabel their data. Such an inadequacy of data labels is known as a few-shot\nscenario; it becomes the key blocker for mobile NLP applications.\n  For the first time, this work investigates federated NLP in the few-shot\nscenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and\nprompt learning, we first establish a training pipeline that delivers\ncompetitive accuracy when only 0.05% (fewer than 100) of the training data is\nlabeled and the remaining is unlabeled. To instantiate the workflow, we further\npresent a system FFNLP, addressing the high execution cost with novel designs.\n(1) Curriculum pacing, which injects pseudo labels to the training workflow at\na rate commensurate to the learning progress; (2) Representational diversity, a\nmechanism for selecting the most learnable data, only for which pseudo labels\nwill be generated; (3) Co-planning of a model's training depth and layer\ncapacity. Together, these designs reduce the training delay, client energy, and\nnetwork traffic by up to 46.0$\\times$, 41.2$\\times$ and 3000.0$\\times$,\nrespectively. Through algorithm/system co-design, FFNLP demonstrates that FL\ncan apply to challenging settings where most training samples are unlabeled.\n","authors":["Dongqi Cai","Shangguang Wang","Yaozong Wu","Felix Xiaozhu Lin","Mengwei Xu"],"pdf_url":"https://arxiv.org/pdf/2212.05974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05961v1","updated":"2022-12-12T15:16:12Z","published":"2022-12-12T15:16:12Z","title":"RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning\n  for Language Understanding","summary":"  This paper presents a new data augmentation algorithm for natural\nunderstanding tasks, called RPN:Random Position Noise algorithm.Due to the\nrelative paucity of current text augmentation methods. Few of the extant\nmethods apply to natural language understanding tasks for all sentence-level\ntasks.RPN applies the traditional augmentation on the original text to the word\nvector level. The RPN algorithm makes a substitution in one or several\ndimensions of some word vectors. As a result, the RPN can introduce a certain\ndegree of perturbation to the sample and can adjust the range of perturbation\non different tasks. The augmented samples are then used to give the model\ntraining.This makes the model more robust. In subsequent experiments, we found\nthat adding RPN to the training or fine-tuning model resulted in a stable boost\non all 8 natural language processing tasks, including TweetEval, CoLA, and\nSST-2 datasets, and more significant improvements than other data augmentation\nalgorithms.The RPN algorithm applies to all sentence-level tasks for language\nunderstanding and is used in any deep learning model with a word embedding\nlayer.\n","authors":["Zhengqing Yuan","Zhuanzhe Zhao","Yongming Liu","Xiaolong Zhang","Xuecong Hou","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2212.05961v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.05956v1","updated":"2022-12-12T15:09:56Z","published":"2022-12-12T15:09:56Z","title":"Improving Generalization of Pre-trained Language Models via Stochastic\n  Weight Averaging","summary":"  Knowledge Distillation (KD) is a commonly used technique for improving the\ngeneralization of compact Pre-trained Language Models (PLMs) on downstream\ntasks. However, such methods impose the additional burden of training a\nseparate teacher model for every new dataset. Alternatively, one may directly\nwork on the improvement of the optimization procedure of the compact model\ntoward better generalization. Recent works observe that the flatness of the\nlocal minimum correlates well with better generalization. In this work, we\nadapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a\nflatter minimum, to fine-tuning PLMs. We conduct extensive experiments on\nvarious NLP tasks (text classification, question answering, and generation) and\ndifferent model architectures and demonstrate that our adaptation improves the\ngeneralization without extra computation cost. Moreover, we observe that this\nsimple optimization technique is able to outperform the state-of-the-art KD\nmethods for compact models.\n","authors":["Peng Lu","Ivan Kobyzev","Mehdi Rezagholizadeh","Ahmad Rashid","Ali Ghodsi","Philippe Langlais"],"pdf_url":"https://arxiv.org/pdf/2212.05956v1.pdf","comment":"Published at EMNLP 2022 (Findings)"},{"id":"http://arxiv.org/abs/2212.05901v1","updated":"2022-12-12T14:00:57Z","published":"2022-12-12T14:00:57Z","title":"Parameter-Efficient Finetuning of Transformers for Source Code","summary":"  Pretrained Transformers achieve state-of-the-art performance in various\ncode-processing tasks but may be too large to be deployed. As software\ndevelopment tools often incorporate modules for various purposes which may\npotentially use a single instance of the pretrained model, it appears relevant\nto utilize parameter-efficient fine-tuning for the pretrained models of code.\nIn this work, we test two widely used approaches, adapters and LoRA, which were\ninitially tested on NLP tasks, on four code-processing tasks. We find that\nthough the efficient fine-tuning approaches may achieve comparable or higher\nperformance than the standard, full, fine-tuning in code understanding tasks,\nthey underperform full fine-tuning in code-generative tasks. These results\nunderline the importance of testing efficient fine-tuning approaches on other\ndomains than NLP and motivate future research in efficient fine-tuning for\nsource code.\n","authors":["Shamil Ayupov","Nadezhda Chirkova"],"pdf_url":"https://arxiv.org/pdf/2212.05901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06765v1","updated":"2022-12-12T13:51:07Z","published":"2022-12-12T13:51:07Z","title":"Earthquake Impact Analysis Based on Text Mining and Social Media\n  Analytics","summary":"  Earthquakes have a deep impact on wide areas, and emergency rescue operations\nmay benefit from social media information about the scope and extent of the\ndisaster. Therefore, this work presents a text miningbased approach to collect\nand analyze social media data for early earthquake impact analysis. First,\ndisasterrelated microblogs are collected from the Sina microblog based on\ncrawler technology. Then, after data cleaning a series of analyses are\nconducted including (1) the hot words analysis, (2) the trend of the number of\nmicroblogs, (3) the trend of public opinion sentiment, and (4) a keyword and\nrule-based text classification for earthquake impact analysis. Finally, two\nrecent earthquakes with the same magnitude and focal depth in China are\nanalyzed to compare their impacts. The results show that the public opinion\ntrend analysis and the trend of public opinion sentiment can estimate the\nearthquake's social impact at an early stage, which will be helpful to\ndecision-making and rescue management.\n","authors":["Zhe Zheng","Hong-Zheng Shi","Yu-Cheng Zhou","Xin-Zheng Lu","Jia-Rui Lin"],"pdf_url":"https://arxiv.org/pdf/2212.06765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05891v1","updated":"2022-12-12T13:48:38Z","published":"2022-12-12T13:48:38Z","title":"Text Mining-Based Patent Analysis for Automated Rule Checking in AEC","summary":"  Automated rule checking (ARC), which is expected to promote the efficiency of\nthe compliance checking process in the architecture, engineering, and\nconstruction (AEC) industry, is gaining increasing attention. Throwing light on\nthe ARC application hotspots and forecasting its trends are useful to the\nrelated research and drive innovations. Therefore, this study takes the patents\nfrom the database of the Derwent Innovations Index database (DII) and China\nnational knowledge infrastructure (CNKI) as data sources and then carried out a\nthree-step analysis including (1) quantitative characteristics (i.e., annual\ndistribution analysis) of patents, (2) identification of ARC topics using a\nlatent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of\nARC topics. The results show that the research hotspots and trends of Chinese\nand English patents are different. The contributions of this study have three\naspects: (1) an approach to a comprehensive analysis of patents by integrating\nmultiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the\napplication hotspots and development trends of ARC are reviewed based on patent\nanalysis; and (3) a signpost for technological development and innovation of\nARC is provided.\n","authors":["Zhe Zheng","Bo-Rui Kang","Qi-Tian Yuan","Yu-Cheng Zhou","Xin-Zheng Lu","Jia-Rui Lin"],"pdf_url":"https://arxiv.org/pdf/2212.05891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05856v1","updated":"2022-12-12T12:41:24Z","published":"2022-12-12T12:41:24Z","title":"\"I think this is the most disruptive technology\": Exploring Sentiments\n  of ChatGPT Early Adopters using Twitter Data","summary":"  Large language models have recently attracted significant attention due to\ntheir impressive performance on a variety of tasks. ChatGPT developed by OpenAI\nis one such implementation of a large, pre-trained language model that has\ngained immense popularity among early adopters, where certain users go to the\nextent of characterizing it as a disruptive technology in many domains.\nUnderstanding such early adopters' sentiments is important because it can\nprovide insights into the potential success or failure of the technology, as\nwell as its strengths and weaknesses. In this paper, we conduct a mixed-method\nstudy using 10,732 tweets from early ChatGPT users. We first use topic\nmodelling to identify the main topics and then perform an in-depth qualitative\nsentiment analysis of each topic. Our results show that the majority of the\nearly adopters have expressed overwhelmingly positive sentiments related to\ntopics such as Disruptions to software development, Entertainment and\nexercising creativity. Only a limited percentage of users expressed concerns\nabout issues such as the potential for misuse of ChatGPT, especially regarding\ntopics such as Impact on educational aspects. We discuss these findings by\nproviding specific examples for each topic and then detail implications related\nto addressing these concerns for both researchers and users.\n","authors":["Mubin Ul Haque","Isuru Dharmadasa","Zarrin Tasnim Sworna","Roshan Namal Rajapakse","Hussain Ahmad"],"pdf_url":"https://arxiv.org/pdf/2212.05856v1.pdf","comment":"This is an early version of this paper"},{"id":"http://arxiv.org/abs/2212.05830v1","updated":"2022-12-12T11:19:05Z","published":"2022-12-12T11:19:05Z","title":"P-Transformer: Towards Better Document-to-Document Neural Machine\n  Translation","summary":"  Directly training a document-to-document (Doc2Doc) neural machine translation\n(NMT) via Transformer from scratch, especially on small datasets usually fails\nto converge. Our dedicated probing tasks show that 1) both the absolute\nposition and relative position information gets gradually weakened or even\nvanished once it reaches the upper encoder layers, and 2) the vanishing of\nabsolute position information in encoder output causes the training failure of\nDoc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer\n(P-Transformer) to enhance both the absolute and relative position information\nin both self-attention and cross-attention. Specifically, we integrate absolute\npositional information, i.e., position embeddings, into the query-key pairs\nboth in self-attention and cross-attention through a simple yet effective\naddition operation. Moreover, we also integrate relative position encoding in\nself-attention. The proposed P-Transformer utilizes sinusoidal position\nencoding and does not require any task-specified position embedding, segment\nembedding, or attention mechanism. Through the above methods, we build a\nDoc2Doc NMT model with P-Transformer, which ingests the source document and\ncompletely generates the target document in a sequence-to-sequence (seq2seq)\nway. In addition, P-Transformer can be applied to seq2seq-based\ndocument-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent)\ntranslation. Extensive experimental results of Doc2Doc NMT show that\nP-Transformer significantly outperforms strong baselines on widely-used 9\ndocument-level datasets in 7 language pairs, covering small-, middle-, and\nlarge-scales, and achieves a new state-of-the-art. Experimentation on discourse\nphenomena shows that our Doc2Doc NMT models improve the translation quality in\nboth BLEU and discourse coherence. We make our code available on Github.\n","authors":["Yachao Li","Junhui Li","Jing Jiang","Shimin Tao","Hao Yang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.05830v1.pdf","comment":"Submitted to TASLP"},{"id":"http://arxiv.org/abs/2212.02017v2","updated":"2022-12-12T11:07:32Z","published":"2022-12-05T04:22:00Z","title":"GNN-SL: Sequence Labeling Based on Nearest Examples via GNN","summary":"  To better handle long-tail cases in the sequence labeling (SL) task, in this\nwork, we introduce graph neural networks sequence labeling (GNN-SL), which\naugments the vanilla SL model output with similar tagging examples retrieved\nfrom the whole training set. Since not all the retrieved tagging examples\nbenefit the model prediction, we construct a heterogeneous graph, and leverage\ngraph neural networks (GNNs) to transfer information between the retrieved\ntagging examples and the input word sequence. The augmented node which\naggregates information from neighbors is used to do prediction. This strategy\nenables the model to directly acquire similar tagging examples and improves the\ngeneral quality of predictions. We conduct a variety of experiments on three\ntypical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech\nTagging (POS), and Chinese Word Segmentation (CWS) to show the significant\nperformance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2)\non PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the\nCWS task, and results comparable to SOTA performances on NER datasets, and POS\ndatasets.\n","authors":["Shuhe Wang","Yuxian Meng","Rongbin Ouyang","Jiwei Li","Tianwei Zhang","Lingjuan Lyu","Guoyin Wang"],"pdf_url":"https://arxiv.org/pdf/2212.02017v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2212.05805v1","updated":"2022-12-12T10:03:10Z","published":"2022-12-12T10:03:10Z","title":"Direct Speech-to-speech Translation without Textual Annotation using\n  Bottleneck Features","summary":"  Speech-to-speech translation directly translates a speech utterance to\nanother between different languages, and has great potential in tasks such as\nsimultaneous interpretation. State-of-art models usually contains an auxiliary\nmodule for phoneme sequences prediction, and this requires textual annotation\nof the training dataset. We propose a direct speech-to-speech translation model\nwhich can be trained without any textual annotation or content information.\nInstead of introducing an auxiliary phoneme prediction task in the model, we\npropose to use bottleneck features as intermediate training objectives for our\nmodel to ensure the translation performance of the system. Experiments on\nMandarin-Cantonese speech translation demonstrate the feasibility of the\nproposed approach and the performance can match a cascaded system with respect\nof translation and synthesis qualities.\n","authors":["Junhui Zhang","Junjie Pan","Xiang Yin","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2212.05805v1.pdf","comment":"4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.05798v1","updated":"2022-12-12T09:49:02Z","published":"2022-12-12T09:49:02Z","title":"BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph","summary":"  Answering complex questions over textual resources remains a challenging\nproblem$\\unicode{x2013}$especially when interpreting the fine-grained\nrelationships among multiple entities that occur within a natural-language\nquestion or clue. Curated knowledge bases (KBs), such as YAGO, DBpedia,\nFreebase and Wikidata, have been widely used in this context and gained great\nacceptance for question-answering (QA) applications in the past decade. While\ncurrent KBs offer a concise representation of structured knowledge, they lack\nthe variety of formulations and semantic nuances as well as the context of\ninformation provided by the natural-language sources. With BigText-QA, we aim\nto develop an integrated QA system which is able to answer questions based on a\nmore redundant form of a knowledge graph (KG) that organizes both structured\nand unstructured (i.e., \"hybrid\") knowledge in a unified graphical\nrepresentation. BigText-QA thereby is able to combine the best of both\nworlds$\\unicode{x2013}$a canonical set of named entities, mapped to a\nstructured background KB (such as YAGO or Wikidata), as well as an open set of\ntextual clauses providing highly diversified relational paraphrases with rich\ncontext information.\n","authors":["Jingjing Xu","Maria Biryukov","Martin Theobald","Vinu Ellampallil Venugopal"],"pdf_url":"https://arxiv.org/pdf/2212.05798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05789v1","updated":"2022-12-12T09:27:50Z","published":"2022-12-12T09:27:50Z","title":"Collaborating Heterogeneous Natural Language Processing Tasks via\n  Federated Learning","summary":"  The increasing privacy concerns on personal private text data promote the\ndevelopment of federated learning (FL) in recent years. However, the existing\nstudies on applying FL in NLP are not suitable to coordinate participants with\nheterogeneous or private learning objectives. In this study, we further broaden\nthe application scope of FL in NLP by proposing an Assign-Then-Contrast\n(denoted as ATC) framework, which enables clients with heterogeneous NLP tasks\nto construct an FL course and learn useful knowledge from each other.\nSpecifically, the clients are suggested to first perform local training with\nthe unified tasks assigned by the server rather than using their own learning\nobjectives, which is called the Assign training stage. After that, in the\nContrast training stage, clients train with different local learning objectives\nand exchange knowledge with other clients who contribute consistent and useful\nmodel updates. We conduct extensive experiments on six widely-used datasets\ncovering both Natural Language Understanding (NLU) and Natural Language\nGeneration (NLG) tasks, and the proposed ATC framework achieves significant\nimprovements compared with various baseline methods. The source code is\navailable at\n\\url{https://github.com/alibaba/FederatedScope/tree/master/federatedscope/nlp/hetero_tasks}.\n","authors":["Chenhe Dong","Yuexiang Xie","Bolin Ding","Ying Shen","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2212.05789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04922v3","updated":"2022-12-12T09:06:43Z","published":"2022-06-10T07:46:34Z","title":"A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural\n  Machine Translation","summary":"  Chinese dialects are different variations of Chinese and can be considered as\ndifferent languages in the same language family with Mandarin. Though they all\nuse Chinese characters, the pronunciations, grammar and idioms can vary\nsignificantly, and even local speakers may find it hard to input correct\nwritten forms of dialect. Besides, using Mandarin text as text-to-speech inputs\nwould generate speech with poor naturalness. In this paper, we propose a novel\nChinese dialect TTS frontend with a translation module, which converts Mandarin\ntext into dialectic expressions to improve the intelligibility and naturalness\nof synthesized speech. A non-autoregressive neural machine translation model\nwith various tricks is proposed for the translation task. It is the first known\nwork to incorporate translation with TTS frontend. Experiments on Cantonese\nshow the proposed model improves 2.56 BLEU and TTS improves 0.27 MOS with\nMandarin inputs.\n","authors":["Junhui Zhang","Wudi Bao","Junjie Pan","Xiang Yin","Zejun Ma"],"pdf_url":"https://arxiv.org/pdf/2206.04922v3.pdf","comment":"4 pages,5 figures"},{"id":"http://arxiv.org/abs/2109.12346v3","updated":"2022-12-12T08:55:20Z","published":"2021-09-25T11:51:35Z","title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect","summary":"  Pre-trained transformers are now the de facto models in Natural Language\nProcessing given their state-of-the-art results in many tasks and languages.\nHowever, most of the current models have been trained on languages for which\nlarge text resources are already available (such as English, French, Arabic,\netc.). Therefore, there are still a number of low-resource languages that need\nmore attention from the community. In this paper, we study the Algerian dialect\nwhich has several specificities that make the use of Arabic or multilingual\nmodels inappropriate. To address this issue, we collected more than one million\nAlgerian tweets, and pre-trained the first Algerian language model: DziriBERT.\nWhen compared with existing models, DziriBERT achieves better results,\nespecially when dealing with the Roman script. The obtained results show that\npre-training a dedicated model on a small dataset (150 MB) can outperform\nexisting models that have been trained on much more data (hundreds of GB).\nFinally, our model is publicly available to the community.\n","authors":["Amine Abdaoui","Mohamed Berrimi","Mourad Oussalah","Abdelouahab Moussaoui"],"pdf_url":"https://arxiv.org/pdf/2109.12346v3.pdf","comment":"4 Pages"},{"id":"http://arxiv.org/abs/2212.05773v1","updated":"2022-12-12T08:51:30Z","published":"2022-12-12T08:51:30Z","title":"A Survey on Natural Language Processing for Programming","summary":"  Natural language processing for programming, which aims to use NLP techniques\nto assist programming, has experienced an explosion in recent years. However,\nthere is no literature that systematically reviews related work from the full\nspectrum. In this paper, we comprehensively investigate existing work, ranging\nfrom early deductive models to the latest competition-level models. Another\nadvantage of this paper is the completeness of the technique category, which\nprovides easy access to locating and comparing future works.\n","authors":["Qingfu Zhu","Xianzhen Luo","Fang Liu","Cuiyun Gao","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2212.05773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05765v1","updated":"2022-12-12T08:38:28Z","published":"2022-12-12T08:38:28Z","title":"Information-Theoretic Text Hallucination Reduction for Video-grounded\n  Dialogue","summary":"  Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question\nregarding a given video and dialogue context. Despite the recent success of\nmulti-modal reasoning to generate answer sentences, existing dialogue systems\nstill suffer from a text hallucination problem, which denotes indiscriminate\ntext-copying from input texts without an understanding of the question. This is\ndue to learning spurious correlations from the fact that answer sentences in\nthe dataset usually include the words of input texts, thus the VGD system\nexcessively relies on copying words from input texts by hoping those words to\noverlap with ground-truth texts. Hence, we design Text Hallucination Mitigating\n(THAM) framework, which incorporates Text Hallucination Regularization (THR)\nloss derived from the proposed information-theoretic text hallucination\nmeasurement approach. Applying THAM with current dialogue systems validates the\neffectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows\nenhanced interpretability.\n","authors":["Sunjae Yoon","Eunseop Yoon","Hee Suk Yoon","Junyeong Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2212.05765v1.pdf","comment":"12 pages, Accepted in EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.05764v1","updated":"2022-12-12T08:32:28Z","published":"2022-12-12T08:32:28Z","title":"Domain Adaptation of Transformer-Based Models using Unlabeled Data for\n  Relevance and Polarity Classification of German Customer Feedback","summary":"  Understanding customer feedback is becoming a necessity for companies to\nidentify problems and improve their products and services. Text classification\nand sentiment analysis can play a major role in analyzing this data by using a\nvariety of machine and deep learning approaches. In this work, different\ntransformer-based models are utilized to explore how efficient these models are\nwhen working with a German customer feedback dataset. In addition, these\npre-trained models are further analyzed to determine if adapting them to a\nspecific domain using unlabeled data can yield better results than\noff-the-shelf pre-trained models. To evaluate the models, two downstream tasks\nfrom the GermEval 2017 are considered. The experimental results show that\ntransformer-based models can reach significant improvements compared to a\nfastText baseline and outperform the published scores and previous models. For\nthe subtask Relevance Classification, the best models achieve a micro-averaged\n$F1$-Score of 96.1 % on the first test set and 95.9 % on the second one, and a\nscore of 85.1 % and 85.3 % for the subtask Polarity Classification.\n","authors":["Ahmad Idrissi-Yaghir","Henning Schäfer","Nadja Bauer","Christoph M. Friedrich"],"pdf_url":"https://arxiv.org/pdf/2212.05764v1.pdf","comment":"Submitted to SN Computer Science"},{"id":"http://arxiv.org/abs/2212.05762v1","updated":"2022-12-12T08:28:22Z","published":"2022-12-12T08:28:22Z","title":"Momentum Contrastive Pre-training for Question Answering","summary":"  Existing pre-training methods for extractive Question Answering (QA) generate\ncloze-like queries different from natural questions in syntax structure, which\ncould overfit pre-trained models to simple keyword matching. In order to\naddress this problem, we propose a novel Momentum Contrastive pRe-training fOr\nqueStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS\nintroduces a momentum contrastive learning framework to align the answer\nprobability between cloze-like and natural query-passage sample pairs. Hence,\nthe pre-trained models can better transfer the knowledge learned in cloze-like\nsamples to answering natural questions. Experimental results on three\nbenchmarking QA datasets show that our method achieves noticeable improvement\ncompared with all baselines in both supervised and zero-shot scenarios.\n","authors":["Minda Hu","Muzhi Li","Yasheng Wang","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2212.05762v1.pdf","comment":"This has been accepted by EMNLP 2022. The reference to ACL Anthology\n  will soon be added"},{"id":"http://arxiv.org/abs/2212.05740v1","updated":"2022-12-12T07:37:45Z","published":"2022-12-12T07:37:45Z","title":"Searching for Effective Multilingual Fine-Tuning Methods: A Case Study\n  in Summarization","summary":"  Recently, a large number of tuning strategies have been proposed to adapt\npre-trained language models to downstream tasks. In this paper, we perform an\nextensive empirical evaluation of various tuning strategies for multilingual\nlearning, particularly in the context of text summarization. Specifically, we\nexplore the relative advantages of three families of multilingual tuning\nstrategies (a total of five models) and empirically evaluate them for\nsummarization over 45 languages. Experimentally, we not only established a new\nstate-of-the-art on the XL-Sum dataset but also derive a series of observations\nthat hopefully can provide hints for future research on the design of\nmultilingual tuning strategies.\n","authors":["Yiwei Qin","Graham Neubig","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2212.05740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05726v1","updated":"2022-12-12T06:29:04Z","published":"2022-12-12T06:29:04Z","title":"T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics","summary":"  Modern embedding-based metrics for evaluation of generated text generally\nfall into one of two paradigms: discriminative metrics that are trained to\ndirectly predict which outputs are of higher quality according to supervised\nhuman annotations, and generative metrics that are trained to evaluate text\nbased on the probabilities of a generative model. Both have their advantages;\ndiscriminative metrics are able to directly optimize for the problem of\ndistinguishing between good and bad outputs, while generative metrics can be\ntrained using abundant raw text. In this paper, we present a framework that\ncombines the best of both worlds, using both supervised and unsupervised\nsignals from whatever data we have available. We operationalize this idea by\ntraining T5Score, a metric that uses these training signals with mT5 as the\nbackbone. We perform an extensive empirical comparison with other existing\nmetrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility\nof our method. Experimental results show that: T5Score achieves the best\nperformance on all datasets against existing top-scoring metrics at the segment\nlevel. We release our code and models at https://github.com/qinyiwei/T5Score.\n","authors":["Yiwei Qin","Weizhe Yuan","Graham Neubig","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2212.05726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05705v3","updated":"2022-12-12T05:06:05Z","published":"2022-11-10T17:18:20Z","title":"DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple\n  Analysis","summary":"  The rapid development of aspect-based sentiment analysis (ABSA) within recent\ndecades shows great potential for real-world society. The current ABSA works,\nhowever, are mostly limited to the scenario of a single text piece, leaving the\nstudy in dialogue contexts unexplored. In this work, we introduce a novel task\nof conversational aspect-based sentiment quadruple analysis, namely DiaASQ,\naiming to detect the sentiment quadruple of target-aspect-opinion-sentiment in\na dialogue. DiaASQ bridges the gap between fine-grained sentiment analysis and\nconversational opinion mining. We manually construct a large-scale high-quality\nDiaASQ dataset in both Chinese and English languages. We deliberately develop a\nneural model to benchmark the task, which advances in effectively performing\nend-to-end quadruple prediction, and manages to incorporate rich\ndialogue-specific and discourse feature representations for better\ncross-utterance quadruple extraction. We finally point out several potential\nfuture works to facilitate the follow-up research of this new task.\n","authors":["Bobo Li","Hao Fei","Fei Li","Yuhan Wu","Jinsong Zhang","Shengqiong Wu","Jingye Li","Yijiang Liu","Lizi Liao","Tat-Seng Chua","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2211.05705v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05702v1","updated":"2022-12-12T04:50:43Z","published":"2022-12-12T04:50:43Z","title":"Implementing Deep Learning-Based Approaches for Article Summarization in\n  Indian Languages","summary":"  The research on text summarization for low-resource Indian languages has been\nlimited due to the availability of relevant datasets. This paper presents a\nsummary of various deep-learning approaches used for the ILSUM 2022 Indic\nlanguage summarization datasets. The ISUM 2022 dataset consists of news\narticles written in Indian English, Hindi, and Gujarati respectively, and their\nground-truth summarizations. In our work, we explore different pre-trained\nseq2seq models and fine-tune those with the ILSUM 2022 datasets. In our case,\nthe fine-tuned SoTA PEGASUS model worked the best for English, the fine-tuned\nIndicBART model with augmented data for Hindi, and again fine-tuned PEGASUS\nmodel along with a translation mapping-based approach for Gujarati. Our scores\non the obtained inferences were evaluated using ROUGE-1, ROUGE-2, and ROUGE-4\nas the evaluation metrics.\n","authors":["Rahul Tangsali","Aabha Pingle","Aditya Vyawahare","Isha Joshi","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2212.05702v1.pdf","comment":"Accepted at ILSUM at FIRE 2022"},{"id":"http://arxiv.org/abs/2202.13758v3","updated":"2022-12-12T04:47:49Z","published":"2022-02-28T13:18:26Z","title":"Logical Fallacy Detection","summary":"  Reasoning is central to human intelligence. However, fallacious arguments are\ncommon, and some exacerbate problems such as spreading misinformation about\nclimate change. In this paper, we propose the task of logical fallacy\ndetection, and provide a new dataset (Logic) of logical fallacies generally\nfound in text, together with an additional challenge set for detecting logical\nfallacies in climate change claims (LogicClimate). Detecting logical fallacies\nis a hard problem as the model must understand the underlying logical structure\nof the argument. We find that existing pretrained large language models perform\npoorly on this task. In contrast, we show that a simple structure-aware\nclassifier outperforms the best language model by 5.46% on Logic and 4.51% on\nLogicClimate. We encourage future work to explore this task as (a) it can serve\nas a new reasoning challenge for language models, and (b) it can have potential\napplications in tackling the spread of misinformation. Our dataset and code are\navailable at https://github.com/causalNLP/logical-fallacy\n","authors":["Zhijing Jin","Abhinav Lalwani","Tejas Vaidhya","Xiaoyu Shen","Yiwen Ding","Zhiheng Lyu","Mrinmaya Sachan","Rada Mihalcea","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2202.13758v3.pdf","comment":"EMNLP 2021 Findings"},{"id":"http://arxiv.org/abs/2212.05696v1","updated":"2022-12-12T04:20:11Z","published":"2022-12-12T04:20:11Z","title":"Ensembling Transformers for Cross-domain Automatic Term Extraction","summary":"  Automatic term extraction plays an essential role in domain language\nunderstanding and several natural language processing downstream tasks. In this\npaper, we propose a comparative study on the predictive power of\nTransformers-based pretrained language models toward term extraction in a\nmulti-language cross-domain setting. Besides evaluating the ability of\nmonolingual models to extract single- and multi-word terms, we also experiment\nwith ensembles of mono- and multilingual models by conducting the intersection\nor union on the term output sets of different language models. Our experiments\nhave been conducted on the ACTER corpus covering four specialized domains\n(Corruption, Wind energy, Equitation, and Heart failure) and three languages\n(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four\nadditional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The\nresults show that the strategy of employing monolingual models outperforms the\nstate-of-the-art approaches from the related work leveraging multilingual\nmodels, regarding all the languages except Dutch and French if the term\nextraction task excludes the extraction of named entity terms. Furthermore, by\ncombining the outputs of the two best performing models, we achieve significant\nimprovements.\n","authors":["Hanh Thi Hong Tran","Matej Martinc","Andraz Pelicon","Antoine Doucet","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2212.05696v1.pdf","comment":"11 pages including references, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2209.08524v2","updated":"2022-12-12T02:32:09Z","published":"2022-09-18T10:19:04Z","title":"A Benchmark for Understanding and Generating Dialogue between Characters\n  in Stories","summary":"  Many classical fairy tales, fiction, and screenplays leverage dialogue to\nadvance story plots and establish characters. We present the first study to\nexplore whether machines can understand and generate dialogue in stories, which\nrequires capturing traits of different characters and the relationships between\nthem. To this end, we propose two new tasks including Masked Dialogue\nGeneration and Dialogue Speaker Recognition, i.e., generating missing dialogue\nturns and predicting speakers for specified dialogue turns, respectively. We\nbuild a new dataset DialStory, which consists of 105k Chinese stories with a\nlarge amount of dialogue weaved into the plots to support the evaluation. We\nshow the difficulty of the proposed tasks by testing existing models with\nautomatic and manual evaluation on DialStory. Furthermore, we propose to learn\nexplicit character representations to improve performance on these tasks.\nExtensive experiments and case studies show that our approach can generate more\ncoherent and informative dialogue, and achieve higher speaker recognition\naccuracy than strong baselines.\n","authors":["Jianzhu Yao","Ziqi Liu","Jian Guan","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2209.08524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07425v1","updated":"2022-12-12T20:27:17Z","published":"2022-12-12T20:27:17Z","title":"Robust and Explainable Identification of Logical Fallacies in Natural\n  Language Arguments","summary":"  The spread of misinformation, propaganda, and flawed argumentation has been\namplified in the Internet era. Given the volume of data and the subtlety of\nidentifying violations of argumentation norms, supporting information analytics\ntasks, like content moderation, with trustworthy methods that can identify\nlogical fallacies is essential. In this paper, we formalize prior theoretical\nwork on logical fallacies into a comprehensive three-stage evaluation framework\nof detection, coarse-grained, and fine-grained classification. We adapt\nexisting evaluation datasets for each stage of the evaluation. We devise three\nfamilies of robust and explainable methods based on prototype reasoning,\ninstance-based reasoning, and knowledge injection. The methods are designed to\ncombine language models with background knowledge and explainable mechanisms.\nMoreover, we address data sparsity with strategies for data augmentation and\ncurriculum learning. Our three-stage framework natively consolidates prior\ndatasets and methods from existing tasks, like propaganda detection, serving as\nan overarching evaluation testbed. We extensively evaluate these methods on our\ndatasets, focusing on their robustness and explainability. Our results provide\ninsight into the strengths and weaknesses of the methods on different\ncomponents and fallacy classes, indicating that fallacy identification is a\nchallenging task that may require specialized forms of reasoning to capture\nvarious classes. We share our open-source code and data on GitHub to support\nfurther work on logical fallacy identification.\n","authors":["Zhivar Sourati","Vishnu Priya Prasanna Venkatesh","Darshan Deshpande","Himanshu Rawlani","Filip Ilievski","Hông-Ân Sandlin","Alain Mermoud"],"pdf_url":"https://arxiv.org/pdf/2212.07425v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.06121v1","updated":"2022-12-12T18:50:03Z","published":"2022-12-12T18:50:03Z","title":"In Defense of Cross-Encoders for Zero-Shot Retrieval","summary":"  Bi-encoders and cross-encoders are widely used in many state-of-the-art\nretrieval pipelines. In this work we study the generalization ability of these\ntwo types of architectures on a wide range of parameter count on both in-domain\nand out-of-domain scenarios. We find that the number of parameters and early\nquery-document interactions of cross-encoders play a significant role in the\ngeneralization ability of retrieval models. Our experiments show that\nincreasing model size results in marginal gains on in-domain test sets, but\nmuch larger gains in new domains never seen during fine-tuning. Furthermore, we\nshow that cross-encoders largely outperform bi-encoders of similar size in\nseveral tasks. In the BEIR benchmark, our largest cross-encoder surpasses a\nstate-of-the-art bi-encoder by more than 4 average points. Finally, we show\nthat using bi-encoders as first-stage retrievers provides no gains in\ncomparison to a simpler retriever such as BM25 on out-of-domain tasks. The code\nis available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n","authors":["Guilherme Rosa","Luiz Bonifacio","Vitor Jeronymo","Hugo Abonizio","Marzieh Fadaee","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2212.06121v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2206.02873"},{"id":"http://arxiv.org/abs/2212.04481v2","updated":"2022-12-12T18:30:49Z","published":"2022-12-08T18:54:15Z","title":"A Survey of Graph Neural Networks for Social Recommender Systems","summary":"  Social recommender systems (SocialRS) simultaneously leverage user-to-item\ninteractions as well as user-to-user social relations for the task of\ngenerating item recommendations to users. Additionally exploiting social\nrelations is clearly effective in understanding users' tastes due to the\neffects of homophily and social influence. For this reason, SocialRS has\nincreasingly attracted attention. In particular, with the advance of Graph\nNeural Networks (GNN), many GNN-based SocialRS methods have been developed\nrecently. Therefore, we conduct a comprehensive and systematic review of the\nliterature on GNN-based SocialRS. In this survey, we first identify 80 papers\non GNN-based SocialRS after annotating 2151 papers by following the PRISMA\nframework (Preferred Reporting Items for Systematic Reviews and Meta-Analysis).\nThen, we comprehensively review them in terms of their inputs and architectures\nto propose a novel taxonomy: (1) input taxonomy includes 5 groups of input type\nnotations and 7 groups of input representation notations; (2) architecture\ntaxonomy includes 8 groups of GNN encoder, 2 groups of decoder, and 12 groups\nof loss function notations. We classify the GNN-based SocialRS methods into\nseveral categories as per the taxonomy and describe their details. Furthermore,\nwe summarize the benchmark datasets and metrics widely used to evaluate the\nGNN-based SocialRS methods. Finally, we conclude this survey by presenting some\nfuture research directions.\n","authors":["Kartik Sharma","Yeon-Chang Lee","Sivagami Nambi","Aditya Salian","Shlok Shah","Sang-Wook Kim","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2212.04481v2.pdf","comment":"GitHub repository with the curated list of papers:\n  https://github.com/claws-lab/awesome-GNN-social-recsys"},{"id":"http://arxiv.org/abs/2206.02873v5","updated":"2022-12-12T17:45:58Z","published":"2022-06-06T19:56:14Z","title":"No Parameter Left Behind: How Distillation and Model Size Affect\n  Zero-Shot Retrieval","summary":"  Recent work has shown that small distilled language models are strong\ncompetitors to models that are orders of magnitude larger and slower in a wide\nrange of information retrieval tasks. This has made distilled and dense models,\ndue to latency constraints, the go-to choice for deployment in real-world\nretrieval applications. In this work, we question this practice by showing that\nthe number of parameters and early query-document interaction play a\nsignificant role in the generalization ability of retrieval models. Our\nexperiments show that increasing model size results in marginal gains on\nin-domain test sets, but much larger gains in new domains never seen during\nfine-tuning. Furthermore, we show that rerankers largely outperform dense ones\nof similar size in several tasks. Our largest reranker reaches the state of the\nart in 12 of the 18 datasets of the Benchmark-IR (BEIR) and surpasses the\nprevious state of the art by 3 average points. Finally, we confirm that\nin-domain effectiveness is not a good indicator of zero-shot effectiveness.\nCode is available at\nhttps://github.com/guilhermemr04/scaling-zero-shot-retrieval.git\n","authors":["Guilherme Moraes Rosa","Luiz Bonifacio","Vitor Jeronymo","Hugo Abonizio","Marzieh Fadaee","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2206.02873v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06002v1","updated":"2022-12-12T16:03:38Z","published":"2022-12-12T16:03:38Z","title":"Effective Seed-Guided Topic Discovery by Integrating Multiple Types of\n  Contexts","summary":"  Instead of mining coherent topics from a given text corpus in a completely\nunsupervised manner, seed-guided topic discovery methods leverage user-provided\nseed words to extract distinctive and coherent topics so that the mined topics\ncan better cater to the user's interest. To model the semantic correlation\nbetween words and seeds for discovering topic-indicative terms, existing\nseed-guided approaches utilize different types of context signals, such as\ndocument-level word co-occurrences, sliding window-based local contexts, and\ngeneric linguistic knowledge brought by pre-trained language models. In this\nwork, we analyze and show empirically that each type of context information has\nits value and limitation in modeling word semantics under seed guidance, but\ncombining three types of contexts (i.e., word embeddings learned from local\ncontexts, pre-trained language model representations obtained from\ngeneral-domain training, and topic-indicative sentences retrieved based on seed\ninformation) allows them to complement each other for discovering quality\ntopics. We propose an iterative framework, SeedTopicMine, which jointly learns\nfrom the three types of contexts and gradually fuses their context signals via\nan ensemble ranking process. Under various sets of seeds and on multiple\ndatasets, SeedTopicMine consistently yields more coherent and accurate topics\nthan existing seed-guided topic discovery approaches.\n","authors":["Yu Zhang","Yunyi Zhang","Martin Michalski","Yucheng Jiang","Yu Meng","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2212.06002v1.pdf","comment":"9 pages; Accepted to WSDM 2023"},{"id":"http://arxiv.org/abs/2212.05996v1","updated":"2022-12-12T15:56:13Z","published":"2022-12-12T15:56:13Z","title":"Dirichlet-Survival Process: Scalable Inference of Topic-Dependent\n  Diffusion Networks","summary":"  Information spread on networks can be efficiently modeled by considering\nthree features: documents' content, time of publication relative to other\npublications, and position of the spreader in the network. Most previous works\nmodel up to two of those jointly, or rely on heavily parametric approaches.\nBuilding on recent Dirichlet-Point processes literature, we introduce the\nHouston (Hidden Online User-Topic Network) model, that jointly considers all\nthose features in a non-parametric unsupervised framework. It infers dynamic\ntopic-dependent underlying diffusion networks in a continuous-time setting\nalong with said topics. It is unsupervised; it considers an unlabeled stream of\ntriplets shaped as \\textit{(time of publication, information's content,\nspreading entity)} as input data. Online inference is conducted using a\nsequential Monte-Carlo algorithm that scales linearly with the size of the\ndataset. Our approach yields consequent improvements over existing baselines on\nboth cluster recovery and subnetworks inference tasks.\n","authors":["Gaël Poux-Médard","Julien Velcin","Sabine Loudcher"],"pdf_url":"https://arxiv.org/pdf/2212.05996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05891v1","updated":"2022-12-12T13:48:38Z","published":"2022-12-12T13:48:38Z","title":"Text Mining-Based Patent Analysis for Automated Rule Checking in AEC","summary":"  Automated rule checking (ARC), which is expected to promote the efficiency of\nthe compliance checking process in the architecture, engineering, and\nconstruction (AEC) industry, is gaining increasing attention. Throwing light on\nthe ARC application hotspots and forecasting its trends are useful to the\nrelated research and drive innovations. Therefore, this study takes the patents\nfrom the database of the Derwent Innovations Index database (DII) and China\nnational knowledge infrastructure (CNKI) as data sources and then carried out a\nthree-step analysis including (1) quantitative characteristics (i.e., annual\ndistribution analysis) of patents, (2) identification of ARC topics using a\nlatent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of\nARC topics. The results show that the research hotspots and trends of Chinese\nand English patents are different. The contributions of this study have three\naspects: (1) an approach to a comprehensive analysis of patents by integrating\nmultiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the\napplication hotspots and development trends of ARC are reviewed based on patent\nanalysis; and (3) a signpost for technological development and innovation of\nARC is provided.\n","authors":["Zhe Zheng","Bo-Rui Kang","Qi-Tian Yuan","Yu-Cheng Zhou","Xin-Zheng Lu","Jia-Rui Lin"],"pdf_url":"https://arxiv.org/pdf/2212.05891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05735v1","updated":"2022-12-12T07:19:14Z","published":"2022-12-12T07:19:14Z","title":"Adaptive Low-Precision Training for Embeddings in Click-Through Rate\n  Prediction","summary":"  Embedding tables are usually huge in click-through rate (CTR) prediction\nmodels. To train and deploy the CTR models efficiently and economically, it is\nnecessary to compress their embedding tables at the training stage. To this\nend, we formulate a novel quantization training paradigm to compress the\nembeddings from the training stage, termed low-precision training (LPT). Also,\nwe provide theoretical analysis on its convergence. The results show that\nstochastic weight quantization has a faster convergence rate and a smaller\nconvergence error than deterministic weight quantization in LPT. Further, to\nreduce the accuracy degradation, we propose adaptive low-precision training\n(ALPT) that learns the step size (i.e., the quantization resolution) through\ngradient descent. Experiments on two real-world datasets confirm our analysis\nand show that ALPT can significantly improve the prediction accuracy,\nespecially at extremely low bit widths. For the first time in CTR models, we\nsuccessfully train 8-bit embeddings without sacrificing prediction accuracy.\nThe code of ALPT is publicly available.\n","authors":["Shiwei Li","Huifeng Guo","Lu Hou","Wei Zhang","Xing Tang","Ruiming Tang","Rui Zhang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2212.05735v1.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2212.05720v1","updated":"2022-12-12T05:55:40Z","published":"2022-12-12T05:55:40Z","title":"Tensor-based Sequential Learning via Hankel Matrix Representation for\n  Next Item Recommendations","summary":"  Self-attentive transformer models have recently been shown to solve the next\nitem recommendation task very efficiently. The learned attention weights\ncapture sequential dynamics in user behavior and generalize well. Motivated by\nthe special structure of learned parameter space, we question if it is possible\nto mimic it with an alternative and more lightweight approach. We develop a new\ntensor factorization-based model that ingrains the structural knowledge about\nsequential data within the learning process. We demonstrate how certain\nproperties of a self-attention network can be reproduced with our approach\nbased on special Hankel matrix representation. The resulting model has a\nshallow linear architecture and compares competitively to its neural\ncounterpart.\n","authors":["Evgeny Frolov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2212.05720v1.pdf","comment":"15 pages, 6 figures, submitted to IEEE Access"},{"id":"http://arxiv.org/abs/2212.05712v1","updated":"2022-12-12T05:35:44Z","published":"2022-12-12T05:35:44Z","title":"A Roadmap to Domain Knowledge Integration in Machine Learning","summary":"  Many machine learning algorithms have been developed in recent years to\nenhance the performance of a model in different aspects of artificial\nintelligence. But the problem persists due to inadequate data and resources.\nIntegrating knowledge in a machine learning model can help to overcome these\nobstacles up to a certain degree. Incorporating knowledge is a complex task\nthough because of various forms of knowledge representation. In this paper, we\nwill give a brief overview of these different forms of knowledge integration\nand their performance in certain machine learning tasks.\n","authors":["Himel Das Gupta","Victor S. Sheng"],"pdf_url":"https://arxiv.org/pdf/2212.05712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05699v1","updated":"2022-12-12T04:36:06Z","published":"2022-12-12T04:36:06Z","title":"MMNet: Multi-modal Fusion with Mutual Learning Network for Fake News\n  Detection","summary":"  The rapid development of social media provides a hotbed for the dissemination\nof fake news, which misleads readers and causes negative effects on society.\nNews usually involves texts and images to be more vivid. Consequently,\nmulti-modal fake news detection has received wide attention. Prior efforts\nprimarily conduct multi-modal fusion by simple concatenation or co-attention\nmechanism, leading to sub-optimal performance. In this paper, we propose a\nnovel mutual learning network based model MMNet, which enhances the multi-modal\nfusion for fake news detection via mutual learning between text- and\nvision-centered views towards the same classification objective. Specifically,\nwe design two detection modules respectively based on text- and vision-centered\nmulti-modal fusion features, and enable the mutual learning of the two modules\nto facilitate the multi-modal fusion, considering the latent consistency\nbetween the two modules towards the same training objective. Moreover, we also\nconsider the influence of the image-text matching degree on news authenticity\njudgement by designing an image-text matching aware co-attention mechanism for\nmulti-modal fusion. Extensive experiments are conducted on three benchmark\ndatasets and the results demonstrate that our proposed MMNet achieves superior\nperformance in fake news detection.\n","authors":["Linmei Hu","Ziwang Zhao","Xinkai Ge","Xuemeng Song","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2212.05699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05696v1","updated":"2022-12-12T04:20:11Z","published":"2022-12-12T04:20:11Z","title":"Ensembling Transformers for Cross-domain Automatic Term Extraction","summary":"  Automatic term extraction plays an essential role in domain language\nunderstanding and several natural language processing downstream tasks. In this\npaper, we propose a comparative study on the predictive power of\nTransformers-based pretrained language models toward term extraction in a\nmulti-language cross-domain setting. Besides evaluating the ability of\nmonolingual models to extract single- and multi-word terms, we also experiment\nwith ensembles of mono- and multilingual models by conducting the intersection\nor union on the term output sets of different language models. Our experiments\nhave been conducted on the ACTER corpus covering four specialized domains\n(Corruption, Wind energy, Equitation, and Heart failure) and three languages\n(English, French, and Dutch), and on the RSDO5 Slovenian corpus covering four\nadditional domains (Biomechanics, Chemistry, Veterinary, and Linguistics). The\nresults show that the strategy of employing monolingual models outperforms the\nstate-of-the-art approaches from the related work leveraging multilingual\nmodels, regarding all the languages except Dutch and French if the term\nextraction task excludes the extraction of named entity terms. Furthermore, by\ncombining the outputs of the two best performing models, we achieve significant\nimprovements.\n","authors":["Hanh Thi Hong Tran","Matej Martinc","Andraz Pelicon","Antoine Doucet","Senja Pollak"],"pdf_url":"https://arxiv.org/pdf/2212.05696v1.pdf","comment":"11 pages including references, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2212.08167v1","updated":"2022-12-12T18:53:10Z","published":"2022-12-12T18:53:10Z","title":"Evaluation of Synthetic Datasets for Conversational Recommender Systems","summary":"  For researchers leveraging Large-Language Models (LLMs) in the generation of\ntraining datasets, especially for conversational recommender systems - the\nabsence of robust evaluation frameworks has been a long-standing problem. The\nefficiency brought about by LLMs in the data generation phase is impeded during\nthe process of evaluation of the generated data, since it generally requires\nhuman-raters to ensure that the data generated is of high quality and has\nsufficient diversity. Since the quality of training data is critical for\ndownstream applications, it is important to develop metrics that evaluate the\nquality holistically and identify biases. In this paper, we present a framework\nthat takes a multi-faceted approach towards evaluating datasets produced by\ngenerative models and discuss the advantages and limitations of various\nevaluation methods.\n","authors":["Harsh Lara","Manoj Tiwari"],"pdf_url":"https://arxiv.org/pdf/2212.08167v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2212.05005v2","updated":"2022-12-12T07:32:57Z","published":"2022-12-09T17:45:36Z","title":"Memories are One-to-Many Mapping Alleviators in Talking Face Generation","summary":"  Talking face generation aims at generating photo-realistic video portraits of\na target person driven by input audio. Due to its nature of one-to-many mapping\nfrom the input audio to the output video (e.g., one speech content may have\nmultiple feasible visual appearances), learning a deterministic mapping like\nprevious works brings ambiguity during training, and thus causes inferior\nvisual results. Although this one-to-many mapping could be alleviated in part\nby a two-stage framework (i.e., an audio-to-expression model followed by a\nneural-rendering model), it is still insufficient since the prediction is\nproduced without enough information (e.g., emotions, wrinkles, etc.). In this\npaper, we propose MemFace to complement the missing information with an\nimplicit memory and an explicit memory that follow the sense of the two stages\nrespectively. More specifically, the implicit memory is employed in the\naudio-to-expression model to capture high-level semantics in the\naudio-expression shared space, while the explicit memory is employed in the\nneural-rendering model to help synthesize pixel-level details. Our experimental\nresults show that our proposed MemFace surpasses all the state-of-the-art\nresults across multiple scenarios consistently and significantly.\n","authors":["Anni Tang","Tianyu He","Xu Tan","Jun Ling","Runnan Li","Sheng Zhao","Li Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2212.05005v2.pdf","comment":"Project page: see https://memoryface.github.io"},{"id":"http://arxiv.org/abs/2212.05699v1","updated":"2022-12-12T04:36:06Z","published":"2022-12-12T04:36:06Z","title":"MMNet: Multi-modal Fusion with Mutual Learning Network for Fake News\n  Detection","summary":"  The rapid development of social media provides a hotbed for the dissemination\nof fake news, which misleads readers and causes negative effects on society.\nNews usually involves texts and images to be more vivid. Consequently,\nmulti-modal fake news detection has received wide attention. Prior efforts\nprimarily conduct multi-modal fusion by simple concatenation or co-attention\nmechanism, leading to sub-optimal performance. In this paper, we propose a\nnovel mutual learning network based model MMNet, which enhances the multi-modal\nfusion for fake news detection via mutual learning between text- and\nvision-centered views towards the same classification objective. Specifically,\nwe design two detection modules respectively based on text- and vision-centered\nmulti-modal fusion features, and enable the mutual learning of the two modules\nto facilitate the multi-modal fusion, considering the latent consistency\nbetween the two modules towards the same training objective. Moreover, we also\nconsider the influence of the image-text matching degree on news authenticity\njudgement by designing an image-text matching aware co-attention mechanism for\nmulti-modal fusion. Extensive experiments are conducted on three benchmark\ndatasets and the results demonstrate that our proposed MMNet achieves superior\nperformance in fake news detection.\n","authors":["Linmei Hu","Ziwang Zhao","Xinkai Ge","Xuemeng Song","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2212.05699v1.pdf","comment":null}]},"2022-12-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.05613v1","updated":"2022-12-11T21:56:44Z","published":"2022-12-11T21:56:44Z","title":"A Study of Slang Representation Methods","summary":"  Warning: this paper contains content that may be offensive or upsetting.\nConsidering the large amount of content created online by the minute,\nslang-aware automatic tools are critically needed to promote social good, and\nassist policymakers and moderators in restricting the spread of offensive\nlanguage, abuse, and hate speech. Despite the success of large language models\nand the spontaneous emergence of slang dictionaries, it is unclear how far\ntheir combination goes in terms of slang understanding for downstream social\ngood tasks. In this paper, we provide a framework to study different\ncombinations of representation learning models and knowledge resources for a\nvariety of downstream tasks that rely on slang understanding. Our experiments\nshow the superiority of models that have been pre-trained on social media data,\nwhile the impact of dictionaries is positive only for static word embeddings.\nOur error analysis identifies core challenges for slang representation\nlearning, including out-of-vocabulary words, polysemy, variance, and annotation\ndisagreements, which can be traced to characteristics of slang as a quickly\nevolving and highly subjective language.\n","authors":["Aravinda Kolla","Filip Ilievski","Hông-Ân Sandlin","Alain Mermoud"],"pdf_url":"https://arxiv.org/pdf/2212.05613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05612v1","updated":"2022-12-11T21:52:21Z","published":"2022-12-11T21:52:21Z","title":"Multimodal and Explainable Internet Meme Classification","summary":"  Warning: this paper contains content that may be offensive or upsetting. In\nthe current context where online platforms have been effectively weaponized in\na variety of geo-political events and social issues, Internet memes make fair\ncontent moderation at scale even more difficult. Existing work on meme\nclassification and tracking has focused on black-box methods that do not\nexplicitly consider the semantics of the memes or the context of their\ncreation. In this paper, we pursue a modular and explainable architecture for\nInternet meme understanding. We design and implement multimodal classification\nmethods that perform example- and prototype-based reasoning over training\ncases, while leveraging both textual and visual SOTA models to represent the\nindividual cases. We study the relevance of our modular and explainable models\nin detecting harmful memes on two existing tasks: Hate Speech Detection and\nMisogyny Classification. We compare the performance between example- and\nprototype-based methods, and between text, vision, and multimodal models,\nacross different categories of harmfulness (e.g., stereotype and\nobjectification). We devise a user-friendly interface that facilitates the\ncomparative analysis of examples retrieved by all of our models for any given\nmeme, informing the community about the strengths and limitations of these\nexplainable methods.\n","authors":["Abhinav Kumar Thakur","Filip Ilievski","Hông-Ân Sandlin","Alain Mermoud","Zhivar Sourati","Luca Luceri","Riccardo Tommasini"],"pdf_url":"https://arxiv.org/pdf/2212.05612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05506v1","updated":"2022-12-11T13:43:22Z","published":"2022-12-11T13:43:22Z","title":"FastClass: A Time-Efficient Approach to Weakly-Supervised Text\n  Classification","summary":"  Weakly-supervised text classification aims to train a classifier using only\nclass descriptions and unlabeled data. Recent research shows that\nkeyword-driven methods can achieve state-of-the-art performance on various\ntasks. However, these methods not only rely on carefully-crafted class\ndescriptions to obtain class-specific keywords but also require substantial\namount of unlabeled data and takes a long time to train. This paper proposes\nFastClass, an efficient weakly-supervised classification approach. It uses\ndense text representation to retrieve class-relevant documents from external\nunlabeled corpus and selects an optimal subset to train a classifier. Compared\nto keyword-driven methods, our approach is less reliant on initial class\ndescriptions as it no longer needs to expand each class description into a set\nof class-specific keywords. Experiments on a wide range of classification tasks\nshow that the proposed approach frequently outperforms keyword-driven models in\nterms of classification accuracy and often enjoys orders-of-magnitude faster\ntraining speed.\n","authors":["Tingyu Xia","Yue Wang","Yuan Tian","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2212.05506v1.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.05479v1","updated":"2022-12-11T11:35:46Z","published":"2022-12-11T11:35:46Z","title":"End-to-End Speech Translation of Arabic to English Broadcast News","summary":"  Speech translation (ST) is the task of directly translating acoustic speech\nsignals in a source language into text in a foreign language. ST task has been\naddressed, for a long time, using a pipeline approach with two modules : first\nan Automatic Speech Recognition (ASR) in the source language followed by a\ntext-to-text Machine translation (MT). In the past few years, we have seen a\nparadigm shift towards the end-to-end approaches using sequence-to-sequence\ndeep neural network models. This paper presents our efforts towards the\ndevelopment of the first Broadcast News end-to-end Arabic to English speech\ntranslation system. Starting from independent ASR and MT LDC releases, we were\nable to identify about 92 hours of Arabic audio recordings for which the manual\ntranscription was also translated into English at the segment level. These data\nwas used to train and compare pipeline and end-to-end speech translation\nsystems under multiple scenarios including transfer learning and data\naugmentation techniques.\n","authors":["Fethi Bougares","Salim Jouili"],"pdf_url":"https://arxiv.org/pdf/2212.05479v1.pdf","comment":"Arabic Natural Language Processing Workshop 2022"},{"id":"http://arxiv.org/abs/2212.03551v2","updated":"2022-12-11T07:57:32Z","published":"2022-12-07T10:01:44Z","title":"Talking About Large Language Models","summary":"  Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere.\n","authors":["Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2212.03551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04267v3","updated":"2022-12-11T07:55:43Z","published":"2022-10-09T13:53:06Z","title":"Spread Love Not Hate: Undermining the Importance of Hateful Pre-training\n  for Hate Speech Detection","summary":"  Pre-training large neural language models, such as BERT, has led to\nimpressive gains on many natural language processing (NLP) tasks. Although this\nmethod has proven to be effective for many domains, it might not always provide\ndesirable benefits. In this paper, we study the effects of hateful pre-training\non low-resource hate speech classification tasks. While previous studies on the\nEnglish language have emphasized its importance, we aim to augment their\nobservations with some non-obvious insights. We evaluate different variations\nof tweet-based BERT models pre-trained on hateful, non-hateful, and mixed\nsubsets of a 40M tweet dataset. This evaluation is carried out for the Indian\nlanguages Hindi and Marathi. This paper is empirical evidence that hateful\npre-training is not the best pre-training option for hate speech detection. We\nshow that pre-training on non-hateful text from the target domain provides\nsimilar or better results. Further, we introduce HindTweetBERT and\nMahaTweetBERT, the first publicly available BERT models pre-trained on Hindi\nand Marathi tweets, respectively. We show that they provide state-of-the-art\nperformance on hate speech classification tasks. We also release hateful BERT\nfor the two languages and a gold hate speech evaluation benchmark HateEval-Hi\nand HateEval-Mr consisting of manually labeled 2000 tweets each. The models and\ndata are available at https://github.com/l3cube-pune/MarathiNLP .\n","authors":["Omkar Gokhale","Aditya Kane","Shantanu Patankar","Tanmay Chavan","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2210.04267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03279v2","updated":"2022-12-11T07:16:44Z","published":"2022-11-14T17:27:07Z","title":"From Knowledge Augmentation to Multi-tasking: Towards Human-like\n  Dialogue Systems","summary":"  The goal of building dialogue agents that can converse with humans naturally\nhas been a long-standing dream of researchers since the early days of\nartificial intelligence. The well-known Turing Test proposed to judge the\nultimate validity of an artificial intelligence agent on the\nindistinguishability of its dialogues from humans'. It should come as no\nsurprise that human-level dialogue systems are very challenging to build. But,\nwhile early effort on rule-based systems found limited success, the emergence\nof deep learning enabled great advance on this topic.\n  In this thesis, we focus on methods that address the numerous issues that\nhave been imposing the gap between artificial conversational agents and\nhuman-level interlocutors. These methods were proposed and experimented with in\nways that were inspired by general state-of-the-art AI methodologies. But they\nalso targeted the characteristics that dialogue systems possess.\n","authors":["Tom Young"],"pdf_url":"https://arxiv.org/pdf/2212.03279v2.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2212.05429v1","updated":"2022-12-11T06:49:29Z","published":"2022-12-11T06:49:29Z","title":"MORTY: Structured Summarization for Targeted Information Extraction from\n  Scholarly Articles","summary":"  Information extraction from scholarly articles is a challenging task due to\nthe sizable document length and implicit information hidden in text, figures,\nand citations. Scholarly information extraction has various applications in\nexploration, archival, and curation services for digital libraries and\nknowledge management systems. We present MORTY, an information extraction\ntechnique that creates structured summaries of text from scholarly articles.\nOur approach condenses the article's full-text to property-value pairs as a\nsegmented text snippet called structured summary. We also present a sizable\nscholarly dataset combining structured summaries retrieved from a scholarly\nknowledge graph and corresponding publicly available scientific articles, which\nwe openly publish as a resource for the research community. Our results show\nthat structured summarization is a suitable approach for targeted information\nextraction that complements other commonly used methods such as question\nanswering and named entity recognition.\n","authors":["Mohamad Yaser Jaradeh","Markus Stocker","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2212.05429v1.pdf","comment":"Published as a short paper in ICADL 2022"},{"id":"http://arxiv.org/abs/2212.05421v1","updated":"2022-12-11T06:16:14Z","published":"2022-12-11T06:16:14Z","title":"Feature-Level Debiased Natural Language Understanding","summary":"  Existing natural language understanding (NLU) models often rely on dataset\nbiases rather than intended task-relevant features to achieve high performance\non specific datasets. As a result, these models perform poorly on datasets\noutside the training distribution. Some recent studies address the above issue\nby reducing the weights of biased samples during the training process. However,\nthese methods still encode biased latent features in representations and\nneglect the dynamic nature of bias, which hinders model prediction. We propose\nan NLU debiasing method, named debiasing contrastive learning (DCT), to\nsimultaneously alleviate the above problems based on contrastive learning. We\ndevise a debiasing positive sampling strategy to mitigate biased latent\nfeatures by selecting the least similar biased positive samples. We also\npropose a dynamic negative sampling strategy to capture the dynamic influence\nof biases by employing a bias-only model to dynamically select the most similar\nbiased negative samples. We conduct experiments on three NLU benchmark\ndatasets. Experimental results show that DCT outperforms state-of-the-art\nbaselines on out-of-distribution datasets while maintaining in-distribution\nperformance. We also verify that DCT can reduce biased latent features from the\nmodel's representations.\n","authors":["Yougang Lyu","Piji Li","Yechang Yang","Maarten de Rijke","Pengjie Ren","Yukun Zhao","Dawei Yin","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2212.05421v1.pdf","comment":"Accepted by AAAI2023"},{"id":"http://arxiv.org/abs/2211.03348v2","updated":"2022-12-11T04:58:00Z","published":"2022-11-07T07:50:30Z","title":"Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for\n  Unsupervised Sentence Embedding","summary":"  Contrastive learning has become a new paradigm for unsupervised sentence\nembeddings. Previous studies focus on instance-wise contrastive learning,\nattempting to construct positive pairs with textual data augmentation. In this\npaper, we propose a novel Contrastive learning method with Prompt-derived\nVirtual semantic Prototypes (ConPVP). Specifically, with the help of prompts,\nwe construct virtual semantic prototypes to each instance, and derive negative\nprototypes by using the negative form of the prompts. Using a prototypical\ncontrastive loss, we enforce the anchor sentence embedding to be close to its\ncorresponding semantic prototypes, and far apart from the negative prototypes\nas well as the prototypes of other sentences. Extensive experimental results on\nsemantic textual similarity, transfer, and clustering tasks demonstrate the\neffectiveness of our proposed model compared to strong baselines. Code is\navailable at https://github.com/lemon0830/promptCSE.\n","authors":["Jiali Zeng","Yongjing Yin","Yufan Jiang","Shuangzhi Wu","Yunbo Cao"],"pdf_url":"https://arxiv.org/pdf/2211.03348v2.pdf","comment":"Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2211.05100v2","updated":"2022-12-11T01:09:36Z","published":"2022-11-09T18:48:09Z","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","summary":"  Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.\n","authors":["BigScience Workshop"," :","Teven Le Scao","Angela Fan","Christopher Akiki","Ellie Pavlick","Suzana Ilić","Daniel Hesslow","Roman Castagné","Alexandra Sasha Luccioni","François Yvon","Matthias Gallé","Jonathan Tow","Alexander M. Rush","Stella Biderman","Albert Webson","Pawan Sasanka Ammanamanchi","Thomas Wang","Benoît Sagot","Niklas Muennighoff","Albert Villanova del Moral","Olatunji Ruwase","Rachel Bawden","Stas Bekman","Angelina McMillan-Major","Iz Beltagy","Huu Nguyen","Lucile Saulnier","Samson Tan","Pedro Ortiz Suarez","Victor Sanh","Hugo Laurençon","Yacine Jernite","Julien Launay","Margaret Mitchell","Colin Raffel","Aaron Gokaslan","Adi Simhi","Aitor Soroa","Alham Fikri Aji","Amit Alfassy","Anna Rogers","Ariel Kreisberg Nitzav","Canwen Xu","Chenghao Mou","Chris Emezue","Christopher Klamm","Colin Leong","Daniel van Strien","David Ifeoluwa Adelani","Dragomir Radev","Eduardo González Ponferrada","Efrat Levkovizh","Ethan Kim","Eyal Bar Natan","Francesco De Toni","Gérard Dupont","Germán Kruszewski","Giada Pistilli","Hady Elsahar","Hamza Benyamina","Hieu Tran","Ian Yu","Idris Abdulmumin","Isaac Johnson","Itziar Gonzalez-Dios","Javier de la Rosa","Jenny Chim","Jesse Dodge","Jian Zhu","Jonathan Chang","Jörg Frohberg","Joseph Tobing","Joydeep Bhattacharjee","Khalid Almubarak","Kimbo Chen","Kyle Lo","Leandro Von Werra","Leon Weber","Long Phan","Loubna Ben allal","Ludovic Tanguy","Manan Dey","Manuel Romero Muñoz","Maraim Masoud","María Grandury","Mario Šaško","Max Huang","Maximin Coavoux","Mayank Singh","Mike Tian-Jian Jiang","Minh Chien Vu","Mohammad A. Jauhar","Mustafa Ghaleb","Nishant Subramani","Nora Kassner","Nurulaqilla Khamis","Olivier Nguyen","Omar Espejel","Ona de Gibert","Paulo Villegas","Peter Henderson","Pierre Colombo","Priscilla Amuok","Quentin Lhoest","Rheza Harliman","Rishi Bommasani","Roberto Luis López","Rui Ribeiro","Salomey Osei","Sampo Pyysalo","Sebastian Nagel","Shamik Bose","Shamsuddeen Hassan Muhammad","Shanya Sharma","Shayne Longpre","Somaieh Nikpoor","Stanislav Silberberg","Suhas Pai","Sydney Zink","Tiago Timponi Torrent","Timo Schick","Tristan Thrush","Valentin Danchev","Vassilina Nikoulina","Veronika Laippala","Violette Lepercq","Vrinda Prabhu","Zaid Alyafeai","Zeerak Talat","Arun Raja","Benjamin Heinzerling","Chenglei Si","Davut Emre Taşar","Elizabeth Salesky","Sabrina J. Mielke","Wilson Y. Lee","Abheesht Sharma","Andrea Santilli","Antoine Chaffin","Arnaud Stiegler","Debajyoti Datta","Eliza Szczechla","Gunjan Chhablani","Han Wang","Harshit Pandey","Hendrik Strobelt","Jason Alan Fries","Jos Rozen","Leo Gao","Lintang Sutawika","M Saiful Bari","Maged S. Al-shaibani","Matteo Manica","Nihal Nayak","Ryan Teehan","Samuel Albanie","Sheng Shen","Srulik Ben-David","Stephen H. Bach","Taewoon Kim","Tali Bers","Thibault Fevry","Trishala Neeraj","Urmish Thakker","Vikas Raunak","Xiangru Tang","Zheng-Xin Yong","Zhiqing Sun","Shaked Brody","Yallow Uri","Hadar Tojarieh","Adam Roberts","Hyung Won Chung","Jaesung Tae","Jason Phang","Ofir Press","Conglong Li","Deepak Narayanan","Hatim Bourfoune","Jared Casper","Jeff Rasley","Max Ryabinin","Mayank Mishra","Minjia Zhang","Mohammad Shoeybi","Myriam Peyrounette","Nicolas Patry","Nouamane Tazi","Omar Sanseviero","Patrick von Platen","Pierre Cornette","Pierre François Lavallée","Rémi Lacroix","Samyam Rajbhandari","Sanchit Gandhi","Shaden Smith","Stéphane Requena","Suraj Patil","Tim Dettmers","Ahmed Baruwa","Amanpreet Singh","Anastasia Cheveleva","Anne-Laure Ligozat","Arjun Subramonian","Aurélie Névéol","Charles Lovering","Dan Garrette","Deepak Tunuguntla","Ehud Reiter","Ekaterina Taktasheva","Ekaterina Voloshina","Eli Bogdanov","Genta Indra Winata","Hailey Schoelkopf","Jan-Christoph Kalo","Jekaterina Novikova","Jessica Zosa Forde","Jordan Clive","Jungo Kasai","Ken Kawamura","Liam Hazan","Marine Carpuat","Miruna Clinciu","Najoung Kim","Newton Cheng","Oleg Serikov","Omer Antverg","Oskar van der Wal","Rui Zhang","Ruochen Zhang","Sebastian Gehrmann","Shachar Mirkin","Shani Pais","Tatiana Shavrina","Thomas Scialom","Tian Yun","Tomasz Limisiewicz","Verena Rieser","Vitaly Protasov","Vladislav Mikhailov","Yada Pruksachatkun","Yonatan Belinkov","Zachary Bamberger","Zdeněk Kasner","Alice Rueda","Amanda Pestana","Amir Feizpour","Ammar Khan","Amy Faranak","Ana Santos","Anthony Hevia","Antigona Unldreaj","Arash Aghagol","Arezoo Abdollahi","Aycha Tammour","Azadeh HajiHosseini","Bahareh Behroozi","Benjamin Ajibade","Bharat Saxena","Carlos Muñoz Ferrandis","Danish Contractor","David Lansky","Davis David","Douwe Kiela","Duong A. Nguyen","Edward Tan","Emi Baylor","Ezinwanne Ozoani","Fatima Mirza","Frankline Ononiwu","Habib Rezanejad","Hessie Jones","Indrani Bhattacharya","Irene Solaiman","Irina Sedenko","Isar Nejadgholi","Jesse Passmore","Josh Seltzer","Julio Bonis Sanz","Livia Dutra","Mairon Samagaio","Maraim Elbadri","Margot Mieskes","Marissa Gerchick","Martha Akinlolu","Michael McKenna","Mike Qiu","Muhammed Ghauri","Mykola Burynok","Nafis Abrar","Nazneen Rajani","Nour Elkott","Nour Fahmy","Olanrewaju Samuel","Ran An","Rasmus Kromann","Ryan Hao","Samira Alizadeh","Sarmad Shubber","Silas Wang","Sourav Roy","Sylvain Viguier","Thanh Le","Tobi Oyebade","Trieu Le","Yoyo Yang","Zach Nguyen","Abhinav Ramesh Kashyap","Alfredo Palasciano","Alison Callahan","Anima Shukla","Antonio Miranda-Escalada","Ayush Singh","Benjamin Beilharz","Bo Wang","Caio Brito","Chenxi Zhou","Chirag Jain","Chuxin Xu","Clémentine Fourrier","Daniel León Periñán","Daniel Molano","Dian Yu","Enrique Manjavacas","Fabio Barth","Florian Fuhrimann","Gabriel Altay","Giyaseddin Bayrak","Gully Burns","Helena U. Vrabec","Imane Bello","Ishani Dash","Jihyun Kang","John Giorgi","Jonas Golde","Jose David Posada","Karthik Rangasai Sivaraman","Lokesh Bulchandani","Lu Liu","Luisa Shinzato","Madeleine Hahn de Bykhovetz","Maiko Takeuchi","Marc Pàmies","Maria A Castillo","Marianna Nezhurina","Mario Sänger","Matthias Samwald","Michael Cullan","Michael Weinberg","Michiel De Wolf","Mina Mihaljcic","Minna Liu","Moritz Freidank","Myungsun Kang","Natasha Seelam","Nathan Dahlberg","Nicholas Michio Broad","Nikolaus Muellner","Pascale Fung","Patrick Haller","Ramya Chandrasekhar","Renata Eisenberg","Robert Martin","Rodrigo Canalli","Rosaline Su","Ruisi Su","Samuel Cahyawijaya","Samuele Garda","Shlok S Deshmukh","Shubhanshu Mishra","Sid Kiblawi","Simon Ott","Sinee Sang-aroonsiri","Srishti Kumar","Stefan Schweter","Sushil Bharati","Tanmay Laud","Théo Gigant","Tomoya Kainuma","Wojciech Kusa","Yanis Labrak","Yash Shailesh Bajaj","Yash Venkatraman","Yifan Xu","Yingxin Xu","Yu Xu","Zhe Tan","Zhongli Xie","Zifan Ye","Mathilde Bras","Younes Belkada","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2211.05100v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2211.10738v3","updated":"2022-12-11T15:55:55Z","published":"2022-11-19T16:30:29Z","title":"Relational Symmetry based Knowledge Graph Contrastive Learning","summary":"  Knowledge graph embedding (KGE) aims to learn powerful representations to\nbenefit various artificial intelligence applications, such as question\nanswering and recommendations. Meanwhile, contrastive learning (CL), as an\neffective mechanism to enhance the discriminative capacity of the learned\nrepresentations, has been leveraged in different fields, especially graph-based\nmodels. However, since the structures of knowledge graphs (KGs) are usually\nmore complicated compared to homogeneous graphs, it is hard to construct\nappropriate contrastive sample pairs. In this paper, we find that the entities\nwithin a symmetrical structure are usually more similar and correlated. This\nkey property can be utilized to construct contrastive positive pairs for\ncontrastive learning. Following the ideas above, we propose a relational\nsymmetrical structure based knowledge graph contrastive learning framework,\ntermed KGE-SymCL, which leverages the symmetrical structure information in KGs\nto enhance the discriminative ability of KGE models. Concretely, a\nplug-and-play approach is designed by taking the entities in the relational\nsymmetrical positions as the positive samples. Besides, a self-supervised\nalignment loss is used to pull together the constructed positive sample pairs\nfor contrastive learning. Extensive experimental results on benchmark datasets\nhave verified the good generalization and superiority of the proposed\nframework.\n","authors":["Ke Liang","Yue Liu","Sihang Zhou","Xinwang Liu","Wenxuan Tu"],"pdf_url":"https://arxiv.org/pdf/2211.10738v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2107.07268v2","updated":"2022-12-11T15:07:42Z","published":"2021-07-15T11:47:43Z","title":"Cross-modal Variational Auto-encoder for Content-based Micro-video\n  Background Music Recommendation","summary":"  In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.\n","authors":["Jing Yi","Yaochen Zhu","Jiayi Xie","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2107.07268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05429v1","updated":"2022-12-11T06:49:29Z","published":"2022-12-11T06:49:29Z","title":"MORTY: Structured Summarization for Targeted Information Extraction from\n  Scholarly Articles","summary":"  Information extraction from scholarly articles is a challenging task due to\nthe sizable document length and implicit information hidden in text, figures,\nand citations. Scholarly information extraction has various applications in\nexploration, archival, and curation services for digital libraries and\nknowledge management systems. We present MORTY, an information extraction\ntechnique that creates structured summaries of text from scholarly articles.\nOur approach condenses the article's full-text to property-value pairs as a\nsegmented text snippet called structured summary. We also present a sizable\nscholarly dataset combining structured summaries retrieved from a scholarly\nknowledge graph and corresponding publicly available scientific articles, which\nwe openly publish as a resource for the research community. Our results show\nthat structured summarization is a suitable approach for targeted information\nextraction that complements other commonly used methods such as question\nanswering and named entity recognition.\n","authors":["Mohamad Yaser Jaradeh","Markus Stocker","Sören Auer"],"pdf_url":"https://arxiv.org/pdf/2212.05429v1.pdf","comment":"Published as a short paper in ICADL 2022"},{"id":"http://arxiv.org/abs/2211.01768v2","updated":"2022-12-11T06:47:50Z","published":"2022-11-03T12:48:25Z","title":"Embedding Knowledge Graph of Patent Metadata to Measure Knowledge\n  Proximity","summary":"  Knowledge proximity refers to the strength of association between any two\nentities in a structural form that embodies certain aspects of a knowledge\nbase. In this work, we operationalize knowledge proximity within the context of\nthe US Patent Database (knowledge base) using a knowledge graph (structural\nform) named PatNet built using patent metadata, including citations, inventors,\nassignees, and domain classifications. We train various graph embedding models\nusing PatNet to obtain the embeddings of entities and relations. The cosine\nsimilarity between the corresponding (or transformed) embeddings of entities\ndenotes the knowledge proximity between these. We compare the embedding models\nin terms of their performances in predicting target entities and explaining\ndomain expansion profiles of inventors and assignees. We then apply the\nembeddings of the best-preferred model to associate homogeneous (e.g.,\npatent-patent) and heterogeneous (e.g., inventor-assignee) pairs of entities.\n","authors":["Guangtong Li","L Siddharth","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2211.01768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05399v1","updated":"2022-12-11T03:39:24Z","published":"2022-12-11T03:39:24Z","title":"Untargeted Attack against Federated Recommendation Systems via Poisonous\n  Item Embeddings and the Defense","summary":"  Federated recommendation (FedRec) can train personalized recommenders without\ncollecting user data, but the decentralized nature makes it susceptible to\npoisoning attacks. Most previous studies focus on the targeted attack to\npromote certain items, while the untargeted attack that aims to degrade the\noverall performance of the FedRec system remains less explored. In fact,\nuntargeted attacks can disrupt the user experience and bring severe financial\nloss to the service provider. However, existing untargeted attack methods are\neither inapplicable or ineffective against FedRec systems. In this paper, we\ndelve into the untargeted attack and its defense for FedRec systems. (i) We\npropose ClusterAttack, a novel untargeted attack method. It uploads poisonous\ngradients that converge the item embeddings into several dense clusters, which\nmake the recommender generate similar scores for these items in the same\ncluster and perturb the ranking order. (ii) We propose a uniformity-based\ndefense mechanism (UNION) to protect FedRec systems from such attacks. We\ndesign a contrastive learning task that regularizes the item embeddings toward\na uniform distribution. Then the server filters out these malicious gradients\nby estimating the uniformity of updated item embeddings. Experiments on two\npublic datasets show that ClusterAttack can effectively degrade the performance\nof FedRec systems while circumventing many defense methods, and UNION can\nimprove the resistance of the system against various untargeted attacks,\nincluding our ClusterAttack.\n","authors":["Yang Yu","Qi Liu","Likang Wu","Runlong Yu","Sanshi Lei Yu","Zaixi Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.05399v1.pdf","comment":"Accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2112.00944v2","updated":"2022-12-11T03:29:37Z","published":"2021-12-02T02:48:25Z","title":"Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation","summary":"  News recommendation is a widely adopted technique to provide personalized\nnews feeds for the user. Recently, pre-trained language models (PLMs) have\ndemonstrated the great capability of natural language understanding and\nbenefited news recommendation via improving news modeling. However, most\nexisting works simply finetune the PLM with the news recommendation task, which\nmay suffer from the known domain shift problem between the pre-training corpus\nand downstream news texts. Moreover, PLMs usually contain a large volume of\nparameters and have high computational overhead, which imposes a great burden\non low-latency online services. In this paper, we propose Tiny-NewsRec, which\ncan improve both the effectiveness and the efficiency of PLM-based news\nrecommendation. We first design a self-supervised domain-specific post-training\nmethod to better adapt the general PLM to the news domain with a contrastive\nmatching task between news titles and news bodies. We further propose a\ntwo-stage knowledge distillation method to improve the efficiency of the large\nPLM-based news recommendation model while maintaining its performance. Multiple\nteacher models originated from different time steps of our post-training\nprocedure are used to transfer comprehensive knowledge to the student in both\nits post-training and finetuning stage. Extensive experiments on two real-world\ndatasets validate the effectiveness and efficiency of our method.\n","authors":["Yang Yu","Fangzhao Wu","Chuhan Wu","Jingwei Yi","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2112.00944v2.pdf","comment":"Accepted by EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.08041v1","updated":"2022-12-11T18:32:00Z","published":"2022-12-11T18:32:00Z","title":"Can REF output quality scores be assigned by AI? Experimental evidence","summary":"  This document describes strategies for using Artificial Intelligence (AI) to\npredict some journal article scores in future research assessment exercises.\nFive strategies have been assessed.\n","authors":["Mike Thelwall","Kayvan Kousha","Mahshid Abdoli","Emma Stuart","Meiko Makita","Paul Wilson","Jonathan Levitt"],"pdf_url":"https://arxiv.org/pdf/2212.08041v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2107.07268v2","updated":"2022-12-11T15:07:42Z","published":"2021-07-15T11:47:43Z","title":"Cross-modal Variational Auto-encoder for Content-based Micro-video\n  Background Music Recommendation","summary":"  In this paper, we propose a cross-modal variational auto-encoder (CMVAE) for\ncontent-based micro-video background music recommendation. CMVAE is a\nhierarchical Bayesian generative model that matches relevant background music\nto a micro-video by projecting these two multimodal inputs into a shared\nlow-dimensional latent space, where the alignment of two corresponding\nembeddings of a matched video-music pair is achieved by cross-generation.\nMoreover, the multimodal information is fused by the product-of-experts (PoE)\nprinciple, where the semantic information in visual and textual modalities of\nthe micro-video are weighted according to their variance estimations such that\nthe modality with a lower noise level is given more weights. Therefore, the\nmicro-video latent variables contain less irrelevant information that results\nin a more robust model generalization. Furthermore, we establish a large-scale\ncontent-based micro-video background music recommendation dataset, TT-150k,\ncomposed of approximately 3,000 different background music clips associated to\n150,000 micro-videos from different users. Extensive experiments on the\nestablished TT-150k dataset demonstrate the effectiveness of the proposed\nmethod. A qualitative assessment of CMVAE by visualizing some recommendation\nresults is also included.\n","authors":["Jing Yi","Yaochen Zhu","Jiayi Xie","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2107.07268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05499v1","updated":"2022-12-11T13:15:06Z","published":"2022-12-11T13:15:06Z","title":"Applicability limitations of differentiable full-reference image-quality","summary":"  Subjective image-quality measurement plays a critical role in the development\nof image-processing applications. The purpose of a visual-quality metric is to\napproximate the results of subjective assessment. In this regard, more and more\nmetrics are under development, but little research has considered their\nlimitations. This paper addresses that deficiency: we show how image\npreprocessing before compression can artificially increase the quality scores\nprovided by the popular metrics DISTS, LPIPS, HaarPSI, and VIF as well as how\nthese scores are inconsistent with subjective-quality scores. We propose a\nseries of neural-network preprocessing models that increase DISTS by up to\n34.5%, LPIPS by up to 36.8%, VIF by up to 98.0%, and HaarPSI by up to 22.6% in\nthe case of JPEG-compressed images. A subjective comparison of preprocessed\nimages showed that for most of the metrics we examined, visual quality drops or\nstays unchanged, limiting the applicability of these metrics.\n","authors":["Siniukov Maksim","Dmitriy Kulikov","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2212.05499v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.05466v1","updated":"2022-12-11T10:45:53Z","published":"2022-12-11T10:45:53Z","title":"Comprehensive Complexity Assessment of Emerging Learned Image\n  Compression on CPU and GPU","summary":"  Learned Compression (LC) is the emerging technology for compressing image and\nvideo content, using deep neural networks. Despite being new, LC methods have\nalready gained a compression efficiency comparable to state-of-the-art image\ncompression, such as HEVC or even VVC. However, the existing solutions often\nrequire a huge computational complexity, which discourages their adoption in\ninternational standards or products. This paper provides a comprehensive\ncomplexity assessment of several notable methods, that shed light on the\nmatter, and guide the future development of this field by presenting key\nfindings. To do so, six existing methods have been evaluated for both encoding\nand decoding, on CPU and GPU platforms. Various aspects of complexity such as\nthe overall complexity, share of each coding module, number of operations,\nnumber of parameters, most demanding GPU kernels, and memory requirements have\nbeen measured and compared on Kodak dataset. The reported results (1) quantify\nthe complexity of LC methods, (2) fairly compare different methods, and (3) a\nmajor contribution of the work is identifying and quantifying the key factors\naffecting the complexity.\n","authors":["Farhad Pakdaman","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2212.05466v1.pdf","comment":null}]},"2022-12-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.08061v1","updated":"2022-12-15T18:59:32Z","published":"2022-12-15T18:59:32Z","title":"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in\n  Zero-Shot Reasoning","summary":"  Generating a chain of thought (CoT) can increase large language model (LLM)\nperformance on a wide range of tasks. Zero-shot CoT evaluations, however, have\nbeen conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In\nthis paper, we perform a controlled evaluation of zero-shot CoT across two\nsensitive domains: harmful questions and stereotype benchmarks. We find that\nusing zero-shot CoT reasoning in a prompt can significantly increase a model's\nlikelihood to produce undesirable output. Without future advances in alignment\nor explicit mitigation instructions, zero-shot CoT should be avoided on tasks\nwhere models can make inferences about marginalized groups or harmful topics.\n","authors":["Omar Shaikh","Hongxin Zhang","William Held","Michael Bernstein","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08061v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2212.08055v1","updated":"2022-12-15T18:58:28Z","published":"2022-12-15T18:58:28Z","title":"UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units","summary":"  Direct speech-to-speech translation (S2ST), in which all components can be\noptimized jointly, is advantageous over cascaded approaches to achieve fast\ninference with a simplified pipeline. We present a novel two-pass direct S2ST\narchitecture, {\\textit UnitY}, which first generates textual representations\nand predicts discrete acoustic units subsequently. We enhance the model\nperformance by subword prediction in the first-pass decoder, advanced two-pass\ndecoder architecture design and search strategy, and better training\nregularization. To leverage large amounts of unlabeled text data, we pre-train\nthe first-pass text decoder based on the self-supervised denoising\nauto-encoding task. Experimental evaluations on benchmark datasets at various\ndata scales demonstrate that UnitY outperforms a single-pass speech-to-unit\ntranslation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show\nthat the proposed methods boost the performance even when predicting\nspectrogram in the second pass. However, predicting discrete units achieves\n2.51x decoding speed-up compared to that case.\n","authors":["Hirofumi Inaguma","Sravya Popuri","Ilia Kulikov","Peng-Jen Chen","Changhan Wang","Yu-An Chung","Yun Tang","Ann Lee","Shinji Watanabe","Juan Pino"],"pdf_url":"https://arxiv.org/pdf/2212.08055v1.pdf","comment":"Early draft. Work in progress"},{"id":"http://arxiv.org/abs/2212.08054v1","updated":"2022-12-15T18:58:07Z","published":"2022-12-15T18:58:07Z","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue","summary":"  Modern virtual assistants use internal semantic parsing engines to convert\nuser utterances to actionable commands. However, prior work has demonstrated\nthat semantic parsing is a difficult multilingual transfer task with low\ntransfer efficiency compared to other tasks. In global markets such as India\nand Latin America, this is a critical issue as switching between languages is\nprevalent for bilingual users. In this work we dramatically improve the\nzero-shot performance of a multilingual and codeswitched semantic parsing\nsystem using two stages of multilingual alignment. First, we show that\nconstrastive alignment pretraining improves both English performance and\ntransfer efficiency. We then introduce a constrained optimization approach for\nhyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned\nMultilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and\n81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing\nbenchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer\nparameters.\n","authors":["William Held","Christopher Hidey","Fei Liu","Eric Zhu","Rahul Goel","Diyi Yang","Rushin Shah"],"pdf_url":"https://arxiv.org/pdf/2212.08054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08037v1","updated":"2022-12-15T18:45:29Z","published":"2022-12-15T18:45:29Z","title":"Attributed Question Answering: Evaluation and Modeling for Attributed\n  Large Language Models","summary":"  Large language models (LLMs) have shown impressive results across a variety\nof tasks while requiring little or no direct supervision. Further, there is\nmounting evidence that LLMs may have potential in information-seeking\nscenarios. We believe the ability of an LLM to attribute the text that it\ngenerates is likely to be crucial for both system developers and users in this\nsetting. We propose and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We develop a reproducable evaluation framework\nfor the task, using human annotations as a gold standard and a correlated\nautomatic metric that we show is suitable for development settings. We describe\nand benchmark a broad set of architectures for the task. Our contributions give\nsome concrete answers to two key questions (How to measure attribution?, and\nHow well do current state-of-the-art methods perform on attribution?), and give\nsome hints as to how to address a third key question (How to build LLMs with\nattribution?).\n","authors":["Bernd Bohnet","Vinh Q. Tran","Pat Verga","Roee Aharoni","Daniel Andor","Livio Baldini Soares","Jacob Eisenstein","Kuzman Ganchev","Jonathan Herzig","Kai Hui","Tom Kwiatkowski","Ji Ma","Jianmo Ni","Tal Schuster","William W. Cohen","Michael Collins","Dipanjan Das","Donald Metzler","Slav Petrov","Kellie Webster"],"pdf_url":"https://arxiv.org/pdf/2212.08037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08011v1","updated":"2022-12-15T18:17:01Z","published":"2022-12-15T18:17:01Z","title":"Multi-VALUE: A Framework for Cross-Dialectal English NLP","summary":"  Dialect differences caused by regional, social, and economic barriers cause\nperformance discrepancies for many groups of users of language technology.\nFair, inclusive, and equitable language technology must critically be dialect\ninvariant, meaning that performance remains constant over dialectal shifts.\nCurrent English systems often fall significantly short of this ideal since they\nare designed and tested on a single dialect: Standard American English. We\nintroduce Multi-VALUE -- a suite of resources for evaluating and achieving\nEnglish dialect invariance. We build a controllable rule-based translation\nsystem spanning 50 English dialects and a total of 189 unique linguistic\nfeatures. Our translation maps Standard American English text to synthetic form\nof each dialect, which uses an upper-bound on the natural density of features\nin that dialect. First, we use this system to build stress tests for question\nanswering, machine translation, and semantic parsing tasks. Stress tests reveal\nsignificant performance disparities for leading models on non-standard\ndialects. Second, we use this system as a data augmentation technique to\nimprove the dialect robustness of existing systems. Finally, we partner with\nnative speakers of Chicano and Indian English to release new gold-standard\nvariants of the popular CoQA task.\n","authors":["Caleb Ziems","William Held","Jingfeng Yang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08011v1.pdf","comment":"24 pages (9 pages + appendix); 21 tables; 5 figures"},{"id":"http://arxiv.org/abs/2210.07792v2","updated":"2022-12-15T17:38:58Z","published":"2022-10-14T13:21:33Z","title":"Robust Preference Learning for Storytelling via Contrastive\n  Reinforcement Learning","summary":"  Controlled automated story generation seeks to generate natural language\nstories satisfying constraints from natural language critiques or preferences.\nExisting methods to control for story preference utilize prompt engineering\nwhich is labor intensive and often inconsistent. They may also use\nlogit-manipulation methods which require annotated datasets to exist for the\ndesired attributes. To address these issues, we first train a contrastive\nbi-encoder model to align stories with corresponding human critiques, named\nCARP, building a general purpose preference model. This is subsequently used as\na reward function to fine-tune a generative language model via reinforcement\nlearning. However, simply fine-tuning a generative language model with a\ncontrastive reward model does not always reliably result in a story generation\nsystem capable of generating stories that meet user preferences. To increase\nstory generation robustness we further fine-tune the contrastive reward model\nusing a prompt-learning technique. A human participant study is then conducted\ncomparing generations from our full system, ablations, and two baselines. We\nshow that the full fine-tuning pipeline results in a story generator preferred\nover a LLM 20x as large as well as logit-based methods. This motivates the use\nof contrastive learning for general purpose human preference modeling.\n","authors":["Louis Castricato","Alexander Havrilla","Shahbuland Matiana","Michael Pieler","Anbang Ye","Ian Yang","Spencer Frazier","Mark Riedl"],"pdf_url":"https://arxiv.org/pdf/2210.07792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07983v1","updated":"2022-12-15T17:31:54Z","published":"2022-12-15T17:31:54Z","title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners","summary":"  Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n","authors":["Yan-Bo Lin","Yi-Lin Sung","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2212.07983v1.pdf","comment":"project page: https://genjib.github.io/project_page/LAVISH/"},{"id":"http://arxiv.org/abs/2212.07981v1","updated":"2022-12-15T17:26:05Z","published":"2022-12-15T17:26:05Z","title":"Revisiting the Gold Standard: Grounding Summarization Evaluation with\n  Robust Human Evaluation","summary":"  Human evaluation is the foundation upon which the evaluation of both\nsummarization systems and automatic metrics rests. However, existing human\nevaluation protocols and benchmarks for summarization either exhibit low\ninter-annotator agreement or lack the scale needed to draw statistically\nsignificant conclusions, and an in-depth analysis of human evaluation is\nlacking. In this work, we address the shortcomings of existing summarization\nevaluation along the following axes: 1) We propose a modified summarization\nsalience protocol, Atomic Content Units (ACUs), which relies on fine-grained\nsemantic units and allows for high inter-annotator agreement. 2) We curate the\nRobust Summarization Evaluation (RoSE) benchmark, a large human evaluation\ndataset consisting of over 22k summary-level annotations over state-of-the-art\nsystems on three datasets. 3) We compare our ACU protocol with three other\nhuman evaluation protocols, underscoring potential confounding factors in\nevaluation setups. 4) We evaluate existing automatic metrics using the\ncollected human annotations across evaluation protocols and demonstrate how our\nbenchmark leads to more statistically stable and significant results.\nFurthermore, our findings have important implications for evaluating large\nlanguage models (LLMs), as we show that LLMs adjusted by human feedback (e.g.,\nGPT-3.5) may overfit unconstrained human evaluation, which is affected by the\nannotators' prior, input-agnostic preferences, calling for more robust,\ntargeted evaluation methods.\n","authors":["Yixin Liu","Alexander R. Fabbri","Pengfei Liu","Yilun Zhao","Linyong Nan","Ruilin Han","Simeng Han","Shafiq Joty","Chien-Sheng Wu","Caiming Xiong","Dragomir Radev"],"pdf_url":"https://arxiv.org/pdf/2212.07981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.11443v4","updated":"2022-12-15T17:26:00Z","published":"2022-05-23T16:33:41Z","title":"Unsupervised Tokenization Learning","summary":"  In the presented study, we discover that the so-called \"transition freedom\"\nmetric appears superior for unsupervised tokenization purposes in comparison to\nstatistical metrics such as mutual information and conditional probability,\nproviding F-measure scores in range from 0.71 to 1.0 across explored\nmultilingual corpora. We find that different languages require different\noffshoots of that metric (such as derivative, variance, and \"peak values\") for\nsuccessful tokenization. Larger training corpora do not necessarily result in\nbetter tokenization quality, while compressing the models by eliminating\nstatistically weak evidence tends to improve performance. The proposed\nunsupervised tokenization technique provides quality better than or comparable\nto lexicon-based ones, depending on the language.\n","authors":["Anton Kolonin","Vignav Ramesh"],"pdf_url":"https://arxiv.org/pdf/2205.11443v4.pdf","comment":"16 pages, 9 figures; Paper accepted to the EMNLP 2022 conference"},{"id":"http://arxiv.org/abs/2212.07939v1","updated":"2022-12-15T16:17:03Z","published":"2022-12-15T16:17:03Z","title":"RWEN-TTS: Relation-aware Word Encoding Network for Natural\n  Text-to-Speech Synthesis","summary":"  With the advent of deep learning, a huge number of text-to-speech (TTS)\nmodels which produce human-like speech have emerged. Recently, by introducing\nsyntactic and semantic information w.r.t the input text, various approaches\nhave been proposed to enrich the naturalness and expressiveness of TTS models.\nAlthough these strategies showed impressive results, they still have some\nlimitations in utilizing language information. First, most approaches only use\ngraph networks to utilize syntactic and semantic information without\nconsidering linguistic features. Second, most previous works do not explicitly\nconsider adjacent words when encoding syntactic and semantic information, even\nthough it is obvious that adjacent words are usually meaningful when encoding\nthe current word. To address these issues, we propose Relation-aware Word\nEncoding Network (RWEN), which effectively allows syntactic and semantic\ninformation based on two modules (i.e., Semantic-level Relation Encoding and\nAdjacent Word Relation Encoding). Experimental results show substantial\nimprovements compared to previous works.\n","authors":["Shinhyeok Oh","HyeongRae Noh","Yoonseok Hong","Insoo Oh"],"pdf_url":"https://arxiv.org/pdf/2212.07939v1.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2212.07937v1","updated":"2022-12-15T16:13:25Z","published":"2022-12-15T16:13:25Z","title":"Visually-augmented pretrained language models for NLP tasks without\n  images","summary":"  Although pre-trained language models (PLMs) have shown impressive performance\nby text-only self-supervised training, they are found lack of visual semantics\nor commonsense, e.g., sizes, shapes, and colors of commonplace objects.\nExisting solutions often rely on explicit images for visual knowledge\naugmentation (requiring time-consuming retrieval or generation), and they also\nconduct the augmentation for the whole input text, without considering whether\nit is actually needed in specific inputs or tasks. To address these issues, we\npropose a novel visually-augmented fine-tuning approach that can be generally\napplied to various PLMs or NLP tasks, without using any retrieved or generated\nimages, namely VAWI. Specifically, we first identify the visually-hungry words\n(VH-words) from input text via a token selector, where three different methods\nhave been proposed, including syntax-, attention- and learning-based\nstrategies. Then, we adopt a fixed CLIP text encoder to generate the\nvisually-augmented representations of these VH-words. As it has been\npre-trained by vision-language alignment task on the large-scale corpus, it is\ncapable of injecting visual semantics into the aligned text representations.\nFinally, the visually-augmented features will be fused and transformed into the\npre-designed visual prompts based on VH-words, which can be inserted into PLMs\nto enrich the visual semantics in word representations. We conduct extensive\nexperiments on ten NLP tasks, i.e., GLUE benchmark, CommonsenseQA, CommonGen,\nand SNLI-VE. Experimental results show that our approach can consistently\nimprove the performance of BERT, RoBERTa, BART, and T5 at different scales, and\noutperform several competitive baselines significantly. Our codes and data are\npublicly available at~\\url{https://github.com/RUCAIBox/VAWI}.\n","authors":["Hangyu Guo","Kun Zhou","Wayne Xin Zhao","Qinyu Zhang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2212.07937v1.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2205.12702v3","updated":"2022-12-15T16:01:49Z","published":"2022-05-25T11:59:39Z","title":"Detecting Label Errors by using Pre-Trained Language Models","summary":"  We show that large pre-trained language models are inherently highly capable\nof identifying label errors in natural language datasets: simply examining\nout-of-sample data points in descending order of fine-tuned task loss\nsignificantly outperforms more complex error-detection mechanisms proposed in\nprevious work.\n  To this end, we contribute a novel method for introducing realistic,\nhuman-originated label noise into existing crowdsourced datasets such as SNLI\nand TweetNLP. We show that this noise has similar properties to real,\nhand-verified label errors, and is harder to detect than existing synthetic\nnoise, creating challenges for model robustness. We argue that human-originated\nnoise is a better standard for evaluation than synthetic noise.\n  Finally, we use crowdsourced verification to evaluate the detection of real\nerrors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models\nperform at a 9-36% higher absolute Area Under the Precision-Recall Curve than\nexisting models.\n","authors":["Derek Chong","Jenny Hong","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2205.12702v3.pdf","comment":"18 pages, 10 figures. Accepted to EMNLP 2022; typesetting of this\n  version slightly differs from conference version"},{"id":"http://arxiv.org/abs/2212.07919v1","updated":"2022-12-15T15:52:39Z","published":"2022-12-15T15:52:39Z","title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning","summary":"  Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.\n","authors":["Olga Golovneva","Moya Chen","Spencer Poff","Martin Corredor","Luke Zettlemoyer","Maryam Fazel-Zarandi","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2212.07919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07914v1","updated":"2022-12-15T15:49:27Z","published":"2022-12-15T15:49:27Z","title":"The Effects of In-domain Corpus Size on pre-training BERT","summary":"  Many prior language modeling efforts have shown that pre-training on an\nin-domain corpus can significantly improve performance on downstream\ndomain-specific NLP tasks. However, the difficulties associated with collecting\nenough in-domain data might discourage researchers from approaching this\npre-training task. In this paper, we conducted a series of experiments by\npre-training Bidirectional Encoder Representations from Transformers (BERT)\nwith different sizes of biomedical corpora. The results demonstrate that\npre-training on a relatively small amount of in-domain data (4GB) with limited\ntraining steps, can lead to better performance on downstream domain-specific\nNLP tasks compared with fine-tuning models pre-trained on general corpora.\n","authors":["Chris Sanchez","Zheyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.07914v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2211.15089v3","updated":"2022-12-15T14:27:19Z","published":"2022-11-28T06:08:54Z","title":"Continuous diffusion for categorical data","summary":"  Diffusion models have quickly become the go-to paradigm for generative\nmodelling of perceptual signals (such as images and sound) through iterative\nrefinement. Their success hinges on the fact that the underlying physical\nphenomena are continuous. For inherently discrete and categorical data such as\nlanguage, various diffusion-inspired alternatives have been proposed. However,\nthe continuous nature of diffusion models conveys many benefits, and in this\nwork we endeavour to preserve it. We propose CDCD, a framework for modelling\ncategorical data with diffusion models that are continuous both in time and\ninput space. We demonstrate its efficacy on several language modelling tasks.\n","authors":["Sander Dieleman","Laurent Sartran","Arman Roshannai","Nikolay Savinov","Yaroslav Ganin","Pierre H. Richemond","Arnaud Doucet","Robin Strudel","Chris Dyer","Conor Durkan","Curtis Hawthorne","Rémi Leblond","Will Grathwohl","Jonas Adler"],"pdf_url":"https://arxiv.org/pdf/2211.15089v3.pdf","comment":"26 pages, 8 figures; corrections and additional information about\n  hyperparameters"},{"id":"http://arxiv.org/abs/2212.07852v1","updated":"2022-12-15T14:19:33Z","published":"2022-12-15T14:19:33Z","title":"The effects of gender bias in word embeddings on depression prediction","summary":"  Word embeddings are extensively used in various NLP problems as a\nstate-of-the-art semantic feature vector representation. Despite their success\non various tasks and domains, they might exhibit an undesired bias for\nstereotypical categories due to statistical and societal biases that exist in\nthe dataset they are trained on. In this study, we analyze the gender bias in\nfour different pre-trained word embeddings specifically for the depression\ncategory in the mental disorder domain. We use contextual and non-contextual\nembeddings that are trained on domain-independent as well as clinical\ndomain-specific data. We observe that embeddings carry bias for depression\ntowards different gender groups depending on the type of embeddings. Moreover,\nwe demonstrate that these undesired correlations are transferred to the\ndownstream task for depression phenotype recognition. We find that data\naugmentation by simply swapping gender words mitigates the bias significantly\nin the downstream task.\n","authors":["Gizem Sogancioglu","Heysem Kaya"],"pdf_url":"https://arxiv.org/pdf/2212.07852v1.pdf","comment":"accepted to and published at \"A Participatory Approach to AI for\n  Mental Health (PAI4MH)\" workshop, co-located with NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07850v1","updated":"2022-12-15T14:18:53Z","published":"2022-12-15T14:18:53Z","title":"Attention as a guide for Simultaneous Speech Translation","summary":"  The study of the attention mechanism has sparked interest in many fields,\nsuch as language modeling and machine translation. Although its patterns have\nbeen exploited to perform different tasks, from neural network understanding to\ntextual alignment, no previous work has analysed the encoder-decoder attention\nbehavior in speech translation (ST) nor used it to improve ST on a specific\ntask. In this paper, we fill this gap by proposing an attention-based policy\n(EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the\nexisting attention relations between audio input and textual output. Its goal\nis to leverage the encoder-decoder attention scores to guide inference in real\ntime. Results on en->{de, es} show that the EDAtt policy achieves overall\nbetter results compared to the SimulST state of the art, especially in terms of\ncomputational-aware latency.\n","authors":["Sara Papi","Matteo Negri","Marco Turchi"],"pdf_url":"https://arxiv.org/pdf/2212.07850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07841v1","updated":"2022-12-15T13:57:07Z","published":"2022-12-15T13:57:07Z","title":"MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are\n  Better Dense Retrievers","summary":"  Dense retrieval aims to map queries and passages into low-dimensional vector\nspace for efficient similarity measuring, showing promising effectiveness in\nvarious large-scale retrieval tasks. Since most existing methods commonly adopt\npre-trained Transformers (e.g. BERT) for parameter initialization, some work\nfocuses on proposing new pre-training tasks for compressing the useful semantic\ninformation from passages into dense vectors, achieving remarkable\nperformances. However, it is still challenging to effectively capture the rich\nsemantic information and relations about passages into the dense vectors via\none single particular pre-training task. In this work, we propose a multi-task\npre-trained model, MASTER, that unifies and integrates multiple pre-training\ntasks with different learning objectives under the bottlenecked masked\nautoencoder architecture. Concretely, MASTER utilizes a multi-decoder\narchitecture to integrate three types of pre-training tasks: corrupted passages\nrecovering, related passage recovering and PLMs outputs recovering. By\nincorporating a shared deep encoder, we construct a representation bottleneck\nin our architecture, compressing the abundant semantic information across tasks\ninto dense vectors. The first two types of tasks concentrate on capturing the\nsemantic information of passages and relationships among them within the\npre-training corpus. The third one can capture the knowledge beyond the corpus\nfrom external PLMs (e.g. GPT-2). Extensive experiments on several large-scale\npassage retrieval datasets have shown that our approach outperforms the\nprevious state-of-the-art dense retrieval methods. Our code and data are\npublicly released in https://github.com/microsoft/SimXNS\n","authors":["Kun Zhou","Xiao Liu","Yeyun Gong","Wayne Xin Zhao","Daxin Jiang","Nan Duan","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2212.07841v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2212.07839v1","updated":"2022-12-15T13:52:03Z","published":"2022-12-15T13:52:03Z","title":"TeTIm-Eval: a novel curated evaluation data set for comparing\n  text-to-image models","summary":"  Evaluating and comparing text-to-image models is a challenging problem.\nSignificant advances in the field have recently been made, piquing interest of\nvarious industrial sectors. As a consequence, a gold standard in the field\nshould cover a variety of tasks and application contexts. In this paper a novel\nevaluation approach is experimented, on the basis of: (i) a curated data set,\nmade by high-quality royalty-free image-text pairs, divided into ten\ncategories; (ii) a quantitative metric, the CLIP-score, (iii) a human\nevaluation task to distinguish, for a given text, the real and the generated\nimages. The proposed method has been applied to the most recent models, i.e.,\nDALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early\nexperimental results show that the accuracy of the human judgement is fully\ncoherent with the CLIP-score. The dataset has been made available to the\npublic.\n","authors":["Federico A. Galatolo","Mario G. C. A. Cimino","Edoardo Cogotti"],"pdf_url":"https://arxiv.org/pdf/2212.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07769v1","updated":"2022-12-15T12:47:18Z","published":"2022-12-15T12:47:18Z","title":"CLAM: Selective Clarification for Ambiguous Questions with Large\n  Language Models","summary":"  State-of-the-art language models are often accurate on many\nquestion-answering benchmarks with well-defined questions. Yet, in real\nsettings questions are often unanswerable without asking the user for\nclarifying information. We show that current SotA models often do not ask the\nuser for clarification when presented with imprecise questions and instead\nprovide incorrect answers or \"hallucinate\". To address this, we introduce CLAM,\na framework that first uses the model to detect ambiguous questions, and if an\nambiguous question is detected, prompts the model to ask the user for\nclarification. Furthermore, we show how to construct a scalable and\ncost-effective automatic evaluation protocol using an oracle language model\nwith privileged information to provide clarifying information. We show that our\nmethod achieves a 20.15 percentage point accuracy improvement over SotA on a\nnovel ambiguous question-answering answering data set derived from TriviaQA.\n","authors":["Lorenz Kuhn","Yarin Gal","Sebastian Farquhar"],"pdf_url":"https://arxiv.org/pdf/2212.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07767v1","updated":"2022-12-15T12:37:28Z","published":"2022-12-15T12:37:28Z","title":"COLA: Improving Conversational Recommender Systems by Collaborative\n  Augmentation","summary":"  Conversational recommender systems (CRS) aim to employ natural language\nconversations to suggest suitable products to users. Understanding user\npreferences for prospective items and learning efficient item representations\nare crucial for CRS. Despite various attempts, earlier studies mostly learned\nitem representations based on individual conversations, ignoring item\npopularity embodied among all others. Besides, they still need support in\nefficiently capturing user preferences since the information reflected in a\nsingle conversation is limited. Inspired by collaborative filtering, we propose\na collaborative augmentation (COLA) method to simultaneously improve both item\nrepresentation learning and user preference modeling to address these issues.\nWe construct an interactive user-item graph from all conversations, which\naugments item representations with user-aware information, i.e., item\npopularity. To improve user preference modeling, we retrieve similar\nconversations from the training corpus, where the involved items and attributes\nthat reflect the user's potential interests are used to augment the user\nrepresentation through gate control. Extensive experiments on two benchmark\ndatasets demonstrate the effectiveness of our method. Our code and data are\navailable at https://github.com/DongdingLin/COLA.\n","authors":["Dongding Lin","Jian Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2212.07767v1.pdf","comment":"Accepted by AAAI-2023"},{"id":"http://arxiv.org/abs/2212.07752v1","updated":"2022-12-15T12:14:25Z","published":"2022-12-15T12:14:25Z","title":"TRIP: Triangular Document-level Pre-training for Multilingual Language\n  Models","summary":"  Despite the current success of multilingual pre-training, most prior works\nfocus on leveraging monolingual data or bilingual parallel data and overlooked\nthe value of trilingual parallel data. This paper presents \\textbf{Tri}angular\nDocument-level \\textbf{P}re-training (\\textbf{TRIP}), which is the first in the\nfield to extend the conventional monolingual and bilingual pre-training to a\ntrilingual setting by (i) \\textbf{Grafting} the same documents in two languages\ninto one mixed document, and (ii) predicting the remaining one language as the\nreference translation. Our experiments on document-level MT and cross-lingual\nabstractive summarization show that TRIP brings by up to 3.65 d-BLEU points and\n6.2 ROUGE-L points on three multilingual document-level machine translation\nbenchmarks and one cross-lingual abstractive summarization benchmark, including\nmultiple strong state-of-the-art (SOTA) scores. In-depth analysis indicates\nthat TRIP improves document-level machine translation and captures better\ndocument contexts in at least three characteristics: (i) tense consistency,\n(ii) noun consistency and (iii) conjunction presence.\n","authors":["Hongyuan Lu","Haoyang Huang","Shuming Ma","Dongdong Zhang","Wai Lam","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2212.07752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06282v2","updated":"2022-12-15T11:38:59Z","published":"2022-10-12T15:05:28Z","title":"Towards Generalized and Explainable Long-Range Context Representation\n  for Dialogue Systems","summary":"  Long-range context modeling is crucial to both dialogue understanding and\ngeneration. The most popular method for dialogue context representation is to\nconcatenate the last-$k$ previous utterances. However, this method may not be\nideal for conversations containing long-range dependencies. In this work, we\npropose DialoGX, a novel encoder-decoder based framework for conversational\nresponse generation with a generalized and explainable context representation\nthat can look beyond the last-$k$ utterances. Hence the method is adaptive to\nconversations with long-range dependencies. The main idea of our approach is to\nidentify and utilize the most relevant historical utterances instead of the\nlast-$k$ utterances in chronological order. We study the effectiveness of our\nproposed method on both dialogue generation (open-domain) and understanding\n(DST) tasks. DialoGX achieves comparable performance with the state-of-the-art\nmodels on DailyDialog dataset. We also observe performance gain in existing DST\nmodels with our proposed context representation strategy on MultiWOZ dataset.\nWe justify our context representation through the lens of psycholinguistics and\nshow that the relevance score of previous utterances agrees well with human\ncognition which makes DialoGX explainable as well.\n","authors":["Suvodip Dey","Maunendra Sankar Desarkar","P. K. Srijith"],"pdf_url":"https://arxiv.org/pdf/2210.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07707v1","updated":"2022-12-15T10:32:29Z","published":"2022-12-15T10:32:29Z","title":"FreCDo: A Large Corpus for French Cross-Domain Dialect Identification","summary":"  We present a novel corpus for French dialect identification comprising\n413,522 French text samples collected from public news websites in Belgium,\nCanada, France and Switzerland. To ensure an accurate estimation of the dialect\nidentification performance of models, we designed the corpus to eliminate\npotential biases related to topic, writing style, and publication source. More\nprecisely, the training, validation and test splits are collected from\ndifferent news websites, while searching for different keywords (topics). This\nleads to a French cross-domain (FreCDo) dialect identification task. We conduct\nexperiments with four competitive baselines, a fine-tuned CamemBERT model, an\nXGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)\nclassifier based on fine-tuned CamemBERT features, and an SVM based on word\nn-grams. Aside from presenting quantitative results, we also make an analysis\nof the most discriminative features learned by CamemBERT. Our corpus is\navailable at https://github.com/MihaelaGaman/FreCDo.\n","authors":["Mihaela Gaman","Adrian-Gabriel Chifu","William Domingues","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2212.07707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07699v1","updated":"2022-12-15T10:20:42Z","published":"2022-12-15T10:20:42Z","title":"Retrieval-based Disentanglement with Distant Supervision","summary":"  Disentangled representation learning remains challenging as ground truth\nfactors of variation do not naturally exist. To address this, we present\nVocabulary Disentanglement Retrieval~(VDR), a simple yet effective\nretrieval-based disentanglement framework that leverages nature language as\ndistant supervision. Our approach is built upon the widely-used bi-encoder\narchitecture with disentanglement heads and is trained on data-text pairs that\nare readily available on the web or in existing datasets. This makes our\napproach task- and modality-agnostic with potential for a wide range of\ndownstream applications. We conduct experiments on 16 datasets in both\ntext-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.\nWith the incorporation of disentanglement heads and a minor increase in\nparameters, VDR achieves significant improvements over the base retriever it is\nbuilt upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text\nretrieval and an average of 13% higher recall in cross-modal retrieval. In\ncomparison to other baselines, VDR outperforms them in most tasks, while also\nimproving explainability and efficiency.\n","authors":["Jiawei Zhou","Xiaoguang Li","Lifeng Shang","Xin Jiang","Qun Liu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.03276v3","updated":"2022-12-15T09:30:58Z","published":"2022-04-07T08:01:13Z","title":"PALBERT: Teaching ALBERT to Ponder","summary":"  Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\nin wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layer's\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from the $i$-th layer, introduces major variance in exit layer indices,\nsignificantly reducing the resulting model's performance. In this paper, we\npropose improving PonderNet with a novel deterministic Q-exit criterion and a\nrevisited model architecture. We adapted the proposed mechanism to ALBERT and\nRoBERTa and compared it with recent methods for performing an early exit. We\nobserved that the proposed changes can be considered significant improvements\non the original PonderNet architecture and outperform PABEE on a wide range of\nGLUE tasks. In addition, we also performed an in-depth ablation study of the\nproposed architecture to further understand Lambda layers and their\nperformance.\n","authors":["Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2204.03276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07677v1","updated":"2022-12-15T09:21:21Z","published":"2022-12-15T09:21:21Z","title":"Transformers learn in-context by gradient descent","summary":"  Transformers have become the state-of-the-art neural network architecture\nacross numerous domains of machine learning. This is partly due to their\ncelebrated ability to transfer and to learn in-context based on few examples.\nNevertheless, the mechanisms by which Transformers become in-context learners\nare not well understood and remain mostly an intuition. Here, we argue that\ntraining Transformers on auto-regressive tasks can be closely related to\nwell-known gradient-based meta-learning formulations. We start by providing a\nsimple weight construction that shows the equivalence of data transformations\ninduced by 1) a single linear self-attention layer and by 2) gradient-descent\n(GD) on a regression loss. Motivated by that construction, we show empirically\nthat when training self-attention-only Transformers on simple regression tasks\neither the models learned by GD and Transformers show great similarity or,\nremarkably, the weights found by optimization match the construction. Thus we\nshow how trained Transformers implement gradient descent in their forward pass.\nThis allows us, at least in the domain of regression problems, to\nmechanistically understand the inner workings of optimized Transformers that\nlearn in-context. Furthermore, we identify how Transformers surpass plain\ngradient descent by an iterative curvature correction and learn linear models\non deep data representations to solve non-linear regression tasks. Finally, we\ndiscuss intriguing parallels to a mechanism identified to be crucial for\nin-context learning termed induction-head (Olsson et al., 2022) and show how it\ncould be understood as a specific case of in-context learning by gradient\ndescent learning within Transformers.\n","authors":["Johannes von Oswald","Eyvind Niklasson","Ettore Randazzo","João Sacramento","Alexander Mordvintsev","Andrey Zhmoginov","Max Vladymyrov"],"pdf_url":"https://arxiv.org/pdf/2212.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07127v2","updated":"2022-12-15T09:10:16Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07672v1","updated":"2022-12-15T09:05:26Z","published":"2022-12-15T09:05:26Z","title":"Summary-Oriented Vision Modeling for Multimodal Abstractive\n  Summarization","summary":"  The goal of multimodal abstractive summarization (MAS) is to produce a\nconcise summary given the multimodal data (text and vision). Existing studies\non MAS mainly focus on how to effectively use the extracted visual features,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the quality of the visual features to\nthe summary, which may limit the model performance especially in the low- and\nzero-resource scenarios. In this paper, we propose to improve the summary\nquality through summary-oriented visual features. To this end, we devise two\nauxiliary tasks including \\emph{vision to summary task} and \\emph{masked image\nmodeling task}. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios.\n","authors":["Yunlong Liang","Fandong Meng","Jinan Xu","Jiaan Wang","Yufeng Chen","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07672v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2212.07669v1","updated":"2022-12-15T08:57:30Z","published":"2022-12-15T08:57:30Z","title":"Using Two Losses and Two Datasets Simultaneously to Improve TempoWiC\n  Accuracy","summary":"  WSD (Word Sense Disambiguation) is the task of identifying which sense of a\nword is meant in a sentence or other segment of text. Researchers have worked\non this task (e.g. Pustejovsky, 2002) for years but it's still a challenging\none even for SOTA (state-of-the-art) LMs (language models). The new dataset,\nTempoWiC introduced by Loureiro et al. (2022b) focuses on the fact that words\nchange over time. Their best baseline achieves 70.33% macro-F1. In this work,\nwe use two different losses simultaneously to train RoBERTa-based\nclassification models. We also improve our model by using another similar\ndataset to generalize better. Our best configuration beats their best baseline\nby 4.23% and reaches 74.56% macroF1.\n","authors":["Mohammad Javad Pirhadi","Motahhare Mirzaei","Sauleh Eetemadi"],"pdf_url":"https://arxiv.org/pdf/2212.07669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07649v1","updated":"2022-12-15T08:15:32Z","published":"2022-12-15T08:15:32Z","title":"Improve Text Classification Accuracy with Intent Information","summary":"  Text classification, a core component of task-oriented dialogue systems,\nattracts continuous research from both the research and industry community, and\nhas resulted in tremendous progress. However, existing method does not consider\nthe use of label information, which may weaken the performance of text\nclassification systems in some token-aware scenarios. To address the problem,\nin this paper, we introduce the use of label information as label embedding for\nthe task of text classification and achieve remarkable performance on benchmark\ndataset.\n","authors":["Yifeng Xie"],"pdf_url":"https://arxiv.org/pdf/2212.07649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11761v2","updated":"2022-12-15T07:32:25Z","published":"2022-08-24T20:14:52Z","title":"IndicSUPERB: A Speech Processing Universal Performance Benchmark for\n  Indian languages","summary":"  A cornerstone in AI research has been the creation and adoption of\nstandardized training and test datasets to earmark the progress of\nstate-of-the-art models. A particularly successful example is the GLUE dataset\nfor training and evaluating Natural Language Understanding (NLU) models for\nEnglish. The large body of research around self-supervised BERT-based language\nmodels revolved around performance improvements on NLU tasks in GLUE. To\nevaluate language models in other languages, several language-specific GLUE\ndatasets were created. The area of speech language understanding (SLU) has\nfollowed a similar trajectory. The success of large self-supervised models such\nas wav2vec2 enable creation of speech models with relatively easy to access\nunlabelled data. These models can then be evaluated on SLU tasks, such as the\nSUPERB benchmark. In this work, we extend this to Indic languages by releasing\nthe IndicSUPERB benchmark. Specifically, we make the following three\ncontributions. (i) We collect Kathbath containing 1,684 hours of labelled\nspeech data across 12 Indian languages from 1,218 contributors located in 203\ndistricts in India. (ii) Using Kathbath, we create benchmarks across 6 speech\ntasks: Automatic Speech Recognition, Speaker Verification, Speaker\nIdentification (mono/multi), Language Identification, Query By Example, and\nKeyword Spotting for 12 languages. (iii) On the released benchmarks, we train\nand evaluate different self-supervised models alongside a commonly used\nbaseline FBANK. We show that language-specific fine-tuned models are more\naccurate than baseline on most of the tasks, including a large gap of 76\\% for\nthe Language Identification task. However, for speaker identification,\nself-supervised models trained on large datasets demonstrate an advantage. We\nhope IndicSUPERB contributes to the progress of developing speech language\nunderstanding models for Indian languages.\n","authors":["Tahir Javed","Kaushal Santosh Bhogale","Abhigyan Raman","Anoop Kunchukuttan","Pratyush Kumar","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2208.11761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07634v1","updated":"2022-12-15T06:52:31Z","published":"2022-12-15T06:52:31Z","title":"Gradient-based Intra-attention Pruning on Pre-trained Language Models","summary":"  Pre-trained language models achieve superior performance, but they are\ncomputationally expensive due to their large size. Techniques such as pruning\nand knowledge distillation (KD) have been developed to reduce their size and\nlatency. In most structural pruning methods, the pruning units, such as\nattention heads and feed-forward hidden dimensions, only span a small model\nstructure space and limit the structures that the pruning algorithm can\nexplore. In this work, we propose Gradient-based Intra-attention pruning\n(GRAIN), which inspects fine intra-attention structures, and allows different\nheads to have different sizes. Intra-attention pruning greatly expands the\nsearching space of model structures and yields highly heterogeneous structures.\nWe further propose structure regularization to encourage generating more\nregular structures, which achieves higher speedups than heterogeneous ones. We\nalso integrate KD into the pruning process with a gradient separation strategy\nto reduce the interference of KD with the pruning process. GRAIN is evaluated\non a variety of tasks. Results show that it notably outperforms other methods\nat the same or similar model size. Even under extreme compression where only\n$3\\%$ weights in transformers remain, the pruned model is still competitive.\n","authors":["Ziqing Yang","Yiming Cui","Xin Yao","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07634v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2212.07617v1","updated":"2022-12-15T05:01:59Z","published":"2022-12-15T05:01:59Z","title":"Efficient Pre-training of Masked Language Model via Concept-based\n  Curriculum Masking","summary":"  Masked language modeling (MLM) has been widely used for pre-training\neffective bidirectional representations, but incurs substantial training costs.\nIn this paper, we propose a novel concept-based curriculum masking (CCM) method\nto efficiently pre-train a language model. CCM has two key differences from\nexisting curriculum learning approaches to effectively reflect the nature of\nMLM. First, we introduce a carefully-designed linguistic difficulty criterion\nthat evaluates the MLM difficulty of each token. Second, we construct a\ncurriculum that gradually masks words related to the previously masked words by\nretrieving a knowledge graph. Experimental results show that CCM significantly\nimproves pre-training efficiency. Specifically, the model trained with CCM\nshows comparative performance with the original BERT on the General Language\nUnderstanding Evaluation benchmark at half of the training cost.\n","authors":["Mingyu Lee","Jun-Hyung Park","Junho Kim","Kang-Min Kim","SangKeun Lee"],"pdf_url":"https://arxiv.org/pdf/2212.07617v1.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.05970v2","updated":"2022-12-15T02:52:34Z","published":"2022-12-09T03:29:38Z","title":"Decomposing a Recurrent Neural Network into Modules for Enabling\n  Reusability and Replacement","summary":"  Can we take a recurrent neural network (RNN) trained to translate between\nlanguages and augment it to support a new natural language without retraining\nthe model from scratch? Can we fix the faulty behavior of the RNN by replacing\nportions associated with the faulty behavior? Recent works on decomposing a\nfully connected neural network (FCNN) and convolutional neural network (CNN)\ninto modules have shown the value of engineering deep models in this manner,\nwhich is standard in traditional SE but foreign for deep learning models.\nHowever, prior works focus on the image-based multiclass classification\nproblems and cannot be applied to RNN due to (a) different layer structures,\n(b) loop structures, (c) different types of input-output architectures, and (d)\nusage of both nonlinear and logistic activation functions. In this work, we\npropose the first approach to decompose an RNN into modules. We study different\ntypes of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN\nmodules can be reused and replaced in various scenarios. We evaluate our\napproach against 5 canonical datasets (i.e., Math QA, Brown Corpus,\nWiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.\nWe found that decomposing a trained model has a small cost (Accuracy: -0.6%,\nBLEU score: +0.10%). Also, the decomposed modules can be reused and replaced\nwithout needing to retrain.\n","authors":["Sayem Mohammad Imtiaz","Fraol Batole","Astha Singh","Rangeet Pan","Breno Dantas Cruz","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2212.05970v2.pdf","comment":"Accepted at 45th international conference on software engineering\n  (ICSE'2023)"},{"id":"http://arxiv.org/abs/2212.05506v2","updated":"2022-12-15T01:07:43Z","published":"2022-12-11T13:43:22Z","title":"FastClass: A Time-Efficient Approach to Weakly-Supervised Text\n  Classification","summary":"  Weakly-supervised text classification aims to train a classifier using only\nclass descriptions and unlabeled data. Recent research shows that\nkeyword-driven methods can achieve state-of-the-art performance on various\ntasks. However, these methods not only rely on carefully-crafted class\ndescriptions to obtain class-specific keywords but also require substantial\namount of unlabeled data and takes a long time to train. This paper proposes\nFastClass, an efficient weakly-supervised classification approach. It uses\ndense text representation to retrieve class-relevant documents from external\nunlabeled corpus and selects an optimal subset to train a classifier. Compared\nto keyword-driven methods, our approach is less reliant on initial class\ndescriptions as it no longer needs to expand each class description into a set\nof class-specific keywords. Experiments on a wide range of classification tasks\nshow that the proposed approach frequently outperforms keyword-driven models in\nterms of classification accuracy and often enjoys orders-of-magnitude faster\ntraining speed.\n","authors":["Tingyu Xia","Yue Wang","Yuan Tian","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2212.05506v2.pdf","comment":"EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.07571v1","updated":"2022-12-15T01:06:55Z","published":"2022-12-15T01:06:55Z","title":"Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual\n  Machine Translation","summary":"  Sparsely gated Mixture of Experts (MoE) models have been shown to be a\ncompute-efficient method to scale model capacity for multilingual machine\ntranslation. However, for low-resource tasks, MoE models severely over-fit. We\nshow effective regularization strategies, namely dropout techniques for MoE\nlayers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods\nthat prevent over-fitting and improve the performance of MoE models on\nlow-resource tasks without adversely affecting high-resource tasks. On a\nmassively multilingual machine translation benchmark, our strategies result in\nabout +1 chrF++ improvement in very low resource language pairs. We perform an\nextensive analysis of the learned MoE routing to better understand the impact\nof our regularization methods and how we can improve them.\n","authors":["Maha Elbayad","Anna Sun","Shruti Bhosale"],"pdf_url":"https://arxiv.org/pdf/2212.07571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.07562v2","updated":"2022-12-15T00:43:13Z","published":"2022-09-15T19:01:21Z","title":"TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for\n  Multilingual Tweet Representations","summary":"  We present TwHIN-BERT, a multilingual language model trained on in-domain\ndata from the popular social network Twitter. TwHIN-BERT differs from prior\npre-trained language models as it is trained with not only text-based\nself-supervision, but also with a social objective based on the rich social\nengagements within a Twitter heterogeneous information network (TwHIN). Our\nmodel is trained on 7 billion tweets covering over 100 distinct languages\nproviding a valuable representation to model short, noisy, user-generated text.\nWe evaluate our model on a variety of multilingual social recommendation and\nsemantic understanding tasks and demonstrate significant metric improvement\nover established pre-trained language models. We will freely open-source\nTwHIN-BERT and our curated hashtag prediction and social engagement benchmark\ndatasets to the research community.\n","authors":["Xinyang Zhang","Yury Malkov","Omar Florez","Serim Park","Brian McWilliams","Jiawei Han","Ahmed El-Kishky"],"pdf_url":"https://arxiv.org/pdf/2209.07562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08196v1","updated":"2022-12-15T23:41:20Z","published":"2022-12-15T23:41:20Z","title":"Saved You A Click: Automatically Answering Clickbait Titles","summary":"  Often clickbait articles have a title that is phrased as a question or vague\nteaser that entices the user to click on the link and read the article to find\nthe explanation. We developed a system that will automatically find the answer\nor explanation of the clickbait hook from the website text so that the user\ndoes not need to read through the text themselves. We fine-tune an extractive\nquestion and answering model (RoBERTa) and an abstractive one (T5), using data\nscraped from the 'StopClickbait' Facebook pages and Reddit's 'SavedYouAClick'\nsubforum. We find that both extractive and abstractive models improve\nsignificantly after finetuning. We find that the extractive model performs\nslightly better according to ROUGE scores, while the abstractive one has a\nslight edge in terms of BERTscores.\n","authors":["Oliver Johnson","Beicheng Lou","Janet Zhong","Andrey Kurenkov"],"pdf_url":"https://arxiv.org/pdf/2212.08196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08195v1","updated":"2022-12-15T23:38:31Z","published":"2022-12-15T23:38:31Z","title":"Improving Chess Commentaries by Combining Language Models with Symbolic\n  Reasoning Engines","summary":"  Despite many recent advancements in language modeling, state-of-the-art\nlanguage models lack grounding in the real world and struggle with tasks\ninvolving complex reasoning. Meanwhile, advances in the symbolic reasoning\ncapabilities of AI have led to systems that outperform humans in games like\nchess and Go (Silver et al., 2018). Chess commentary provides an interesting\ndomain for bridging these two fields of research, as it requires reasoning over\na complex board state and providing analyses in natural language. In this work\nwe demonstrate how to combine symbolic reasoning engines with controllable\nlanguage models to generate chess commentaries. We conduct experiments to\ndemonstrate that our approach generates commentaries that are preferred by\nhuman judges over previous baselines.\n","authors":["Andrew Lee","David Wu","Emily Dinan","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2212.08195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08192v1","updated":"2022-12-15T23:26:54Z","published":"2022-12-15T23:26:54Z","title":"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources\n  in Natural Language Understanding Systems","summary":"  Many state-of-the-art natural language understanding (NLU) models are based\non pretrained neural language models. These models often make inferences using\ninformation from multiple sources. An important class of such inferences are\nthose that require both background knowledge, presumably contained in a model's\npretrained parameters, and instance-specific information that is supplied at\ninference time. However, the integration and reasoning abilities of NLU models\nin the presence of multiple knowledge sources have been largely understudied.\nIn this work, we propose a test suite of coreference resolution tasks that\nrequire reasoning over multiple facts. Our dataset is organized into subtasks\nthat differ in terms of which knowledge sources contain relevant facts. We\nevaluate state-of-the-art coreference resolution models on our dataset. Our\nresults indicate that several models struggle to reason on-the-fly over\nknowledge observed both at pretrain time and at inference time. However, with\ntask-specific training, a subset of models demonstrates the ability to\nintegrate certain knowledge types from multiple sources.\n","authors":["Akshatha Arodi","Martin Pömsl","Kaheer Suleman","Adam Trischler","Alexandra Olteanu","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2212.08192v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2210.01959v2","updated":"2022-12-15T23:16:30Z","published":"2022-10-04T23:33:52Z","title":"Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot\n  Document-Level Question Answering","summary":"  Researchers produce thousands of scholarly documents containing valuable\ntechnical knowledge. The community faces the laborious task of reading these\ndocuments to identify, extract, and synthesize information. To automate\ninformation gathering, document-level question answering (QA) offers a flexible\nframework where human-posed questions can be adapted to extract diverse\nknowledge. Finetuning QA systems requires access to labeled data (tuples of\ncontext, question and answer). However, data curation for document QA is\nuniquely challenging because the context (i.e. answer evidence passage) needs\nto be retrieved from potentially long, ill-formatted documents. Existing QA\ndatasets sidestep this challenge by providing short, well-defined contexts that\nare unrealistic in real-world applications. We present a three-stage document\nQA approach: (1) text extraction from PDF; (2) evidence retrieval from\nextracted texts to form well-posed contexts; (3) QA to extract knowledge from\ncontexts to return high-quality answers -- extractive, abstractive, or Boolean.\nUsing QASPER for evaluation, our detect-retrieve-comprehend (DRC) system\nachieves a +7.19 improvement in Answer-F1 over existing baselines while\ndelivering superior context selection. Our results demonstrate that DRC holds\ntremendous promise as a flexible framework for practical scientific document\nQA.\n","authors":["Tavish McDonald","Brian Tsan","Amar Saini","Juanita Ordonez","Luis Gutierrez","Phan Nguyen","Blake Mason","Brenda Ng"],"pdf_url":"https://arxiv.org/pdf/2210.01959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08184v1","updated":"2022-12-15T23:00:33Z","published":"2022-12-15T23:00:33Z","title":"NBC-Softmax : Darkweb Author fingerprinting and migration tracking","summary":"  Metric learning aims to learn distances from the data, which enhances the\nperformance of similarity-based algorithms. An author style detection task is a\nmetric learning problem, where learning style features with small intra-class\nvariations and larger inter-class differences is of great importance to achieve\nbetter performance. Recently, metric learning based on softmax loss has been\nused successfully for style detection. While softmax loss can produce separable\nrepresentations, its discriminative power is relatively poor. In this work, we\npropose NBC-Softmax, a contrastive loss based clustering technique for softmax\nloss, which is more intuitive and able to achieve superior performance. Our\ntechnique meets the criterion for larger number of samples, thus achieving\nblock contrastiveness, which is proven to outperform pair-wise losses. It uses\nmini-batch sampling effectively and is scalable. Experiments on 4 darkweb\nsocial forums, with NBCSAuthor that uses the proposed NBC-Softmax for author\nand sybil detection, shows that our negative block contrastive approach\nconstantly outperforms state-of-the-art methods using the same network\narchitecture.\n  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax\n","authors":["Gayan K. Kulatilleke","Shekhar S. Chandra","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2212.08184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09718v2","updated":"2022-12-15T22:45:46Z","published":"2022-11-02T00:58:02Z","title":"Numerical Optimizations for Weighted Low-rank Estimation on Language\n  Model","summary":"  Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.\n","authors":["Ting Hua","Yen-Chang Hsu","Felicity Wang","Qian Lou","Yilin Shen","Hongxia Jin"],"pdf_url":"https://arxiv.org/pdf/2211.09718v2.pdf","comment":"long paper EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.08172v1","updated":"2022-12-15T22:15:11Z","published":"2022-12-15T22:15:11Z","title":"Reliable Measures of Spread in High Dimensional Latent Spaces","summary":"  Understanding geometric properties of natural language processing models'\nlatent spaces allows the manipulation of these properties for improved\nperformance on downstream tasks. One such property is the amount of data spread\nin a model's latent space, or how fully the available latent space is being\nused. In this work, we define data spread and demonstrate that the commonly\nused measures of data spread, Average Cosine Similarity and a partition\nfunction min/max ratio I(V), do not provide reliable metrics to compare the use\nof latent space across models. We propose and examine eight alternative\nmeasures of data spread, all but one of which improve over these current\nmetrics when applied to seven synthetic data distributions. Of our proposed\nmeasures, we recommend one principal component-based measure and one\nentropy-based measure that provide reliable, relative measures of spread and\ncan be used to compare models of different sizes and dimensionalities.\n","authors":["Anna C. Marbut","Katy McKinney-Bock","Travis J. Wheeler"],"pdf_url":"https://arxiv.org/pdf/2212.08172v1.pdf","comment":"24 pages, 11 figures, 13 tables"},{"id":"http://arxiv.org/abs/2211.16822v2","updated":"2022-12-15T21:53:05Z","published":"2022-11-30T08:44:30Z","title":"A Probabilistic-Logic based Commonsense Representation Framework for\n  Modelling Inferences with Multiple Antecedents and Varying Likelihoods","summary":"  Commonsense knowledge-graphs (CKGs) are important resources towards building\nmachines that can 'reason' on text or environmental inputs and make inferences\nbeyond perception. While current CKGs encode world knowledge for a large number\nof concepts and have been effectively utilized for incorporating commonsense in\nneural models, they primarily encode declarative or single-condition\ninferential knowledge and assume all conceptual beliefs to have the same\nlikelihood. Further, these CKGs utilize a limited set of relations shared\nacross concepts and lack a coherent knowledge organization structure resulting\nin redundancies as well as sparsity across the larger knowledge graph.\nConsequently, today's CKGs, while useful for a first level of reasoning, do not\nadequately capture deeper human-level commonsense inferences which can be more\nnuanced and influenced by multiple contextual or situational factors.\n  Accordingly, in this work, we study how commonsense knowledge can be better\nrepresented by -- (i) utilizing a probabilistic logic representation scheme to\nmodel composite inferential knowledge and represent conceptual beliefs with\nvarying likelihoods and (ii) incorporating a hierarchical conceptual ontology\nto identify salient concept-relevant relations and organize beliefs at\ndifferent conceptual levels. Our resulting knowledge representation framework\ncan encode a wider variety of world knowledge and represent beliefs flexibly\nusing grounded concepts as well as free-text phrases. As a result, the\nframework can be utilized as both a traditional free-text knowledge graph and a\ngrounded logic-based inference system more suitable for neuro-symbolic\napplications. We describe how we extend the PrimeNet knowledge base with our\nframework through crowd-sourcing and expert-annotation, and demonstrate its\napplication for more interpretable passage-based semantic parsing and question\nanswering.\n","authors":["Shantanu Jaiswal","Liu Yan","Dongkyu Choi","Kenneth Kwok"],"pdf_url":"https://arxiv.org/pdf/2211.16822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08158v1","updated":"2022-12-15T21:41:06Z","published":"2022-12-15T21:41:06Z","title":"MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal\n  Contributions in Vision and Language Models & Tasks","summary":"  Vision and language models (VL) are known to exploit unrobust indicators in\nindividual modalities (e.g., introduced by distributional biases), instead of\nfocusing on relevant information in each modality. A small drop in accuracy\nobtained on a VL task with a unimodal model suggests that so-called unimodal\ncollapse occurred. But how to quantify the amount of unimodal collapse\nreliably, at dataset and instance-level, to diagnose and combat unimodal\ncollapse in a targeted way? We present MM-SHAP, a performance-agnostic\nmultimodality score that quantifies the proportion by which a model uses\nindividual modalities in multimodal tasks. MM-SHAP is based on Shapley values\nand will be applied in two ways: (1) to compare models for their degree of\nmultimodality, and (2) to measure the contribution of individual modalities for\na given task and dataset. Experiments with 6 VL models -- LXMERT, CLIP and four\nALBEF variants -- on four VL tasks highlight that unimodal collapse can occur\nto different degrees and in different directions, contradicting the wide-spread\nassumption that unimodal collapse is one-sided. We recommend MM-SHAP for\nanalysing multimodal tasks, to diagnose and guide progress towards multimodal\nintegration. Code available at: https://github.com/Heidelberg-NLP/MM-SHAP\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2212.08158v1.pdf","comment":"10 pages, 13 appendix pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2212.08153v1","updated":"2022-12-15T21:35:46Z","published":"2022-12-15T21:35:46Z","title":"FiDO: Fusion-in-Decoder optimized for stronger performance and faster\n  inference","summary":"  Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD\nsuffers from very expensive inference. We show that the majority of inference\ntime results from memory bandwidth constraints in the decoder, and propose two\nsimple changes to the FiD architecture to speed up inference by 7x. The faster\ndecoder inference then allows for a much larger decoder. We denote FiD with the\nabove modifications as FiDO, and show that it strongly improves performance\nover existing FiD models for a wide range of inference budgets. For example,\nFiDO-Large-XXL performs faster inference than FiD-Base and achieves better\nperformance than FiD-Large.\n","authors":["Michiel de Jong","Yury Zemlyanskiy","Joshua Ainslie","Nicholas FitzGerald","Sumit Sanghai","Fei Sha","William Cohen"],"pdf_url":"https://arxiv.org/pdf/2212.08153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08136v1","updated":"2022-12-15T20:51:27Z","published":"2022-12-15T20:51:27Z","title":"Efficient Long Sequence Modeling via State Space Augmented Transformer","summary":"  Transformer models have achieved superior performance in various natural\nlanguage processing tasks. However, the quadratic computational cost of the\nattention mechanism limits its practicality for long sequences. There are\nexisting attention variants that improve the computational efficiency, but they\nhave limited ability to effectively compute global information. In parallel to\nTransformer models, state space models (SSMs) are tailored for long sequences,\nbut they are not flexible enough to capture complicated local information. We\npropose SPADE, short for $\\underline{\\textbf{S}}$tate\ns$\\underline{\\textbf{P}}$ace\n$\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$\nTransform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the\nbottom layer of SPADE, and we employ efficient local attention methods for the\nother layers. The SSM augments global information, which complements the lack\nof long-range dependency issue in local attention methods. Experimental results\non the Long Range Arena benchmark and language modeling tasks demonstrate the\neffectiveness of the proposed method. To further demonstrate the scalability of\nSPADE, we pre-train large encoder-decoder models and present fine-tuning\nresults on natural language understanding and natural language generation\ntasks.\n","authors":["Simiao Zuo","Xiaodong Liu","Jian Jiao","Denis Charles","Eren Manavoglu","Tuo Zhao","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2212.08136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.05952v3","updated":"2022-12-15T20:38:08Z","published":"2021-09-13T13:26:30Z","title":"Adapting the Tesseract Open-Source OCR Engine for Tamil and Sinhala\n  Legacy Fonts and Creating a Parallel Corpus for Tamil-Sinhala-English","summary":"  Most low-resource languages do not have the necessary resources to create\neven a substantial monolingual corpus. These languages may often be found in\ngovernment proceedings but mainly in Portable Document Format (PDF) that\ncontains legacy fonts. Extracting text from these documents to create a\nmonolingual corpus is challenging due to legacy font usage and printer-friendly\nencoding, which are not optimized for text extraction. Therefore, we propose a\nsimple, automatic, and novel idea that can scale for Tamil, Sinhala, English\nlanguages, and many documents along with parallel corpora. Since Tamil and\nSinhala are Low-Resource Languages, we improved the performance of Tesseract by\nemploying LSTM-based training on more than 20 legacy fonts to recognize printed\ncharacters in these languages. Especially, our model detects code-mixed text,\nnumbers, and special characters from the printed document. It is shown that\nthis approach can reduce the character-level error rate of Tesseract from 6.03\nto 2.61 for Tamil (-3.42% relative change) and 7.61 to 4.74 for Sinhala (-2.87%\nrelative change), as well as the word-level error rate from 39.68 to 20.61 for\nTamil (-19.07% relative change) and 35.04 to 26.58 for Sinhala (-8.46% relative\nchange) on the test set. Also, our newly created parallel corpus consists of\n185.4k, 168.9k, and 181.04k sentences and 2.11M, 2.22M, and 2.33M Words in\nTamil, Sinhala, and English respectively. This study shows that fine-tuning\nTesseract models on multiple new fonts help to understand the texts and\nenhances the performance of the OCR. We made newly trained models and the\nsource code for fine-tuning Tesseract, freely available.\n","authors":["Charangan Vasantharajan","Laksika Tharmalingam","Uthayasanker Thayasivam"],"pdf_url":"https://arxiv.org/pdf/2109.05952v3.pdf","comment":"7 Pages"},{"id":"http://arxiv.org/abs/2212.08120v1","updated":"2022-12-15T20:15:05Z","published":"2022-12-15T20:15:05Z","title":"Injecting Domain Knowledge in Language Models for Task-Oriented Dialogue\n  Systems","summary":"  Pre-trained language models (PLM) have advanced the state-of-the-art across\nNLP applications, but lack domain-specific knowledge that does not naturally\noccur in pre-training data. Previous studies augmented PLMs with symbolic\nknowledge for different downstream NLP tasks. However, knowledge bases (KBs)\nutilized in these studies are usually large-scale and static, in contrast to\nsmall, domain-specific, and modifiable knowledge bases that are prominent in\nreal-world task-oriented dialogue (TOD) systems. In this paper, we showcase the\nadvantages of injecting domain-specific knowledge prior to fine-tuning on TOD\ntasks. To this end, we utilize light-weight adapters that can be easily\nintegrated with PLMs and serve as a repository for facts learned from different\nKBs. To measure the efficacy of proposed knowledge injection methods, we\nintroduce Knowledge Probing using Response Selection (KPRS) -- a probe designed\nspecifically for TOD models. Experiments on KPRS and the response generation\ntask show improvements of knowledge injection with adapters over strong\nbaselines.\n","authors":["Denis Emelin","Daniele Bonadiman","Sawsan Alqahtani","Yi Zhang","Saab Mansour"],"pdf_url":"https://arxiv.org/pdf/2212.08120v1.pdf","comment":"Published at EMNLP 2022 (main conference)"},{"id":"http://arxiv.org/abs/2212.08094v1","updated":"2022-12-15T19:13:42Z","published":"2022-12-15T19:13:42Z","title":"Joint processing of linguistic properties in brains and language models","summary":"  Language models have been shown to be very effective in predicting brain\nrecordings of subjects experiencing complex language stimuli. For a deeper\nunderstanding of this alignment, it is important to understand the alignment\nbetween the detailed processing of linguistic information by the human brain\nversus language models. In NLP, linguistic probing tasks have revealed a\nhierarchy of information processing in neural language models that progresses\nfrom simple to complex with an increase in depth. On the other hand, in\nneuroscience, the strongest alignment with high-level language brain regions\nhas consistently been observed in the middle layers. These findings leave an\nopen question as to what linguistic information actually underlies the observed\nalignment between brains and language models. We investigate this question via\na direct approach, in which we eliminate information related to specific\nlinguistic properties in the language model representations and observe how\nthis intervention affects the alignment with fMRI brain recordings obtained\nwhile participants listened to a story. We investigate a range of linguistic\nproperties (surface, syntactic and semantic) and find that the elimination of\neach one results in a significant decrease in brain alignment across all layers\nof a language model. These findings provide direct evidence for the role of\nspecific linguistic information in the alignment between brain and language\nmodels, and opens new avenues for mapping the joint information processing in\nboth systems.\n","authors":["Subba Reddy Oota","Manish Gupta","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2212.08094v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.08073v1","updated":"2022-12-15T06:19:23Z","published":"2022-12-15T06:19:23Z","title":"Constitutional AI: Harmlessness from AI Feedback","summary":"  As AI systems become more capable, we would like to enlist their help to\nsupervise other AIs. We experiment with methods for training a harmless AI\nassistant through self-improvement, without any human labels identifying\nharmful outputs. The only human oversight is provided through a list of rules\nor principles, and so we refer to the method as 'Constitutional AI'. The\nprocess involves both a supervised learning and a reinforcement learning phase.\nIn the supervised phase we sample from an initial model, then generate\nself-critiques and revisions, and then finetune the original model on revised\nresponses. In the RL phase, we sample from the finetuned model, use a model to\nevaluate which of the two samples is better, and then train a preference model\nfrom this dataset of AI preferences. We then train with RL using the preference\nmodel as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a\nresult we are able to train a harmless but non-evasive AI assistant that\nengages with harmful queries by explaining its objections to them. Both the SL\nand RL methods can leverage chain-of-thought style reasoning to improve the\nhuman-judged performance and transparency of AI decision making. These methods\nmake it possible to control AI behavior more precisely and with far fewer human\nlabels.\n","authors":["Yuntao Bai","Saurav Kadavath","Sandipan Kundu","Amanda Askell","Jackson Kernion","Andy Jones","Anna Chen","Anna Goldie","Azalia Mirhoseini","Cameron McKinnon","Carol Chen","Catherine Olsson","Christopher Olah","Danny Hernandez","Dawn Drain","Deep Ganguli","Dustin Li","Eli Tran-Johnson","Ethan Perez","Jamie Kerr","Jared Mueller","Jeffrey Ladish","Joshua Landau","Kamal Ndousse","Kamile Lukosuite","Liane Lovitt","Michael Sellitto","Nelson Elhage","Nicholas Schiefer","Noemi Mercado","Nova DasSarma","Robert Lasenby","Robin Larson","Sam Ringer","Scott Johnston","Shauna Kravec","Sheer El Showk","Stanislav Fort","Tamera Lanham","Timothy Telleen-Lawton","Tom Conerly","Tom Henighan","Tristan Hume","Samuel R. Bowman","Zac Hatfield-Dodds","Ben Mann","Dario Amodei","Nicholas Joseph","Sam McCandlish","Tom Brown","Jared Kaplan"],"pdf_url":"https://arxiv.org/pdf/2212.08073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08475v1","updated":"2022-12-15T02:28:52Z","published":"2022-12-15T02:28:52Z","title":"Best-Answer Prediction in Q&A Sites Using User Information","summary":"  Community Question Answering (CQA) sites have spread and multiplied\nsignificantly in recent years. Sites like Reddit, Quora, and Stack Exchange are\nbecoming popular amongst people interested in finding answers to diverse\nquestions. One practical way of finding such answers is automatically\npredicting the best candidate given existing answers and comments. Many studies\nwere conducted on answer prediction in CQA but with limited focus on using the\nbackground information of the questionnaires. We address this limitation using\na novel method for predicting the best answers using the questioner's\nbackground information and other features, such as the textual content or the\nrelationships with other participants. Our answer classification model was\ntrained using the Stack Exchange dataset and validated using the Area Under the\nCurve (AUC) metric. The experimental results show that the proposed method\ncomplements previous methods by pointing out the importance of the\nrelationships between users, particularly throughout the level of involvement\nin different communities on Stack Exchange. Furthermore, we point out that\nthere is little overlap between user-relation information and the information\nrepresented by the shallow text features and the meta-features, such as time\ndifferences.\n","authors":["Rafik Hadfi","Ahmed Moustafa","Kai Yoshino","Takayuki Ito"],"pdf_url":"https://arxiv.org/pdf/2212.08475v1.pdf","comment":"22 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.07571v1","updated":"2022-12-15T01:06:55Z","published":"2022-12-15T01:06:55Z","title":"Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual\n  Machine Translation","summary":"  Sparsely gated Mixture of Experts (MoE) models have been shown to be a\ncompute-efficient method to scale model capacity for multilingual machine\ntranslation. However, for low-resource tasks, MoE models severely over-fit. We\nshow effective regularization strategies, namely dropout techniques for MoE\nlayers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods\nthat prevent over-fitting and improve the performance of MoE models on\nlow-resource tasks without adversely affecting high-resource tasks. On a\nmassively multilingual machine translation benchmark, our strategies result in\nabout +1 chrF++ improvement in very low resource language pairs. We perform an\nextensive analysis of the learned MoE routing to better understand the impact\nof our regularization methods and how we can improve them.\n","authors":["Maha Elbayad","Anna Sun","Shruti Bhosale"],"pdf_url":"https://arxiv.org/pdf/2212.07571v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2207.04672"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2212.08071v1","updated":"2022-12-15T18:59:59Z","published":"2022-12-15T18:59:59Z","title":"MAViL: Masked Audio-Video Learners","summary":"  We present Masked Audio-Video Learners (MAViL) to train audio-visual\nrepresentations. Our approach learns with three complementary forms of\nself-supervision: (1) reconstruction of masked audio and video input data, (2)\nintra- and inter-modal contrastive learning with masking, and (3) self-training\nby reconstructing joint audio-video contextualized features learned from the\nfirst two objectives. Pre-training with MAViL not only enables the model to\nperform well in audio-visual classification and retrieval tasks but also\nimproves representations of each modality in isolation, without using\ninformation from the other modality for fine-tuning or inference. Empirically,\nMAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%\naccuracy). For the first time, a self-supervised audio-visual model outperforms\nones that use external supervision on these benchmarks. Code will be available\nsoon.\n","authors":["Po-Yao Huang","Vasu Sharma","Hu Xu","Chaitanya Ryali","Haoqi Fan","Yanghao Li","Shang-Wen Li","Gargi Ghosh","Jitendra Malik","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2212.08071v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2212.08070v1","updated":"2022-12-15T18:59:58Z","published":"2022-12-15T18:59:58Z","title":"NeRF-Art: Text-Driven Neural Radiance Fields Stylization","summary":"  As a powerful representation of 3D scenes, the neural radiance field (NeRF)\nenables high-quality novel view synthesis from multi-view images. Stylizing\nNeRF, however, remains challenging, especially on simulating a text-guided\nstyle with both the appearance and the geometry altered simultaneously. In this\npaper, we present NeRF-Art, a text-guided NeRF stylization approach that\nmanipulates the style of a pre-trained NeRF model with a simple text prompt.\nUnlike previous approaches that either lack sufficient geometry deformations\nand texture details or require meshes to guide the stylization, our method can\nshift a 3D scene to the target style characterized by desired geometry and\nappearance variations without any mesh guidance. This is achieved by\nintroducing a novel global-local contrastive learning strategy, combined with\nthe directional constraint to simultaneously control both the trajectory and\nthe strength of the target style. Moreover, we adopt a weight regularization\nmethod to effectively suppress cloudy artifacts and geometry noises which arise\neasily when the density field is transformed during geometry stylization.\nThrough extensive experiments on various styles, we demonstrate that our method\nis effective and robust regarding both single-view stylization quality and\ncross-view consistency. The code and more results can be found in our project\npage: https://cassiepython.github.io/nerfart/.\n","authors":["Can Wang","Ruixiang Jiang","Menglei Chai","Mingming He","Dongdong Chen","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2212.08070v1.pdf","comment":"Project page: https://cassiepython.github.io/nerfart/"},{"id":"http://arxiv.org/abs/2212.08067v1","updated":"2022-12-15T18:59:54Z","published":"2022-12-15T18:59:54Z","title":"VolRecon: Volume Rendering of Signed Ray Distance Functions for\n  Generalizable Multi-View Reconstruction","summary":"  With the success of neural volume rendering in novel view synthesis, neural\nimplicit reconstruction with volume rendering has become popular. However, most\nmethods optimize per-scene functions and are unable to generalize to novel\nscenes. We introduce VolRecon, a generalizable implicit reconstruction method\nwith Signed Ray Distance Function (SRDF). To reconstruct with fine details and\nlittle noise, we combine projection features, aggregated from multi-view\nfeatures with a view transformer, and volume features interpolated from a\ncoarse global feature volume. A ray transformer computes SRDF values of all the\nsamples along a ray to estimate the surface location, which are used for volume\nrendering of color and depth. Extensive experiments on DTU and ETH3D\ndemonstrate the effectiveness and generalization ability of our method. On DTU,\nour method outperforms SparseNeuS by about 30% in sparse view reconstruction\nand achieves comparable quality as MVSNet in full view reconstruction. Besides,\nour method shows good generalization ability on the large-scale ETH3D\nbenchmark. Project page: https://fangjinhuawang.github.io/VolRecon.\n","authors":["Yufan Ren","Fangjinhua Wang","Tong Zhang","Marc Pollefeys","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2212.08067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08066v1","updated":"2022-12-15T18:59:52Z","published":"2022-12-15T18:59:52Z","title":"Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners","summary":"  Optimization in multi-task learning (MTL) is more challenging than\nsingle-task learning (STL), as the gradient from different tasks can be\ncontradictory. When tasks are related, it can be beneficial to share some\nparameters among them (cooperation). However, some tasks require additional\nparameters with expertise in a specific type of data or discrimination\n(specialization). To address the MTL challenge, we propose Mod-Squad, a new\nmodel that is Modularized into groups of experts (a 'Squad'). This structure\nallows us to formalize cooperation and specialization as the process of\nmatching experts and tasks. We optimize this matching process during the\ntraining of a single model. Specifically, we incorporate mixture of experts\n(MoE) layers into a transformer model, with a new loss that incorporates the\nmutual dependence between tasks and experts. As a result, only a small set of\nexperts are activated for each task. This prevents the sharing of the entire\nbackbone model between all tasks, which strengthens the model, especially when\nthe training set size and the number of tasks scale up. More interestingly, for\neach task, we can extract the small set of experts as a standalone model that\nmaintains the same performance as the large model. Extensive experiments on the\nTaskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5\nvision tasks show the superiority of our approach.\n","authors":["Zitian Chen","Yikang Shen","Mingyu Ding","Zhenfang Chen","Hengshuang Zhao","Erik Learned-Miller","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2212.08066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08062v1","updated":"2022-12-15T18:59:33Z","published":"2022-12-15T18:59:33Z","title":"MetaPortrait: Identity-Preserving Talking Head Generation with Fast\n  Personalized Adaptation","summary":"  In this work, we propose an ID-preserving talking head generation framework,\nwhich advances previous methods in two aspects. First, as opposed to\ninterpolating from sparse flow, we claim that dense landmarks are crucial to\nachieving accurate geometry-aware flow fields. Second, inspired by\nface-swapping methods, we adaptively fuse the source identity during synthesis,\nso that the network better preserves the key characteristics of the image\nportrait. Although the proposed model surpasses prior generation fidelity on\nestablished benchmarks, to further make the talking head generation qualified\nfor real usage, personalized fine-tuning is usually needed. However, this\nprocess is rather computationally demanding that is unaffordable to standard\nusers. To solve this, we propose a fast adaptation model using a meta-learning\napproach. The learned model can be adapted to a high-quality personalized model\nas fast as 30 seconds. Last but not the least, a spatial-temporal enhancement\nmodule is proposed to improve the fine details while ensuring temporal\ncoherency. Extensive experiments prove the significant superiority of our\napproach over the state of the arts in both one-shot and personalized settings.\n","authors":["Bowen Zhang","Chenyang Qi","Pan Zhang","Bo Zhang","HsiangTao Wu","Dong Chen","Qifeng Chen","Yong Wang","Fang Wen"],"pdf_url":"https://arxiv.org/pdf/2212.08062v1.pdf","comment":"Project Page: https://meta-portrait.github.io"},{"id":"http://arxiv.org/abs/2212.08059v1","updated":"2022-12-15T18:59:12Z","published":"2022-12-15T18:59:12Z","title":"Rethinking Vision Transformers for MobileNet Size and Speed","summary":"  With the success of Vision Transformers (ViTs) in computer vision tasks,\nrecent arts try to optimize the performance and complexity of ViTs to enable\nefficient deployment on mobile devices. Multiple approaches are proposed to\naccelerate attention mechanism, improve inefficient designs, or incorporate\nmobile-friendly lightweight convolutions to form hybrid architectures. However,\nViT and its variants still have higher latency or considerably more parameters\nthan lightweight CNNs, even true for the years-old MobileNet. In practice,\nlatency and size are both crucial for efficient deployment on\nresource-constraint hardware. In this work, we investigate a central question,\ncan transformer models run as fast as MobileNet and maintain a similar size? We\nrevisit the design choices of ViTs and propose an improved supernet with low\nlatency and high parameter efficiency. We further introduce a fine-grained\njoint search strategy that can find efficient architectures by optimizing\nlatency and number of parameters simultaneously. The proposed models,\nEfficientFormerV2, achieve about $4\\%$ higher top-1 accuracy than MobileNetV2\nand MobileNetV2$\\times1.4$ on ImageNet-1K with similar latency and parameters.\nWe demonstrate that properly designed and optimized vision transformers can\nachieve high performance with MobileNet-level size and speed.\n","authors":["Yanyu Li","Ju Hu","Yang Wen","Georgios Evangelidis","Kamyar Salahi","Yanzhi Wang","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2212.08059v1.pdf","comment":"Code is available at:\n  https://github.com/snap-research/EfficientFormer"},{"id":"http://arxiv.org/abs/2212.08058v1","updated":"2022-12-15T18:59:07Z","published":"2022-12-15T18:59:07Z","title":"Learning a Fast 3D Spectral Approach to Object Segmentation and Tracking\n  over Space and Time","summary":"  We pose video object segmentation as spectral graph clustering in space and\ntime, with one graph node for each pixel and edges forming local space-time\nneighborhoods. We claim that the strongest cluster in this video graph\nrepresents the salient object. We start by introducing a novel and efficient\nmethod based on 3D filtering for approximating the spectral solution, as the\nprincipal eigenvector of the graph's adjacency matrix, without explicitly\nbuilding the matrix. This key property allows us to have a fast parallel\nimplementation on GPU, orders of magnitude faster than classical approaches for\ncomputing the eigenvector. Our motivation for a spectral space-time clustering\napproach, unique in video semantic segmentation literature, is that such\nclustering is dedicated to preserving object consistency over time, which we\nevaluate using our novel segmentation consistency measure. Further on, we show\nhow to efficiently learn the solution over multiple input feature channels.\nFinally, we extend the formulation of our approach beyond the segmentation\ntask, into the realm of object tracking. In extensive experiments we show\nsignificant improvements over top methods, as well as over powerful ensembles\nthat combine them, achieving state-of-the-art on multiple benchmarks, both for\ntracking and segmentation.\n","authors":["Elena Burceanu","Marius Leordeanu"],"pdf_url":"https://arxiv.org/pdf/2212.08058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08057v1","updated":"2022-12-15T18:58:56Z","published":"2022-12-15T18:58:56Z","title":"Real-Time Neural Light Field on Mobile Devices","summary":"  Recent efforts in Neural Rendering Fields (NeRF) have shown impressive\nresults on novel view synthesis by utilizing implicit neural representation to\nrepresent 3D scenes. Due to the process of volumetric rendering, the inference\nspeed for NeRF is extremely slow, limiting the application scenarios of\nutilizing NeRF on resource-constrained hardware, such as mobile devices. Many\nworks have been conducted to reduce the latency of running NeRF models.\nHowever, most of them still require high-end GPU for acceleration or extra\nstorage memory, which is all unavailable on mobile devices. Another emerging\ndirection utilizes the neural light field (NeLF) for speedup, as only one\nforward pass is performed on a ray to predict the pixel color. Nevertheless, to\nreach a similar rendering quality as NeRF, the network in NeLF is designed with\nintensive computation, which is not mobile-friendly. In this work, we propose\nan efficient network that runs in real-time on mobile devices for neural\nrendering. We follow the setting of NeLF to train our network. Unlike existing\nworks, we introduce a novel network architecture that runs efficiently on\nmobile devices with low latency and small size, i.e., saving $15\\times \\sim\n24\\times$ storage compared with MobileNeRF. Our model achieves high-resolution\ngeneration while maintaining real-time inference for both synthetic and\nreal-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering\none $1008\\times756$ image of real 3D scenes. Additionally, we achieve similar\nimage quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.\n$25.91$ on the real-world forward-facing dataset).\n","authors":["Junli Cao","Huan Wang","Pavlo Chemerys","Vladislav Shakhrai","Ju Hu","Yun Fu","Denys Makoviichuk","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2212.08057v1.pdf","comment":"Project page: https://snap-research.github.io/MobileR2L/"},{"id":"http://arxiv.org/abs/2212.08051v1","updated":"2022-12-15T18:56:53Z","published":"2022-12-15T18:56:53Z","title":"Objaverse: A Universe of Annotated 3D Objects","summary":"  Massive data corpora like WebText, Wikipedia, Conceptual Captions,\nWebImageText, and LAION have propelled recent dramatic progress in AI. Large\nneural models trained on such datasets produce impressive results and top many\nof today's benchmarks. A notable omission within this family of large-scale\ndatasets is 3D data. Despite considerable interest and potential applications\nin 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with\nlimited diversity of object categories. Addressing this gap, we present\nObjaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models\nwith descriptive captions, tags, and animations. Objaverse improves upon\npresent day 3D repositories in terms of scale, number of categories, and in the\nvisual diversity of instances within a category. We demonstrate the large\npotential of Objaverse via four diverse applications: training generative 3D\nmodels, improving tail category segmentation on the LVIS benchmark, training\nopen-vocabulary object-navigation models for Embodied AI, and creating a new\nbenchmark for robustness analysis of vision models. Objaverse can open new\ndirections for research and enable new applications across the field of AI.\n","authors":["Matt Deitke","Dustin Schwenk","Jordi Salvador","Luca Weihs","Oscar Michel","Eli VanderBilt","Ludwig Schmidt","Kiana Ehsani","Aniruddha Kembhavi","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2212.08051v1.pdf","comment":"Website: objaverse.allenai.org"},{"id":"http://arxiv.org/abs/2212.08045v1","updated":"2022-12-15T18:52:08Z","published":"2022-12-15T18:52:08Z","title":"Image-and-Language Understanding from Pixels Only","summary":"  Multimodal models are becoming increasingly effective, in part due to unified\ncomponents, such as the Transformer architecture. However, multimodal models\nstill often consist of many task- and modality-specific pieces and training\nprocedures. For example, CLIP (Radford et al., 2021) trains independent text\nand image towers via a contrastive loss. We explore an additional unification:\nthe use of a pure pixel-based model to perform image, text, and multimodal\ntasks. Our model is trained with contrastive loss alone, so we call it\nCLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both\nregular images and text rendered as images. CLIPPO performs image-based tasks\nsuch as retrieval and zero-shot image classification almost as well as CLIP,\nwith half the number of parameters and no text-specific tower or embedding.\nWhen trained jointly via image-text contrastive learning and next-sentence\ncontrastive learning, CLIPPO can perform well on natural language understanding\ntasks, without any word-level loss (language modelling or masked language\nmodelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can\nobtain good accuracy in visual question answering, simply by rendering the\nquestion and image together. Finally, we exploit the fact that CLIPPO does not\nrequire a tokenizer to show that it can achieve strong performance on\nmultilingual multimodal retrieval without\n","authors":["Michael Tschannen","Basil Mustafa","Neil Houlsby"],"pdf_url":"https://arxiv.org/pdf/2212.08045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08044v1","updated":"2022-12-15T18:52:03Z","published":"2022-12-15T18:52:03Z","title":"Are Multimodal Models Robust to Image and Text Perturbations?","summary":"  Multimodal image-text models have shown remarkable performance in the past\nfew years. However, evaluating their robustness against distribution shifts is\ncrucial before adopting them in real-world applications. In this paper, we\ninvestigate the robustness of 9 popular open-sourced image-text models under\ncommon perturbations on five tasks (image-text retrieval, visual reasoning,\nvisual entailment, image captioning, and text-to-image generation). In\nparticular, we propose several new multimodal robustness benchmarks by applying\n17 image perturbation and 16 text perturbation techniques on top of existing\ndatasets. We observe that multimodal models are not robust to image and text\nperturbations, especially to image perturbations. Among the tested perturbation\nmethods, character-level perturbations constitute the most severe distribution\nshift for text, and zoom blur is the most severe shift for image data. We also\nintroduce two new robustness metrics (MMI and MOR) for proper evaluations of\nmultimodal models. We hope our extensive study sheds light on new directions\nfor the development of robust multimodal models.\n","authors":["Jielin Qiu","Yi Zhu","Xingjian Shi","Florian Wenzel","Zhiqiang Tang","Ding Zhao","Bo Li","Mu Li"],"pdf_url":"https://arxiv.org/pdf/2212.08044v1.pdf","comment":"The project webpage is at: https://mmrobustness.github.io/"},{"id":"http://arxiv.org/abs/2212.08013v1","updated":"2022-12-15T18:18:38Z","published":"2022-12-15T18:18:38Z","title":"FlexiViT: One Model for All Patch Sizes","summary":"  Vision Transformers convert images to sequences by slicing them into patches.\nThe size of these patches controls a speed/accuracy tradeoff, with smaller\npatches leading to higher accuracy at greater computational cost, but changing\nthe patch size typically requires retraining the model. In this paper, we\ndemonstrate that simply randomizing the patch size at training time leads to a\nsingle set of weights that performs well across a wide range of patch sizes,\nmaking it possible to tailor the model to different compute budgets at\ndeployment time. We extensively evaluate the resulting model, which we call\nFlexiViT, on a wide range of tasks, including classification, image-text\nretrieval, open-world detection, panoptic segmentation, and semantic\nsegmentation, concluding that it usually matches, and sometimes outperforms,\nstandard ViT models trained at a single patch size in an otherwise identical\nsetup. Hence, FlexiViT training is a simple drop-in improvement for ViT that\nmakes it easy to add compute-adaptive capabilities to most models relying on a\nViT backbone architecture. Code and pre-trained models are available at\nhttps://github.com/google-research/big_vision\n","authors":["Lucas Beyer","Pavel Izmailov","Alexander Kolesnikov","Mathilde Caron","Simon Kornblith","Xiaohua Zhai","Matthias Minderer","Michael Tschannen","Ibrahim Alabdulmohsin","Filip Pavetic"],"pdf_url":"https://arxiv.org/pdf/2212.08013v1.pdf","comment":"Code and pre-trained models available at\n  https://github.com/google-research/big_vision. All authors made significant\n  technical contributions"},{"id":"http://arxiv.org/abs/2212.08008v1","updated":"2022-12-15T18:14:51Z","published":"2022-12-15T18:14:51Z","title":"A New Deep Boosted CNN and Ensemble Learning based IoT Malware Detection","summary":"  Security issues are threatened in various types of networks, especially in\nthe Internet of Things (IoT) environment that requires early detection. IoT is\nthe network of real-time devices like home automation systems and can be\ncontrolled by open-source android devices, which can be an open ground for\nattackers. Attackers can access the network, initiate a different kind of\nsecurity breach, and compromises network control. Therefore, timely detecting\nthe increasing number of sophisticated malware attacks is the challenge to\nensure the credibility of network protection. In this regard, we have developed\na new malware detection framework, Deep Squeezed-Boosted and Ensemble Learning\n(DSBEL), comprised of novel Squeezed-Boosted Boundary-Region\nSplit-Transform-Merge (SB-BR-STM) CNN and ensemble learning. The proposed\nS.T.M. block employs multi-path dilated convolutional, Boundary, and regional\noperations to capture the homogenous and heterogeneous global malicious\npatterns. Moreover, diverse feature maps are achieved using transfer learning\nand multi-path-based squeezing and boosting at initial and final levels to\nlearn minute pattern variations. Finally, the boosted discriminative features\nare extracted from the developed deep SB-BR-STM CNN and provided to the\nensemble classifiers (SVM, M.L.P., and AdaboostM1) to improve the hybrid\nlearning generalization. The performance analysis of the proposed DSBEL\nframework and SB-BR-STM CNN against the existing techniques have been evaluated\nby the IOT_Malware dataset on standard performance measures. Evaluation results\nshow progressive performance as 98.50% accuracy, 97.12% F1-Score, 91.91% MCC,\n95.97 % Recall, and 98.42 % Precision. The proposed malware analysis framework\nis helpful for the timely detection of malicious activity and suggests future\nstrategies.\n","authors":["Saddam Hussain Khan","Wasi Ullah"],"pdf_url":"https://arxiv.org/pdf/2212.08008v1.pdf","comment":"20 pages, 10 figures, 6 tables; Corresponding saddamhkhan@ueas.edu.pk"},{"id":"http://arxiv.org/abs/2201.10737v5","updated":"2022-12-15T17:45:50Z","published":"2022-01-26T03:50:02Z","title":"Class-Aware Adversarial Transformers for Medical Image Segmentation","summary":"  Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: (1) existing\nmethods fail to capture the important features of the images due to the naive\ntokenization scheme; (2) the models suffer from information loss because they\nonly consider single-scale feature representations; and (3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCASTformer, a novel type of adversarial transformers, for 2D medical image\nsegmentation. First, we take advantage of the pyramid structure to construct\nmulti-scale representations and handle multi-scale variations. We then design a\nnovel class-aware transformer module to better learn the discriminative regions\nof objects with semantic structures. Lastly, we utilize an adversarial training\nstrategy that boosts segmentation accuracy and correspondingly allows a\ntransformer-based discriminator to capture high-level semantically correlated\ncontents and low-level anatomical features. Our experiments demonstrate that\nCASTformer dramatically outperforms previous state-of-the-art transformer-based\napproaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in\nDice over previous models. Further qualitative experiments provide a more\ndetailed picture of the model's inner workings, shed light on the challenges in\nimproved transparency, and demonstrate that transfer learning can greatly\nimprove performance and reduce the size of medical image datasets in training,\nmaking CASTformer a strong starting point for downstream medical image analysis\ntasks.\n","authors":["Chenyu You","Ruihan Zhao","Fenglin Liu","Siyuan Dong","Sandeep Chinchali","Ufuk Topcu","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2201.10737v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07992v1","updated":"2022-12-15T17:44:31Z","published":"2022-12-15T17:44:31Z","title":"Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks","summary":"  Designing powerful adversarial attacks is of paramount importance for the\nevaluation of $\\ell_p$-bounded adversarial defenses. Projected Gradient Descent\n(PGD) is one of the most effective and conceptually simple algorithms to\ngenerate such adversaries. The search space of PGD is dictated by the steepest\nascent directions of an objective. Despite the plethora of objective function\nchoices, there is no universally superior option and robustness overestimation\nmay arise from ill-suited objective selection. Driven by this observation, we\npostulate that the combination of different objectives through a simple loss\nalternating scheme renders PGD more robust towards design choices. We\nexperimentally verify this assertion on a synthetic-data example and by\nevaluating our proposed method across 25 different $\\ell_{\\infty}$-robust\nmodels and 3 datasets. The performance improvement is consistent, when compared\nto the single loss counterparts. In the CIFAR-10 dataset, our strongest\nadversarial attack outperforms all of the white-box components of AutoAttack\n(AA) ensemble, as well as the most powerful attacks existing on the literature,\nachieving state-of-the-art results in the computational budget of our study\n($T=100$, no restarts).\n","authors":["Nikolaos Antoniou","Efthymios Georgiou","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2212.07992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07983v1","updated":"2022-12-15T17:31:54Z","published":"2022-12-15T17:31:54Z","title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners","summary":"  Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n","authors":["Yan-Bo Lin","Yi-Lin Sung","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2212.07983v1.pdf","comment":"project page: https://genjib.github.io/project_page/LAVISH/"},{"id":"http://arxiv.org/abs/2211.16359v3","updated":"2022-12-15T16:58:04Z","published":"2022-11-29T16:42:53Z","title":"POLCOVID: a multicenter multiclass chest X-ray database (Poland,\n  2020-2021)","summary":"  The outbreak of the SARS-CoV-2 pandemic has put healthcare systems worldwide\nto their limits, resulting in increased waiting time for diagnosis and required\nmedical assistance. With chest radiographs (CXR) being one of the most common\nCOVID-19 diagnosis methods, many artificial intelligence tools for image-based\nCOVID-19 detection have been developed, often trained on a small number of\nimages from COVID-19-positive patients. Thus, the need for high-quality and\nwell-annotated CXR image databases increased. This paper introduces POLCOVID\ndataset, containing chest X-ray (CXR) images of patients with COVID-19 or\nother-type pneumonia, and healthy individuals gathered from 15 Polish\nhospitals. The original radiographs are accompanied by the preprocessed images\nlimited to the lung area and the corresponding lung masks obtained with the\nsegmentation model. Moreover, the manually created lung masks are provided for\na part of POLCOVID dataset and the other four publicly available CXR image\ncollections. POLCOVID dataset can help in pneumonia or COVID-19 diagnosis,\nwhile the set of matched images and lung masks may serve for the development of\nlung segmentation solutions.\n","authors":["Aleksandra Suwalska","Joanna Tobiasz","Wojciech Prazuch","Marek Socha","Pawel Foszner","Damian Piotrowski","Katarzyna Gruszczynska","Magdalena Sliwinska","Jerzy Walecki","Tadeusz Popiela","Grzegorz Przybylski","Mateusz Nowak","Piotr Fiedor","Malgorzata Pawlowska","Robert Flisiak","Krzysztof Simon","Gabriela Zapolska","Barbara Gizycka","Edyta Szurowska","POLCOVID Study Group","Michal Marczyk","Andrzej Cieszanowski","Joanna Polanska"],"pdf_url":"https://arxiv.org/pdf/2211.16359v3.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2201.03644v2","updated":"2022-12-15T16:24:29Z","published":"2022-01-10T20:55:59Z","title":"3D Segmentation with Fully Trainable Gabor Kernels and Pearson's\n  Correlation Coefficient","summary":"  The convolutional layer and loss function are two fundamental components in\ndeep learning. Because of the success of conventional deep learning kernels,\nthe less versatile Gabor kernels become less popular despite the fact that they\ncan provide abundant features at different frequencies, orientations, and\nscales with much fewer parameters. For existing loss functions for multi-class\nimage segmentation, there is usually a tradeoff among accuracy, robustness to\nhyperparameters, and manual weight selections for combining different losses.\nTherefore, to gain the benefits of using Gabor kernels while keeping the\nadvantage of automatic feature generation in deep learning, we propose a fully\ntrainable Gabor-based convolutional layer where all Gabor parameters are\ntrainable through backpropagation. Furthermore, we propose a loss function\nbased on the Pearson's correlation coefficient, which is accurate, robust to\nlearning rates, and does not require manual weight selections. Experiments on\n43 3D brain magnetic resonance images with 19 anatomical structures show that,\nusing the proposed loss function with a proper combination of conventional and\nGabor-based kernels, we can train a network with only 1.6 million parameters to\nachieve an average Dice coefficient of 83%. This size is 44 times smaller than\nthe original V-Net which has 71 million parameters. This paper demonstrates the\npotentials of using learnable parametric kernels in deep learning for 3D\nsegmentation.\n","authors":["Ken C. L. Wong","Mehdi Moradi"],"pdf_url":"https://arxiv.org/pdf/2201.03644v2.pdf","comment":"This paper was accepted by the International Workshop on Machine\n  Learning in Medical Imaging (MLMI 2022)"},{"id":"http://arxiv.org/abs/2212.07923v1","updated":"2022-12-15T15:55:44Z","published":"2022-12-15T15:55:44Z","title":"The Effects of Character-Level Data Augmentation on Style-Based Dating\n  of Historical Manuscripts","summary":"  Identifying the production dates of historical manuscripts is one of the main\ngoals for paleographers when studying ancient documents. Automatized methods\ncan provide paleographers with objective tools to estimate dates more\naccurately. Previously, statistical features have been used to date digitized\nhistorical manuscripts based on the hypothesis that handwriting styles change\nover periods. However, the sparse availability of such documents poses a\nchallenge in obtaining robust systems. Hence, the research of this article\nexplores the influence of data augmentation on the dating of historical\nmanuscripts. Linear Support Vector Machines were trained with k-fold\ncross-validation on textural and grapheme-based features extracted from\nhistorical manuscripts of different collections, including the Medieval\nPaleographical Scale, early Aramaic manuscripts, and the Dead Sea Scrolls.\nResults show that training models with augmented data improve the performance\nof historical manuscripts dating by 1% - 3% in cumulative scores. Additionally,\nthis indicates further enhancement possibilities by considering models specific\nto the features and the documents' scripts.\n","authors":["Lisa Koopmans","Maruf A. Dhali","Lambert Schomaker"],"pdf_url":"https://arxiv.org/pdf/2212.07923v1.pdf","comment":"Accepted after the peer-review process for ICPRAM 2023; scheduled to\n  be presented on 22 February 2023"},{"id":"http://arxiv.org/abs/2212.07911v1","updated":"2022-12-15T15:43:42Z","published":"2022-12-15T15:43:42Z","title":"Urban Scene Semantic Segmentation with Low-Cost Coarse Annotation","summary":"  For best performance, today's semantic segmentation methods use large and\ncarefully labeled datasets, requiring expensive annotation budgets. In this\nwork, we show that coarse annotation is a low-cost but highly effective\nalternative for training semantic segmentation models. Considering the urban\nscene segmentation scenario, we leverage cheap coarse annotations for\nreal-world captured data, as well as synthetic data to train our model and show\ncompetitive performance compared with finely annotated real-world data.\nSpecifically, we propose a coarse-to-fine self-training framework that\ngenerates pseudo labels for unlabeled regions of the coarsely annotated data,\nusing synthetic data to improve predictions around the boundaries between\nsemantic classes, and using cross-domain data augmentation to increase\ndiversity. Our extensive experimental results on Cityscapes and BDD100k\ndatasets demonstrate that our method achieves a significantly better\nperformance vs annotation cost tradeoff, yielding a comparable performance to\nfully annotated data with only a small fraction of the annotation budget. Also,\nwhen used as pretraining, our framework performs better compared to the\nstandard fully supervised setting.\n","authors":["Anurag Das","Yongqin Xian","Yang He","Zeynep Akata","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2212.07911v1.pdf","comment":"Accepted at WACV 2023"},{"id":"http://arxiv.org/abs/2212.07907v1","updated":"2022-12-15T15:39:55Z","published":"2022-12-15T15:39:55Z","title":"Automatic vehicle trajectory data reconstruction at scale","summary":"  Vehicle trajectory data has received increasing research attention over the\npast decades. With the technological sensing improvements such as\nhigh-resolution video cameras, in-vehicle radars and lidars, abundant\nindividual and contextual traffic data is now available. However, though the\ndata quantity is massive, it is by itself of limited utility for traffic\nresearch because of noise and systematic sensing errors, thus necessitates\nproper processing to ensure data quality. We draw particular attention to\nextracting high-resolution vehicle trajectory data from video cameras as\ntraffic monitoring cameras are becoming increasingly ubiquitous. We explore\nmethods for automatic trajectory data reconciliation, given \"raw\" vehicle\ndetection and tracking information from automatic video processing algorithms.\nWe propose a pipeline including a) an online data association algorithm to\nmatch fragments that are associated to the same object (vehicle), which is\nformulated as a min-cost network flow problem of a graph, and b) a trajectory\nreconciliation method formulated as a quadratic program to enhance raw\ndetection data. The pipeline leverages vehicle dynamics and physical\nconstraints to associate tracked objects when they become fragmented, remove\nmeasurement noise on trajectories and impute missing data due to\nfragmentations. The accuracy is benchmarked on a sample of manually-labeled\ndata, which shows that the reconciled trajectories improve the accuracy on all\nthe tested input data for a wide range of measures. An online version of the\nreconciliation pipeline is implemented and will be applied in a continuous\nvideo processing system running on a camera network covering a 4-mile stretch\nof Interstate-24 near Nashville, Tennessee.\n","authors":["Yanbing Wang","Derek Gloudemans","Zi Nean Teoh","Lisa Liu","Gergely Zachár","William Barbour","Daniel Work"],"pdf_url":"https://arxiv.org/pdf/2212.07907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07900v1","updated":"2022-12-15T15:35:25Z","published":"2022-12-15T15:35:25Z","title":"EVAL: Explainable Video Anomaly Localization","summary":"  We develop a novel framework for single-scene video anomaly localization that\nallows for human-understandable reasons for the decisions the system makes. We\nfirst learn general representations of objects and their motions (using deep\nnetworks) and then use these representations to build a high-level,\nlocation-dependent model of any particular scene. This model can be used to\ndetect anomalies in new videos of the same scene. Importantly, our approach is\nexplainable - our high-level appearance and motion features can provide\nhuman-understandable reasons for why any part of a video is classified as\nnormal or anomalous. We conduct experiments on standard video anomaly detection\ndatasets (Street Scene, CUHK Avenue, ShanghaiTech and UCSD Ped1, Ped2) and show\nsignificant improvements over the previous state-of-the-art.\n","authors":["Ashish Singh","Michael J. Jones","Erik Learned-Miller"],"pdf_url":"https://arxiv.org/pdf/2212.07900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10535v2","updated":"2022-12-15T15:25:28Z","published":"2022-06-21T17:08:23Z","title":"EpiGRAF: Rethinking training of 3D GANs","summary":"  A very recent trend in generative modeling is building 3D-aware generators\nfrom 2D image collections. To induce the 3D bias, such models typically rely on\nvolumetric rendering, which is expensive to employ at high resolutions. During\nthe past months, there appeared more than 10 works that address this scaling\nissue by training a separate 2D decoder to upsample a low-resolution image (or\na feature tensor) produced from a pure 3D generator. But this solution comes at\na cost: not only does it break multi-view consistency (i.e. shape and texture\nchange when the camera moves), but it also learns the geometry in a low\nfidelity. In this work, we show that it is possible to obtain a high-resolution\n3D generator with SotA image quality by following a completely different route\nof simply training the model patch-wise. We revisit and improve this\noptimization scheme in two ways. First, we design a location- and scale-aware\ndiscriminator to work on patches of different proportions and spatial\npositions. Second, we modify the patch sampling strategy based on an annealed\nbeta distribution to stabilize training and accelerate the convergence. The\nresulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D\ngenerator, and we test it on four datasets (two introduced in this work) at\n$256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality,\nhigh-fidelity geometry and trains ${\\approx} 2.5 \\times$ faster than the\nupsampler-based counterparts. Project website:\nhttps://universome.github.io/epigraf.\n","authors":["Ivan Skorokhodov","Sergey Tulyakov","Yiqun Wang","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2206.10535v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07891v1","updated":"2022-12-15T15:20:58Z","published":"2022-12-15T15:20:58Z","title":"Emergent Behaviors in Multi-Agent Target Acquisition","summary":"  Only limited studies and superficial evaluations are available on agents'\nbehaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using\nReinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)\ngame, which shares task goals with target acquisition, and we create different\nadversarial scenarios by replacing RL-trained pursuers' policies with two\ndistinct (non-RL) analytical strategies. Using heatmaps of agents' positions\n(state-space variable) over time, we are able to categorize an RL-trained\nevader's behaviors. The novelty of our approach entails the creation of an\ninfluential feature set that reveals underlying data regularities, which allow\nus to classify an agent's behavior. This classification may aid in catching the\n(enemy) targets by enabling us to identify and predict their behaviors, and\nwhen extended to pursuers, this approach towards identifying teammates'\nbehavior may allow agents to coordinate more effectively.\n","authors":["Piyush K. Sharma","Erin Zaroukian","Derrik E. Asher","Bryson Howell"],"pdf_url":"https://arxiv.org/pdf/2212.07891v1.pdf","comment":"This article appeared in the news at:\n  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior"},{"id":"http://arxiv.org/abs/2212.07890v1","updated":"2022-12-15T15:19:09Z","published":"2022-12-15T15:19:09Z","title":"Full Contextual Attention for Multi-resolution Transformers in Semantic\n  Segmentation","summary":"  Transformers have proved to be very effective for visual recognition tasks.\nIn particular, vision transformers construct compressed global representations\nthrough self-attention and learnable class tokens. Multi-resolution\ntransformers have shown recent successes in semantic segmentation but can only\ncapture local interactions in high-resolution feature maps. This paper extends\nthe notion of global tokens to build GLobal Attention Multi-resolution (GLAM)\ntransformers. GLAM is a generic module that can be integrated into most\nexisting transformer backbones. GLAM includes learnable global tokens, which\nunlike previous methods can model interactions between all image regions, and\nextracts powerful representations during training. Extensive experiments show\nthat GLAM-Swin or GLAM-Swin-UNet exhibit substantially better performances than\ntheir vanilla counterparts on ADE20K and Cityscapes. Moreover, GLAM can be used\nto segment large 3D medical images, and GLAM-nnFormer achieves new\nstate-of-the-art performance on the BCV dataset.\n","authors":["Loic Themyr","Clement Rambour","Nicolas Thome","Toby Collins","Alexandre Hostettler"],"pdf_url":"https://arxiv.org/pdf/2212.07890v1.pdf","comment":"Winter Conference on Applications of Computer Vision (WACV 2023)"},{"id":"http://arxiv.org/abs/2212.07886v1","updated":"2022-12-15T15:11:38Z","published":"2022-12-15T15:11:38Z","title":"Meta-Learned Kernel For Blind Super-Resolution Kernel Estimation","summary":"  Recent image degradation estimation methods have enabled single-image\nsuper-resolution (SR) approaches to better upsample real-world images. Among\nthese methods, explicit kernel estimation approaches have demonstrated\nunprecedented performance at handling unknown degradations. Nonetheless, a\nnumber of limitations constrain their efficacy when used by downstream SR\nmodels. Specifically, this family of methods yields i) excessive inference time\ndue to long per-image adaptation times and ii) inferior image fidelity due to\nkernel mismatch. In this work, we introduce a learning-to-learn approach that\nmeta-learns from the information contained in a distribution of images, thereby\nenabling significantly faster adaptation to new images with substantially\nimproved performance in both kernel estimation and image fidelity.\nSpecifically, we meta-train a kernel-generating GAN, named MetaKernelGAN, on a\nrange of tasks, such that when a new image is presented, the generator starts\nfrom an informed kernel estimate and the discriminator starts with a strong\ncapability to distinguish between patch distributions. Compared with\nstate-of-the-art methods, our experiments show that MetaKernelGAN better\nestimates the magnitude and covariance of the kernel, leading to\nstate-of-the-art blind SR results within a similar computational regime when\ncombined with a non-blind SR model. Through supervised learning of an\nunsupervised learner, our method maintains the generalizability of the\nunsupervised learner, improves the optimization stability of kernel estimation,\nand hence image adaptation, and leads to a faster inference with a speedup\nbetween 14.24 to 102.1x over existing methods.\n","authors":["Royson Lee","Rui Li","Stylianos I. Venieris","Timothy Hospedales","Ferenc Huszár","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2212.07886v1.pdf","comment":"Preprint: under review"},{"id":"http://arxiv.org/abs/2212.07867v1","updated":"2022-12-15T14:34:12Z","published":"2022-12-15T14:34:12Z","title":"Localizing Scan Targets from Human Pose for Autonomous Lung Ultrasound\n  Imaging","summary":"  Ultrasound is progressing toward becoming an affordable and versatile\nsolution to medical imaging. With the advent of COVID-19 global pandemic, there\nis a need to fully automate ultrasound imaging as it requires trained operators\nin close proximity to patients for long period of time. In this work, we\ninvestigate the important yet seldom-studied problem of scan target\nlocalization, under the setting of lung ultrasound imaging. We propose a purely\nvision-based, data driven method that incorporates learning-based computer\nvision techniques. We combine a human pose estimation model with a specially\ndesigned regression model to predict the lung ultrasound scan targets, and\ndeploy multiview stereo vision to enhance the consistency of 3D target\nlocalization. While related works mostly focus on phantom experiments, we\ncollect data from 30 human subjects for testing. Our method attains an accuracy\nlevel of 15.52 (9.47) mm for probe positioning and 4.32 (3.69){\\deg} for probe\norientation, with a success rate above 80% under an error threshold of 25mm for\nall scan targets. Moreover, our approach can serve as a general solution to\nother types of ultrasound modalities. The code for implementation has been\nreleased.\n","authors":["Jianzhi Long","Jicang Cai","Abdullah Al-Battal","Shiwei Jin","Jing Zhang","Dacheng Tao","Truong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2212.07867v1.pdf","comment":"First arxiv submission"},{"id":"http://arxiv.org/abs/2212.07855v1","updated":"2022-12-15T14:22:49Z","published":"2022-12-15T14:22:49Z","title":"QueryPose: Sparse Multi-Person Pose Regression via Spatial-Aware\n  Part-Level Query","summary":"  We propose a sparse end-to-end multi-person pose regression framework, termed\nQueryPose, which can directly predict multi-person keypoint sequences from the\ninput image. The existing end-to-end methods rely on dense representations to\npreserve the spatial detail and structure for precise keypoint localization.\nHowever, the dense paradigm introduces complex and redundant post-processes\nduring inference. In our framework, each human instance is encoded by several\nlearnable spatial-aware part-level queries associated with an instance-level\nquery. First, we propose the Spatial Part Embedding Generation Module (SPEGM)\nthat considers the local spatial attention mechanism to generate several\nspatial-sensitive part embeddings, which contain spatial details and structural\ninformation for enhancing the part-level queries. Second, we introduce the\nSelective Iteration Module (SIM) to adaptively update the sparse part-level\nqueries via the generated spatial-sensitive part embeddings stage-by-stage.\nBased on the two proposed modules, the part-level queries are able to fully\nencode the spatial details and structural information for precise keypoint\nregression. With the bipartite matching, QueryPose avoids the hand-designed\npost-processes and surpasses the existing dense end-to-end methods with 73.6 AP\non MS COCO mini-val set and 72.7 AP on CrowdPose test set. Code is available at\nhttps://github.com/buptxyb666/QueryPose.\n","authors":["Yabo Xiao","Kai Su","Xiaojuan Wang","Dongdong Yu","Lei Jin","Mingshu He","Zehuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2212.07855v1.pdf","comment":"Published on NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07849v1","updated":"2022-12-15T14:18:47Z","published":"2022-12-15T14:18:47Z","title":"DETR4D: Direct Multi-View 3D Object Detection with Sparse Attention","summary":"  3D object detection with surround-view images is an essential task for\nautonomous driving. In this work, we propose DETR4D, a Transformer-based\nframework that explores sparse attention and direct feature query for 3D object\ndetection in multi-view images. We design a novel projective cross-attention\nmechanism for query-image interaction to address the limitations of existing\nmethods in terms of geometric cue exploitation and information loss for\ncross-view objects. In addition, we introduce a heatmap generation technique\nthat bridges 3D and 2D spaces efficiently via query initialization.\nFurthermore, unlike the common practice of fusing intermediate spatial features\nfor temporal aggregation, we provide a new perspective by introducing a novel\nhybrid approach that performs cross-frame fusion over past object queries and\nimage features, enabling efficient and robust modeling of temporal information.\nExtensive experiments on the nuScenes dataset demonstrate the effectiveness and\nefficiency of the proposed DETR4D.\n","authors":["Zhipeng Luo","Changqing Zhou","Gongjie Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2212.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00842v2","updated":"2022-12-15T14:01:12Z","published":"2022-12-01T20:00:00Z","title":"3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models","summary":"  Diffusion models have shown great promise for image generation, beating GANs\nin terms of generation diversity, with comparable image quality. However, their\napplication to 3D shapes has been limited to point or voxel representations\nthat can in practice not accurately represent a 3D surface. We propose a\ndiffusion model for neural implicit representations of 3D shapes that operates\nin the latent space of an auto-decoder. This allows us to generate diverse and\nhigh quality 3D surfaces. We additionally show that we can condition our model\non images or text to enable image-to-3D generation and text-to-3D generation\nusing CLIP embeddings. Furthermore, adding noise to the latent codes of\nexisting shapes allows us to explore shape variations.\n","authors":["Gimin Nam","Mariem Khlifi","Andrew Rodriguez","Alberto Tono","Linqi Zhou","Paul Guerrero"],"pdf_url":"https://arxiv.org/pdf/2212.00842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07839v1","updated":"2022-12-15T13:52:03Z","published":"2022-12-15T13:52:03Z","title":"TeTIm-Eval: a novel curated evaluation data set for comparing\n  text-to-image models","summary":"  Evaluating and comparing text-to-image models is a challenging problem.\nSignificant advances in the field have recently been made, piquing interest of\nvarious industrial sectors. As a consequence, a gold standard in the field\nshould cover a variety of tasks and application contexts. In this paper a novel\nevaluation approach is experimented, on the basis of: (i) a curated data set,\nmade by high-quality royalty-free image-text pairs, divided into ten\ncategories; (ii) a quantitative metric, the CLIP-score, (iii) a human\nevaluation task to distinguish, for a given text, the real and the generated\nimages. The proposed method has been applied to the most recent models, i.e.,\nDALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early\nexperimental results show that the accuracy of the human judgement is fully\ncoherent with the CLIP-score. The dataset has been made available to the\npublic.\n","authors":["Federico A. Galatolo","Mario G. C. A. Cimino","Edoardo Cogotti"],"pdf_url":"https://arxiv.org/pdf/2212.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07834v1","updated":"2022-12-15T13:43:11Z","published":"2022-12-15T13:43:11Z","title":"Unsupervised Object Localization: Observing the Background to Discover\n  Objects","summary":"  Recent advances in self-supervised visual representation learning have paved\nthe way for unsupervised methods tackling tasks such as object discovery and\ninstance segmentation. However, discovering objects in an image with no\nsupervision is a very hard task; what are the desired objects, when to separate\nthem into parts, how many are there, and of what classes? The answers to these\nquestions depend on the tasks and datasets of evaluation. In this work, we take\na different approach and propose to look for the background instead. This way,\nthe salient objects emerge as a by-product without any strong assumption on\nwhat an object should be. We propose FOUND, a simple model made of a single\n$conv1\\times1$ initialized with coarse background masks extracted from\nself-supervised patch-based representations. After fast training and refining\nthese seed masks, the model reaches state-of-the-art results on unsupervised\nsaliency detection and object discovery benchmarks. Moreover, we show that our\napproach yields good results in the unsupervised semantic segmentation\nretrieval task. The code to reproduce our results is available at\nhttps://github.com/valeoai/FOUND.\n","authors":["Oriane Siméoni","Chloé Sekkat","Gilles Puy","Antonin Vobecky","Éloi Zablocki","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2212.07834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.13279v4","updated":"2022-12-15T12:58:41Z","published":"2022-01-31T14:42:35Z","title":"UQGAN: A Unified Model for Uncertainty Quantification of Deep\n  Classifiers trained via Conditional GANs","summary":"  We present an approach to quantifying both aleatoric and epistemic\nuncertainty for deep neural networks in image classification, based on\ngenerative adversarial networks (GANs). While most works in the literature that\nuse GANs to generate out-of-distribution (OoD) examples only focus on the\nevaluation of OoD detection, we present a GAN based approach to learn a\nclassifier that produces proper uncertainties for OoD examples as well as for\nfalse positives (FPs). Instead of shielding the entire in-distribution data\nwith GAN generated OoD examples which is state-of-the-art, we shield each class\nseparately with out-of-class examples generated by a conditional GAN and\ncomplement this with a one-vs-all image classifier. In our experiments, in\nparticular on CIFAR10, CIFAR100 and Tiny ImageNet, we improve over the OoD\ndetection and FP detection performance of state-of-the-art GAN-training based\nclassifiers. Furthermore, we also find that the generated GAN examples do not\nsignificantly affect the calibration error of our classifier and result in a\nsignificant gain in model accuracy.\n","authors":["Philipp Oberdiek","Gernot A. Fink","Matthias Rottmann"],"pdf_url":"https://arxiv.org/pdf/2201.13279v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07778v1","updated":"2022-12-15T12:54:21Z","published":"2022-12-15T12:54:21Z","title":"Efficient Visual Computing with Camera RAW Snapshots","summary":"  Conventional cameras capture image irradiance on a sensor and convert it to\nRGB images using an image signal processor (ISP). The images can then be used\nfor photography or visual computing tasks in a variety of applications, such as\npublic safety surveillance and autonomous driving. One can argue that since RAW\nimages contain all the captured information, the conversion of RAW to RGB using\nan ISP is not necessary for visual computing. In this paper, we propose a novel\n$\\rho$-Vision framework to perform high-level semantic understanding and\nlow-level compression using RAW images without the ISP subsystem used for\ndecades. Considering the scarcity of available RAW image datasets, we first\ndevelop an unpaired CycleR2R network based on unsupervised CycleGAN to train\nmodular unrolled ISP and inverse ISP (invISP) models using unpaired RAW and RGB\nimages. We can then flexibly generate simulated RAW images (simRAW) using any\nexisting RGB image dataset and finetune different models originally trained for\nthe RGB domain to process real-world camera RAW images. We demonstrate object\ndetection and image compression capabilities in RAW-domain using RAW-domain\nYOLOv3 and RAW image compressor (RIC) on snapshots from various cameras.\nQuantitative results reveal that RAW-domain task inference provides better\ndetection accuracy and compression compared to RGB-domain processing.\nFurthermore, the proposed \\r{ho}-Vision generalizes across various camera\nsensors and different task-specific models. Additional advantages of the\nproposed $\\rho$-Vision that eliminates the ISP are the potential reductions in\ncomputations and processing times.\n","authors":["Zhihao Li","Ming Lu","Xu Zhang","Xin Feng","M. Salman Asif","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2212.07778v1.pdf","comment":"home page: https://njuvision.github.io/rho-vision"},{"id":"http://arxiv.org/abs/2212.07776v1","updated":"2022-12-15T12:53:26Z","published":"2022-12-15T12:53:26Z","title":"Enhancing Indic Handwritten Text Recognition Using Global Semantic\n  Information","summary":"  Handwritten Text Recognition (HTR) is more interesting and challenging than\nprinted text due to uneven variations in the handwriting style of the writers,\ncontent, and time. HTR becomes more challenging for the Indic languages because\nof (i) multiple characters combined to form conjuncts which increase the number\nof characters of respective languages, and (ii) near to 100 unique basic\nUnicode characters in each Indic script. Recently, many recognition methods\nbased on the encoder-decoder framework have been proposed to handle such\nproblems. They still face many challenges, such as image blur and incomplete\ncharacters due to varying writing styles and ink density. We argue that most\nencoder-decoder methods are based on local visual features without explicit\nglobal semantic information.\n  In this work, we enhance the performance of Indic handwritten text\nrecognizers using global semantic information. We use a semantic module in an\nencoder-decoder framework for extracting global semantic information to\nrecognize the Indic handwritten texts. The semantic information is used in both\nthe encoder for supervision and the decoder for initialization. The semantic\ninformation is predicted from the word embedding of a pre-trained language\nmodel. Extensive experiments demonstrate that the proposed framework achieves\nstate-of-the-art results on handwritten texts of ten Indic languages.\n","authors":["Ajoy Mondal","C. V. Jawahar"],"pdf_url":"https://arxiv.org/pdf/2212.07776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07768v1","updated":"2022-12-15T12:46:31Z","published":"2022-12-15T12:46:31Z","title":"A scalable framework for annotating photovoltaic cell defects in\n  electroluminescence images","summary":"  The correct functioning of photovoltaic (PV) cells is critical to ensuring\nthe optimal performance of a solar plant. Anomaly detection techniques for PV\ncells can result in significant cost savings in operation and maintenance\n(O&M). Recent research has focused on deep learning techniques for\nautomatically detecting anomalies in Electroluminescence (EL) images. Automated\nanomaly annotations can improve current O&M methodologies and help develop\ndecision-making systems to extend the life-cycle of the PV cells and predict\nfailures. This paper addresses the lack of anomaly segmentation annotations in\nthe literature by proposing a combination of state-of-the-art data-driven\ntechniques to create a Golden Standard benchmark. The proposed method stands\nout for (1) its adaptability to new PV cell types, (2) cost-efficient\nfine-tuning, and (3) leverage public datasets to generate advanced annotations.\nThe methodology has been validated in the annotation of a widely used dataset,\nobtaining a reduction of the annotation cost by 60%.\n","authors":["Urtzi Otamendi","Inigo Martinez","Igor G. Olaizola","Marco Quartulli"],"pdf_url":"https://arxiv.org/pdf/2212.07768v1.pdf","comment":"10 pages, 10 figures, 1 table, accepted at IEEE Transactions on\n  Industrial Informatics"},{"id":"http://arxiv.org/abs/2206.13500v2","updated":"2022-12-15T12:37:36Z","published":"2022-06-27T17:59:45Z","title":"Neural Neural Textures Make Sim2Real Consistent","summary":"  Unpaired image translation algorithms can be used for sim2real tasks, but\nmany fail to generate temporally consistent results. We present a new approach\nthat combines differentiable rendering with image translation to achieve\ntemporal consistency over indefinite timescales, using surface consistency\nlosses and \\emph{neural neural textures}. We call this algorithm TRITON\n(Texture Recovering Image Translation Network): an unsupervised, end-to-end,\nstateless sim2real algorithm that leverages the underlying 3D geometry of input\nscenes by generating realistic-looking learnable neural textures. By settling\non a particular texture for the objects in a scene, we ensure consistency\nbetween frames statelessly. Unlike previous algorithms, TRITON is not limited\nto camera movements -- it can handle the movement of objects as well, making it\nuseful for downstream tasks such as robotic manipulation.\n","authors":["Ryan Burgert","Jinghuan Shang","Xiang Li","Michael Ryoo"],"pdf_url":"https://arxiv.org/pdf/2206.13500v2.pdf","comment":"9 pages, 10 figures (without references or appendix); 16 pages, 16\n  figures (with appendix)"},{"id":"http://arxiv.org/abs/2212.07766v1","updated":"2022-12-15T12:36:49Z","published":"2022-12-15T12:36:49Z","title":"DeepLSD: Line Segment Detection and Refinement with Deep Image Gradients","summary":"  Line segments are ubiquitous in our human-made world and are increasingly\nused in vision tasks. They are complementary to feature points thanks to their\nspatial extent and the structural information they provide. Traditional line\ndetectors based on the image gradient are extremely fast and accurate, but lack\nrobustness in noisy images and challenging conditions. Their learned\ncounterparts are more repeatable and can handle challenging images, but at the\ncost of a lower accuracy and a bias towards wireframe lines. We propose to\ncombine traditional and learned approaches to get the best of both worlds: an\naccurate and robust line detector that can be trained in the wild without\nground truth lines. Our new line segment detector, DeepLSD, processes images\nwith a deep network to generate a line attraction field, before converting it\nto a surrogate image gradient magnitude and angle, which is then fed to any\nexisting handcrafted line detector. Additionally, we propose a new optimization\ntool to refine line segments based on the attraction field and vanishing\npoints. This refinement improves the accuracy of current deep detectors by a\nlarge margin. We demonstrate the performance of our method on low-level line\ndetection metrics, as well as on several downstream tasks using multiple\nchallenging datasets. The source code and models are available at\nhttps://github.com/cvg/DeepLSD.\n","authors":["Rémi Pautrat","Daniel Barath","Viktor Larsson","Martin R. Oswald","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2212.07766v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2207.09280v3","updated":"2022-12-15T12:20:33Z","published":"2022-07-19T13:49:30Z","title":"Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain\n  Adaptation","summary":"  Universal domain adaptation (UDA) aims to transfer the knowledge of common\nclasses from source domain to target domain without any prior knowledge on the\nlabel set, which requires to distinguish the unknown samples from the known\nones in the target domain. Recent methods preferred to increase the\ninter-sample affinity within a known class, while they ignored the inter-sample\naffinity between the unknown samples and the known ones. This paper reveals\nthat exploiting such inter-sample affinity can significantly improve the\nperformance of UDA and proposes a knowability-aware UDA framework based on it.\nFirst, we estimate the knowability of each target sample by searching its\nneighboring samples in the source domain. Then, we propose an auto-thresholding\nscheme applied to the estimated knowability to determine whether a target\nsample is unknown or known. Next, in addition to increasing the inter-sample\naffinity within each known class like previous methods, we design new losses\nbased on the estimated knowability to reduce the inter-sample affinity between\nthe unknown target samples and the known ones. Finally, experiments on four\npublic datasets demonstrate that our method significantly outperforms existing\nstate-of-the-art methods.\n","authors":["Yifan Wang","Lin Zhang","Ran Song","Lin Ma","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.09280v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07754v1","updated":"2022-12-15T12:18:13Z","published":"2022-12-15T12:18:13Z","title":"Event-based Visual Tracking in Dynamic Environments","summary":"  Visual object tracking under challenging conditions of motion and light can\nbe hindered by the capabilities of conventional cameras, prone to producing\nimages with motion blur. Event cameras are novel sensors suited to robustly\nperform vision tasks under these conditions. However, due to the nature of\ntheir output, applying them to object detection and tracking is non-trivial. In\nthis work, we propose a framework to take advantage of both event cameras and\noff-the-shelf deep learning for object tracking. We show that reconstructing\nevent data into intensity frames improves the tracking performance in\nconditions under which conventional cameras fail to provide acceptable results.\n","authors":["Irene Perez-Salesa","Rodrigo Aldana-Lopez","Carlos Sagues"],"pdf_url":"https://arxiv.org/pdf/2212.07754v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in ROBOT2022: Fifth Iberian Robotics Conference"},{"id":"http://arxiv.org/abs/2212.07751v1","updated":"2022-12-15T12:09:02Z","published":"2022-12-15T12:09:02Z","title":"Combating Uncertainty and Class Imbalance in Facial Expression\n  Recognition","summary":"  Recognition of facial expression is a challenge when it comes to computer\nvision. The primary reasons are class imbalance due to data collection and\nuncertainty due to inherent noise such as fuzzy facial expressions and\ninconsistent labels. However, current research has focused either on the\nproblem of class imbalance or on the problem of uncertainty, ignoring the\nintersection of how to address these two problems. Therefore, in this paper, we\npropose a framework based on Resnet and Attention to solve the above problems.\nWe design weight for each class. Through the penalty mechanism, our model will\npay more attention to the learning of small samples during training, and the\nresulting decrease in model accuracy can be improved by a Convolutional Block\nAttention Module (CBAM). Meanwhile, our backbone network will also learn an\nuncertain feature for each sample. By mixing uncertain features between\nsamples, the model can better learn those features that can be used for\nclassification, thus suppressing uncertainty. Experiments show that our method\nsurpasses most basic methods in terms of accuracy on facial expression data\nsets (e.g., AffectNet, RAF-DB), and it also solves the problem of class\nimbalance well.\n","authors":["Jiaxiang Fan","Jian Zhou","Xiaoyu Deng","Huabin Wang","Liang Tao","Hon Keung Kwan"],"pdf_url":"https://arxiv.org/pdf/2212.07751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11582v3","updated":"2022-12-15T11:18:41Z","published":"2022-10-20T20:51:48Z","title":"Deep Learning for Diagonal Earlobe Crease Detection","summary":"  An article published on Medical News Today in June 2022 presented a\nfundamental question in its title: Can an earlobe crease predict heart attacks?\nThe author explained that end arteries supply the heart and ears. In other\nwords, if they lose blood supply, no other arteries can take over, resulting in\ntissue damage. Consequently, some earlobes have a diagonal crease, line, or\ndeep fold that resembles a wrinkle. In this paper, we take a step toward\ndetecting this specific marker, commonly known as DELC or Frank's Sign. For\nthis reason, we have made the first DELC dataset available to the public. In\naddition, we have investigated the performance of numerous cutting-edge\nbackbones on annotated photos. Experimentally, we demonstrate that it is\npossible to solve this challenge by combining pre-trained encoders with a\ncustomized classifier to achieve 97.7% accuracy. Moreover, we have analyzed the\nbackbone trade-off between performance and size, estimating MobileNet as the\nmost promising encoder.\n","authors":["Sara L. Almonacid-Uribe","Oliverio J. Santana","Daniel Hernández-Sosa","David Freire-Obregón"],"pdf_url":"https://arxiv.org/pdf/2210.11582v3.pdf","comment":"Accepted at 12th International Conference on Pattern Recognition\n  Applications (ICPRAM 2023)"},{"id":"http://arxiv.org/abs/2204.02810v4","updated":"2022-12-15T11:17:07Z","published":"2022-04-06T13:22:24Z","title":"Expression-preserving face frontalization improves visually assisted\n  speech processing","summary":"  Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a frontalization\nmethodology that preserves non-rigid facial deformations in order to boost the\nperformance of visually assisted speech communication. The method alternates\nbetween the estimation of (i)~the rigid transformation (scale, rotation, and\ntranslation) and (ii)~the non-rigid deformation between an arbitrarily-viewed\nface and a face model. The method has two important merits: it can deal with\nnon-Gaussian errors in the data and it incorporates a dynamical face\ndeformation model. For that purpose, we use the generalized Student\nt-distribution in combination with a linear dynamic system in order to account\nfor both rigid head motions and time-varying facial deformations caused by\nspeech production. We propose to use the zero-mean normalized cross-correlation\n(ZNCC) score to evaluate the ability of the method to preserve facial\nexpressions. The method is thoroughly evaluated and compared with several state\nof the art methods, either based on traditional geometric models or on deep\nlearning. Moreover, we show that the method, when incorporated into deep\nlearning pipelines, namely lip reading and speech enhancement, improves word\nrecognition and speech intelligibilty scores by a considerable margin.\nSupplemental material is accessible at\nhttps://team.inria.fr/robotlearn/research/facefrontalization/\n","authors":["Zhiqi Kang","Mostafa Sadeghi","Radu Horaud","Xavier Alameda-Pineda"],"pdf_url":"https://arxiv.org/pdf/2204.02810v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.00538"},{"id":"http://arxiv.org/abs/2212.07729v1","updated":"2022-12-15T11:15:14Z","published":"2022-12-15T11:15:14Z","title":"HUM3DIL: Semi-supervised Multi-modal 3D Human Pose Estimation for\n  Autonomous Driving","summary":"  Autonomous driving is an exciting new industry, posing important research\nquestions. Within the perception module, 3D human pose estimation is an\nemerging technology, which can enable the autonomous vehicle to perceive and\nunderstand the subtle and complex behaviors of pedestrians. While hardware\nsystems and sensors have dramatically improved over the decades -- with cars\npotentially boasting complex LiDAR and vision systems and with a growing\nexpansion of the available body of dedicated datasets for this newly available\ninformation -- not much work has been done to harness these novel signals for\nthe core problem of 3D human pose estimation. Our method, which we coin HUM3DIL\n(HUMan 3D from Images and LiDAR), efficiently makes use of these complementary\nsignals, in a semi-supervised fashion and outperforms existing methods with a\nlarge margin. It is a fast and compact model for onboard deployment.\nSpecifically, we embed LiDAR points into pixel-aligned multi-modal features,\nwhich we pass through a sequence of Transformer refinement stages. Quantitative\nexperiments on the Waymo Open Dataset support these claims, where we achieve\nstate-of-the-art results on the task of 3D pose estimation.\n","authors":["Andrei Zanfir","Mihai Zanfir","Alexander Gorban","Jingwei Ji","Yin Zhou","Dragomir Anguelov","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2212.07729v1.pdf","comment":"Published at the 6th Conference on Robot Learning (CoRL 2022),\n  Auckland, New Zealand"},{"id":"http://arxiv.org/abs/2212.07724v1","updated":"2022-12-15T11:07:23Z","published":"2022-12-15T11:07:23Z","title":"Attention-based Multiple Instance Learning for Survival Prediction on\n  Lung Cancer Tissue Microarrays","summary":"  Attention-based multiple instance learning (AMIL) algorithms have proven to\nbe successful in utilizing gigapixel whole-slide images (WSIs) for a variety of\ndifferent computational pathology tasks such as outcome prediction and cancer\nsubtyping problems. We extended an AMIL approach to the task of survival\nprediction by utilizing the classical Cox partial likelihood as a loss\nfunction, converting the AMIL model into a nonlinear proportional hazards\nmodel. We applied the model to tissue microarray (TMA) slides of 330 lung\ncancer patients. The results show that AMIL approaches can handle very small\namounts of tissue from a TMA and reach similar C-index performance compared to\nestablished survival prediction methods trained with highly discriminative\nclinical factors such as age, cancer grade, and cancer stage\n","authors":["Jonas Ammeling","Lars-Henning Schmidt","Jonathan Ganz","Tanja Niedermair","Christoph Brochhausen-Delius","Christian Schulz","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2212.07724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07721v1","updated":"2022-12-15T10:56:47Z","published":"2022-12-15T10:56:47Z","title":"Deep Learning-Based Automatic Assessment of AgNOR-scores in\n  Histopathology Images","summary":"  Nucleolar organizer regions (NORs) are parts of the DNA that are involved in\nRNA transcription. Due to the silver affinity of associated proteins,\nargyrophilic NORs (AgNORs) can be visualized using silver-based staining. The\naverage number of AgNORs per nucleus has been shown to be a prognostic factor\nfor predicting the outcome of many tumors. Since manual detection of AgNORs is\nlaborious, automation is of high interest. We present a deep learning-based\npipeline for automatically determining the AgNOR-score from histopathological\nsections. An additional annotation experiment was conducted with six\npathologists to provide an independent performance evaluation of our approach.\nAcross all raters and images, we found a mean squared error of 0.054 between\nthe AgNOR- scores of the experts and those of the model, indicating that our\napproach offers performance comparable to humans.\n","authors":["Jonathan Ganz","Karoline Lipnik","Jonas Ammeling","Barbara Richter","Chloé Puget","Eda Parlak","Laura Diehl","Robert Klopfleisch","Taryn A. Donovan","Matti Kiupel","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2212.07721v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2212.07700v1","updated":"2022-12-15T10:23:32Z","published":"2022-12-15T10:23:32Z","title":"Colab NAS: Obtaining lightweight task-specific convolutional neural\n  networks following Occam's razor","summary":"  The current trend of applying transfer learning from CNNs trained on large\ndatasets can be an overkill when the target application is a custom and\ndelimited problem with enough data to train a network from scratch. On the\nother hand, the training of custom and lighter CNNs requires expertise, in the\nfrom-scratch case, and or high-end resources, as in the case of hardware-aware\nneural architecture search (HW NAS), limiting access to the technology by\nnon-habitual NN developers.\n  For this reason, we present Colab NAS, an affordable HW NAS technique for\nproducing lightweight task-specific CNNs. Its novel derivative-free search\nstrategy, inspired by Occam's razor, allows it to obtain state-of-the-art\nresults on the Visual Wake Word dataset in just 4.5 GPU hours using free online\nGPU services such as Google Colaboratory and Kaggle Kernel.\n","authors":["Andrea Mattia Garavagno","Daniele Leonardis","Antonio Frisoli"],"pdf_url":"https://arxiv.org/pdf/2212.07700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07699v1","updated":"2022-12-15T10:20:42Z","published":"2022-12-15T10:20:42Z","title":"Retrieval-based Disentanglement with Distant Supervision","summary":"  Disentangled representation learning remains challenging as ground truth\nfactors of variation do not naturally exist. To address this, we present\nVocabulary Disentanglement Retrieval~(VDR), a simple yet effective\nretrieval-based disentanglement framework that leverages nature language as\ndistant supervision. Our approach is built upon the widely-used bi-encoder\narchitecture with disentanglement heads and is trained on data-text pairs that\nare readily available on the web or in existing datasets. This makes our\napproach task- and modality-agnostic with potential for a wide range of\ndownstream applications. We conduct experiments on 16 datasets in both\ntext-to-text and cross-modal scenarios and evaluate VDR in a zero-shot setting.\nWith the incorporation of disentanglement heads and a minor increase in\nparameters, VDR achieves significant improvements over the base retriever it is\nbuilt upon, with a 9% higher on NDCG@10 scores in zero-shot text-to-text\nretrieval and an average of 13% higher recall in cross-modal retrieval. In\ncomparison to other baselines, VDR outperforms them in most tasks, while also\nimproving explainability and efficiency.\n","authors":["Jiawei Zhou","Xiaoguang Li","Lifeng Shang","Xin Jiang","Qun Liu","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.01992v7","updated":"2022-12-15T10:15:34Z","published":"2022-06-04T13:45:08Z","title":"CAINNFlow: Convolutional block Attention modules and Invertible Neural\n  Networks Flow for anomaly detection and localization tasks","summary":"  Detection of object anomalies is crucial in industrial processes, but\nunsupervised anomaly detection and localization is particularly important due\nto the difficulty of obtaining a large number of defective samples and the\nunpredictable types of anomalies in real life. Among the existing unsupervised\nanomaly detection and localization methods, the NF-based scheme has achieved\nbetter results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and\n$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze\nthe input visual features from 2D flattening to 1D, destroying the spatial\nlocation relationship in the feature map and losing the spatial structure\ninformation. In order to retain and effectively extract spatial structure\ninformation, we design in this study a complex function model with alternating\nCBAM embedded in a stacked $3\\times3$ full convolution, which is able to retain\nand effectively extract spatial structure information in the normalized flow\nmodel. Extensive experimental results on the MVTec AD dataset show that\nCAINNFlow achieves advanced levels of accuracy and inference efficiency based\non CNN and Transformer backbone networks as feature extractors, and CAINNFlow\nachieves a pixel-level AUC of $98.64\\%$ for anomaly detection in MVTec AD.\n","authors":["Ruiqing Yan","Fan Zhang","Mengyuan Huang","Wu Liu","Dongyu Hu","Jinfeng Li","Qiang Liu","Jinrong Jiang","Qianjin Guo","Linghan Zheng"],"pdf_url":"https://arxiv.org/pdf/2206.01992v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15444v2","updated":"2022-12-15T10:03:25Z","published":"2022-11-23T17:59:12Z","title":"DAMO-YOLO : A Report on Real-Time Object Detection Design","summary":"  In this report, we present a fast and accurate object detection method dubbed\nDAMO-YOLO, which achieves higher performance than the state-of-the-art YOLO\nseries. DAMO-YOLO is extended from YOLO with some new technologies, including\nNeural Architecture Search (NAS), efficient Reparameterized Generalized-FPN\n(RepGFPN), a lightweight head with AlignedOTA label assignment, and\ndistillation enhancement. In particular, we use MAE-NAS, a method guided by the\nprinciple of maximum entropy, to search our detection backbone under the\nconstraints of low latency and high performance, producing ResNet-like /\nCSP-like structures with spatial pyramid pooling and focus modules. In the\ndesign of necks and heads, we follow the rule of \"large neck, small head\". We\nimport Generalized-FPN with accelerated queen-fusion to build the detector neck\nand upgrade its CSPNet with efficient layer aggregation networks (ELAN) and\nreparameterization. Then we investigate how detector head size affects\ndetection performance and find that a heavy neck with only one task projection\nlayer would yield better results. In addition, AlignedOTA is proposed to solve\nthe misalignment problem in label assignment. And a distillation schema is\nintroduced to improve performance to a higher level. Based on these new techs,\nwe build a suite of models at various scales to meet the needs of different\nscenarios, i.e., DAMO-YOLO-Tiny/Small/Medium. They can achieve 43.0/46.8/50.0\nmAPs on COCO with the latency of 2.78/3.83/5.62 ms on T4 GPUs respectively. The\ncode is available at https://github.com/tinyvision/damo-yolo.\n","authors":["Xianzhe Xu","Yiqi Jiang","Weihua Chen","Yilun Huang","Yuan Zhang","Xiuyu Sun"],"pdf_url":"https://arxiv.org/pdf/2211.15444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07692v1","updated":"2022-12-15T09:57:19Z","published":"2022-12-15T09:57:19Z","title":"CNN-based real-time 2D-3D deformable registration from a single X-ray\n  projection","summary":"  Purpose: The purpose of this paper is to present a method for real-time 2D-3D\nnon-rigid registration using a single fluoroscopic image. Such a method can\nfind applications in surgery, interventional radiology and radiotherapy. By\nestimating a three-dimensional displacement field from a 2D X-ray image,\nanatomical structures segmented in the preoperative scan can be projected onto\nthe 2D image, thus providing a mixed reality view. Methods: A dataset composed\nof displacement fields and 2D projections of the anatomy is generated from the\npreoperative scan. From this dataset, a neural network is trained to recover\nthe unknown 3D displacement field from a single projection image. Results: Our\nmethod is validated on lung 4D CT data at different stages of the lung\ndeformation. The training is performed on a 3D CT using random (non\ndomain-specific) diffeomorphic deformations, to which perturbations mimicking\nthe pose uncertainty are added. The model achieves a mean TRE over a series of\nlandmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.\nConclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid\nregistration is presented. This method is able to cope with pose estimation\nuncertainties, making it applicable to actual clinical scenarios, such as lung\nsurgery, where the C-arm pose is planned before the intervention.\n","authors":["François Lecomte","Jean-Louis Dillenseger","Stéphane Cotin"],"pdf_url":"https://arxiv.org/pdf/2212.07692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10911v4","updated":"2022-12-15T09:21:22Z","published":"2022-06-22T08:33:52Z","title":"Influence of uncertainty estimation techniques on false-positive\n  reduction in liver lesion detection","summary":"  Deep learning techniques show success in detecting objects in medical images,\nbut still suffer from false-positive predictions that may hinder accurate\ndiagnosis. The estimated uncertainty of the neural network output has been used\nto flag incorrect predictions. We study the role played by features computed\nfrom neural network uncertainty estimates and shape-based features computed\nfrom binary predictions in reducing false positives in liver lesion detection\nby developing a classification-based post-processing step for different\nuncertainty estimation methods. We demonstrate an improvement in the lesion\ndetection performance of the neural network (with respect to F1-score) for all\nuncertainty estimation methods on two datasets, comprising abdominal MR and CT\nimages, respectively. We show that features computed from neural network\nuncertainty estimates tend not to contribute much toward reducing false\npositives. Our results show that factors like class imbalance (true over false\npositive ratio) and shape-based features extracted from uncertainty maps play\nan important role in distinguishing false positive from true positive\npredictions. Our code can be found at https://github.com/ishaanb92/FPCPipeline.\n","authors":["Ishaan Bhat","Josien P. W. Pluim","Max A. Viergever","Hugo J. Kuijf"],"pdf_url":"https://arxiv.org/pdf/2206.10911v4.pdf","comment":"Accepted for publication in the Journal of Machine Learning for\n  Biomedical Imaging (MELBA)"},{"id":"http://arxiv.org/abs/2004.08554v3","updated":"2022-12-15T09:09:19Z","published":"2020-04-18T08:25:25Z","title":"Realistic Large-Scale Fine-Depth Dehazing Dataset from 3D Videos","summary":"  Image dehazing is one of the important and popular topics in computer vision\nand machine learning. A reliable real-time dehazing method with reliable\nperformance is highly desired for many applications such as autonomous driving,\nsecurity surveillance, etc. While recent learning-based methods require\ndatasets containing pairs of hazy images and clean ground truth, it is\nimpossible to capture them in real scenes. Many existing works compromise this\ndifficulty to generate hazy images by rendering the haze from depth on common\nRGBD datasets using the haze imaging model. However, there is still a gap\nbetween the synthetic datasets and real hazy images as large datasets with\nhigh-quality depth are mostly indoor and depth maps for outdoor are imprecise.\nIn this paper, we complement the existing datasets with a new, large, and\ndiverse dehazing dataset containing real outdoor scenes from High-Definition\n(HD) 3D movies. We select a large number of high-quality frames of real outdoor\nscenes and render haze on them using depth from stereo. Our dataset is clearly\nmore realistic and more diversified with better visual quality than existing\nones. More importantly, we demonstrate that using this dataset greatly improves\nthe dehazing performance on real scenes. In addition to the dataset, we also\nevaluate a series state of the art methods on the proposed benchmarking\ndatasets.\n","authors":["Ruoteng Li","Xiaoyi Zhang","Shaodi You","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2004.08554v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07672v1","updated":"2022-12-15T09:05:26Z","published":"2022-12-15T09:05:26Z","title":"Summary-Oriented Vision Modeling for Multimodal Abstractive\n  Summarization","summary":"  The goal of multimodal abstractive summarization (MAS) is to produce a\nconcise summary given the multimodal data (text and vision). Existing studies\non MAS mainly focus on how to effectively use the extracted visual features,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the quality of the visual features to\nthe summary, which may limit the model performance especially in the low- and\nzero-resource scenarios. In this paper, we propose to improve the summary\nquality through summary-oriented visual features. To this end, we devise two\nauxiliary tasks including \\emph{vision to summary task} and \\emph{masked image\nmodeling task}. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios.\n","authors":["Yunlong Liang","Fandong Meng","Jinan Xu","Jiaan Wang","Yufeng Chen","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07672v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2212.07671v1","updated":"2022-12-15T09:04:45Z","published":"2022-12-15T09:04:45Z","title":"Multi-task Fusion for Efficient Panoptic-Part Segmentation","summary":"  In this paper, we introduce a novel network that generates semantic,\ninstance, and part segmentation using a shared encoder and effectively fuses\nthem to achieve panoptic-part segmentation. Unifying these three segmentation\nproblems allows for mutually improved and consistent representation learning.\nTo fuse the predictions of all three heads efficiently, we introduce a\nparameter-free joint fusion module that dynamically balances the logits and\nfuses them to create panoptic-part segmentation. Our method is evaluated on the\nCityscapes Panoptic Parts (CPP) and Pascal Panoptic Parts (PPP) datasets. For\nCPP, the PartPQ of our proposed model with joint fusion surpasses the previous\nstate-of-the-art by 1.6 and 4.7 percentage points for all areas and segments\nwith parts, respectively. On PPP, our joint fusion outperforms a model using\nthe previous top-down merging strategy by 3.3 percentage points in PartPQ and\n10.5 percentage points in PartPQ for partitionable classes.\n","authors":["Sravan Kumar Jagadeesh","René Schuster","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2212.07671v1.pdf","comment":"Accepted in ICPRAM 2023"},{"id":"http://arxiv.org/abs/2212.07664v1","updated":"2022-12-15T08:42:25Z","published":"2022-12-15T08:42:25Z","title":"Writer Retrieval and Writer Identification in Greek Papyri","summary":"  The analysis of digitized historical manuscripts is typically addressed by\npaleographic experts. Writer identification refers to the classification of\nknown writers while writer retrieval seeks to find the writer by means of image\nsimilarity in a dataset of images. While automatic writer\nidentification/retrieval methods already provide promising results for many\nhistorical document types, papyri data is very challenging due to the fiber\nstructures and severe artifacts. Thus, an important step for an improved writer\nidentification is the preprocessing and feature sampling process. We\ninvestigate several methods and show that a good binarization is key to an\nimproved writer identification in papyri writings. We focus mainly on writer\nretrieval using unsupervised feature methods based on traditional or\nself-supervised-based methods. It is, however, also comparable to the state of\nthe art supervised deep learning-based method in the case of writer\nclassification/re-identification.\n","authors":["Vincent Christlein","Isabelle Marthot-Santaniello","Martin Mayr","Anguelos Nicolaou","Mathias Seuret"],"pdf_url":"https://arxiv.org/pdf/2212.07664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07652v1","updated":"2022-12-15T08:19:02Z","published":"2022-12-15T08:19:02Z","title":"Body-Part Joint Detection and Association via Extended Object\n  Representation","summary":"  The detection of human body and its related parts (e.g., face, head or hands)\nhave been intensively studied and greatly improved since the breakthrough of\ndeep CNNs. However, most of these detectors are trained independently, making\nit a challenging task to associate detected body parts with people. This paper\nfocuses on the problem of joint detection of human body and its corresponding\nparts. Specifically, we propose a novel extended object representation that\nintegrates the center location offsets of body or its parts, and construct a\ndense single-stage anchor-based Body-Part Joint Detector (BPJDet). Body-part\nassociations in BPJDet are embedded into the unified representation which\ncontains both the semantic and geometric information. Therefore, BPJDet does\nnot suffer from error-prone association post-matching, and has a better\naccuracy-speed trade-off. Furthermore, BPJDet can be seamlessly generalized to\njointly detect any body part. To verify the effectiveness and superiority of\nour method, we conduct extensive experiments on the CityPersons, CrowdHuman and\nBodyHands datasets. The proposed BPJDet detector achieves state-of-the-art\nassociation performance on these three benchmarks while maintains high accuracy\nof detection. Code will be released to facilitate further studies.\n","authors":["Huayi Zhou","Fei Jiang","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2212.07652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07651v1","updated":"2022-12-15T08:18:37Z","published":"2022-12-15T08:18:37Z","title":"Two-stage Contextual Transformer-based Convolutional Neural Network for\n  Airway Extraction from CT Images","summary":"  Accurate airway extraction from computed tomography (CT) images is a critical\nstep for planning navigation bronchoscopy and quantitative assessment of\nairway-related chronic obstructive pulmonary disease (COPD). The existing\nmethods are challenging to sufficiently segment the airway, especially the\nhigh-generation airway, with the constraint of the limited label and cannot\nmeet the clinical use in COPD. We propose a novel two-stage 3D contextual\ntransformer-based U-Net for airway segmentation using CT images. The method\nconsists of two stages, performing initial and refined airway segmentation. The\ntwo-stage model shares the same subnetwork with different airway masks as\ninput. Contextual transformer block is performed both in the encoder and\ndecoder path of the subnetwork to finish high-quality airway segmentation\neffectively. In the first stage, the total airway mask and CT images are\nprovided to the subnetwork, and the intrapulmonary airway mask and\ncorresponding CT scans to the subnetwork in the second stage. Then the\npredictions of the two-stage method are merged as the final prediction.\nExtensive experiments were performed on in-house and multiple public datasets.\nQuantitative and qualitative analysis demonstrate that our proposed method\nextracted much more branches and lengths of the tree while accomplishing\nstate-of-the-art airway segmentation performance. The code is available at\nhttps://github.com/zhaozsq/airway_segmentation.\n","authors":["Yanan Wu","Shuiqing Zhao","Shouliang Qi","Jie Feng","Haowen Pang","Runsheng Chang","Long Bai","Mengqi Li","Shuyue Xia","Wei Qian","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2212.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07648v1","updated":"2022-12-15T08:06:03Z","published":"2022-12-15T08:06:03Z","title":"Relightable Neural Human Assets from Multi-view Gradient Illuminations","summary":"  Human modeling and relighting are two fundamental problems in computer vision\nand graphics, where high-quality datasets can largely facilitate related\nresearch. However, most existing human datasets only provide multi-view human\nimages captured under the same illumination. Although valuable for modeling\ntasks, they are not readily used in relighting problems. To promote research in\nboth fields, in this paper, we present UltraStage, a new 3D human dataset that\ncontains more than 2K high-quality human assets captured under both multi-view\nand multi-illumination settings. Specifically, for each example, we provide 32\nsurrounding views illuminated with one white light and two gradient\nilluminations. In addition to regular multi-view images, gradient illuminations\nhelp recover detailed surface normal and spatially-varying material maps,\nenabling various relighting applications. Inspired by recent advances in neural\nrepresentation, we further interpret each example into a neural human asset\nwhich allows novel view synthesis under arbitrary lighting conditions. We show\nour neural human assets can achieve extremely high capture performance and are\ncapable of representing fine details such as facial wrinkles and cloth folds.\nWe also validate UltraStage in single image relighting tasks, training neural\nnetworks with virtual relighted data from neural assets and demonstrating\nrealistic rendering improvements over prior arts. UltraStage will be publicly\navailable to the community to stimulate significant future developments in\nvarious human modeling and rendering tasks.\n","authors":["Taotao Zhou","Kai He","Di Wu","Teng Xu","Qixuan Zhang","Kuixiang Shao","Wenzheng Chen","Lan Xu","Jingyi Yi"],"pdf_url":"https://arxiv.org/pdf/2212.07648v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.07646v1","updated":"2022-12-15T07:39:50Z","published":"2022-12-15T07:39:50Z","title":"Memory-like Adaptive Modeling Multi-Agent Learning System","summary":"  In this work, we propose a self-supervised multi-agent system, termed a\nmemory-like adaptive modeling multi-agent learning system (MAMMALS), that\nrealizes online learning towards behavioral pattern clustering tasks for time\nseries. Encoding the visual behaviors as discrete time series(DTS), and\ntraining and modeling them in the multi-agent system with a bio-memory-like\nform. We finally implemented a fully decentralized multi-agent system design\nframework and completed its feasibility verification in a surveillance video\napplication scenario on vehicle path clustering. In multi-agent learning, using\nlearning methods designed for individual agents will typically perform poorly\nglobally because of the behavior of ignoring the synergy between agents.\n","authors":["Xingyu Qian","Aximu Yuemaier","Longfei Liang","Wen-Chi Yang","Xiaogang Chen","Shunfen Li","Weibang Dai","Zhitang Song"],"pdf_url":"https://arxiv.org/pdf/2212.07646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.08772v3","updated":"2022-12-15T07:31:37Z","published":"2022-10-17T06:29:07Z","title":"Signal Processing for Implicit Neural Representations","summary":"  Implicit Neural Representations (INRs) encoding continuous multi-media data\nvia multi-layer perceptrons has shown undebatable promise in various computer\nvision tasks. Despite many successful applications, editing and processing an\nINR remains intractable as signals are represented by latent parameters of a\nneural network. Existing works manipulate such continuous representations via\nprocessing on their discretized instance, which breaks down the compactness and\ncontinuous nature of INR. In this work, we present a pilot study on the\nquestion: how to directly modify an INR without explicit decoding? We answer\nthis question by proposing an implicit neural signal processing network, dubbed\nINSP-Net, via differential operators on INR. Our key insight is that spatial\ngradients of neural networks can be computed analytically and are invariant to\ntranslation, while mathematically we show that any continuous convolution\nfilter can be uniformly approximated by a linear combination of high-order\ndifferential operators. With these two knobs, INSP-Net instantiates the signal\nprocessing operator as a weighted composition of computational graphs\ncorresponding to the high-order derivatives of INRs, where the weighting\nparameters can be data-driven learned. Based on our proposed INSP-Net, we\nfurther build the first Convolutional Neural Network (CNN) that implicitly runs\non INRs, named INSP-ConvNet. Our experiments validate the expressiveness of\nINSP-Net and INSP-ConvNet in fitting low-level image and geometry processing\nkernels (e.g. blurring, deblurring, denoising, inpainting, and smoothening) as\nwell as for high-level tasks on implicit fields such as image classification.\n","authors":["Dejia Xu","Peihao Wang","Yifan Jiang","Zhiwen Fan","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2210.08772v3.pdf","comment":"Advances in Neural Information Processing Systems (NeurIPS), 2022"},{"id":"http://arxiv.org/abs/2206.09604v2","updated":"2022-12-15T07:21:01Z","published":"2022-06-20T07:20:02Z","title":"Distortion-Aware Network Pruning and Feature Reuse for Real-time Video\n  Segmentation","summary":"  Real-time video segmentation is a crucial task for many real-world\napplications such as autonomous driving and robot control. Since\nstate-of-the-art semantic segmentation models are often too heavy for real-time\napplications despite their impressive performance, researchers have proposed\nlightweight architectures with speed-accuracy trade-offs, achieving real-time\nspeed at the expense of reduced accuracy. In this paper, we propose a novel\nframework to speed up any architecture with skip-connections for real-time\nvision tasks by exploiting the temporal locality in videos. Specifically, at\nthe arrival of each frame, we transform the features from the previous frame to\nreuse them at specific spatial bins. We then perform partial computation of the\nbackbone network on the regions of the current frame that captures temporal\ndifferences between the current and previous frame. This is done by dynamically\ndropping out residual blocks using a gating mechanism which decides which\nblocks to drop based on inter-frame distortion. We validate our\nSpatial-Temporal Mask Generator (STMG) on video semantic segmentation\nbenchmarks with multiple backbone networks, and show that our method largely\nspeeds up inference with minimal loss of accuracy.\n","authors":["Hyunsu Rhee","Dongchan Min","Sunil Hwang","Bruno Andreis","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2206.09604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07629v1","updated":"2022-12-15T06:30:11Z","published":"2022-12-15T06:30:11Z","title":"EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level\n  Weakly Supervised Instance Segmentation","summary":"  We propose EM-PASTE: an Expectation Maximization(EM) guided Cut-Paste\ncompositional dataset augmentation approach for weakly-supervised instance\nsegmentation using only image-level supervision. The proposed method consists\nof three main components. The first component generates high-quality foreground\nobject masks. To this end, an EM-like approach is proposed that iteratively\nrefines an initial set of object mask proposals generated by a generic region\nproposal method. Next, in the second component, high-quality context-aware\nbackground images are generated using a text-to-image compositional synthesis\nmethod like DALL-E. Finally, the third component creates a large-scale\npseudo-labeled instance segmentation training dataset by compositing the\nforeground object masks onto the original and generated background images. The\nproposed approach achieves state-of-the-art weakly-supervised instance\nsegmentation results on both the PASCAL VOC 2012 and MS COCO datasets by using\nonly image-level, weak label information. In particular, it outperforms the\nbest baseline by +7.4 and +2.8 mAP0.50 on PASCAL and COCO, respectively.\nFurther, the method provides a new solution to the long-tail weakly-supervised\ninstance segmentation problem (when many classes may only have few training\nsamples), by selectively augmenting under-represented classes.\n","authors":["Yunhao Ge","Jiashu Xu","Brian Nlong Zhao","Laurent Itti","Vibhav Vineet"],"pdf_url":"https://arxiv.org/pdf/2212.07629v1.pdf","comment":"15 pages (including appendix), 7 figures"},{"id":"http://arxiv.org/abs/2212.07626v1","updated":"2022-12-15T05:58:45Z","published":"2022-12-15T05:58:45Z","title":"NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object\n  Interactions","summary":"  Humans constantly interact with objects in daily life tasks. Capturing such\nprocesses and subsequently conducting visual inferences from a fixed viewpoint\nsuffers from occlusions, shape and texture ambiguities, motions, etc. To\nmitigate the problem, it is essential to build a training dataset that captures\nfree-viewpoint interactions. We construct a dense multi-view dome to acquire a\ncomplex human object interaction dataset, named HODome, that consists of\n$\\sim$75M frames on 10 subjects interacting with 23 objects. To process the\nHODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline\ntailored for multi-view video inputs to conduct accurate tracking, geometry\nreconstruction and free-view rendering, for both human subjects and objects.\nExtensive experiments on the HODome dataset demonstrate the effectiveness of\nNeuralDome on a variety of inference, modeling, and rendering tasks. Both the\ndataset and the NeuralDome tools will be disseminated to the community for\nfurther development.\n","authors":["Juze Zhang","Haimin Luo","Hongdi Yang","Xinru Xu","Qianyang Wu","Ye Shi","Jingyi Yu","Lan Xu","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07623v1","updated":"2022-12-15T05:43:21Z","published":"2022-12-15T05:43:21Z","title":"SBSS: Stacking-Based Semantic Segmentation Framework for Very High\n  Resolution Remote Sensing Image","summary":"  Semantic segmentation of Very High Resolution (VHR) remote sensing images is\na fundamental task for many applications. However, large variations in the\nscales of objects in those VHR images pose a challenge for performing accurate\nsemantic segmentation. Existing semantic segmentation networks are able to\nanalyse an input image at up to four resizing scales, but this may be\ninsufficient given the diversity of object scales. Therefore, Multi Scale (MS)\ntest-time data augmentation is often used in practice to obtain more accurate\nsegmentation results, which makes equal use of the segmentation results\nobtained at the different resizing scales. However, it was found in this study\nthat different classes of objects had their preferred resizing scale for more\naccurate semantic segmentation. Based on this behaviour, a Stacking-Based\nSemantic Segmentation (SBSS) framework is proposed to improve the segmentation\nresults by learning this behaviour, which contains a learnable Error Correction\nModule (ECM) for segmentation result fusion and an Error Correction Scheme\n(ECS) for computational complexity control. Two ECS, i.e., ECS-MS and ECS-SS,\nare proposed and investigated in this study. The Floating-point operations\n(Flops) required for ECS-MS and ECS-SS are similar to the commonly used MS test\nand the Single-Scale (SS) test, respectively. Extensive experiments on four\ndatasets (i.e., Cityscapes, UAVid, LoveDA and Potsdam) show that SBSS is an\neffective and flexible framework. It achieved higher accuracy than MS when\nusing ECS-MS, and similar accuracy as SS with a quarter of the memory footprint\nwhen using ECS-SS.\n","authors":["Yuanzhi Cai","Lei Fan","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2212.07623v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2212.02886v2","updated":"2022-12-15T05:10:36Z","published":"2022-12-06T11:23:16Z","title":"GAS-NeXt: Few-Shot Cross-Lingual Font Generator","summary":"  Generating new fonts is a time-consuming and labor-intensive task, especially\nin a language with a huge amount of characters like Chinese. Various deep\nlearning models have demonstrated the ability to efficiently generate new fonts\nwith a few reference characters of that style, but few models support\ncross-lingual font generation. This paper presents GAS-NeXt, a novel few-shot\ncross-lingual font generator based on AGIS-Net and Font Translator GAN, and\nimprove the performance metrics such as Fr\\'echet Inception Distance (FID),\nStructural Similarity Index Measure(SSIM), and Pixel-level Accuracy (pix-acc).\nOur approaches include replacing the original encoder and decoder with the idea\nof layer attention and context-aware attention from Font Translator GAN, while\nutilizing the shape, texture, and local discriminators of AGIS-Net. In our\nexperiment on English-to-Chinese font translation, we observed better results\nin fonts with distinct local features than conventional Chinese fonts compared\nto results obtained from Font Translator GAN. We also validate our method on\nmultiple languages and datasets.\n","authors":["Haoyang He","Xin Jin","Angela Chen"],"pdf_url":"https://arxiv.org/pdf/2212.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07618v1","updated":"2022-12-15T05:09:11Z","published":"2022-12-15T05:09:11Z","title":"Proposal Distribution Calibration for Few-Shot Object Detection","summary":"  Adapting object detectors learned with sufficient supervision to novel\nclasses under low data regimes is charming yet challenging. In few-shot object\ndetection (FSOD), the two-step training paradigm is widely adopted to mitigate\nthe severe sample imbalance, i.e., holistic pre-training on base classes, then\npartial fine-tuning in a balanced setting with all classes. Since unlabeled\ninstances are suppressed as backgrounds in the base training phase, the learned\nRPN is prone to produce biased proposals for novel instances, resulting in\ndramatic performance degradation. Unfortunately, the extreme data scarcity\naggravates the proposal distribution bias, hindering the RoI head from evolving\ntoward novel classes. In this paper, we introduce a simple yet effective\nproposal distribution calibration (PDC) approach to neatly enhance the\nlocalization and classification abilities of the RoI head by recycling its\nlocalization ability endowed in base training and enriching high-quality\npositive samples for semantic fine-tuning. Specifically, we sample proposals\nbased on the base proposal statistics to calibrate the distribution bias and\nimpose additional localization and classification losses upon the sampled\nproposals for fast expanding the base detector to novel classes. Experiments on\nthe commonly used Pascal VOC and MS COCO datasets with explicit\nstate-of-the-art performances justify the efficacy of our PDC for FSOD. Code is\navailable at github.com/Bohao-Lee/PDC.\n","authors":["Bohao Li","Chang Liu","Mengnan Shi","Xiaozhong Chen","Xiangyang Ji","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2212.07618v1.pdf","comment":"This paper is under review in IEEE TNNLS"},{"id":"http://arxiv.org/abs/2212.07613v1","updated":"2022-12-15T04:34:57Z","published":"2022-12-15T04:34:57Z","title":"DCS-RISR: Dynamic Channel Splitting for Efficient Real-world Image\n  Super-Resolution","summary":"  Real-world image super-resolution (RISR) has received increased focus for\nimproving the quality of SR images under unknown complex degradation. Existing\nmethods rely on the heavy SR models to enhance low-resolution (LR) images of\ndifferent degradation levels, which significantly restricts their practical\ndeployments on resource-limited devices. In this paper, we propose a novel\nDynamic Channel Splitting scheme for efficient Real-world Image\nSuper-Resolution, termed DCS-RISR. Specifically, we first introduce the light\ndegradation prediction network to regress the degradation vector to simulate\nthe real-world degradations, upon which the channel splitting vector is\ngenerated as the input for an efficient SR model. Then, a learnable octave\nconvolution block is proposed to adaptively decide the channel splitting scale\nfor low- and high-frequency features at each block, reducing computation\noverhead and memory cost by offering the large scale to low-frequency features\nand the small scale to the high ones. To further improve the RISR performance,\nNon-local regularization is employed to supplement the knowledge of patches\nfrom LR and HR subspace with free-computation inference. Extensive experiments\ndemonstrate the effectiveness of DCS-RISR on different benchmark datasets. Our\nDCS-RISR not only achieves the best trade-off between computation/parameter and\nPSNR/SSIM metric, and also effectively handles real-world images with different\ndegradation levels.\n","authors":["Junbo Qiao","Shaohui Lin","Yunlun Zhang","Wei Li","Hu Jie","Gaoqi He","Changbo Wang","Zhuangli Ma"],"pdf_url":"https://arxiv.org/pdf/2212.07613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06872v2","updated":"2022-12-15T04:12:59Z","published":"2022-12-13T19:38:13Z","title":"Examining the Difference Among Transformers and CNNs with Explanation\n  Methods","summary":"  We propose a methodology that systematically applies deep explanation\nalgorithms on a dataset-wide basis, to compare different types of visual\nrecognition backbones, such as convolutional networks (CNNs), global attention\nnetworks, and local attention networks. Examination of both qualitative\nvisualizations and quantitative statistics across the dataset helps us to gain\nintuitions that are not just anecdotal, but are supported by the statistics\ncomputed on the entire dataset. Specifically, we propose two methods. The first\none, sub-explanation counting, systematically searches for minimally-sufficient\nexplanations of all images and count the amount of sub-explanations for each\nnetwork. The second one, called cross-testing, computes salient regions using\none network and then evaluates the performance by only showing these regions as\nan image to other networks. Through a combination of qualitative insights and\nquantitative statistics, we illustrate that 1) there are significant\ndifferences between the salient features of CNNs and attention models; 2) the\nocclusion-robustness in local attention models and global attention models may\ncome from different decision-making mechanisms.\n","authors":["Mingqi Jiang","Saeed Khorram","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2212.06872v2.pdf","comment":"28 pages with 39 figures"},{"id":"http://arxiv.org/abs/2212.07603v1","updated":"2022-12-15T03:26:53Z","published":"2022-12-15T03:26:53Z","title":"Text-guided mask-free local image retouching","summary":"  In the realm of multi-modality, text-guided image retouching techniques\nemerged with the advent of deep learning. Most currently available text-guided\nmethods, however, rely on object-level supervision to constrain the region that\nmay be modified. This not only makes it more challenging to develop these\nalgorithms, but it also limits how widely deep learning can be used for image\nretouching. In this paper, we offer a text-guided mask-free image retouching\napproach that yields consistent results to address this concern. In order to\nperform image retouching without mask supervision, our technique can construct\nplausible and edge-sharp masks based on the text for each object in the image.\nExtensive experiments have shown that our method can produce high-quality,\naccurate images based on spoken language. The source code will be released\nsoon.\n","authors":["Zerun Liu","Fan Zhang","Jingxuan He","Jin Wang","Zhangye Wang","Lechao Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.07603v1.pdf","comment":"7 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2212.07599v1","updated":"2022-12-15T03:04:48Z","published":"2022-12-15T03:04:48Z","title":"Universal Generative Modeling in Dual-domain for Dynamic MR Imaging","summary":"  Dynamic magnetic resonance image reconstruction from incomplete k-space data\nhas generated great research interest due to its capability to reduce scan\ntime. Never-theless, the reconstruction problem is still challenging due to its\nill-posed nature. Recently, diffusion models espe-cially score-based generative\nmodels have exhibited great potential in algorithm robustness and usage\nflexi-bility. Moreover, the unified framework through the variance exploding\nstochastic differential equation (VE-SDE) is proposed to enable new sampling\nmethods and further extend the capabilities of score-based gener-ative models.\nTherefore, by taking advantage of the uni-fied framework, we proposed a k-space\nand image Du-al-Domain collaborative Universal Generative Model (DD-UGM) which\ncombines the score-based prior with low-rank regularization penalty to\nreconstruct highly under-sampled measurements. More precisely, we extract prior\ncomponents from both image and k-space domains via a universal generative model\nand adaptively handle these prior components for faster processing while\nmaintaining good generation quality. Experimental comparisons demonstrated the\nnoise reduction and detail preservation abilities of the proposed method. Much\nmore than that, DD-UGM can reconstruct data of differ-ent frames by only\ntraining a single frame image, which reflects the flexibility of the proposed\nmodel.\n","authors":["Chuanming Yu","Yu Guan","Ziwen Ke","Dong Liang","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2212.07599v1.pdf","comment":"12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2210.08585v3","updated":"2022-12-15T02:56:29Z","published":"2022-10-16T17:10:52Z","title":"A new trigonometric kernel function for support vector machine","summary":"  In the last few years, various types of machine learning algorithms, such as\nSupport Vector Machine (SVM), Support Vector Regression (SVR), and Non-negative\nMatrix Factorization (NMF) have been introduced. The kernel approach is an\neffective method for increasing the classification accuracy of machine learning\nalgorithms. This paper introduces a family of one-parameter kernel functions\nfor improving the accuracy of SVM classification. The proposed kernel function\nconsists of a trigonometric term and differs from all existing kernel\nfunctions. We show this function is a positive definite kernel function.\nFinally, we evaluate the SVM method based on the new trigonometric kernel, the\nGaussian kernel, the polynomial kernel, and a convex combination of the new\nkernel function and the Gaussian kernel function on various types of datasets.\nEmpirical results show that the SVM based on the new trigonometric kernel\nfunction and the mixed kernel function achieve the best classification\naccuracy. Moreover, some numerical results of performing the SVR based on the\nnew trigonometric kernel function and the mixed kernel function are presented.\n","authors":["Sajad Fathi Hafshejani","Zahra Moberfard"],"pdf_url":"https://arxiv.org/pdf/2210.08585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05245v2","updated":"2022-12-15T02:48:41Z","published":"2022-12-10T08:49:19Z","title":"Joint Spatio-Temporal Modeling for the Semantic Change Detection in\n  Remote Sensing Images","summary":"  Semantic Change Detection (SCD) refers to the task of simultaneously\nextracting the changed areas and the semantic categories (before and after the\nchanges) in Remote Sensing Images (RSIs). This is more meaningful than Binary\nChange Detection (BCD) since it enables detailed change analysis in the\nobserved areas. Previous works established triple-branch Convolutional Neural\nNetwork (CNN) architectures as the paradigm for SCD. However, it remains\nchallenging to exploit semantic information with a limited amount of change\nsamples. In this work, we investigate to jointly consider the spatio-temporal\ndependencies to improve the accuracy of SCD. First, we propose a Semantic\nChange Transformer (SCanFormer) to explicitly model the 'from-to' semantic\ntransitions between the bi-temporal RSIs. Then, we introduce a semantic\nlearning scheme to leverage the spatio-temporal constraints, which are coherent\nto the SCD task, to guide the learning of semantic changes. The resulting\nnetwork (SCanNet) significantly outperforms the baseline method in terms of\nboth detection of critical semantic changes and semantic consistency in the\nobtained bi-temporal results. It achieves the SOTA accuracy on two benchmark\ndatasets for the SCD.\n","authors":["Lei Ding","Jing Zhang","Kai Zhang","Haitao Guo","Bing Liu","Lorenzo Bruzzone"],"pdf_url":"https://arxiv.org/pdf/2212.05245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07593v1","updated":"2022-12-15T02:45:57Z","published":"2022-12-15T02:45:57Z","title":"Enhanced Training of Query-Based Object Detection via Selective Query\n  Recollection","summary":"  This paper investigates a phenomenon where query-based object detectors\nmispredict at the last decoding stage while predicting correctly at an\nintermediate stage. We review the training process and attribute the overlooked\nphenomenon to two limitations: lack of training emphasis and cascading errors\nfrom decoding sequence. We design and present Selective Query Recollection\n(SQR), a simple and effective training strategy for query-based object\ndetectors. It cumulatively collects intermediate queries as decoding stages go\ndeeper and selectively forwards the queries to the downstream stages aside from\nthe sequential structure. Such-wise, SQR places training emphasis on later\nstages and allows later stages to work with intermediate queries from earlier\nstages directly. SQR can be easily plugged into various query-based object\ndetectors and significantly enhances their performance while leaving the\ninference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,\nand Deformable-DETR across various settings (backbone, number of queries,\nschedule) and consistently brings 1.4-2.8 AP improvement.\n","authors":["Fangyi Chen","Han Zhang","Kai Hu","Yu-kai Huang","Chenchen Zhu","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2212.07593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07592v1","updated":"2022-12-15T02:44:13Z","published":"2022-12-15T02:44:13Z","title":"Solve the Puzzle of Instance Segmentation in Videos: A Weakly Supervised\n  Framework with Spatio-Temporal Collaboration","summary":"  Instance segmentation in videos, which aims to segment and track multiple\nobjects in video frames, has garnered a flurry of research attention in recent\nyears. In this paper, we present a novel weakly supervised framework with\n\\textbf{S}patio-\\textbf{T}emporal \\textbf{C}ollaboration for instance\n\\textbf{Seg}mentation in videos, namely \\textbf{STC-Seg}. Concretely, STC-Seg\ndemonstrates four contributions. First, we leverage the complementary\nrepresentations from unsupervised depth estimation and optical flow to produce\neffective pseudo-labels for training deep networks and predicting high-quality\ninstance masks. Second, to enhance the mask generation, we devise a puzzle\nloss, which enables end-to-end training using box-level annotations. Third, our\ntracking module jointly utilizes bounding-box diagonal points with\nspatio-temporal discrepancy to model movements, which largely improves the\nrobustness to different object appearances. Finally, our framework is flexible\nand enables image-level instance segmentation methods to operate the\nvideo-level task. We conduct an extensive set of experiments on the KITTI MOTS\nand YT-VIS datasets. Experimental results demonstrate that our method achieves\nstrong performance and even outperforms fully supervised TrackR-CNN and\nMaskTrack R-CNN. We believe that STC-Seg can be a valuable addition to the\ncommunity, as it reflects the tip of an iceberg about the innovative\nopportunities in the weakly supervised paradigm for instance segmentation in\nvideos.\n","authors":["Liqi Yan","Qifan Wang","Siqi Ma","Jingang Wang","Changbin Yu"],"pdf_url":"https://arxiv.org/pdf/2212.07592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04993v4","updated":"2022-12-15T02:33:40Z","published":"2022-10-10T19:58:23Z","title":"Continual Learning with Evolving Class Ontologies","summary":"  Lifelong learners must recognize concept vocabularies that evolve over time.\nA common yet underexplored scenario is learning with class labels that\ncontinually refine/expand old classes. For example, humans learn to recognize\n${\\tt dog}$ before dog breeds. In practical settings, dataset\n$\\textit{versioning}$ often introduces refinement to ontologies, such as\nautonomous vehicle benchmarks that refine a previous ${\\tt vehicle}$ class into\n${\\tt school-bus}$ as autonomous operations expand to new cities. This paper\nformalizes a protocol for studying the problem of $\\textit{Learning with\nEvolving Class Ontology}$ (LECO). LECO requires learning classifiers in\ndistinct time periods (TPs); each TP introduces a new ontology of \"fine\" labels\nthat refines old ontologies of \"coarse\" labels (e.g., dog breeds that refine\nthe previous ${\\tt dog}$). LECO explores such questions as whether to annotate\nnew data or relabel the old, how to leverage coarse labels, and whether to\nfinetune the previous TP's model or train from scratch. To answer these\nquestions, we leverage insights from related problems such as class-incremental\nlearning. We validate them under the LECO protocol through the lens of image\nclassification (CIFAR and iNaturalist) and semantic segmentation (Mapillary).\nOur experiments lead to surprising conclusions; while the current status quo is\nto relabel existing datasets with new ontologies (such as COCO-to-LVIS or\nMapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to\nannotate $\\textit{new}$ data with the new ontology. However, this produces an\naggregate dataset with inconsistent old-vs-new labels, complicating learning.\nTo address this challenge, we adopt methods from semi-supervised and\npartial-label learning. Such strategies can surprisingly be made near-optimal,\napproaching an \"oracle\" that learns on the aggregate dataset exhaustively\nlabeled with the newest ontology.\n","authors":["Zhiqiu Lin","Deepak Pathak","Yu-Xiong Wang","Deva Ramanan","Shu Kong"],"pdf_url":"https://arxiv.org/pdf/2210.04993v4.pdf","comment":"NeurIPS 2022; Website: https://linzhiqiu.github.io/papers/leco/"},{"id":"http://arxiv.org/abs/2212.07585v1","updated":"2022-12-15T02:25:22Z","published":"2022-12-15T02:25:22Z","title":"Co-Learning with Pre-Trained Networks Improves Source-Free Domain\n  Adaptation","summary":"  Source-free domain adaptation aims to adapt a source model trained on\nfully-labeled source domain data to a target domain with unlabeled target\ndomain data. Source data is assumed inaccessible due to proprietary or privacy\nreasons. Existing works use the source model to pseudolabel target data, but\nthe pseudolabels are unreliable due to data distribution shift between source\nand target domain. In this work, we propose to leverage an ImageNet pre-trained\nfeature extractor in a new co-learning framework to improve target pseudolabel\nquality for finetuning the source model. Benefits of the ImageNet feature\nextractor include that it is not source-biased and it provides an alternate\nview of features and classification decisions different from the source model.\nSuch pre-trained feature extractors are also publicly available, which allows\nus to readily leverage modern network architectures that have strong\nrepresentation learning ability. After co-learning, we sharpen predictions of\nnon-pseudolabeled samples by entropy minimization. Evaluation on 3 benchmark\ndatasets show that our proposed method can outperform existing source-free\ndomain adaptation methods, as well as unsupervised domain adaptation methods\nwhich assume joint access to source and target data.\n","authors":["Wenyu Zhang","Li Shen","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2212.07585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07582v1","updated":"2022-12-15T02:05:12Z","published":"2022-12-15T02:05:12Z","title":"Edema Estimation From Facial Images Taken Before and After Dialysis via\n  Contrastive Multi-Patient Pre-Training","summary":"  Edema is a common symptom of kidney disease, and quantitative measurement of\nedema is desired. This paper presents a method to estimate the degree of edema\nfrom facial images taken before and after dialysis of renal failure patients.\nAs tasks to estimate the degree of edema, we perform pre- and post-dialysis\nclassification and body weight prediction. We develop a multi-patient\npre-training framework for acquiring knowledge of edema and transfer the\npre-trained model to a model for each patient. For effective pre-training, we\npropose a novel contrastive representation learning, called weight-aware\nsupervised momentum contrast (WeightSupMoCo). WeightSupMoCo aims to make\nfeature representations of facial images closer in similarity of patient weight\nwhen the pre- and post-dialysis labels are the same. Experimental results show\nthat our pre-training approach improves the accuracy of pre- and post-dialysis\nclassification by 15.1% and reduces the mean absolute error of weight\nprediction by 0.243 kg compared with training from scratch. The proposed method\naccurately estimate the degree of edema from facial images; our edema\nestimation system could thus be beneficial to dialysis patients.\n","authors":["Yusuke Akamatsu","Yoshifumi Onishi","Hitoshi Imaoka","Junko Kameyama","Hideo Tsurushima"],"pdf_url":"https://arxiv.org/pdf/2212.07582v1.pdf","comment":"Published in IEEE Journal of Biomedical and Health Informatics\n  (J-BHI)"},{"id":"http://arxiv.org/abs/2104.03509v2","updated":"2022-12-15T02:03:57Z","published":"2021-04-08T04:52:21Z","title":"Py-Feat: Python Facial Expression Analysis Toolbox","summary":"  Studying facial expressions is a notoriously difficult endeavor. Recent\nadvances in the field of affective computing have yielded impressive progress\nin automatically detecting facial expressions from pictures and videos.\nHowever, much of this work has yet to be widely disseminated in social science\ndomains such as psychology. Current state of the art models require\nconsiderable domain expertise that is not traditionally incorporated into\nsocial science training programs. Furthermore, there is a notable absence of\nuser-friendly and open-source software that provides a comprehensive set of\ntools and functions that support facial expression research. In this paper, we\nintroduce Py-Feat, an open-source Python toolbox that provides support for\ndetecting, preprocessing, analyzing, and visualizing facial expression data.\nPy-Feat makes it easy for domain experts to disseminate and benchmark computer\nvision models and also for end users to quickly process, analyze, and visualize\nface expression data. We hope this platform will facilitate increased use of\nfacial expression data in human behavior research.\n","authors":["Eshin Jolly","Jin Hyun Cheong","Tiankang Xie","Sophie Byrne","Matthew Kenny","Luke J. Chang"],"pdf_url":"https://arxiv.org/pdf/2104.03509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07579v1","updated":"2022-12-15T01:56:22Z","published":"2022-12-15T01:56:22Z","title":"Learning to Detect Semantic Boundaries with Image-level Class Labels","summary":"  This paper presents the first attempt to learn semantic boundary detection\nusing image-level class labels as supervision. Our method starts by estimating\ncoarse areas of object classes through attentions drawn by an image\nclassification network. Since boundaries will locate somewhere between such\nareas of different classes, our task is formulated as a multiple instance\nlearning (MIL) problem, where pixels on a line segment connecting areas of two\ndifferent classes are regarded as a bag of boundary candidates. Moreover, we\ndesign a new neural network architecture that can learn to estimate semantic\nboundaries reliably even with uncertain supervision given by the MIL strategy.\nOur network is used to generate pseudo semantic boundary labels of training\nimages, which are in turn used to train fully supervised models. The final\nmodel trained with our pseudo labels achieves an outstanding performance on the\nSBD dataset, where it is as competitive as some of previous arts trained with\nstronger supervision.\n","authors":["Namyup Kim","Sehyun Hwang","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2212.07579v1.pdf","comment":"International Journal of Computer Vision (IJCV), 2022"},{"id":"http://arxiv.org/abs/2212.07575v1","updated":"2022-12-15T01:26:09Z","published":"2022-12-15T01:26:09Z","title":"Evaluation of direct attacks to fingerprint verification systems","summary":"  The vulnerabilities of fingerprint-based recognition systems to direct\nattacks with and without the cooperation of the user are studied. Two different\nsystems, one minutiae-based and one ridge feature-based, are evaluated on a\ndatabase of real and fake fingerprints. Based on the fingerprint images quality\nand on the results achieved on different operational scenarios, we obtain a\nnumber of statistically significant observations regarding the robustness of\nthe systems.\n","authors":["J. Galbally","J. Fierrez","F. Alonso-Fernandez","M. Martinez-Diaz"],"pdf_url":"https://arxiv.org/pdf/2212.07575v1.pdf","comment":"Published at Springer Journal of Telecommunication Systems, Special\n  Issue of Biometrics Systems & Applications"},{"id":"http://arxiv.org/abs/2212.07567v1","updated":"2022-12-15T00:53:42Z","published":"2022-12-15T00:53:42Z","title":"Learning Markerless Robot-Depth Camera Calibration and End-Effector Pose\n  Estimation","summary":"  Traditional approaches to extrinsic calibration use fiducial markers and\nlearning-based approaches rely heavily on simulation data. In this work, we\npresent a learning-based markerless extrinsic calibration system that uses a\ndepth camera and does not rely on simulation data. We learn models for\nend-effector (EE) segmentation, single-frame rotation prediction and keypoint\ndetection, from automatically generated real-world data. We use a\ntransformation trick to get EE pose estimates from rotation predictions and a\nmatching algorithm to get EE pose estimates from keypoint predictions. We\nfurther utilize the iterative closest point algorithm, multiple-frames,\nfiltering and outlier detection to increase calibration robustness. Our\nevaluations with training data from multiple camera poses and test data from\npreviously unseen poses give sub-centimeter and sub-deciradian average\ncalibration and pose estimation errors. We also show that a carefully selected\nsingle training pose gives comparable results.\n","authors":["Bugra C. Sefercik","Baris Akgun"],"pdf_url":"https://arxiv.org/pdf/2212.07567v1.pdf","comment":"8 pages, 6 figures, Conference on Robot Learning"},{"id":"http://arxiv.org/abs/2212.07564v1","updated":"2022-12-15T00:41:09Z","published":"2022-12-15T00:41:09Z","title":"AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for\n  Approximating Reynolds-Averaged Navier-Stokes Solutions","summary":"  Surrogate models are necessary to optimize meaningful quantities in physical\ndynamics as their recursive numerical resolutions are often prohibitively\nexpensive. It is mainly the case for fluid dynamics and the resolution of\nNavier-Stokes equations. However, despite the fast-growing field of data-driven\nmodels for physical systems, reference datasets representing real-world\nphenomena are lacking. In this work, we develop AirfRANS, a dataset for\nstudying the two-dimensional incompressible steady-state Reynolds-Averaged\nNavier-Stokes equations over airfoils at a subsonic regime and for different\nangles of attacks. We also introduce metrics on the stress forces at the\nsurface of geometries and visualization of boundary layers to assess the\ncapabilities of models to accurately predict the meaningful information of the\nproblem. Finally, we propose deep learning baselines on four machine learning\ntasks to study AirfRANS under different constraints for generalization\nconsiderations: big and scarce data regime, Reynolds number, and angle of\nattack extrapolation.\n","authors":["Florent Bonnet","Ahmed Jocelyn Mazari","Paola Cinnella","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2212.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07560v1","updated":"2022-12-15T00:25:05Z","published":"2022-12-15T00:25:05Z","title":"Multi-level and multi-modal feature fusion for accurate 3D object\n  detection in Connected and Automated Vehicles","summary":"  Aiming at highly accurate object detection for connected and automated\nvehicles (CAVs), this paper presents a Deep Neural Network based 3D object\ndetection model that leverages a three-stage feature extractor by developing a\nnovel LIDAR-Camera fusion scheme. The proposed feature extractor extracts\nhigh-level features from two input sensory modalities and recovers the\nimportant features discarded during the convolutional process. The novel fusion\nscheme effectively fuses features across sensory modalities and convolutional\nlayers to find the best representative global features. The fused features are\nshared by a two-stage network: the region proposal network (RPN) and the\ndetection head (DH). The RPN generates high-recall proposals, and the DH\nproduces final detection results. The experimental results show the proposed\nmodel outperforms more recent research on the KITTI 2D and 3D detection\nbenchmark, particularly for distant and highly occluded instances.\n","authors":["Yiming Hou","Mahdi Rezaei","Richard Romano"],"pdf_url":"https://arxiv.org/pdf/2212.07560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.00114v5","updated":"2022-12-15T00:13:44Z","published":"2021-04-30T22:34:45Z","title":"Improved Real-Time Monocular SLAM Using Semantic Segmentation on\n  Selective Frames","summary":"  Monocular simultaneous localization and mapping (SLAM) is emerging in\nadvanced driver assistance systems and autonomous driving, because a single\ncamera is cheap and easy to install. Conventional monocular SLAM has two major\nchallenges leading inaccurate localization and mapping. First, it is\nchallenging to estimate scales in localization and mapping. Second,\nconventional monocular SLAM uses inappropriate mapping factors such as dynamic\nobjects and low-parallax areas in mapping. This paper proposes an improved\nreal-time monocular SLAM that resolves the aforementioned challenges by\nefficiently using deep learning-based semantic segmentation. To achieve the\nreal-time execution of the proposed method, we apply semantic segmentation only\nto downsampled keyframes in parallel with mapping processes. In addition, the\nproposed method corrects scales of camera poses and three-dimensional (3D)\npoints, using estimated ground plane from road-labeled 3D points and the real\ncamera height. The proposed method also removes inappropriate corner features\nlabeled as moving objects and low parallax areas. Experiments with eight video\nsequences demonstrate that the proposed monocular SLAM system achieves\nsignificantly improved and comparable trajectory tracking accuracy, compared to\nexisting state-of-the-art monocular and stereo SLAM systems, respectively. The\nproposed system can achieve real-time tracking on a standard CPU potentially\nwith a standard GPU support, whereas existing segmentation-aided monocular SLAM\ndoes not.\n","authors":["Jinkyu Lee","Muhyun Back","Sung Soo Hwang","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2105.00114v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.10047v3","updated":"2022-12-15T23:47:39Z","published":"2022-10-18T17:59:55Z","title":"From Play to Policy: Conditional Behavior Generation from Uncurated\n  Robot Data","summary":"  While large-scale sequence modeling from offline data has led to impressive\nperformance gains in natural language and image generation, directly\ntranslating such ideas to robotics has been challenging. One critical reason\nfor this is that uncurated robot demonstration data, i.e. play data, collected\nfrom non-expert human demonstrators are often noisy, diverse, and\ndistributionally multi-modal. This makes extracting useful, task-centric\nbehaviors from such data a difficult generative modeling problem. In this work,\nwe present Conditional Behavior Transformers (C-BeT), a method that combines\nthe multi-modal generation ability of Behavior Transformer with\nfuture-conditioned goal specification. On a suite of simulated benchmark tasks,\nwe find that C-BeT improves upon prior state-of-the-art work in learning from\nplay data by an average of 45.7%. Further, we demonstrate for the first time\nthat useful task-centric behaviors can be learned on a real-world robot purely\nfrom play data without any task labels or reward information. Robot videos are\nbest viewed on our project website: https://play-to-policy.github.io\n","authors":["Zichen Jeff Cui","Yibin Wang","Nur Muhammad Mahi Shafiullah","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2210.10047v3.pdf","comment":"Code and data available at: https://play-to-policy.github.io; (fixed\n  metadata author name format)"},{"id":"http://arxiv.org/abs/2009.04709v4","updated":"2022-12-15T23:35:23Z","published":"2020-09-10T07:48:42Z","title":"Quantifying the Preferential Direction of the Model Gradient in\n  Adversarial Training With Projected Gradient Descent","summary":"  Adversarial training, especially projected gradient descent (PGD), has proven\nto be a successful approach for improving robustness against adversarial\nattacks. After adversarial training, gradients of models with respect to their\ninputs have a preferential direction. However, the direction of alignment is\nnot mathematically well established, making it difficult to evaluate\nquantitatively. We propose a novel definition of this direction as the\ndirection of the vector pointing toward the closest point of the support of the\nclosest inaccurate class in decision space. To evaluate the alignment with this\ndirection after adversarial training, we apply a metric that uses generative\nadversarial networks to produce the smallest residual needed to change the\nclass present in the image. We show that PGD-trained models have a higher\nalignment than the baseline according to our definition, that our metric\npresents higher alignment values than a competing metric formulation, and that\nenforcing this alignment increases the robustness of models.\n","authors":["Ricardo Bigolin Lanfredi","Joyce D. Schroeder","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2009.04709v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08187v1","updated":"2022-12-15T23:20:13Z","published":"2022-12-15T23:20:13Z","title":"Dual Moving Average Pseudo-Labeling for Source-Free Inductive Domain\n  Adaptation","summary":"  Unsupervised domain adaptation reduces the reliance on data annotation in\ndeep learning by adapting knowledge from a source to a target domain. For\nprivacy and efficiency concerns, source-free domain adaptation extends\nunsupervised domain adaptation by adapting a pre-trained source model to an\nunlabeled target domain without accessing the source data. However, most\nexisting source-free domain adaptation methods to date focus on the\ntransductive setting, where the target training set is also the testing set. In\nthis paper, we address source-free domain adaptation in the more realistic\ninductive setting, where the target training and testing sets are mutually\nexclusive. We propose a new semi-supervised fine-tuning method named Dual\nMoving Average Pseudo-Labeling (DMAPL) for source-free inductive domain\nadaptation. We first split the unlabeled training set in the target domain into\na pseudo-labeled confident subset and an unlabeled less-confident subset\naccording to the prediction confidence scores from the pre-trained source\nmodel. Then we propose a soft-label moving-average updating strategy for the\nunlabeled subset based on a moving-average prototypical classifier, which\ngradually adapts the source model towards the target domain. Experiments show\nthat our proposed method achieves state-of-the-art performance and outperforms\nprevious methods by large margins.\n","authors":["Hao Yan","Yuhong Guo"],"pdf_url":"https://arxiv.org/pdf/2212.08187v1.pdf","comment":"BMVC 2022"},{"id":"http://arxiv.org/abs/2212.08158v1","updated":"2022-12-15T21:41:06Z","published":"2022-12-15T21:41:06Z","title":"MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal\n  Contributions in Vision and Language Models & Tasks","summary":"  Vision and language models (VL) are known to exploit unrobust indicators in\nindividual modalities (e.g., introduced by distributional biases), instead of\nfocusing on relevant information in each modality. A small drop in accuracy\nobtained on a VL task with a unimodal model suggests that so-called unimodal\ncollapse occurred. But how to quantify the amount of unimodal collapse\nreliably, at dataset and instance-level, to diagnose and combat unimodal\ncollapse in a targeted way? We present MM-SHAP, a performance-agnostic\nmultimodality score that quantifies the proportion by which a model uses\nindividual modalities in multimodal tasks. MM-SHAP is based on Shapley values\nand will be applied in two ways: (1) to compare models for their degree of\nmultimodality, and (2) to measure the contribution of individual modalities for\na given task and dataset. Experiments with 6 VL models -- LXMERT, CLIP and four\nALBEF variants -- on four VL tasks highlight that unimodal collapse can occur\nto different degrees and in different directions, contradicting the wide-spread\nassumption that unimodal collapse is one-sided. We recommend MM-SHAP for\nanalysing multimodal tasks, to diagnose and guide progress towards multimodal\nintegration. Code available at: https://github.com/Heidelberg-NLP/MM-SHAP\n","authors":["Letitia Parcalabescu","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2212.08158v1.pdf","comment":"10 pages, 13 appendix pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2205.11495v3","updated":"2022-12-15T20:57:59Z","published":"2022-05-23T17:51:48Z","title":"Flexible Diffusion Modeling of Long Videos","summary":"  We present a framework for video modeling based on denoising diffusion\nprobabilistic models that produces long-duration video completions in a variety\nof realistic environments. We introduce a generative model that can at\ntest-time sample any arbitrary subset of video frames conditioned on any other\nsubset and present an architecture adapted for this purpose. Doing so allows us\nto efficiently compare and optimize a variety of schedules for the order in\nwhich frames in a long video are sampled and use selective sparse and\nlong-range conditioning on previously sampled frames. We demonstrate improved\nvideo modeling over prior work on a number of datasets and sample temporally\ncoherent videos over 25 minutes in length. We additionally release a new video\nmodeling dataset and semantically meaningful metrics based on videos generated\nin the CARLA autonomous driving simulator.\n","authors":["William Harvey","Saeid Naderiparizi","Vaden Masrani","Christian Weilbach","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2205.11495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08130v1","updated":"2022-12-15T20:35:48Z","published":"2022-12-15T20:35:48Z","title":"On Evaluating Adversarial Robustness of Chest X-ray Classification:\n  Pitfalls and Best Practices","summary":"  Vulnerability to adversarial attacks is a well-known weakness of Deep Neural\nNetworks. While most of the studies focus on natural images with standardized\nbenchmarks like ImageNet and CIFAR, little research has considered real world\napplications, in particular in the medical domain. Our research shows that,\ncontrary to previous claims, robustness of chest x-ray classification is much\nharder to evaluate and leads to very different assessments based on the\ndataset, the architecture and robustness metric. We argue that previous studies\ndid not take into account the peculiarity of medical diagnosis, like the\nco-occurrence of diseases, the disagreement of labellers (domain experts), the\nthreat model of the attacks and the risk implications for each successful\nattack.\n  In this paper, we discuss the methodological foundations, review the pitfalls\nand best practices, and suggest new methodological considerations for\nevaluating the robustness of chest xray classification models. Our evaluation\non 3 datasets, 7 models, and 18 diseases is the largest evaluation of\nrobustness of chest x-ray classification models.\n","authors":["Salah Ghamizi","Maxime Cordy","Michail Papadakis","Yves Le Traon"],"pdf_url":"https://arxiv.org/pdf/2212.08130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08123v1","updated":"2022-12-15T20:23:09Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For CIFAR, the stochastic ensembles are\nquantitatively compared to published Hamiltonian Monte Carlo results for a\nResNet-20 architecture. We also test the quality of the posteriors directly\nagainst Hamiltonian Monte Carlo simulations in a simplified toy model. Our\nresults show that in a number of settings, stochastic ensembles provide more\naccurate posterior estimates than regular deep ensembles.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2212.08121v1","updated":"2022-12-15T20:20:18Z","published":"2022-12-15T20:20:18Z","title":"Backdoor Attack Detection in Computer Vision by Applying Matrix\n  Factorization on the Weights of Deep Networks","summary":"  The increasing importance of both deep neural networks (DNNs) and cloud\nservices for training them means that bad actors have more incentive and\nopportunity to insert backdoors to alter the behavior of trained models. In\nthis paper, we introduce a novel method for backdoor detection that extracts\nfeatures from pre-trained DNN's weights using independent vector analysis (IVA)\nfollowed by a machine learning classifier. In comparison to other detection\ntechniques, this has a number of benefits, such as not requiring any training\ndata, being applicable across domains, operating with a wide range of network\narchitectures, not assuming the nature of the triggers used to change network\nbehavior, and being highly scalable. We discuss the detection pipeline, and\nthen demonstrate the results on two computer vision datasets regarding image\nclassification and object detection. Our method outperforms the competing\nalgorithms in terms of efficiency and is more accurate, helping to ensure the\nsafe application of deep learning and AI.\n","authors":["Khondoker Murad Hossain","Tim Oates"],"pdf_url":"https://arxiv.org/pdf/2212.08121v1.pdf","comment":"7 pages, 4 figures, 5 tables, AAAI Workshop on Safe AI 2023"},{"id":"http://arxiv.org/abs/2205.14100v5","updated":"2022-12-15T19:21:35Z","published":"2022-05-27T17:03:38Z","title":"GIT: A Generative Image-to-text Transformer for Vision and Language","summary":"  In this paper, we design and train a Generative Image-to-text Transformer,\nGIT, to unify vision-language tasks such as image/video captioning and question\nanswering. While generative models provide a consistent network architecture\nbetween pre-training and fine-tuning, existing work typically contains complex\nstructures (uni/multi-modal encoder/decoder) and depends on external modules\nsuch as object detectors/taggers and optical character recognition (OCR). In\nGIT, we simplify the architecture as one image encoder and one text decoder\nunder a single language modeling task. We also scale up the pre-training data\nand the model size to boost the model performance. Without bells and whistles,\nour GIT establishes new state of the arts on 12 challenging benchmarks with a\nlarge margin. For instance, our model surpasses the human performance for the\nfirst time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a\nnew scheme of generation-based image classification and scene text recognition,\nachieving decent performance on standard benchmarks. Codes are released at\n\\url{https://github.com/microsoft/GenerativeImage2Text}.\n","authors":["Jianfeng Wang","Zhengyuan Yang","Xiaowei Hu","Linjie Li","Kevin Lin","Zhe Gan","Zicheng Liu","Ce Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2205.14100v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.05701v2","updated":"2022-12-15T19:03:06Z","published":"2022-03-11T01:19:04Z","title":"6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An\n  Accessible Dataset and Benchmark","summary":"  We present a new dataset for 6-DoF pose estimation of known objects, with a\nfocus on robotic manipulation research. We propose a set of toy grocery\nobjects, whose physical instantiations are readily available for purchase and\nare appropriately sized for robotic grasping and manipulation. We provide 3D\nscanned textured models of these objects, suitable for generating synthetic\ntraining data, as well as RGBD images of the objects in challenging, cluttered\nscenes exhibiting partial occlusion, extreme lighting variations, multiple\ninstances per image, and a large variety of poses. Using semi-automated\nRGBD-to-model texture correspondences, the images are annotated with ground\ntruth poses accurate within a few millimeters. We also propose a new pose\nevaluation metric called ADD-H based on the Hungarian assignment algorithm that\nis robust to symmetries in object geometry without requiring their explicit\nenumeration. We share pre-trained pose estimators for all the toy grocery\nobjects, along with their baseline performance on both validation and test\nsets. We offer this dataset to the community to help connect the efforts of\ncomputer vision researchers with the needs of roboticists.\n","authors":["Stephen Tyree","Jonathan Tremblay","Thang To","Jia Cheng","Terry Mosier","Jeffrey Smith","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2203.05701v2.pdf","comment":"IROS 2022. Project page is at https://github.com/swtyree/hope-dataset"},{"id":"http://arxiv.org/abs/2102.07085v2","updated":"2022-12-15T15:44:21Z","published":"2021-02-14T06:44:47Z","title":"Light Field Reconstruction via Deep Adaptive Fusion of Hybrid Lenses","summary":"  This paper explores the problem of reconstructing high-resolution light field\n(LF) images from hybrid lenses, including a high-resolution camera surrounded\nby multiple low-resolution cameras. The performance of existing methods is\nstill limited, as they produce either blurry results on plain textured areas or\ndistortions around depth discontinuous boundaries. To tackle this challenge, we\npropose a novel end-to-end learning-based approach, which can comprehensively\nutilize the specific characteristics of the input from two complementary and\nparallel perspectives. Specifically, one module regresses a spatially\nconsistent intermediate estimation by learning a deep multidimensional and\ncross-domain feature representation, while the other module warps another\nintermediate estimation, which maintains the high-frequency textures, by\npropagating the information of the high-resolution view. We finally leverage\nthe advantages of the two intermediate estimations adaptively via the learned\nattention maps, leading to the final high-resolution LF image with satisfactory\nresults on both plain textured areas and depth discontinuous boundaries.\nBesides, to promote the effectiveness of our method trained with simulated\nhybrid data on real hybrid data captured by a hybrid LF imaging system, we\ncarefully design the network architecture and the training strategy. Extensive\nexperiments on both real and simulated hybrid data demonstrate the significant\nsuperiority of our approach over state-of-the-art ones. To the best of our\nknowledge, this is the first end-to-end deep learning method for LF\nreconstruction from a real hybrid input. We believe our framework could\npotentially decrease the cost of high-resolution LF data acquisition and\nbenefit LF data storage and transmission.\n","authors":["Jing Jin","Mantang Guo","Hui Liu","Junhui Hou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2102.07085v2.pdf","comment":"18 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:1907.09640"},{"id":"http://arxiv.org/abs/2212.07409v2","updated":"2022-12-15T12:22:08Z","published":"2022-12-14T18:49:50Z","title":"Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion","summary":"  StyleGAN has achieved great progress in 2D face reconstruction and semantic\nediting via image inversion and latent editing. While studies over extending 2D\nStyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion\nframework is still missing, limiting the applications of 3D face reconstruction\nand semantic editing. In this paper, we study the challenging problem of 3D GAN\ninversion where a latent code is predicted given a single face image to\nfaithfully recover its 3D shapes and detailed textures. The problem is\nill-posed: innumerable compositions of shape and texture could be rendered to\nthe current image. Furthermore, with the limited capacity of a global latent\ncode, 2D inversion methods cannot preserve faithful shape and texture at the\nsame time when applied to 3D models. To solve this problem, we devise an\neffective self-training scheme to constrain the learning of inversion. The\nlearning is done efficiently without any real-world 2D-3D training pairs but\nproxy samples generated from a 3D GAN. In addition, apart from a global latent\ncode that captures the coarse shape and texture information, we augment the\ngeneration network with a local branch, where pixel-aligned features are added\nto faithfully reconstruct face details. We further consider a new pipeline to\nperform 3D view-consistent editing. Extensive experiments show that our method\noutperforms state-of-the-art inversion methods in both shape and texture\nreconstruction quality. Code and data will be released.\n","authors":["Yushi Lan","Xuyi Meng","Shuai Yang","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2212.07409v2.pdf","comment":"An encoder-based 3D GAN inversion method. Project page:\n  https://nirvanalan.github.io/projects/E3DGE/index.html"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.07841v1","updated":"2022-12-15T13:57:07Z","published":"2022-12-15T13:57:07Z","title":"MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are\n  Better Dense Retrievers","summary":"  Dense retrieval aims to map queries and passages into low-dimensional vector\nspace for efficient similarity measuring, showing promising effectiveness in\nvarious large-scale retrieval tasks. Since most existing methods commonly adopt\npre-trained Transformers (e.g. BERT) for parameter initialization, some work\nfocuses on proposing new pre-training tasks for compressing the useful semantic\ninformation from passages into dense vectors, achieving remarkable\nperformances. However, it is still challenging to effectively capture the rich\nsemantic information and relations about passages into the dense vectors via\none single particular pre-training task. In this work, we propose a multi-task\npre-trained model, MASTER, that unifies and integrates multiple pre-training\ntasks with different learning objectives under the bottlenecked masked\nautoencoder architecture. Concretely, MASTER utilizes a multi-decoder\narchitecture to integrate three types of pre-training tasks: corrupted passages\nrecovering, related passage recovering and PLMs outputs recovering. By\nincorporating a shared deep encoder, we construct a representation bottleneck\nin our architecture, compressing the abundant semantic information across tasks\ninto dense vectors. The first two types of tasks concentrate on capturing the\nsemantic information of passages and relationships among them within the\npre-training corpus. The third one can capture the knowledge beyond the corpus\nfrom external PLMs (e.g. GPT-2). Extensive experiments on several large-scale\npassage retrieval datasets have shown that our approach outperforms the\nprevious state-of-the-art dense retrieval methods. Our code and data are\npublicly released in https://github.com/microsoft/SimXNS\n","authors":["Kun Zhou","Xiao Liu","Yeyun Gong","Wayne Xin Zhao","Daxin Jiang","Nan Duan","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2212.07841v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2212.07835v1","updated":"2022-12-15T13:44:28Z","published":"2022-12-15T13:44:28Z","title":"You were saying? -- Spoken Language in the V3C Dataset","summary":"  This paper presents an analysis of the distribution of spoken language in the\nV3C video retrieval benchmark dataset based on automatically generated\ntranscripts. It finds that a large portion of the dataset is covered by spoken\nlanguage. Since language transcripts can be quickly and accurately described,\nthis has implications for retrieval tasks such as known-item search.\n","authors":["Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2212.07835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07767v1","updated":"2022-12-15T12:37:28Z","published":"2022-12-15T12:37:28Z","title":"COLA: Improving Conversational Recommender Systems by Collaborative\n  Augmentation","summary":"  Conversational recommender systems (CRS) aim to employ natural language\nconversations to suggest suitable products to users. Understanding user\npreferences for prospective items and learning efficient item representations\nare crucial for CRS. Despite various attempts, earlier studies mostly learned\nitem representations based on individual conversations, ignoring item\npopularity embodied among all others. Besides, they still need support in\nefficiently capturing user preferences since the information reflected in a\nsingle conversation is limited. Inspired by collaborative filtering, we propose\na collaborative augmentation (COLA) method to simultaneously improve both item\nrepresentation learning and user preference modeling to address these issues.\nWe construct an interactive user-item graph from all conversations, which\naugments item representations with user-aware information, i.e., item\npopularity. To improve user preference modeling, we retrieve similar\nconversations from the training corpus, where the involved items and attributes\nthat reflect the user's potential interests are used to augment the user\nrepresentation through gate control. Extensive experiments on two benchmark\ndatasets demonstrate the effectiveness of our method. Our code and data are\navailable at https://github.com/DongdingLin/COLA.\n","authors":["Dongding Lin","Jian Wang","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2212.07767v1.pdf","comment":"Accepted by AAAI-2023"},{"id":"http://arxiv.org/abs/2212.07742v1","updated":"2022-12-15T11:45:10Z","published":"2022-12-15T11:45:10Z","title":"Analysis of information cascading and propagation barriers across\n  distinctive news events","summary":"  News reporting on events that occur in our society can have different styles\nand structures as well as different dynamics of news spreading over time. News\npublishers have the potential to spread their news and reach out to a large\nnumber of readers worldwide. In this paper we would like to understand how well\nthey are doing it and which kind of obstacles the news may encounter when\nspreading. The news to be spread wider cross multiple barriers such as\nlinguistic (the most evident one as they get published in other natural\nlanguages), economic, geographical, political, time zone, and cultural\nbarriers. Observing potential differences between spreading of news on\ndifferent events published by multiple publishers can bring insights into what\nmay influence the differences in the spreading patterns. There are multiple\nreasons, possibly many hidden, influencing the speed and geographical spread of\nnews. This paper studies information cascading and propagation barriers,\napplying the proposed methodology on three distinctive kinds of events: Global\nWarming, earthquakes, and FIFA World Cup.\n","authors":["Abdul Sittar","Dunja Mladenic","Marko Grobelnik"],"pdf_url":"https://arxiv.org/pdf/2212.07742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07679v1","updated":"2022-12-15T09:23:31Z","published":"2022-12-15T09:23:31Z","title":"Exact fixed-radius nearest neighbor search with an application to\n  clustering","summary":"  Fixed-radius nearest-neighbor search is a common database operation that\nretrieves all data points within a user-specified distance to a query point.\nThere are efficient approximate nearest neighbor search algorithms that provide\nfast query responses but they often have a very compute-intensive indexing\nphase and require parameter tuning. Therefore, exact brute force and tree-based\nsearch methods are still widely used. Here we propose a new fixed-radius\nnearest neighbor search method that significantly improves over brute force and\ntree-based methods in terms of index and query time, returns exact results, and\nrequires no parameter tuning. The method exploits a sorting of the data points\nby their first principal component, thereby facilitating a reduction in query\nsearch space. Further speedup is gained from an efficient implementation using\nhigh-level Basic Linear Algebra Subprograms (BLAS). We provide theoretical\nanalysis of our method and demonstrate its practical performance when used\nstand-alone and when applied within a clustering algorithm.\n","authors":["Xinye Chen","Stefan Güttel"],"pdf_url":"https://arxiv.org/pdf/2212.07679v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2202.01456"},{"id":"http://arxiv.org/abs/2212.08184v1","updated":"2022-12-15T23:00:33Z","published":"2022-12-15T23:00:33Z","title":"NBC-Softmax : Darkweb Author fingerprinting and migration tracking","summary":"  Metric learning aims to learn distances from the data, which enhances the\nperformance of similarity-based algorithms. An author style detection task is a\nmetric learning problem, where learning style features with small intra-class\nvariations and larger inter-class differences is of great importance to achieve\nbetter performance. Recently, metric learning based on softmax loss has been\nused successfully for style detection. While softmax loss can produce separable\nrepresentations, its discriminative power is relatively poor. In this work, we\npropose NBC-Softmax, a contrastive loss based clustering technique for softmax\nloss, which is more intuitive and able to achieve superior performance. Our\ntechnique meets the criterion for larger number of samples, thus achieving\nblock contrastiveness, which is proven to outperform pair-wise losses. It uses\nmini-batch sampling effectively and is scalable. Experiments on 4 darkweb\nsocial forums, with NBCSAuthor that uses the proposed NBC-Softmax for author\nand sybil detection, shows that our negative block contrastive approach\nconstantly outperforms state-of-the-art methods using the same network\narchitecture.\n  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax\n","authors":["Gayan K. Kulatilleke","Shekhar S. Chandra","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2212.08184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08475v1","updated":"2022-12-15T02:28:52Z","published":"2022-12-15T02:28:52Z","title":"Best-Answer Prediction in Q&A Sites Using User Information","summary":"  Community Question Answering (CQA) sites have spread and multiplied\nsignificantly in recent years. Sites like Reddit, Quora, and Stack Exchange are\nbecoming popular amongst people interested in finding answers to diverse\nquestions. One practical way of finding such answers is automatically\npredicting the best candidate given existing answers and comments. Many studies\nwere conducted on answer prediction in CQA but with limited focus on using the\nbackground information of the questionnaires. We address this limitation using\na novel method for predicting the best answers using the questioner's\nbackground information and other features, such as the textual content or the\nrelationships with other participants. Our answer classification model was\ntrained using the Stack Exchange dataset and validated using the Area Under the\nCurve (AUC) metric. The experimental results show that the proposed method\ncomplements previous methods by pointing out the importance of the\nrelationships between users, particularly throughout the level of involvement\nin different communities on Stack Exchange. Furthermore, we point out that\nthere is little overlap between user-relation information and the information\nrepresented by the shallow text features and the meta-features, such as time\ndifferences.\n","authors":["Rafik Hadfi","Ahmed Moustafa","Kai Yoshino","Takayuki Ito"],"pdf_url":"https://arxiv.org/pdf/2212.08475v1.pdf","comment":"22 pages, 3 figures, 4 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2212.08066v1","updated":"2022-12-15T18:59:52Z","published":"2022-12-15T18:59:52Z","title":"Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners","summary":"  Optimization in multi-task learning (MTL) is more challenging than\nsingle-task learning (STL), as the gradient from different tasks can be\ncontradictory. When tasks are related, it can be beneficial to share some\nparameters among them (cooperation). However, some tasks require additional\nparameters with expertise in a specific type of data or discrimination\n(specialization). To address the MTL challenge, we propose Mod-Squad, a new\nmodel that is Modularized into groups of experts (a 'Squad'). This structure\nallows us to formalize cooperation and specialization as the process of\nmatching experts and tasks. We optimize this matching process during the\ntraining of a single model. Specifically, we incorporate mixture of experts\n(MoE) layers into a transformer model, with a new loss that incorporates the\nmutual dependence between tasks and experts. As a result, only a small set of\nexperts are activated for each task. This prevents the sharing of the entire\nbackbone model between all tasks, which strengthens the model, especially when\nthe training set size and the number of tasks scale up. More interestingly, for\neach task, we can extract the small set of experts as a standalone model that\nmaintains the same performance as the large model. Extensive experiments on the\nTaskonomy dataset with 13 vision tasks and the PASCAL-Context dataset with 5\nvision tasks show the superiority of our approach.\n","authors":["Zitian Chen","Yikang Shen","Mingyu Ding","Zhenfang Chen","Hengshuang Zhao","Erik Learned-Miller","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2212.08066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08059v1","updated":"2022-12-15T18:59:12Z","published":"2022-12-15T18:59:12Z","title":"Rethinking Vision Transformers for MobileNet Size and Speed","summary":"  With the success of Vision Transformers (ViTs) in computer vision tasks,\nrecent arts try to optimize the performance and complexity of ViTs to enable\nefficient deployment on mobile devices. Multiple approaches are proposed to\naccelerate attention mechanism, improve inefficient designs, or incorporate\nmobile-friendly lightweight convolutions to form hybrid architectures. However,\nViT and its variants still have higher latency or considerably more parameters\nthan lightweight CNNs, even true for the years-old MobileNet. In practice,\nlatency and size are both crucial for efficient deployment on\nresource-constraint hardware. In this work, we investigate a central question,\ncan transformer models run as fast as MobileNet and maintain a similar size? We\nrevisit the design choices of ViTs and propose an improved supernet with low\nlatency and high parameter efficiency. We further introduce a fine-grained\njoint search strategy that can find efficient architectures by optimizing\nlatency and number of parameters simultaneously. The proposed models,\nEfficientFormerV2, achieve about $4\\%$ higher top-1 accuracy than MobileNetV2\nand MobileNetV2$\\times1.4$ on ImageNet-1K with similar latency and parameters.\nWe demonstrate that properly designed and optimized vision transformers can\nachieve high performance with MobileNet-level size and speed.\n","authors":["Yanyu Li","Ju Hu","Yang Wen","Georgios Evangelidis","Kamyar Salahi","Yanzhi Wang","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2212.08059v1.pdf","comment":"Code is available at:\n  https://github.com/snap-research/EfficientFormer"},{"id":"http://arxiv.org/abs/2212.08057v1","updated":"2022-12-15T18:58:56Z","published":"2022-12-15T18:58:56Z","title":"Real-Time Neural Light Field on Mobile Devices","summary":"  Recent efforts in Neural Rendering Fields (NeRF) have shown impressive\nresults on novel view synthesis by utilizing implicit neural representation to\nrepresent 3D scenes. Due to the process of volumetric rendering, the inference\nspeed for NeRF is extremely slow, limiting the application scenarios of\nutilizing NeRF on resource-constrained hardware, such as mobile devices. Many\nworks have been conducted to reduce the latency of running NeRF models.\nHowever, most of them still require high-end GPU for acceleration or extra\nstorage memory, which is all unavailable on mobile devices. Another emerging\ndirection utilizes the neural light field (NeLF) for speedup, as only one\nforward pass is performed on a ray to predict the pixel color. Nevertheless, to\nreach a similar rendering quality as NeRF, the network in NeLF is designed with\nintensive computation, which is not mobile-friendly. In this work, we propose\nan efficient network that runs in real-time on mobile devices for neural\nrendering. We follow the setting of NeLF to train our network. Unlike existing\nworks, we introduce a novel network architecture that runs efficiently on\nmobile devices with low latency and small size, i.e., saving $15\\times \\sim\n24\\times$ storage compared with MobileNeRF. Our model achieves high-resolution\ngeneration while maintaining real-time inference for both synthetic and\nreal-world scenes on mobile devices, e.g., $18.04$ms (iPhone 13) for rendering\none $1008\\times756$ image of real 3D scenes. Additionally, we achieve similar\nimage quality as NeRF and better quality than MobileNeRF (PSNR $26.15$ vs.\n$25.91$ on the real-world forward-facing dataset).\n","authors":["Junli Cao","Huan Wang","Pavlo Chemerys","Vladislav Shakhrai","Ju Hu","Yun Fu","Denys Makoviichuk","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2212.08057v1.pdf","comment":"Project page: https://snap-research.github.io/MobileR2L/"},{"id":"http://arxiv.org/abs/2212.08054v1","updated":"2022-12-15T18:58:07Z","published":"2022-12-15T18:58:07Z","title":"DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue","summary":"  Modern virtual assistants use internal semantic parsing engines to convert\nuser utterances to actionable commands. However, prior work has demonstrated\nthat semantic parsing is a difficult multilingual transfer task with low\ntransfer efficiency compared to other tasks. In global markets such as India\nand Latin America, this is a critical issue as switching between languages is\nprevalent for bilingual users. In this work we dramatically improve the\nzero-shot performance of a multilingual and codeswitched semantic parsing\nsystem using two stages of multilingual alignment. First, we show that\nconstrastive alignment pretraining improves both English performance and\ntransfer efficiency. We then introduce a constrained optimization approach for\nhyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned\nMultilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and\n81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing\nbenchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer\nparameters.\n","authors":["William Held","Christopher Hidey","Fei Liu","Eric Zhu","Rahul Goel","Diyi Yang","Rushin Shah"],"pdf_url":"https://arxiv.org/pdf/2212.08054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04486v2","updated":"2022-12-15T18:57:30Z","published":"2022-12-08T18:56:37Z","title":"DP-RAFT: A Differentially Private Recipe for Accelerated Fine-Tuning","summary":"  A major direction in differentially private machine learning is\ndifferentially private fine-tuning: pretraining a model on a source of \"public\ndata\" and transferring the extracted features to downstream tasks.\n  This is an important setting because many industry deployments fine-tune\npublicly available feature extractors on proprietary data for downstream tasks.\n  In this paper, we carefully integrate techniques, both new and from prior\nwork, to solve benchmark tasks in computer vision and natural language\nprocessing using differentially private fine-tuning. Our key insight is that by\naccelerating training with the choice of key hyperparameters, we can quickly\ndrive the model parameters to regions in parameter space where the impact of\nnoise is minimized. We obtain new state-of-the art performance on CIFAR10,\nCIFAR100, FashionMNIST, STL10, and PersonaChat, including $99 \\%$ on CIFAR10\nfor $\\varepsilon=1, \\delta=1e-5$-DP.\n","authors":["Ashwinee Panda","Xinyu Tang","Vikash Sehwag","Saeed Mahloujifar","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2212.04486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08049v1","updated":"2022-12-15T18:55:23Z","published":"2022-12-15T18:55:23Z","title":"Sliced Optimal Partial Transport","summary":"  Optimal transport (OT) has become exceedingly popular in machine learning,\ndata science, and computer vision. The core assumption in the OT problem is the\nequal total amount of mass in source and target measures, which limits its\napplication. Optimal Partial Transport (OPT) is a recently proposed solution to\nthis limitation. Similar to the OT problem, the computation of OPT relies on\nsolving a linear programming problem (often in high dimensions), which can\nbecome computationally prohibitive. In this paper, we propose an efficient\nalgorithm for calculating the OPT problem between two non-negative measures in\none dimension. Next, following the idea of sliced OT distances, we utilize\nslicing to define the sliced OPT distance. Finally, we demonstrate the\ncomputational and accuracy benefits of the sliced OPT-based method in various\nnumerical experiments. In particular, we show an application of our proposed\nSliced-OPT in noisy point cloud registration.\n","authors":["Yikun Bai","Bernard Schmitzer","Mathew Thorpe","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2212.08049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08046v1","updated":"2022-12-15T18:52:27Z","published":"2022-12-15T18:52:27Z","title":"Silhouette: Toward Performance-Conscious and Transferable CPU Embeddings","summary":"  Learned embeddings are widely used to obtain concise data representation and\nenable transfer learning between different data sets and tasks. In this paper,\nwe present Silhouette, our approach that leverages publicly-available\nperformance data sets to learn CPU embeddings. We show how these embeddings\nenable transfer learning between data sets of different types and sizes. Each\nof these scenarios leads to an improvement in accuracy for the target data set.\n","authors":["Tarikul Islam Papon","Abdul Wasay"],"pdf_url":"https://arxiv.org/pdf/2212.08046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08032v1","updated":"2022-12-15T18:41:15Z","published":"2022-12-15T18:41:15Z","title":"Demonstration of machine-learning-enhanced Bayesian quantum state\n  estimation","summary":"  Machine learning (ML) has found broad applicability in quantum information\nscience in topics as diverse as experimental design, state classification, and\neven studies on quantum foundations. Here, we experimentally realize an\napproach for defining custom prior distributions that are automatically tuned\nusing ML for use with Bayesian quantum state estimation methods. Previously,\nresearchers have looked to Bayesian quantum state tomography due to its unique\nadvantages like natural uncertainty quantification, the return of reliable\nestimates under any measurement condition, and minimal mean-squared error.\nHowever, practical challenges related to long computation times and conceptual\nissues concerning how to incorporate prior knowledge most suitably can\novershadow these benefits. Using both simulated and experimental measurement\nresults, we demonstrate that ML-defined prior distributions reduce net\nconvergence times and provide a natural way to incorporate both implicit and\nexplicit information directly into the prior distribution. These results\nconstitute a promising path toward practical implementations of Bayesian\nquantum state tomography.\n","authors":["Sanjaya Lohani","Joseph M. Lukens","Atiyya A. Davis","Amirali Khannejad","Sangita Regmi","Daniel E. Jones","Ryan T. Glasser","Thomas A. Searles","Brian T. Kirby"],"pdf_url":"https://arxiv.org/pdf/2212.08032v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.08013v1","updated":"2022-12-15T18:18:38Z","published":"2022-12-15T18:18:38Z","title":"FlexiViT: One Model for All Patch Sizes","summary":"  Vision Transformers convert images to sequences by slicing them into patches.\nThe size of these patches controls a speed/accuracy tradeoff, with smaller\npatches leading to higher accuracy at greater computational cost, but changing\nthe patch size typically requires retraining the model. In this paper, we\ndemonstrate that simply randomizing the patch size at training time leads to a\nsingle set of weights that performs well across a wide range of patch sizes,\nmaking it possible to tailor the model to different compute budgets at\ndeployment time. We extensively evaluate the resulting model, which we call\nFlexiViT, on a wide range of tasks, including classification, image-text\nretrieval, open-world detection, panoptic segmentation, and semantic\nsegmentation, concluding that it usually matches, and sometimes outperforms,\nstandard ViT models trained at a single patch size in an otherwise identical\nsetup. Hence, FlexiViT training is a simple drop-in improvement for ViT that\nmakes it easy to add compute-adaptive capabilities to most models relying on a\nViT backbone architecture. Code and pre-trained models are available at\nhttps://github.com/google-research/big_vision\n","authors":["Lucas Beyer","Pavel Izmailov","Alexander Kolesnikov","Mathilde Caron","Simon Kornblith","Xiaohua Zhai","Matthias Minderer","Michael Tschannen","Ibrahim Alabdulmohsin","Filip Pavetic"],"pdf_url":"https://arxiv.org/pdf/2212.08013v1.pdf","comment":"Code and pre-trained models available at\n  https://github.com/google-research/big_vision. All authors made significant\n  technical contributions"},{"id":"http://arxiv.org/abs/2201.10737v5","updated":"2022-12-15T17:45:50Z","published":"2022-01-26T03:50:02Z","title":"Class-Aware Adversarial Transformers for Medical Image Segmentation","summary":"  Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: (1) existing\nmethods fail to capture the important features of the images due to the naive\ntokenization scheme; (2) the models suffer from information loss because they\nonly consider single-scale feature representations; and (3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCASTformer, a novel type of adversarial transformers, for 2D medical image\nsegmentation. First, we take advantage of the pyramid structure to construct\nmulti-scale representations and handle multi-scale variations. We then design a\nnovel class-aware transformer module to better learn the discriminative regions\nof objects with semantic structures. Lastly, we utilize an adversarial training\nstrategy that boosts segmentation accuracy and correspondingly allows a\ntransformer-based discriminator to capture high-level semantically correlated\ncontents and low-level anatomical features. Our experiments demonstrate that\nCASTformer dramatically outperforms previous state-of-the-art transformer-based\napproaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in\nDice over previous models. Further qualitative experiments provide a more\ndetailed picture of the model's inner workings, shed light on the challenges in\nimproved transparency, and demonstrate that transfer learning can greatly\nimprove performance and reduce the size of medical image datasets in training,\nmaking CASTformer a strong starting point for downstream medical image analysis\ntasks.\n","authors":["Chenyu You","Ruihan Zhao","Fenglin Liu","Siyuan Dong","Sandeep Chinchali","Ufuk Topcu","Lawrence Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2201.10737v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07992v1","updated":"2022-12-15T17:44:31Z","published":"2022-12-15T17:44:31Z","title":"Alternating Objectives Generates Stronger PGD-Based Adversarial Attacks","summary":"  Designing powerful adversarial attacks is of paramount importance for the\nevaluation of $\\ell_p$-bounded adversarial defenses. Projected Gradient Descent\n(PGD) is one of the most effective and conceptually simple algorithms to\ngenerate such adversaries. The search space of PGD is dictated by the steepest\nascent directions of an objective. Despite the plethora of objective function\nchoices, there is no universally superior option and robustness overestimation\nmay arise from ill-suited objective selection. Driven by this observation, we\npostulate that the combination of different objectives through a simple loss\nalternating scheme renders PGD more robust towards design choices. We\nexperimentally verify this assertion on a synthetic-data example and by\nevaluating our proposed method across 25 different $\\ell_{\\infty}$-robust\nmodels and 3 datasets. The performance improvement is consistent, when compared\nto the single loss counterparts. In the CIFAR-10 dataset, our strongest\nadversarial attack outperforms all of the white-box components of AutoAttack\n(AA) ensemble, as well as the most powerful attacks existing on the literature,\nachieving state-of-the-art results in the computational budget of our study\n($T=100$, no restarts).\n","authors":["Nikolaos Antoniou","Efthymios Georgiou","Alexandros Potamianos"],"pdf_url":"https://arxiv.org/pdf/2212.07992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07983v1","updated":"2022-12-15T17:31:54Z","published":"2022-12-15T17:31:54Z","title":"Vision Transformers are Parameter-Efficient Audio-Visual Learners","summary":"  Vision transformers (ViTs) have achieved impressive results on various\ncomputer vision tasks in the last several years. In this work, we study the\ncapability of frozen ViTs, pretrained only on visual data, to generalize to\naudio-visual data without finetuning any of its original parameters. To do so,\nwe propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained\nViTs to audio-visual tasks by injecting a small number of trainable parameters\ninto every layer of a frozen ViT. To efficiently fuse visual and audio cues,\nour LAVISH adapter uses a small set of latent tokens, which form an attention\nbottleneck, thus, eliminating the quadratic cost of standard cross-attention.\nCompared to the existing modality-specific audio-visual methods, our approach\nachieves competitive or even better performance on various audio-visual tasks\nwhile using fewer tunable parameters and without relying on costly audio\npretraining or external audio encoders. Our code is available at\nhttps://genjib.github.io/project_page/LAVISH/\n","authors":["Yan-Bo Lin","Yi-Lin Sung","Jie Lei","Mohit Bansal","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2212.07983v1.pdf","comment":"project page: https://genjib.github.io/project_page/LAVISH/"},{"id":"http://arxiv.org/abs/2212.07967v1","updated":"2022-12-15T17:01:56Z","published":"2022-12-15T17:01:56Z","title":"Distributed-Training-and-Execution Multi-Agent Reinforcement Learning\n  for Power Control in HetNet","summary":"  In heterogeneous networks (HetNets), the overlap of small cells and the macro\ncell causes severe cross-tier interference. Although there exist some\napproaches to address this problem, they usually require global channel state\ninformation, which is hard to obtain in practice, and get the sub-optimal power\nallocation policy with high computational complexity. To overcome these\nlimitations, we propose a multi-agent deep reinforcement learning (MADRL) based\npower control scheme for the HetNet, where each access point makes power\ncontrol decisions independently based on local information. To promote\ncooperation among agents, we develop a penalty-based Q learning (PQL) algorithm\nfor MADRL systems. By introducing regularization terms in the loss function,\neach agent tends to choose an experienced action with high reward when\nrevisiting a state, and thus the policy updating speed slows down. In this way,\nan agent's policy can be learned by other agents more easily, resulting in a\nmore efficient collaboration process. We then implement the proposed PQL in the\nconsidered HetNet and compare it with other distributed-training-and-execution\n(DTE) algorithms. Simulation results show that our proposed PQL can learn the\ndesired power control policy from a dynamic environment where the locations of\nusers change episodically and outperform existing DTE MADRL algorithms.\n","authors":["Kaidi Xu","Nguyen Van Huynh","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2212.07967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07959v1","updated":"2022-12-15T16:49:23Z","published":"2022-12-15T16:49:23Z","title":"Scalable Bayesian Uncertainty Quantification for Neural Network\n  Potentials: Promise and Pitfalls","summary":"  Neural network (NN) potentials promise highly accurate molecular dynamics\n(MD) simulations within the computational complexity of classical MD force\nfields. However, when applied outside their training domain, NN potential\npredictions can be inaccurate, increasing the need for Uncertainty\nQuantification (UQ). Bayesian modeling provides the mathematical framework for\nUQ, but classical Bayesian methods based on Markov chain Monte Carlo (MCMC) are\ncomputationally intractable for NN potentials. By training graph NN potentials\nfor coarse-grained systems of liquid water and alanine dipeptide, we\ndemonstrate here that scalable Bayesian UQ via stochastic gradient MCMC\n(SG-MCMC) yields reliable uncertainty estimates for MD observables. We show\nthat cold posteriors can reduce the required training data size and that for\nreliable UQ, multiple Markov chains are needed. Additionally, we find that\nSG-MCMC and the Deep Ensemble method achieve comparable results, despite\nshorter training and less hyperparameter tuning of the latter. We show that\nboth methods can capture aleatoric and epistemic uncertainty reliably, but not\nsystematic uncertainty, which needs to be minimized by adequate modeling to\nobtain accurate credible intervals for MD observables. Our results represent a\nstep towards accurate UQ that is of vital importance for trustworthy NN\npotential-based MD simulations required for decision-making in practice.\n","authors":["Stephan Thaler","Gregor Doehner","Julija Zavadlav"],"pdf_url":"https://arxiv.org/pdf/2212.07959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07951v1","updated":"2022-12-15T16:34:39Z","published":"2022-12-15T16:34:39Z","title":"A Data Source Dependency Analysis Framework for Large Scale Data Science\n  Projects","summary":"  Dependency hell is a well-known pain point in the development of large\nsoftware projects and machine learning (ML) code bases are not immune from it.\nIn fact, ML applications suffer from an additional form, namely, \"data source\ndependency hell\". This term refers to the central role played by data and its\nunique quirks that often lead to unexpected failures of ML models which cannot\nbe explained by code changes. In this paper, we present an automated dependency\nmapping framework that allows MLOps engineers to monitor the whole dependency\nmap of their models in a fast paced engineering environment and thus mitigate\nahead of time the consequences of any data source changes (e.g., re-train\nmodel, ignore data, set default data etc.). Our system is based on a unified\nand generic approach, employing techniques from static analysis, from which\ndata sources can be identified reliably for any type of dependency on a wide\nrange of source languages and artefacts. The dependency mapping framework is\nexposed as a REST web API where the only input is the path to the Git\nrepository hosting the code base. Currently used by MLOps engineers at\nMicrosoft, we expect such dependency map APIs to be adopted more widely by\nMLOps engineers in the future.\n","authors":["Laurent Boué","Pratap Kunireddy","Pavle Subotić"],"pdf_url":"https://arxiv.org/pdf/2212.07951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1901.11457v8","updated":"2022-12-15T16:30:46Z","published":"2019-01-31T16:30:41Z","title":"Improving SGD convergence by online linear regression of gradients in\n  multiple statistically relevant directions","summary":"  Deep neural networks are usually trained with stochastic gradient descent\n(SGD), which minimizes objective function using very rough approximations of\ngradient, only averaging to the real gradient. Standard approaches like\nmomentum or ADAM only consider a single direction, and do not try to model\ndistance from extremum - neglecting valuable information from calculated\nsequence of gradients, often stagnating in some suboptimal plateau. Second\norder methods could exploit these missed opportunities, however, beside\nsuffering from very large cost and numerical instabilities, many of them\nattract to suboptimal points like saddles due to negligence of signs of\ncurvatures (as eigenvalues of Hessian).\n  Saddle-free Newton method is a rare example of addressing this issue -\nchanges saddle attraction into repulsion, and was shown to provide essential\nimprovement for final value this way. However, it neglects noise while\nmodelling second order behavior, focuses on Krylov subspace for numerical\nreasons, and requires costly eigendecomposion.\n  Maintaining SFN advantages, there are proposed inexpensive ways for\nexploiting these opportunities. Second order behavior is linear dependence of\nfirst derivative - we can optimally estimate it from sequence of noisy\ngradients with least square linear regression, in online setting here: with\nweakening weights of old gradients. Statistically relevant subspace is\nsuggested by PCA of recent noisy gradients - in online setting it can be made\nby slowly rotating considered directions toward new gradients, gradually\nreplacing old directions with recent statistically relevant. Eigendecomposition\ncan be also performed online: with regularly performed step of QR method to\nmaintain diagonal Hessian. Outside the second order modeled subspace we can\nsimultaneously perform gradient descent.\n","authors":["Jarek Duda"],"pdf_url":"https://arxiv.org/pdf/1901.11457v8.pdf","comment":"11 pages, 2 figure"},{"id":"http://arxiv.org/abs/2212.07946v1","updated":"2022-12-15T16:28:06Z","published":"2022-12-15T16:28:06Z","title":"Combining information-seeking exploration and reward maximization:\n  Unified inference on continuous state and action spaces under partial\n  observability","summary":"  Reinforcement learning (RL) gained considerable attention by creating\ndecision-making agents that maximize rewards received from fully observable\nenvironments. However, many real-world problems are partially or noisily\nobservable by nature, where agents do not receive the true and complete state\nof the environment. Such problems are formulated as partially observable Markov\ndecision processes (POMDPs). Some studies applied RL to POMDPs by recalling\nprevious decisions and observations or inferring the true state of the\nenvironment from received observations. Nevertheless, aggregating observations\nand decisions over time is impractical for environments with high-dimensional\ncontinuous state and action spaces. Moreover, so-called inference-based RL\napproaches require large number of samples to perform well since agents eschew\nuncertainty in the inferred state for the decision-making. Active inference is\na framework that is naturally formulated in POMDPs and directs agents to select\ndecisions by minimising expected free energy (EFE). This supplies\nreward-maximising (exploitative) behaviour in RL, with an information-seeking\n(exploratory) behaviour. Despite this exploratory behaviour of active\ninference, its usage is limited to discrete state and action spaces due to the\ncomputational difficulty of the EFE. We propose a unified principle for joint\ninformation-seeking and reward maximization that clarifies a theoretical\nconnection between active inference and RL, unifies active inference and RL,\nand overcomes their aforementioned limitations. Our findings are supported by\nstrong theoretical analysis. The proposed framework's superior exploration\nproperty is also validated by experimental results on partial observable tasks\nwith high-dimensional continuous state and action spaces. Moreover, the results\nshow that our model solves reward-free problems, making task reward design\noptional.\n","authors":["Parvin Malekzadeh","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2212.07946v1.pdf","comment":"34 pages, 7 figures"},{"id":"http://arxiv.org/abs/2212.07944v1","updated":"2022-12-15T16:23:25Z","published":"2022-12-15T16:23:25Z","title":"Variable Clustering via Distributionally Robust Nodewise Regression","summary":"  We study a multi-factor block model for variable clustering and connect it to\nthe regularized subspace clustering by formulating a distributionally robust\nversion of the nodewise regression. To solve the latter problem, we derive a\nconvex relaxation, provide guidance on selecting the size of the robust region,\nand hence the regularization weighting parameter, based on the data, and\npropose an ADMM algorithm for implementation. We validate our method in an\nextensive simulation study. Finally, we propose and apply a variant of our\nmethod to stock return data, obtain interpretable clusters that facilitate\nportfolio selection and compare its out-of-sample performance with other\nclustering methods in an empirical study.\n","authors":["Kaizheng Wang","Xiao Xu","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2212.07944v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2212.07939v1","updated":"2022-12-15T16:17:03Z","published":"2022-12-15T16:17:03Z","title":"RWEN-TTS: Relation-aware Word Encoding Network for Natural\n  Text-to-Speech Synthesis","summary":"  With the advent of deep learning, a huge number of text-to-speech (TTS)\nmodels which produce human-like speech have emerged. Recently, by introducing\nsyntactic and semantic information w.r.t the input text, various approaches\nhave been proposed to enrich the naturalness and expressiveness of TTS models.\nAlthough these strategies showed impressive results, they still have some\nlimitations in utilizing language information. First, most approaches only use\ngraph networks to utilize syntactic and semantic information without\nconsidering linguistic features. Second, most previous works do not explicitly\nconsider adjacent words when encoding syntactic and semantic information, even\nthough it is obvious that adjacent words are usually meaningful when encoding\nthe current word. To address these issues, we propose Relation-aware Word\nEncoding Network (RWEN), which effectively allows syntactic and semantic\ninformation based on two modules (i.e., Semantic-level Relation Encoding and\nAdjacent Word Relation Encoding). Experimental results show substantial\nimprovements compared to previous works.\n","authors":["Shinhyeok Oh","HyeongRae Noh","Yoonseok Hong","Insoo Oh"],"pdf_url":"https://arxiv.org/pdf/2212.07939v1.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2212.07936v1","updated":"2022-12-15T16:11:40Z","published":"2022-12-15T16:11:40Z","title":"A Study on the Intersection of GPU Utilization and CNN Inference","summary":"  There has been significant progress in developing neural network\narchitectures that both achieve high predictive performance and that also\nachieve high application-level inference throughput (e.g., frames per second).\nAnother metric of increasing importance is GPU utilization during inference:\nthe measurement of how well a deployed neural network uses the computational\ncapabilities of the GPU on which it runs. Achieving high GPU utilization is\ncritical to increasing application-level throughput and ensuring a good return\non investment for deploying GPUs.\n  This paper analyzes the GPU utilization of convolutional neural network (CNN)\ninference. We first survey the GPU utilization of CNNs to show that there is\nroom to improve the GPU utilization of many of these CNNs. We then investigate\nthe GPU utilization of networks within a neural architecture search (NAS)\nsearch space, and explore how using GPU utilization as a metric could\npotentially be used to accelerate NAS itself. Our study makes the case that\nthere is room to improve the inference-time GPU utilization of CNNs and that\nknowledge of GPU utilization has the potential to benefit even applications\nthat do not target utilization itself. We hope that the results of this study\nwill spur future innovation in designing GPU-efficient neural networks.\n","authors":["Jack Kosaian","Amar Phanishayee"],"pdf_url":"https://arxiv.org/pdf/2212.07936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07933v1","updated":"2022-12-15T16:09:47Z","published":"2022-12-15T16:09:47Z","title":"Bridging POMDPs and Bayesian decision making for robust maintenance\n  planning under model uncertainty: An application to railway systems","summary":"  Structural Health Monitoring (SHM) describes a process for inferring\nquantifiable metrics of structural condition, which can serve as input to\nsupport decisions on the operation and maintenance of infrastructure assets.\nGiven the long lifespan of critical structures, this problem can be cast as a\nsequential decision making problem over prescribed horizons. Partially\nObservable Markov Decision Processes (POMDPs) offer a formal framework to solve\nthe underlying optimal planning task. However, two issues can undermine the\nPOMDP solutions. Firstly, the need for a model that can adequately describe the\nevolution of the structural condition under deterioration or corrective actions\nand, secondly, the non-trivial task of recovery of the observation process\nparameters from available monitoring data. Despite these potential challenges,\nthe adopted POMDP models do not typically account for uncertainty on model\nparameters, leading to solutions which can be unrealistically confident. In\nthis work, we address both key issues. We present a framework to estimate POMDP\ntransition and observation model parameters directly from available data, via\nMarkov Chain Monte Carlo (MCMC) sampling of a Hidden Markov Model (HMM)\nconditioned on actions. The MCMC inference estimates distributions of the\ninvolved model parameters. We then form and solve the POMDP problem by\nexploiting the inferred distributions, to derive solutions that are robust to\nmodel uncertainty. We successfully apply our approach on maintenance planning\nfor railway track assets on the basis of a \"fractal value\" indicator, which is\ncomputed from actual railway monitoring data.\n","authors":["Giacomo Arcieri","Cyprien Hoelzl","Oliver Schwery","Daniel Straub","Konstantinos G. Papakonstantinou","Eleni Chatzi"],"pdf_url":"https://arxiv.org/pdf/2212.07933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.10904v3","updated":"2022-12-15T16:00:42Z","published":"2020-02-25T14:44:25Z","title":"Reward Shaping for Human Learning via Inverse Reinforcement Learning","summary":"  Humans are spectacular reinforcement learners, constantly learning from and\nadjusting to experience and feedback. Unfortunately, this doesn't necessarily\nmean humans are fast learners. When tasks are challenging, learning can become\nunacceptably slow. Fortunately, humans do not have to learn tabula rasa, and\nlearning speed can be greatly increased with learning aids. In this work we\nvalidate a new type of learning aid -- reward shaping for humans via inverse\nreinforcement learning (IRL). The goal of this aid is to increase the speed\nwith which humans can learn good policies for specific tasks. Furthermore this\napproach compliments alternative machine learning techniques such as safety\nfeatures that try to prevent individuals from making poor decisions. To achieve\nour results we first extend a well known IRL algorithm via kernel methods.\nAfterwards we conduct two human subjects experiments using an online game where\nplayers have limited time to learn a good policy. We show with statistical\nsignificance that players who receive our learning aid are able to approach\ndesired policies more quickly than the control group.\n","authors":["Mark A. Rucker","Layne T. Watson","Matthew S. Gerber","Laura E. Barnes"],"pdf_url":"https://arxiv.org/pdf/2002.10904v3.pdf","comment":"This paper has been modified considerably for resubmission to Journal\n  of Machine Learning Research, for source code, see\n  https://github.com/mrucker/kpirl-kla"},{"id":"http://arxiv.org/abs/2212.07923v1","updated":"2022-12-15T15:55:44Z","published":"2022-12-15T15:55:44Z","title":"The Effects of Character-Level Data Augmentation on Style-Based Dating\n  of Historical Manuscripts","summary":"  Identifying the production dates of historical manuscripts is one of the main\ngoals for paleographers when studying ancient documents. Automatized methods\ncan provide paleographers with objective tools to estimate dates more\naccurately. Previously, statistical features have been used to date digitized\nhistorical manuscripts based on the hypothesis that handwriting styles change\nover periods. However, the sparse availability of such documents poses a\nchallenge in obtaining robust systems. Hence, the research of this article\nexplores the influence of data augmentation on the dating of historical\nmanuscripts. Linear Support Vector Machines were trained with k-fold\ncross-validation on textural and grapheme-based features extracted from\nhistorical manuscripts of different collections, including the Medieval\nPaleographical Scale, early Aramaic manuscripts, and the Dead Sea Scrolls.\nResults show that training models with augmented data improve the performance\nof historical manuscripts dating by 1% - 3% in cumulative scores. Additionally,\nthis indicates further enhancement possibilities by considering models specific\nto the features and the documents' scripts.\n","authors":["Lisa Koopmans","Maruf A. Dhali","Lambert Schomaker"],"pdf_url":"https://arxiv.org/pdf/2212.07923v1.pdf","comment":"Accepted after the peer-review process for ICPRAM 2023; scheduled to\n  be presented on 22 February 2023"},{"id":"http://arxiv.org/abs/2212.07919v1","updated":"2022-12-15T15:52:39Z","published":"2022-12-15T15:52:39Z","title":"ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning","summary":"  Large language models show improved downstream task performance when prompted\nto generate step-by-step reasoning to justify their final answers. These\nreasoning steps greatly improve model interpretability and verification, but\nobjectively studying their correctness (independent of the final answer) is\ndifficult without reliable methods for automatic evaluation. We simply do not\nknow how often the stated reasoning steps actually support the final end task\npredictions. In this work, we present ROSCOE, a suite of interpretable,\nunsupervised automatic scores that improve and extend previous text generation\nevaluation metrics. To evaluate ROSCOE against baseline metrics, we design a\ntypology of reasoning errors and collect synthetic and human evaluation scores\non commonly used reasoning datasets. In contrast with existing metrics, ROSCOE\ncan measure semantic consistency, logicality, informativeness, fluency, and\nfactuality - among other traits - by leveraging properties of step-by-step\nrationales. We empirically verify the strength of our metrics on five human\nannotated and six programmatically perturbed diagnostics datasets - covering a\ndiverse set of tasks that require reasoning skills and show that ROSCOE can\nconsistently outperform baseline metrics.\n","authors":["Olga Golovneva","Moya Chen","Spencer Poff","Martin Corredor","Luke Zettlemoyer","Maryam Fazel-Zarandi","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2212.07919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07918v1","updated":"2022-12-15T15:52:18Z","published":"2022-12-15T15:52:18Z","title":"Construction of a Surrogate Model: Multivariate Time Series Prediction\n  with a Hybrid Model","summary":"  Recent developments of advanced driver-assistance systems necessitate an\nincreasing number of tests to validate new technologies. These tests cannot be\ncarried out on track in a reasonable amount of time and automotive groups rely\non simulators to perform most tests. The reliability of these simulators for\nconstantly refined tasks is becoming an issue and, to increase the number of\ntests, the industry is now developing surrogate models, that should mimic the\nbehavior of the simulator while being much faster to run on specific tasks.\n  In this paper we aim to construct a surrogate model to mimic and replace the\nsimulator. We first test several classical methods such as random forests,\nridge regression or convolutional neural networks. Then we build three hybrid\nmodels that use all these methods and combine them to obtain an efficient\nhybrid surrogate model.\n","authors":["Clara Carlier","Arnaud Franju","Matthieu Lerasle","Mathias Obrebski"],"pdf_url":"https://arxiv.org/pdf/2212.07918v1.pdf","comment":"6 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2212.06653v2","updated":"2022-12-15T15:36:36Z","published":"2022-12-10T22:50:00Z","title":"Spatiotemporal Residual Regularization with Dynamic Mixtures for Traffic\n  Forecasting","summary":"  Existing deep learning-based traffic forecasting models are mainly trained\nwith MSE (or MAE) as the loss function, assuming that residuals/errors follow\nindependent and isotropic Gaussian (or Laplacian) distribution for simplicity.\nHowever, this assumption rarely holds for real-world traffic forecasting tasks,\nwhere the unexplained residuals are often correlated in both space and time. In\nthis study, we propose Spatiotemporal Residual Regularization by modeling\nresiduals with a dynamic (e.g., time-varying) mixture of zero-mean multivariate\nGaussian distribution with learnable spatiotemporal covariance matrices. This\napproach allows us to directly capture spatiotemporally correlated residuals.\nFor scalability, we model the spatiotemporal covariance for each mixture\ncomponent using a Kronecker product structure, which significantly reduces the\nnumber of parameters and computation complexity. We evaluate the performance of\nthe proposed method on a traffic speed forecasting task. Our results show that,\nby properly modeling residual distribution, the proposed method not only\nimproves the model performance but also provides interpretable structures.\n","authors":["Seongjin Choi","Nicolas Saunier","Martin Trepanier","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2212.06653v2.pdf","comment":"8 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2206.10535v2","updated":"2022-12-15T15:25:28Z","published":"2022-06-21T17:08:23Z","title":"EpiGRAF: Rethinking training of 3D GANs","summary":"  A very recent trend in generative modeling is building 3D-aware generators\nfrom 2D image collections. To induce the 3D bias, such models typically rely on\nvolumetric rendering, which is expensive to employ at high resolutions. During\nthe past months, there appeared more than 10 works that address this scaling\nissue by training a separate 2D decoder to upsample a low-resolution image (or\na feature tensor) produced from a pure 3D generator. But this solution comes at\na cost: not only does it break multi-view consistency (i.e. shape and texture\nchange when the camera moves), but it also learns the geometry in a low\nfidelity. In this work, we show that it is possible to obtain a high-resolution\n3D generator with SotA image quality by following a completely different route\nof simply training the model patch-wise. We revisit and improve this\noptimization scheme in two ways. First, we design a location- and scale-aware\ndiscriminator to work on patches of different proportions and spatial\npositions. Second, we modify the patch sampling strategy based on an annealed\nbeta distribution to stabilize training and accelerate the convergence. The\nresulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D\ngenerator, and we test it on four datasets (two introduced in this work) at\n$256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality,\nhigh-fidelity geometry and trains ${\\approx} 2.5 \\times$ faster than the\nupsampler-based counterparts. Project website:\nhttps://universome.github.io/epigraf.\n","authors":["Ivan Skorokhodov","Sergey Tulyakov","Yiqun Wang","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2206.10535v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07892v1","updated":"2022-12-15T15:21:28Z","published":"2022-12-15T15:21:28Z","title":"Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical\n  Systems","summary":"  Many, if not most, systems of interest in science are naturally described as\nnonlinear dynamical systems (DS). Empirically, we commonly access these systems\nthrough time series measurements, where often we have time series from\ndifferent types of data modalities simultaneously. For instance, we may have\nevent counts in addition to some continuous signal. While by now there are many\npowerful machine learning (ML) tools for integrating different data modalities\ninto predictive models, this has rarely been approached so far from the\nperspective of uncovering the underlying, data-generating DS (aka DS\nreconstruction). Recently, sparse teacher forcing (TF) has been suggested as an\nefficient control-theoretic method for dealing with exploding loss gradients\nwhen training ML models on chaotic DS. Here we incorporate this idea into a\nnovel recurrent neural network (RNN) training framework for DS reconstruction\nbased on multimodal variational autoencoders (MVAE). The forcing signal for the\nRNN is generated by the MVAE which integrates different types of simultaneously\ngiven time series data into a joint latent code optimal for DS reconstruction.\nWe show that this training method achieves significantly better reconstructions\non multimodal datasets generated from chaotic DS benchmarks than various\nalternative methods.\n","authors":["Manuel Brenner","Georgia Koppe","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2212.07892v1.pdf","comment":"Published as a workshop paper for the AAAI 2023 Workshop MLmDS"},{"id":"http://arxiv.org/abs/2212.07891v1","updated":"2022-12-15T15:20:58Z","published":"2022-12-15T15:20:58Z","title":"Emergent Behaviors in Multi-Agent Target Acquisition","summary":"  Only limited studies and superficial evaluations are available on agents'\nbehaviors and roles within a Multi-Agent System (MAS). We simulate a MAS using\nReinforcement Learning (RL) in a pursuit-evasion (a.k.a predator-prey pursuit)\ngame, which shares task goals with target acquisition, and we create different\nadversarial scenarios by replacing RL-trained pursuers' policies with two\ndistinct (non-RL) analytical strategies. Using heatmaps of agents' positions\n(state-space variable) over time, we are able to categorize an RL-trained\nevader's behaviors. The novelty of our approach entails the creation of an\ninfluential feature set that reveals underlying data regularities, which allow\nus to classify an agent's behavior. This classification may aid in catching the\n(enemy) targets by enabling us to identify and predict their behaviors, and\nwhen extended to pursuers, this approach towards identifying teammates'\nbehavior may allow agents to coordinate more effectively.\n","authors":["Piyush K. Sharma","Erin Zaroukian","Derrik E. Asher","Bryson Howell"],"pdf_url":"https://arxiv.org/pdf/2212.07891v1.pdf","comment":"This article appeared in the news at:\n  https://www.army.mil/article/258408/u_s_army_scientists_invent_a_method_to_characterize_ai_behavior"},{"id":"http://arxiv.org/abs/2211.10760v2","updated":"2022-12-15T15:00:12Z","published":"2022-11-19T18:18:52Z","title":"An experimental study on Synthetic Tabular Data Evaluation","summary":"  In this paper, we present the findings of various methodologies for measuring\nthe similarity of synthetic data generated from tabular data samples. We\nparticularly apply our research to the case where the synthetic data has many\nmore samples than the real data. This task has a special complexity: validating\nthe reliability of this synthetically generated data with a much higher number\nof samples than the original. We evaluated the most commonly used global\nmetrics found in the literature. We introduced a novel approach based on the\ndata's topological signature analysis. Topological data analysis has several\nadvantages in addressing this latter challenge. The study of qualitative\ngeometric information focuses on geometric properties while neglecting\nquantitative distance function values. This is especially useful with\nhigh-dimensional synthetic data where the sample size has been significantly\nincreased. It is comparable to introducing new data points into the data space\nwithin the limits set by the original data. Then, in large synthetic data\nspaces, points will be much more concentrated than in the original space, and\ntheir analysis will become much more sensitive to both the metrics used and\nnoise. Instead, the concept of \"closeness\" between points is used for\nqualitative geometric information. Finally, we suggest an approach based on\ndata Eigen vectors for evaluating the level of noise in synthetic data. This\napproach can also be used to assess the similarity of original and synthetic\ndata.\n","authors":["Javier Marin"],"pdf_url":"https://arxiv.org/pdf/2211.10760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07876v1","updated":"2022-12-15T14:49:01Z","published":"2022-12-15T14:49:01Z","title":"Forgetful Forests: high performance learning data structures for\n  streaming data under concept drift","summary":"  Database research can help machine learning performance in many ways. One way\nis to design better data structures. This paper combines the use of incremental\ncomputation and sequential and probabilistic filtering to enable \"forgetful\"\ntree-based learning algorithms to cope with concept drift data (i.e., data\nwhose function from input to classification changes over time).\n  The forgetful algorithms described in this paper achieve high time\nperformance while maintaining high quality predictions on streaming data.\nSpecifically, the algorithms are up to 24 times faster than state-of-the-art\nincremental algorithms with at most a 2% loss of accuracy, or at least twice\nfaster without any loss of accuracy. This makes such structures suitable for\nhigh volume streaming applications.\n","authors":["Zhehu Yuan","Yinqi Sun","Dennis Shasha"],"pdf_url":"https://arxiv.org/pdf/2212.07876v1.pdf","comment":"21 pages, 12 Figures, 7 algorithms"},{"id":"http://arxiv.org/abs/2206.05124v2","updated":"2022-12-15T14:34:52Z","published":"2022-06-10T14:00:06Z","title":"Stochastic Zeroth order Descent with Structured Directions","summary":"  We introduce and analyze Structured Stochastic Zeroth order Descent (S-SZD),\na finite difference approach which approximates a stochastic gradient on a set\nof $l\\leq d$ orthogonal directions, where $d$ is the dimension of the ambient\nspace. These directions are randomly chosen, and may change at each step. For\nsmooth convex functions we prove almost sure convergence of the iterates and a\nconvergence rate on the function values of the form $O(d/l k^{-c})$ for every\n$c<1/2$, which is arbitrarily close to the one of Stochastic Gradient Descent\n(SGD) in terms of number of iterations. Our bound also shows the benefits of\nusing $l$ multiple directions instead of one. For non-convex functions\nsatisfying the Polyak-{\\L}ojasiewicz condition, we establish the first\nconvergence rates for stochastic zeroth order algorithms under such an\nassumption. We corroborate our theoretical findings in numerical simulations\nwhere assumptions are satisfied and on the real-world problem of\nhyper-parameter optimization, observing that S-SZD has very good practical\nperformances.\n","authors":["Marco Rando","Cesare Molinari","Silvia Villa","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2206.05124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07860v1","updated":"2022-12-15T14:28:56Z","published":"2022-12-15T14:28:56Z","title":"Multi-Level Association Rule Mining for Wireless Network Time Series\n  Data","summary":"  Key performance indicators(KPIs) are of great significance in the monitoring\nof wireless network service quality. The network service quality can be\nimproved by adjusting relevant configuration parameters(CPs) of the base\nstation. However, there are numerous CPs and different cells may affect each\nother, which bring great challenges to the association analysis of wireless\nnetwork data. In this paper, we propose an adjustable multi-level association\nrule mining framework, which can quantitatively mine association rules at each\nlevel with environmental information, including engineering parameters and\nperformance management(PMs), and it has interpretability at each level.\nSpecifically, We first cluster similar cells, then quantify KPIs and CPs, and\nintegrate expert knowledge into the association rule mining model, which\nimprove the robustness of the model. The experimental results in real world\ndataset prove the effectiveness of our method.\n","authors":["Chen Zhu","Chengbo Qiu","Shaoyu Dou","Minghao Liao"],"pdf_url":"https://arxiv.org/pdf/2212.07860v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.15089v3","updated":"2022-12-15T14:27:19Z","published":"2022-11-28T06:08:54Z","title":"Continuous diffusion for categorical data","summary":"  Diffusion models have quickly become the go-to paradigm for generative\nmodelling of perceptual signals (such as images and sound) through iterative\nrefinement. Their success hinges on the fact that the underlying physical\nphenomena are continuous. For inherently discrete and categorical data such as\nlanguage, various diffusion-inspired alternatives have been proposed. However,\nthe continuous nature of diffusion models conveys many benefits, and in this\nwork we endeavour to preserve it. We propose CDCD, a framework for modelling\ncategorical data with diffusion models that are continuous both in time and\ninput space. We demonstrate its efficacy on several language modelling tasks.\n","authors":["Sander Dieleman","Laurent Sartran","Arman Roshannai","Nikolay Savinov","Yaroslav Ganin","Pierre H. Richemond","Arnaud Doucet","Robin Strudel","Chris Dyer","Conor Durkan","Curtis Hawthorne","Rémi Leblond","Will Grathwohl","Jonas Adler"],"pdf_url":"https://arxiv.org/pdf/2211.15089v3.pdf","comment":"26 pages, 8 figures; corrections and additional information about\n  hyperparameters"},{"id":"http://arxiv.org/abs/2212.07852v1","updated":"2022-12-15T14:19:33Z","published":"2022-12-15T14:19:33Z","title":"The effects of gender bias in word embeddings on depression prediction","summary":"  Word embeddings are extensively used in various NLP problems as a\nstate-of-the-art semantic feature vector representation. Despite their success\non various tasks and domains, they might exhibit an undesired bias for\nstereotypical categories due to statistical and societal biases that exist in\nthe dataset they are trained on. In this study, we analyze the gender bias in\nfour different pre-trained word embeddings specifically for the depression\ncategory in the mental disorder domain. We use contextual and non-contextual\nembeddings that are trained on domain-independent as well as clinical\ndomain-specific data. We observe that embeddings carry bias for depression\ntowards different gender groups depending on the type of embeddings. Moreover,\nwe demonstrate that these undesired correlations are transferred to the\ndownstream task for depression phenotype recognition. We find that data\naugmentation by simply swapping gender words mitigates the bias significantly\nin the downstream task.\n","authors":["Gizem Sogancioglu","Heysem Kaya"],"pdf_url":"https://arxiv.org/pdf/2212.07852v1.pdf","comment":"accepted to and published at \"A Participatory Approach to AI for\n  Mental Health (PAI4MH)\" workshop, co-located with NeurIPS 2022"},{"id":"http://arxiv.org/abs/2210.03430v2","updated":"2022-12-15T14:14:55Z","published":"2022-10-07T10:01:06Z","title":"Monitoring MBE substrate deoxidation via RHEED image-sequence analysis\n  by deep learning","summary":"  Reflection high-energy electron diffraction (RHEED) is a powerful tool in\nmolecular beam epitaxy (MBE), but RHEED images are often difficult to\ninterpret, requiring experienced operators. We present an approach for\nautomated surveillance of GaAs substrate deoxidation in MBE reactors using deep\nlearning based RHEED image-sequence classification. Our approach consists of an\nnon-supervised auto-encoder (AE) for feature extraction, combined with a\nsupervised convolutional classifier network. We demonstrate that our\nlightweight network model can accurately identify the exact deoxidation moment.\nFurthermore we show that the approach is very robust and allows accurate\ndeoxidation detection during months without requiring re-training. The main\nadvantage of the approach is that it can be applied to raw RHEED images without\nrequiring further information such as the rotation angle, temperature, etc.\n","authors":["Abdourahman Khaireh-Walieh","Alexandre Arnoult","Sébastien Plissard","Peter R. Wiecha"],"pdf_url":"https://arxiv.org/pdf/2210.03430v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.07844v1","updated":"2022-12-15T14:05:32Z","published":"2022-12-15T14:05:32Z","title":"Differentiating Nonsmooth Solutions to Parametric Monotone Inclusion\n  Problems","summary":"  We leverage path differentiability and a recent result on nonsmooth implicit\ndifferentiation calculus to give sufficient conditions ensuring that the\nsolution to a monotone inclusion problem will be path differentiable, with\nformulas for computing its generalized gradient. A direct consequence of our\nresult is that these solutions happen to be differentiable almost everywhere.\nOur approach is fully compatible with automatic differentiation and comes with\nassumptions which are easy to check, roughly speaking: semialgebraicity and\nstrong monotonicity. We illustrate the scope of our results by considering\nthree fundamental composite problem settings: strongly convex problems, dual\nsolutions to convex minimization problems and primal-dual solutions to min-max\nproblems.\n","authors":["Jérôme Bolte","Edouard Pauwels","Antonio José Silveti-Falls"],"pdf_url":"https://arxiv.org/pdf/2212.07844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07839v1","updated":"2022-12-15T13:52:03Z","published":"2022-12-15T13:52:03Z","title":"TeTIm-Eval: a novel curated evaluation data set for comparing\n  text-to-image models","summary":"  Evaluating and comparing text-to-image models is a challenging problem.\nSignificant advances in the field have recently been made, piquing interest of\nvarious industrial sectors. As a consequence, a gold standard in the field\nshould cover a variety of tasks and application contexts. In this paper a novel\nevaluation approach is experimented, on the basis of: (i) a curated data set,\nmade by high-quality royalty-free image-text pairs, divided into ten\ncategories; (ii) a quantitative metric, the CLIP-score, (iii) a human\nevaluation task to distinguish, for a given text, the real and the generated\nimages. The proposed method has been applied to the most recent models, i.e.,\nDALLE2, Latent Diffusion, Stable Diffusion, GLIDE and Craiyon. Early\nexperimental results show that the accuracy of the human judgement is fully\ncoherent with the CLIP-score. The dataset has been made available to the\npublic.\n","authors":["Federico A. Galatolo","Mario G. C. A. Cimino","Edoardo Cogotti"],"pdf_url":"https://arxiv.org/pdf/2212.07839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07836v1","updated":"2022-12-15T13:46:15Z","published":"2022-12-15T13:46:15Z","title":"Spatially-resolved Thermometry from Line-of-Sight Emission Spectroscopy\n  via Machine Learning","summary":"  A methodology is proposed, which addresses the caveat that line-of-sight\nemission spectroscopy presents in that it cannot provide spatially resolved\ntemperature measurements in nonhomogeneous temperature fields. The aim of this\nresearch is to explore the use of data-driven models in measuring temperature\ndistributions in a spatially resolved manner using emission spectroscopy data.\nTwo categories of data-driven methods are analyzed: (i) Feature engineering and\nclassical machine learning algorithms, and (ii) end-to-end convolutional neural\nnetworks (CNN). In total, combinations of fifteen feature groups and fifteen\nclassical machine learning models, and eleven CNN models are considered and\ntheir performances explored. The results indicate that the combination of\nfeature engineering and machine learning provides better performance than the\ndirect use of CNN. Notably, feature engineering which is comprised of\nphysics-guided transformation, signal representation-based feature extraction\nand Principal Component Analysis is found to be the most effective. Moreover,\nit is shown that when using the extracted features, the ensemble-based, light\nblender learning model offers the best performance with RMSE, RE, RRMSE and R\nvalues of 64.3, 0.017, 0.025 and 0.994, respectively. The proposed method,\nbased on feature engineering and the light blender model, is capable of\nmeasuring nonuniform temperature distributions from low-resolution spectra,\neven when the species concentration distribution in the gas mixtures is\nunknown.\n","authors":["Ruiyuan Kang","Dimitrios C. Kyritsis","Panos Liatsis"],"pdf_url":"https://arxiv.org/pdf/2212.07836v1.pdf","comment":"19 pages, 10 figures, systematical investigation of feature\n  engineering and machine learning for realizing spatially-resolved thermometry\n  from line-of-sight spectroscopy"},{"id":"http://arxiv.org/abs/2205.12934v4","updated":"2022-12-15T13:39:22Z","published":"2022-05-25T17:37:08Z","title":"Amortized Inference for Causal Structure Learning","summary":"  Inferring causal structure poses a combinatorial search problem that\ntypically involves evaluating structures with a score or independence test. The\nresulting search is costly, and designing suitable scores or tests that capture\nprior knowledge is difficult. In this work, we propose to amortize causal\nstructure learning. Rather than searching over structures, we train a\nvariational inference model to directly predict the causal structure from\nobservational or interventional data. This allows our inference model to\nacquire domain-specific inductive biases for causal discovery solely from data\ngenerated by a simulator, bypassing both the hand-engineering of suitable score\nfunctions and the search over graphs. The architecture of our inference model\nemulates permutation invariances that are crucial for statistical efficiency in\nstructure learning, which facilitates generalization to significantly larger\nproblem instances than seen during training. On synthetic data and\nsemisynthetic gene expression data, our models exhibit robust generalization\ncapabilities when subject to substantial distribution shifts and significantly\noutperform existing algorithms, especially in the challenging genomics domain.\nOur code and models are publicly available at:\nhttps://github.com/larslorch/avici.\n","authors":["Lars Lorch","Scott Sussex","Jonas Rothfuss","Andreas Krause","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2205.12934v4.pdf","comment":"NeurIPS 2022, fixed formatting of Figure 5"},{"id":"http://arxiv.org/abs/2212.07826v1","updated":"2022-12-15T13:36:35Z","published":"2022-12-15T13:36:35Z","title":"Hybrid Quantum Generative Adversarial Networks for Molecular Simulation\n  and Drug Discovery","summary":"  In molecular research, simulation \\& design of molecules are key areas with\nsignificant implications for drug development, material science, and other\nfields. Current classical computational power falls inadequate to simulate any\nmore than small molecules, let alone protein chains on hundreds of peptide.\nTherefore these experiment are done physically in wet-lab, but it takes a lot\nof time \\& not possible to examine every molecule due to the size of the search\narea, tens of billions of dollars are spent every year in these research\nexperiments. Molecule simulation \\& design has lately advanced significantly by\nmachine learning models, A fresh perspective on the issue of chemical synthesis\nis provided by deep generative models for graph-structured data. By optimising\ndifferentiable models that produce molecular graphs directly, it is feasible to\navoid costly search techniques in the discrete and huge space of chemical\nstructures. But these models also suffer from computational limitations when\ndimensions become huge and consume huge amount of resources. Quantum Generative\nmachine learning in recent years have shown some empirical results promising\nsignificant advantages over classical counterparts.\n","authors":["Prateek Jain","Srinjoy Ganguly"],"pdf_url":"https://arxiv.org/pdf/2212.07826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07818v1","updated":"2022-12-15T13:34:02Z","published":"2022-12-15T13:34:02Z","title":"Towards Hardware-Specific Automatic Compression of Neural Networks","summary":"  Compressing neural network architectures is important to allow the deployment\nof models to embedded or mobile devices, and pruning and quantization are the\nmajor approaches to compress neural networks nowadays. Both methods benefit\nwhen compression parameters are selected specifically for each layer. Finding\ngood combinations of compression parameters, so-called compression policies, is\nhard as the problem spans an exponentially large search space. Effective\ncompression policies consider the influence of the specific hardware\narchitecture on the used compression methods. We propose an algorithmic\nframework called Galen to search such policies using reinforcement learning\nutilizing pruning and quantization, thus providing automatic compression for\nneural networks. Contrary to other approaches we use inference latency measured\non the target hardware device as an optimization goal. With that, the framework\nsupports the compression of models specific to a given hardware target. We\nvalidate our approach using three different reinforcement learning agents for\npruning, quantization and joint pruning and quantization. Besides proving the\nfunctionality of our approach we were able to compress a ResNet18 for CIFAR-10,\non an embedded ARM processor, to 20% of the original inference latency without\nsignificant loss of accuracy. Moreover, we can demonstrate that a joint search\nand compression using pruning and quantization is superior to an individual\nsearch for policies using a single compression method.\n","authors":["Torben Krieger","Bernhard Klein","Holger Fröning"],"pdf_url":"https://arxiv.org/pdf/2212.07818v1.pdf","comment":"To be published at the AAAI Conference on Artificial Intelligence\n  2023, at the 2nd International Workshop on Practical Deep Learning in the\n  Wild"},{"id":"http://arxiv.org/abs/2212.07816v1","updated":"2022-12-15T13:32:36Z","published":"2022-12-15T13:32:36Z","title":"DUIDD: Deep-Unfolded Interleaved Detection and Decoding for MIMO\n  Wireless Systems","summary":"  Iterative detection and decoding (IDD) is known to achieve near-capacity\nperformance in multi-antenna wireless systems. We propose deep-unfolded\ninterleaved detection and decoding (DUIDD), a new paradigm that reduces the\ncomplexity of IDD while achieving even lower error rates. DUIDD interleaves the\ninner stages of the data detector and channel decoder, which expedites\nconvergence and reduces complexity. Furthermore, DUIDD applies deep unfolding\nto automatically optimize algorithmic hyperparameters, soft-information\nexchange, message damping, and state forwarding. We demonstrate the efficacy of\nDUIDD using NVIDIA's Sionna link-level simulator in a 5G-near multi-user\nMIMO-OFDM wireless system with a novel low-complexity soft-input soft-output\ndata detector, an optimized low-density parity-check decoder, and channel\nvectors from a commercial ray-tracer. Our results show that DUIDD outperforms\nclassical IDD both in terms of block error rate and computational complexity.\n","authors":["Reinhard Wiesmayr","Chris Dick","Jakob Hoydis","Christoph Studer"],"pdf_url":"https://arxiv.org/pdf/2212.07816v1.pdf","comment":"This work has been presented at the Asilomar Conference on Signals,\n  Systems, and Computers 2022"},{"id":"http://arxiv.org/abs/2105.15013v6","updated":"2022-12-15T13:29:24Z","published":"2021-05-31T14:50:52Z","title":"SHAQ: Incorporating Shapley Value Theory into Multi-Agent Q-Learning","summary":"  Value factorisation is a useful technique for multi-agent reinforcement\nlearning (MARL) in global reward game, however its underlying mechanism is not\nyet fully understood. This paper studies a theoretical framework for value\nfactorisation with interpretability via Shapley value theory. We generalise\nShapley value to Markov convex game called Markov Shapley value (MSV) and apply\nit as a value factorisation method in global reward game, which is obtained by\nthe equivalence between the two games. Based on the properties of MSV, we\nderive Shapley-Bellman optimality equation (SBOE) to evaluate the optimal MSV,\nwhich corresponds to an optimal joint deterministic policy. Furthermore, we\npropose Shapley-Bellman operator (SBO) that is proved to solve SBOE. With a\nstochastic approximation and some transformations, a new MARL algorithm called\nShapley Q-learning (SHAQ) is established, the implementation of which is guided\nby the theoretical results of SBO and MSV. We also discuss the relationship\nbetween SHAQ and relevant value factorisation methods. In the experiments, SHAQ\nexhibits not only superior performances on all tasks but also the\ninterpretability that agrees with the theoretical analysis. The implementation\nof this paper is on https://github.com/hsvgbkhgbv/shapley-q-learning.\n","authors":["Jianhong Wang","Yuan Zhang","Yunjie Gu","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2105.15013v6.pdf","comment":"Accepted paper for NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.07802v1","updated":"2022-12-15T13:15:45Z","published":"2022-12-15T13:15:45Z","title":"Chaotic Variational Auto Encoder based One Class Classifier for\n  Insurance Fraud Detection","summary":"  Of late, insurance fraud detection has assumed immense significance owing to\nthe huge financial & reputational losses fraud entails and the phenomenal\nsuccess of the fraud detection techniques. Insurance is majorly divided into\ntwo categories: (i) Life and (ii) Non-life. Non-life insurance in turn includes\nhealth insurance and auto insurance among other things. In either of the\ncategories, the fraud detection techniques should be designed in such a way\nthat they capture as many fraudulent transactions as possible. Owing to the\nrarity of fraudulent transactions, in this paper, we propose a chaotic\nvariational autoencoder (C-VAE to perform one-class classification (OCC) on\ngenuine transactions. Here, we employed the logistic chaotic map to generate\nrandom noise in the latent space. The effectiveness of C-VAE is demonstrated on\nthe health insurance fraud and auto insurance datasets. We considered vanilla\nVariational Auto Encoder (VAE) as the baseline. It is observed that C-VAE\noutperformed VAE in both datasets. C-VAE achieved a classification rate of\n77.9% and 87.25% in health and automobile insurance datasets respectively.\nFurther, the t-test conducted at 1% level of significance and 18 degrees of\nfreedom infers that C-VAE is statistically significant than the VAE.\n","authors":["K. S. N. V. K. Gangadhar","B. Akhil Kumar","Yelleti Vivek","Vadlamani Ravi"],"pdf_url":"https://arxiv.org/pdf/2212.07802v1.pdf","comment":"19 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2212.07775v1","updated":"2022-12-15T12:52:23Z","published":"2022-12-15T12:52:23Z","title":"Calibrating AI Models for Wireless Communications via Conformal\n  Prediction","summary":"  When used in complex engineered systems, such as communication networks,\nartificial intelligence (AI) models should be not only as accurate as possible,\nbut also well calibrated. A well-calibrated AI model is one that can reliably\nquantify the uncertainty of its decisions, assigning high confidence levels to\ndecisions that are likely to be correct and low confidence levels to decisions\nthat are likely to be erroneous. This paper investigates the application of\nconformal prediction as a general framework to obtain AI models that produce\ndecisions with formal calibration guarantees. Conformal prediction transforms\nprobabilistic predictors into set predictors that are guaranteed to contain the\ncorrect answer with a probability chosen by the designer. Such formal\ncalibration guarantees hold irrespective of the true, unknown, distribution\nunderlying the generation of the variables of interest, and can be defined in\nterms of ensemble or time-averaged probabilities. In this paper, conformal\nprediction is applied for the first time to the design of AI for communication\nsystems in conjunction to both frequentist and Bayesian learning, focusing on\ndemodulation, modulation classification, and channel prediction.\n","authors":["Kfir M. Cohen","Sangwoo Park","Osvaldo Simeone","Shlomo Shamai"],"pdf_url":"https://arxiv.org/pdf/2212.07775v1.pdf","comment":"Submitted for a journal review"},{"id":"http://arxiv.org/abs/2212.07773v1","updated":"2022-12-15T12:50:42Z","published":"2022-12-15T12:50:42Z","title":"Runtime Monitoring for Out-of-Distribution Detection in Object Detection\n  Neural Networks","summary":"  Runtime monitoring provides a more realistic and applicable alternative to\nverification in the setting of real neural networks used in industry. It is\nparticularly useful for detecting out-of-distribution (OOD) inputs, for which\nthe network was not trained and can yield erroneous results. We extend a\nruntime-monitoring approach previously proposed for classification networks to\nperception systems capable of identification and localization of multiple\nobjects. Furthermore, we analyze its adequacy experimentally on different kinds\nof OOD settings, documenting the overall efficacy of our approach.\n","authors":["Vahid Hashemi","Jan Křetínsky","Sabine Rieder","Jessica Schmidt"],"pdf_url":"https://arxiv.org/pdf/2212.07773v1.pdf","comment":"14 Pages, 1 Table, 5 Figures. Accepted at the International Symposium\n  of Formal Methods 2023 (FM 2023)"},{"id":"http://arxiv.org/abs/2212.07771v1","updated":"2022-12-15T12:47:59Z","published":"2022-12-15T12:47:59Z","title":"Put Attention to Temporal Saliency Patterns of Multi-Horizon Time Series","summary":"  Time series, sets of sequences in chronological order, are essential data in\nstatistical research with many forecasting applications. Although recent\nperformance in many Transformer-based models has been noticeable, long\nmulti-horizon time series forecasting remains a very challenging task. Going\nbeyond transformers in sequence translation and transduction research, we\nobserve the effects of down-and-up samplings that can nudge temporal saliency\npatterns to emerge in time sequences. Motivated by the mentioned observation,\nin this paper, we propose a novel architecture, Temporal Saliency Detection\n(TSD), on top of the attention mechanism and apply it to multi-horizon time\nseries prediction. We renovate the traditional encoder-decoder architecture by\nmaking as a series of deep convolutional blocks to work in tandem with the\nmulti-head self-attention. The proposed TSD approach facilitates the\nmultiresolution of saliency patterns upon condensed multi-heads, thus\nprogressively enhancing complex time series forecasting. Experimental results\nillustrate that our proposed approach has significantly outperformed existing\nstate-of-the-art methods across multiple standard benchmark datasets in many\nfar-horizon forecasting settings. Overall, TSD achieves 31% and 46% relative\nimprovement over the current state-of-the-art models in multivariate and\nunivariate time series forecasting scenarios on standard benchmarks. The Git\nrepository is available at\nhttps://github.com/duongtrung/time-series-temporal-saliency-patterns.\n","authors":["Nghia Duong-Trung","Stefan Born","Kiran Madhusudhanan","Randolf Scholz","Johannes Burchert","Danh Le-Phuoc","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2212.07771v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2212.07769v1","updated":"2022-12-15T12:47:18Z","published":"2022-12-15T12:47:18Z","title":"CLAM: Selective Clarification for Ambiguous Questions with Large\n  Language Models","summary":"  State-of-the-art language models are often accurate on many\nquestion-answering benchmarks with well-defined questions. Yet, in real\nsettings questions are often unanswerable without asking the user for\nclarifying information. We show that current SotA models often do not ask the\nuser for clarification when presented with imprecise questions and instead\nprovide incorrect answers or \"hallucinate\". To address this, we introduce CLAM,\na framework that first uses the model to detect ambiguous questions, and if an\nambiguous question is detected, prompts the model to ask the user for\nclarification. Furthermore, we show how to construct a scalable and\ncost-effective automatic evaluation protocol using an oracle language model\nwith privileged information to provide clarifying information. We show that our\nmethod achieves a 20.15 percentage point accuracy improvement over SotA on a\nnovel ambiguous question-answering answering data set derived from TriviaQA.\n","authors":["Lorenz Kuhn","Yarin Gal","Sebastian Farquhar"],"pdf_url":"https://arxiv.org/pdf/2212.07769v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07768v1","updated":"2022-12-15T12:46:31Z","published":"2022-12-15T12:46:31Z","title":"A scalable framework for annotating photovoltaic cell defects in\n  electroluminescence images","summary":"  The correct functioning of photovoltaic (PV) cells is critical to ensuring\nthe optimal performance of a solar plant. Anomaly detection techniques for PV\ncells can result in significant cost savings in operation and maintenance\n(O&M). Recent research has focused on deep learning techniques for\nautomatically detecting anomalies in Electroluminescence (EL) images. Automated\nanomaly annotations can improve current O&M methodologies and help develop\ndecision-making systems to extend the life-cycle of the PV cells and predict\nfailures. This paper addresses the lack of anomaly segmentation annotations in\nthe literature by proposing a combination of state-of-the-art data-driven\ntechniques to create a Golden Standard benchmark. The proposed method stands\nout for (1) its adaptability to new PV cell types, (2) cost-efficient\nfine-tuning, and (3) leverage public datasets to generate advanced annotations.\nThe methodology has been validated in the annotation of a widely used dataset,\nobtaining a reduction of the annotation cost by 60%.\n","authors":["Urtzi Otamendi","Inigo Martinez","Igor G. Olaizola","Marco Quartulli"],"pdf_url":"https://arxiv.org/pdf/2212.07768v1.pdf","comment":"10 pages, 10 figures, 1 table, accepted at IEEE Transactions on\n  Industrial Informatics"},{"id":"http://arxiv.org/abs/2206.13500v2","updated":"2022-12-15T12:37:36Z","published":"2022-06-27T17:59:45Z","title":"Neural Neural Textures Make Sim2Real Consistent","summary":"  Unpaired image translation algorithms can be used for sim2real tasks, but\nmany fail to generate temporally consistent results. We present a new approach\nthat combines differentiable rendering with image translation to achieve\ntemporal consistency over indefinite timescales, using surface consistency\nlosses and \\emph{neural neural textures}. We call this algorithm TRITON\n(Texture Recovering Image Translation Network): an unsupervised, end-to-end,\nstateless sim2real algorithm that leverages the underlying 3D geometry of input\nscenes by generating realistic-looking learnable neural textures. By settling\non a particular texture for the objects in a scene, we ensure consistency\nbetween frames statelessly. Unlike previous algorithms, TRITON is not limited\nto camera movements -- it can handle the movement of objects as well, making it\nuseful for downstream tasks such as robotic manipulation.\n","authors":["Ryan Burgert","Jinghuan Shang","Xiang Li","Michael Ryoo"],"pdf_url":"https://arxiv.org/pdf/2206.13500v2.pdf","comment":"9 pages, 10 figures (without references or appendix); 16 pages, 16\n  figures (with appendix)"},{"id":"http://arxiv.org/abs/2212.07757v1","updated":"2022-12-15T12:21:27Z","published":"2022-12-15T12:21:27Z","title":"Spatial-Temporal Anomaly Detection for Sensor Attacks in Autonomous\n  Vehicles","summary":"  Time-of-flight (ToF) distance measurement devices such as ultrasonics, LiDAR\nand radar are widely used in autonomous vehicles for environmental perception,\nnavigation and assisted braking control. Despite their relative importance in\nmaking safer driving decisions, these devices are vulnerable to multiple attack\ntypes including spoofing, triggering and false data injection. When these\nattacks are successful they can compromise the security of autonomous vehicles\nleading to severe consequences for the driver, nearby vehicles and pedestrians.\nTo handle these attacks and protect the measurement devices, we propose a\nspatial-temporal anomaly detection model \\textit{STAnDS} which incorporates a\nresidual error spatial detector, with a time-based expected change detection.\nThis approach is evaluated using a simulated quantitative environment and the\nresults show that \\textit{STAnDS} is effective at detecting multiple attack\ntypes.\n","authors":["Martin Higgins","Devki Jha","David Wallom"],"pdf_url":"https://arxiv.org/pdf/2212.07757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07743v1","updated":"2022-12-15T11:50:31Z","published":"2022-12-15T11:50:31Z","title":"Interpretable ML for Imbalanced Data","summary":"  Deep learning models are being increasingly applied to imbalanced data in\nhigh stakes fields such as medicine, autonomous driving, and intelligence\nanalysis. Imbalanced data compounds the black-box nature of deep networks\nbecause the relationships between classes may be highly skewed and unclear.\nThis can reduce trust by model users and hamper the progress of developers of\nimbalanced learning algorithms. Existing methods that investigate imbalanced\ndata complexity are geared toward binary classification, shallow learning\nmodels and low dimensional data. In addition, current eXplainable Artificial\nIntelligence (XAI) techniques mainly focus on converting opaque deep learning\nmodels into simpler models (e.g., decision trees) or mapping predictions for\nspecific instances to inputs, instead of examining global data properties and\ncomplexities. Therefore, there is a need for a framework that is tailored to\nmodern deep networks, that incorporates large, high dimensional, multi-class\ndatasets, and uncovers data complexities commonly found in imbalanced data\n(e.g., class overlap, sub-concepts, and outlier instances). We propose a set of\ntechniques that can be used by both deep learning model users to identify,\nvisualize and understand class prototypes, sub-concepts and outlier instances;\nand by imbalanced learning algorithm developers to detect features and class\nexemplars that are key to model performance. Our framework also identifies\ninstances that reside on the border of class decision boundaries, which can\ncarry highly discriminative information. Unlike many existing XAI techniques\nwhich map model decisions to gray-scale pixel locations, we use saliency\nthrough back-propagation to identify and aggregate image color bands across\nentire classes. Our framework is publicly available at\n\\url{https://github.com/dd1github/XAI_for_Imbalanced_Learning}\n","authors":["Damien A. Dablain","Colin Bellinger","Bartosz Krawczyk","David W. Aha","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2212.07743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07740v1","updated":"2022-12-15T11:44:11Z","published":"2022-12-15T11:44:11Z","title":"Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer","summary":"  Deep reinforcement learning has recently emerged as an appealing alternative\nfor legged locomotion over multiple terrains by training a policy in physical\nsimulation and then transferring it to the real world (i.e., sim-to-real\ntransfer). Despite considerable progress, the capacity and scalability of\ntraditional neural networks are still limited, which may hinder their\napplications in more complex environments. In contrast, the Transformer\narchitecture has shown its superiority in a wide range of large-scale sequence\nmodeling tasks, including natural language processing and decision-making\nproblems. In this paper, we propose Terrain Transformer (TERT), a high-capacity\nTransformer model for quadrupedal locomotion control on various terrains.\nFurthermore, to better leverage Transformer in sim-to-real scenarios, we\npresent a novel two-stage training framework consisting of an offline\npretraining stage and an online correction stage, which can naturally integrate\nTransformer with privileged training. Extensive experiments in simulation\ndemonstrate that TERT outperforms state-of-the-art baselines on different\nterrains in terms of return, energy consumption and control smoothness. In\nfurther real-world validation, TERT successfully traverses nine challenging\nterrains, including sand pit and stair down, which can not be accomplished by\nstrong baselines.\n","authors":["Hang Lai","Weinan Zhang","Xialin He","Chen Yu","Zheng Tian","Yong Yu","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2212.07740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07738v1","updated":"2022-12-15T11:40:40Z","published":"2022-12-15T11:40:40Z","title":"A large-scale and PCR-referenced vocal audio dataset for COVID-19","summary":"  The UK COVID-19 Vocal Audio Dataset is designed for the training and\nevaluation of machine learning models that classify SARS-CoV-2 infection status\nor associated respiratory symptoms using vocal audio. The UK Health Security\nAgency recruited voluntary participants through the national Test and Trace\nprogramme and the REACT-1 survey in England from March 2021 to March 2022,\nduring dominant transmission of the Alpha and Delta SARS-CoV-2 variants and\nsome Omicron variant sublineages. Audio recordings of volitional coughs,\nexhalations, and speech were collected in the 'Speak up to help beat\ncoronavirus' digital survey alongside demographic, self-reported symptom and\nrespiratory condition data, and linked to SARS-CoV-2 test results. The UK\nCOVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2\nPCR-referenced audio recordings to date. PCR results were linked to 70,794 of\n72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms\nwere reported by 45.62% of participants. This dataset has additional potential\nuses for bioacoustics research, with 11.30% participants reporting asthma, and\n27.20% with linked influenza PCR test results.\n","authors":["Jobie Budd","Kieran Baker","Emma Karoune","Harry Coppock","Selina Patel","Ana Tendero Cañadas","Alexander Titcomb","Richard Payne","David Hurley","Sabrina Egglestone","Lorraine Butler","Jonathon Mellor","George Nicholson","Ivan Kiskin","Vasiliki Koutra","Radka Jersakova","Rachel A. McKendry","Peter Diggle","Sylvia Richardson","Björn W. Schuller","Steven Gilmour","Davide Pigoli","Stephen Roberts","Josef Packham","Tracey Thornley","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2212.07738v1.pdf","comment":"36 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.02486v3","updated":"2022-12-15T11:19:19Z","published":"2022-11-04T14:27:11Z","title":"Decorrelation with conditional normalizing flows","summary":"  The sensitivity of many physics analyses can be enhanced by constructing\ndiscriminants that preferentially select signal events. Such discriminants\nbecome much more useful if they are uncorrelated with a set of protected\nattributes. In this paper we show that a normalizing flow conditioned on the\nprotected attributes can be used to find a decorrelated representation for any\ndiscriminant. As a normalizing flow is invertible the separation power of the\nresulting discriminant will be unchanged at any fixed value of the protected\nattributes. We demonstrate the efficacy of our approach by building supervised\njet taggers that produce almost no sculpting in the mass distribution of the\nbackground.\n","authors":["Samuel Klein","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2211.02486v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07723v1","updated":"2022-12-15T11:01:32Z","published":"2022-12-15T11:01:32Z","title":"Physics-Informed Neural Networks for Material Model Calibration from\n  Full-Field Displacement Data","summary":"  The identification of material parameters occurring in constitutive models\nhas a wide range of applications in practice. One of these applications is the\nmonitoring and assessment of the actual condition of infrastructure buildings,\nas the material parameters directly reflect the resistance of the structures to\nexternal impacts. Physics-informed neural networks (PINNs) have recently\nemerged as a suitable method for solving inverse problems. The advantages of\nthis method are a straightforward inclusion of observation data. Unlike\ngrid-based methods, such as the finite element method updating (FEMU) approach,\nno computational grid and no interpolation of the data is required. In the\ncurrent work, we aim to further develop PINNs towards the calibration of the\nlinear-elastic constitutive model from full-field displacement and global force\ndata in a realistic regime. We show that normalization and conditioning of the\noptimization problem play a crucial role in this process. Therefore, among\nothers, we identify the material parameters for initial estimates and balance\nthe individual terms in the loss function. In order to reduce the dependence of\nthe identified material parameters on local errors in the displacement\napproximation, we base the identification not on the stress boundary conditions\nbut instead on the global balance of internal and external work. In addition,\nwe found that we get a better posed inverse problem if we reformulate it in\nterms of bulk and shear modulus instead of Young's modulus and Poisson's ratio.\nWe demonstrate that the enhanced PINNs are capable of identifying material\nparameters from both experimental one-dimensional data and synthetic full-field\ndisplacement data in a realistic regime. Since displacement data measured by,\ne.g., a digital image correlation (DIC) system is noisy, we additionally\ninvestigate the robustness of the method to different levels of noise.\n","authors":["David Anton","Henning Wessels"],"pdf_url":"https://arxiv.org/pdf/2212.07723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.13802v3","updated":"2022-12-15T10:54:22Z","published":"2021-11-27T03:34:13Z","title":"Factorized Fourier Neural Operators","summary":"  We propose the Factorized Fourier Neural Operator (F-FNO), a learning-based\napproach for simulating partial differential equations (PDEs). Starting from a\nrecently proposed Fourier representation of flow fields, the F-FNO bridges the\nperformance gap between pure machine learning approaches to that of the best\nnumerical or hybrid solvers. This is achieved with new representations -\nseparable spectral layers and improved residual connections - and a combination\nof training strategies such as the Markov assumption, Gaussian noise, and\ncosine learning rate decay. On several challenging benchmark PDEs on regular\ngrids, structured meshes, and point clouds, the F-FNO can scale to deeper\nnetworks and outperform both the FNO and the geo-FNO, reducing the error by 83%\non the Navier-Stokes problem, 31% on the elasticity problem, 57% on the airfoil\nflow problem, and 60% on the plastic forging problem. Compared to the\nstate-of-the-art pseudo-spectral method, the F-FNO can take a step size that is\nan order of magnitude larger in time and achieve an order of magnitude speedup\nto produce the same solution quality.\n","authors":["Alasdair Tran","Alexander Mathews","Lexing Xie","Cheng Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2111.13802v3.pdf","comment":"Under review. Code is available at\n  https://github.com/alasdairtran/fourierflow"},{"id":"http://arxiv.org/abs/2211.02024v2","updated":"2022-12-15T10:48:12Z","published":"2022-10-23T15:11:37Z","title":"fMRI from EEG is only Deep Learning away: the use of interpretable DL to\n  unravel EEG-fMRI relationships","summary":"  The access to activity of subcortical structures offers unique opportunity\nfor building intention dependent brain-computer interfaces, renders abundant\noptions for exploring a broad range of cognitive phenomena in the realm of\naffective neuroscience including complex decision making processes and the\neternal free-will dilemma and facilitates diagnostics of a range of\nneurological deceases. So far this was possible only using bulky, expensive and\nimmobile fMRI equipment. Here we present an interpretable domain grounded\nsolution to recover the activity of several subcortical regions from the\nmultichannel EEG data and demonstrate up to 60% correlation between the actual\nsubcortical blood oxygenation level dependent sBOLD signal and its EEG-derived\ntwin. Then, using the novel and theoretically justified weight interpretation\nmethodology we recover individual spatial and time-frequency patterns of scalp\nEEG predictive of the hemodynamic signal in the subcortical nuclei. The\ndescribed results not only pave the road towards wearable subcortical activity\nscanners but also showcase an automatic knowledge discovery process facilitated\nby deep learning technology in combination with an interpretable domain\nconstrained architecture and the appropriate downstream task.\n","authors":["Alexander Kovalev","Ilia Mikheev","Alexei Ossadtchi"],"pdf_url":"https://arxiv.org/pdf/2211.02024v2.pdf","comment":"11 pages. Add acknowledgment"},{"id":"http://arxiv.org/abs/2212.07707v1","updated":"2022-12-15T10:32:29Z","published":"2022-12-15T10:32:29Z","title":"FreCDo: A Large Corpus for French Cross-Domain Dialect Identification","summary":"  We present a novel corpus for French dialect identification comprising\n413,522 French text samples collected from public news websites in Belgium,\nCanada, France and Switzerland. To ensure an accurate estimation of the dialect\nidentification performance of models, we designed the corpus to eliminate\npotential biases related to topic, writing style, and publication source. More\nprecisely, the training, validation and test splits are collected from\ndifferent news websites, while searching for different keywords (topics). This\nleads to a French cross-domain (FreCDo) dialect identification task. We conduct\nexperiments with four competitive baselines, a fine-tuned CamemBERT model, an\nXGBoost based on fine-tuned CamemBERT features, a Support Vector Machines (SVM)\nclassifier based on fine-tuned CamemBERT features, and an SVM based on word\nn-grams. Aside from presenting quantitative results, we also make an analysis\nof the most discriminative features learned by CamemBERT. Our corpus is\navailable at https://github.com/MihaelaGaman/FreCDo.\n","authors":["Mihaela Gaman","Adrian-Gabriel Chifu","William Domingues","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2212.07707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.02248v2","updated":"2022-12-15T10:10:21Z","published":"2021-07-05T19:49:14Z","title":"A comparison of LSTM and GRU networks for learning symbolic sequences","summary":"  We explore relations between the hyper-parameters of a recurrent neural\nnetwork (RNN) and the complexity of string sequences it is able to memorize. We\ncompare long short-term memory (LSTM) networks and gated recurrent units\n(GRUs). We find that an increase of RNN depth does not necessarily result in\nbetter memorization capability when the training time is constrained. Our\nresults also indicate that the learning rate and the number of units per layer\nare among the most important hyper-parameters to be tuned. Generally, GRUs\noutperform LSTM networks on low complexity sequences while on high complexity\nsequences LSTMs perform better.\n","authors":["Roberto Cahuantzi","Xinye Chen","Stefan Güttel"],"pdf_url":"https://arxiv.org/pdf/2107.02248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07691v1","updated":"2022-12-15T09:53:49Z","published":"2022-12-15T09:53:49Z","title":"Anomaly Detection in Driving by Cluster Analysis Twice","summary":"  Events deviating from normal traffic patterns in driving, anomalies, such as\naggressive driving or bumpy roads, may harm delivery efficiency for\ntransportation and logistics (T&L) business. Thus, detecting anomalies in\ndriving is critical for the T&L industry. So far numerous researches have used\nvehicle sensor data to identify anomalies. Most previous works captured\nanomalies by using deep learning or machine learning algorithms, which require\nprior training processes and huge computational costs. This study proposes a\nmethod namely Anomaly Detection in Driving by Cluster Analysis Twice (ADDCAT)\nwhich clusters the processed sensor data in different physical properties. An\nevent is said to be an anomaly if it never fits with the major cluster, which\nis considered as the pattern of normality in driving. This method provides a\nway to detect anomalies in driving with no prior training processes and huge\ncomputational costs needed. This paper validated the performance of the method\non an open dataset.\n","authors":["Chung-Hao Lee","Yen-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2212.07691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07684v1","updated":"2022-12-15T09:35:54Z","published":"2022-12-15T09:35:54Z","title":"Multi-Agent Reinforcement Learning with Shared Resources for Inventory\n  Management","summary":"  In this paper, we consider the inventory management (IM) problem where we\nneed to make replenishment decisions for a large number of stock keeping units\n(SKUs) to balance their supply and demand. In our setting, the constraint on\nthe shared resources (such as the inventory capacity) couples the otherwise\nindependent control for each SKU. We formulate the problem with this structure\nas Shared-Resource Stochastic Game (SRSG)and propose an efficient algorithm\ncalled Context-aware Decentralized PPO (CD-PPO). Through extensive experiments,\nwe demonstrate that CD-PPO can accelerate the learning procedure compared with\nstandard MARL algorithms.\n","authors":["Yuandong Ding","Mingxiao Feng","Guozi Liu","Wei Jiang","Chuheng Zhang","Li Zhao","Lei Song","Houqiang Li","Yan Jin","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2212.07684v1.pdf","comment":"Published in RL4RealLife@NeurIPS 2022"},{"id":"http://arxiv.org/abs/2204.03276v3","updated":"2022-12-15T09:30:58Z","published":"2022-04-07T08:01:13Z","title":"PALBERT: Teaching ALBERT to Ponder","summary":"  Currently, pre-trained models can be considered the default choice for a wide\nrange of NLP tasks. Despite their SoTA results, there is practical evidence\nthat these models may require a different number of computing layers for\ndifferent input sequences, since evaluating all layers leads to overconfidence\nin wrong predictions (namely overthinking). This problem can potentially be\nsolved by implementing adaptive computation time approaches, which were first\ndesigned to improve inference speed. Recently proposed PonderNet may be a\npromising solution for performing an early exit by treating the exit layer's\nindex as a latent variable. However, the originally proposed exit criterion,\nrelying on sampling from trained posterior distribution on the probability of\nexiting from the $i$-th layer, introduces major variance in exit layer indices,\nsignificantly reducing the resulting model's performance. In this paper, we\npropose improving PonderNet with a novel deterministic Q-exit criterion and a\nrevisited model architecture. We adapted the proposed mechanism to ALBERT and\nRoBERTa and compared it with recent methods for performing an early exit. We\nobserved that the proposed changes can be considered significant improvements\non the original PonderNet architecture and outperform PABEE on a wide range of\nGLUE tasks. In addition, we also performed an in-depth ablation study of the\nproposed architecture to further understand Lambda layers and their\nperformance.\n","authors":["Nikita Balagansky","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2204.03276v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.04487v2","updated":"2022-12-15T09:25:15Z","published":"2022-01-12T14:16:47Z","title":"Smoothness and continuity of cost functionals for ECG mismatch\n  computation","summary":"  The field of cardiac electrophysiology tries to abstract, describe and\nfinally model the electrical characteristics of a heartbeat. With recent\nadvances in cardiac electrophysiology, models have become more powerful and\ndescriptive as ever. However, to advance to the field of inverse\nelectrophysiological modeling, i.e. creating models from electrical\nmeasurements such as the ECG, the less investigated field of smoothness of the\nsimulated ECGs w.r.t. model parameters need to be further explored. The present\npaper discusses smoothness in terms of the whole pipeline which describes how\nfrom physiological parameters, we arrive at the simulated ECG. Employing such a\npipeline, we create a test-bench of a simplified idealized left ventricle model\nand demonstrate the most important factors for efficient inverse modeling\nthrough smooth cost functionals. Such knowledge will be important for designing\nand creating inverse models in future optimization and machine learning\nmethods.\n","authors":["Thomas Grandits","Simone Pezzuto","Gernot Plank"],"pdf_url":"https://arxiv.org/pdf/2201.04487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10911v4","updated":"2022-12-15T09:21:22Z","published":"2022-06-22T08:33:52Z","title":"Influence of uncertainty estimation techniques on false-positive\n  reduction in liver lesion detection","summary":"  Deep learning techniques show success in detecting objects in medical images,\nbut still suffer from false-positive predictions that may hinder accurate\ndiagnosis. The estimated uncertainty of the neural network output has been used\nto flag incorrect predictions. We study the role played by features computed\nfrom neural network uncertainty estimates and shape-based features computed\nfrom binary predictions in reducing false positives in liver lesion detection\nby developing a classification-based post-processing step for different\nuncertainty estimation methods. We demonstrate an improvement in the lesion\ndetection performance of the neural network (with respect to F1-score) for all\nuncertainty estimation methods on two datasets, comprising abdominal MR and CT\nimages, respectively. We show that features computed from neural network\nuncertainty estimates tend not to contribute much toward reducing false\npositives. Our results show that factors like class imbalance (true over false\npositive ratio) and shape-based features extracted from uncertainty maps play\nan important role in distinguishing false positive from true positive\npredictions. Our code can be found at https://github.com/ishaanb92/FPCPipeline.\n","authors":["Ishaan Bhat","Josien P. W. Pluim","Max A. Viergever","Hugo J. Kuijf"],"pdf_url":"https://arxiv.org/pdf/2206.10911v4.pdf","comment":"Accepted for publication in the Journal of Machine Learning for\n  Biomedical Imaging (MELBA)"},{"id":"http://arxiv.org/abs/2212.07677v1","updated":"2022-12-15T09:21:21Z","published":"2022-12-15T09:21:21Z","title":"Transformers learn in-context by gradient descent","summary":"  Transformers have become the state-of-the-art neural network architecture\nacross numerous domains of machine learning. This is partly due to their\ncelebrated ability to transfer and to learn in-context based on few examples.\nNevertheless, the mechanisms by which Transformers become in-context learners\nare not well understood and remain mostly an intuition. Here, we argue that\ntraining Transformers on auto-regressive tasks can be closely related to\nwell-known gradient-based meta-learning formulations. We start by providing a\nsimple weight construction that shows the equivalence of data transformations\ninduced by 1) a single linear self-attention layer and by 2) gradient-descent\n(GD) on a regression loss. Motivated by that construction, we show empirically\nthat when training self-attention-only Transformers on simple regression tasks\neither the models learned by GD and Transformers show great similarity or,\nremarkably, the weights found by optimization match the construction. Thus we\nshow how trained Transformers implement gradient descent in their forward pass.\nThis allows us, at least in the domain of regression problems, to\nmechanistically understand the inner workings of optimized Transformers that\nlearn in-context. Furthermore, we identify how Transformers surpass plain\ngradient descent by an iterative curvature correction and learn linear models\non deep data representations to solve non-linear regression tasks. Finally, we\ndiscuss intriguing parallels to a mechanism identified to be crucial for\nin-context learning termed induction-head (Olsson et al., 2022) and show how it\ncould be understood as a specific case of in-context learning by gradient\ndescent learning within Transformers.\n","authors":["Johannes von Oswald","Eyvind Niklasson","Ettore Randazzo","João Sacramento","Alexander Mordvintsev","Andrey Zhmoginov","Max Vladymyrov"],"pdf_url":"https://arxiv.org/pdf/2212.07677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07127v2","updated":"2022-12-15T09:10:16Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06691v2","updated":"2022-12-15T08:35:42Z","published":"2022-12-13T16:04:16Z","title":"Quantum Clustering with k-Means: a Hybrid Approach","summary":"  Quantum computing is a promising paradigm based on quantum theory for\nperforming fast computations. Quantum algorithms are expected to surpass their\nclassical counterparts in terms of computational complexity for certain tasks,\nincluding machine learning. In this paper, we design, implement, and evaluate\nthree hybrid quantum k-Means algorithms, exploiting different degree of\nparallelism. Indeed, each algorithm incrementally leverages quantum parallelism\nto reduce the complexity of the cluster assignment step up to a constant cost.\nIn particular, we exploit quantum phenomena to speed up the computation of\ndistances. The core idea is that the computation of distances between records\nand centroids can be executed simultaneously, thus saving time, especially for\nbig datasets. We show that our hybrid quantum k-Means algorithms can be more\nefficient than the classical version, still obtaining comparable clustering\nresults.\n","authors":["Alessandro Poggiali","Alessandro Berti","Anna Bernasconi","Gianna M. Del Corso","Riccardo Guidotti"],"pdf_url":"https://arxiv.org/pdf/2212.06691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07658v1","updated":"2022-12-15T08:30:23Z","published":"2022-12-15T08:30:23Z","title":"Interpolation with the polynomial kernels","summary":"  The polynomial kernels are widely used in machine learning and they are one\nof the default choices to develop kernel-based classification and regression\nmodels. However, they are rarely used and considered in numerical analysis due\nto their lack of strict positive definiteness. In particular they do not enjoy\nthe usual property of unisolvency for arbitrary point sets, which is one of the\nkey properties used to build kernel-based interpolation methods. This paper is\ndevoted to establish some initial results for the study of these kernels, and\ntheir related interpolation algorithms, in the context of approximation theory.\nWe will first prove necessary and sufficient conditions on point sets which\nguarantee the existence and uniqueness of an interpolant. We will then study\nthe Reproducing Kernel Hilbert Spaces (or native spaces) of these kernels and\ntheir norms, and provide inclusion relations between spaces corresponding to\ndifferent kernel parameters. With these spaces at hand, it will be further\npossible to derive generic error estimates which apply to sufficiently smooth\nfunctions, thus escaping the native space. Finally, we will show how to employ\nan efficient stable algorithm to these kernels to obtain accurate interpolants,\nand we will test them in some numerical experiment. After this analysis several\ncomputational and theoretical aspects remain open, and we will outline possible\nfurther research directions in a concluding section. This work builds some\nbridges between kernel and polynomial interpolation, two topics to which the\nauthors, to different extents, have been introduced under the supervision or\nthrough the work of Stefano De Marchi. For this reason, they wish to dedicate\nthis work to him in the occasion of his 60th birthday.\n","authors":["Giacomo Elefante","Wolfgang Erb","Francesco Marchetti","Emma Perracchione","Davide Poggiali","Gabriele Santin"],"pdf_url":"https://arxiv.org/pdf/2212.07658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07651v1","updated":"2022-12-15T08:18:37Z","published":"2022-12-15T08:18:37Z","title":"Two-stage Contextual Transformer-based Convolutional Neural Network for\n  Airway Extraction from CT Images","summary":"  Accurate airway extraction from computed tomography (CT) images is a critical\nstep for planning navigation bronchoscopy and quantitative assessment of\nairway-related chronic obstructive pulmonary disease (COPD). The existing\nmethods are challenging to sufficiently segment the airway, especially the\nhigh-generation airway, with the constraint of the limited label and cannot\nmeet the clinical use in COPD. We propose a novel two-stage 3D contextual\ntransformer-based U-Net for airway segmentation using CT images. The method\nconsists of two stages, performing initial and refined airway segmentation. The\ntwo-stage model shares the same subnetwork with different airway masks as\ninput. Contextual transformer block is performed both in the encoder and\ndecoder path of the subnetwork to finish high-quality airway segmentation\neffectively. In the first stage, the total airway mask and CT images are\nprovided to the subnetwork, and the intrapulmonary airway mask and\ncorresponding CT scans to the subnetwork in the second stage. Then the\npredictions of the two-stage method are merged as the final prediction.\nExtensive experiments were performed on in-house and multiple public datasets.\nQuantitative and qualitative analysis demonstrate that our proposed method\nextracted much more branches and lengths of the tree while accomplishing\nstate-of-the-art airway segmentation performance. The code is available at\nhttps://github.com/zhaozsq/airway_segmentation.\n","authors":["Yanan Wu","Shuiqing Zhao","Shouliang Qi","Jie Feng","Haowen Pang","Runsheng Chang","Long Bai","Mengqi Li","Shuyue Xia","Wei Qian","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2212.07651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08624v2","updated":"2022-12-15T07:52:42Z","published":"2022-11-16T02:29:05Z","title":"Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral\n  Mapping for Single-channel Speech Enhancement","summary":"  Most speech enhancement (SE) models learn a point estimate, and do not make\nuse of uncertainty estimation in the learning process. In this paper, we show\nthat modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian\nnegative log-likelihood (NLL) improves SE performance at no extra cost. During\ntraining, our approach augments a model learning complex spectral mapping with\na temporary submodel to predict the covariance of the enhancement error at each\ntime-frequency bin. Due to unrestricted heteroscedastic uncertainty, the\ncovariance introduces an undersampling effect, detrimental to SE performance.\nTo mitigate undersampling, our approach inflates the uncertainty lower bound\nand weights each loss component with their uncertainty, effectively\ncompensating severely undersampled components with more penalties. Our\nmultivariate setting reveals common covariance assumptions such as scalar and\ndiagonal matrices. By weakening these assumptions, we show that the NLL\nachieves superior performance compared to popular losses including the mean\nsquared error (MSE), mean absolute error (MAE), and scale-invariant\nsignal-to-distortion ratio (SI-SDR).\n","authors":["Kuan-Lin Chen","Daniel D. E. Wong","Ke Tan","Buye Xu","Anurag Kumar","Vamsi Krishna Ithapu"],"pdf_url":"https://arxiv.org/pdf/2211.08624v2.pdf","comment":"5 pages. Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2211.00641v4","updated":"2022-12-15T07:18:28Z","published":"2022-10-30T13:15:19Z","title":"Transposed Variational Auto-encoder with Intrinsic Feature Learning for\n  Traffic Forecasting","summary":"  In this technical report, we present our solutions to the Traffic4cast 2022\ncore challenge and extended challenge. In this competition, the participants\nare required to predict the traffic states for the future 15-minute based on\nthe vehicle counter data in the previous hour. Compared to other competitions\nin the same series, this year focuses on the prediction of different data\nsources and sparse vertex-to-edge generalization. To address these issues, we\nintroduce the Transposed Variational Auto-encoder (TVAE) model to reconstruct\nthe missing data and Graph Attention Networks (GAT) to strengthen the\ncorrelations between learned representations. We further apply feature\nselection to learn traffic patterns from diverse but easily available data.\n  Our solutions have ranked first in both challenges on the final leaderboard.\nThe source code is available at \\url{https://github.com/Daftstone/Traffic4cast}\n","authors":["Leyan Deng","Chenwang Wu","Defu Lian","Min Zhou"],"pdf_url":"https://arxiv.org/pdf/2211.00641v4.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.07635v1","updated":"2022-12-15T07:03:54Z","published":"2022-12-15T07:03:54Z","title":"Let's consider more general nonlinear approaches to study\n  teleconnections of climate variables","summary":"  The recent work by (Rieger et al 2021) is concerned with the problem of\nextracting features from spatio-temporal geophysical signals. The authors\nintroduce the complex rotated MCA (xMCA) to deal with lagged effects and\nnon-orthogonality of the feature representation. This method essentially (1)\ntransforms the signals to a complex plane with the Hilbert transform; (2)\napplies an oblique (Varimax and Promax) rotation to remove the orthogonality\nconstraint; and (3) performs the eigendecomposition in this complex space\n(Horel et al, 1984). We argue that this method is essentially a particular case\nof the method called rotated complex kernel principal component analysis\n(ROCK-PCA) introduced in (Bueso et al., 2019, 2020), where we proposed the same\napproach: first transform the data to the complex plane with the Hilbert\ntransform and then apply the varimax rotation, with the only difference that\nthe eigendecomposition is performed in the dual (kernel) Hilbert space. The\nlatter allows us to generalize the xMCA solution by extracting nonlinear\n(curvilinear) features when nonlinear kernel functions are used. Hence, the\nsolution of xMCA boils down to ROCK-PCA when the inner product is computed in\nthe input data space instead of in the high-dimensional (possibly infinite)\nkernel Hilbert space to which data has been mapped. In this short\ncorrespondence we show theoretical proof that xMCA is a special case of\nROCK-PCA and provide quantitative evidence that more expressive and informative\nfeatures can be extracted when working with kernels; results of the\ndecomposition of global sea surface temperature (SST) fields are shown to\nillustrate the capabilities of ROCK-PCA to cope with nonlinear processes,\nunlike xMCA.\n","authors":["D. Bueso","M. Piles","G. Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2212.07635v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2011.10219v2","updated":"2022-12-15T06:53:30Z","published":"2020-11-20T04:58:13Z","title":"Certified Monotonic Neural Networks","summary":"  Learning monotonic models with respect to a subset of the inputs is a\ndesirable feature to effectively address the fairness, interpretability, and\ngeneralization issues in practice. Existing methods for learning monotonic\nneural networks either require specifically designed model structures to ensure\nmonotonicity, which can be too restrictive/complicated, or enforce monotonicity\nby adjusting the learning process, which cannot provably guarantee the learned\nmodel is monotonic on selected features. In this work, we propose to certify\nthe monotonicity of the general piece-wise linear neural networks by solving a\nmixed integer linear programming problem.This provides a new general approach\nfor learning monotonic neural networks with arbitrary model structures. Our\nmethod allows us to train neural networks with heuristic monotonicity\nregularizations, and we can gradually increase the regularization magnitude\nuntil the learned network is certified monotonic. Compared to prior works, our\napproach does not require human-designed constraints on the weight space and\nalso yields more accurate approximation. Empirical studies on various datasets\ndemonstrate the efficiency of our approach over the state-of-the-art methods,\nsuch as Deep Lattice Networks.\n","authors":["Xingchao Liu","Xing Han","Na Zhang","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2011.10219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07632v1","updated":"2022-12-15T06:36:14Z","published":"2022-12-15T06:36:14Z","title":"Ungeneralizable Contextual Logistic Bandit in Credit Scoring","summary":"  The application of reinforcement learning in credit scoring has created a\nunique setting for contextual logistic bandit that does not conform to the\nusual exploration-exploitation tradeoff but rather favors exploration-free\nalgorithms. Through sufficient randomness in a pool of observable contexts, the\nreinforcement learning agent can simultaneously exploit an action with the\nhighest reward while still learning more about the structure governing that\nenvironment. Thus, it is the case that greedy algorithms consistently\noutperform algorithms with efficient exploration, such as Thompson sampling.\nHowever, in a more pragmatic scenario in credit scoring, lenders can, to a\ndegree, classify each borrower as a separate group, and learning about the\ncharacteristics of each group does not infer any information to another group.\nThrough extensive simulations, we show that Thompson sampling dominates over\ngreedy algorithms given enough timesteps which increase with the complexity of\nunderlying features.\n","authors":["Pojtanut Manopanjasiri","Kantapong Visantavarakul","Seksan Kiatsupaibul"],"pdf_url":"https://arxiv.org/pdf/2212.07632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05814v2","updated":"2022-12-15T06:02:33Z","published":"2022-12-12T10:24:47Z","title":"GWRBoost:A geographically weighted gradient boosting method for\n  explainable quantification of spatially-varying relationships","summary":"  The geographically weighted regression (GWR) is an essential tool for\nestimating the spatial variation of relationships between dependent and\nindependent variables in geographical contexts. However, GWR suffers from the\nproblem that classical linear regressions, which compose the GWR model, are\nmore prone to be underfitting, especially for significant volume and complex\nnonlinear data, causing inferior comparative performance. Nevertheless, some\nadvanced models, such as the decision tree and the support vector machine, can\nlearn features from complex data more effectively while they cannot provide\nexplainable quantification for the spatial variation of localized\nrelationships. To address the above issues, we propose a geographically\ngradient boosting weighted regression model, GWRBoost, that applies the\nlocalized additive model and gradient boosting optimization method to alleviate\nunderfitting problems and retains explainable quantification capability for\nspatially-varying relationships between geographically located variables.\nFurthermore, we formulate the computation method of the Akaike information\nscore for the proposed model to conduct the comparative analysis with the\nclassic GWR algorithm. Simulation experiments and the empirical case study are\napplied to prove the efficient performance and practical value of GWRBoost. The\nresults show that our proposed model can reduce the RMSE by 18.3% in parameter\nestimation accuracy and AICc by 67.3% in the goodness of fit.\n","authors":["Han Wang","Zhou Huang","Ganmin Yin","Yi Bao","Xiao Zhou","Yong Gao"],"pdf_url":"https://arxiv.org/pdf/2212.05814v2.pdf","comment":"13 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.07624v1","updated":"2022-12-15T05:54:16Z","published":"2022-12-15T05:54:16Z","title":"JAX-Accelerated Neuroevolution of Physics-informed Neural Networks:\n  Benchmarks and Experimental Results","summary":"  This paper introduces the use of evolutionary algorithms for solving\ndifferential equations. The solution is obtained by optimizing a deep neural\nnetwork whose loss function is defined by the residual terms from the\ndifferential equations. Recent studies have used stochastic gradient descent\n(SGD) variants to train these physics-informed neural networks (PINNs), but\nthese methods can struggle to find accurate solutions due to optimization\nchallenges. When solving differential equations, it is important to find the\nglobally optimum parameters of the network, rather than just finding a solution\nthat works well during training. SGD only searches along a single gradient\ndirection, so it may not be the best approach for training PINNs with their\naccompanying complex optimization landscapes. In contrast, evolutionary\nalgorithms perform a parallel exploration of different solutions in order to\navoid getting stuck in local optima and can potentially find more accurate\nsolutions. However, evolutionary algorithms can be slow, which can make them\ndifficult to use in practice. To address this, we provide a set of five\nbenchmark problems with associated performance metrics and baseline results to\nsupport the development of evolutionary algorithms for enhanced PINN training.\nAs a baseline, we evaluate the performance and speed of using the widely\nadopted Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for solving\nPINNs. We provide the loss and training time for CMA-ES run on TensorFlow, and\nCMA-ES and SGD run on JAX (with GPU acceleration) for the five benchmark\nproblems. Our results show that JAX-accelerated evolutionary algorithms,\nparticularly CMA-ES, can be a useful approach for solving differential\nequations. We hope that our work will support the exploration and development\nof alternative optimization algorithms for the complex task of optimizing\nPINNs.\n","authors":["Nicholas Sung Wei Yong","Jian Cheng Wong","Pao-Hsiung Chiu","Abhishek Gupta","Chinchun Ooi","Yew-Soon Ong"],"pdf_url":"https://arxiv.org/pdf/2212.07624v1.pdf","comment":"11 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2212.07619v1","updated":"2022-12-15T05:11:04Z","published":"2022-12-15T05:11:04Z","title":"Curriculum Learning Meets Weakly Supervised Modality Correlation\n  Learning","summary":"  In the field of multimodal sentiment analysis (MSA), a few studies have\nleveraged the inherent modality correlation information stored in samples for\nself-supervised learning. However, they feed the training pairs in a random\norder without consideration of difficulty. Without human annotation, the\ngenerated training pairs of self-supervised learning often contain noise. If\nnoisy or hard pairs are used for training at the easy stage, the model might be\nstuck in bad local optimum. In this paper, we inject curriculum learning into\nweakly supervised modality correlation learning. The weakly supervised\ncorrelation learning leverages the label information to generate scores for\nnegative pairs to learn a more discriminative embedding space, where negative\npairs are defined as two unimodal embeddings from different samples. To assist\nthe correlation learning, we feed the training pairs to the model according to\ndifficulty by the proposed curriculum learning, which consists of elaborately\ndesigned scoring and feeding functions. The scoring function computes the\ndifficulty of pairs using pre-trained and current correlation predictors, where\nthe pairs with large losses are defined as hard pairs. Notably, the hardest\npairs are discarded in our algorithm, which are assumed as noisy pairs.\nMoreover, the feeding function takes the difference of correlation losses as\nfeedback to determine the feeding actions (`stay', `step back', or `step\nforward'). The proposed method reaches state-of-the-art performance on MSA.\n","authors":["Sijie Mai","Ya Sun","Haifeng Hu"],"pdf_url":"https://arxiv.org/pdf/2212.07619v1.pdf","comment":"Accepted by EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.02886v2","updated":"2022-12-15T05:10:36Z","published":"2022-12-06T11:23:16Z","title":"GAS-NeXt: Few-Shot Cross-Lingual Font Generator","summary":"  Generating new fonts is a time-consuming and labor-intensive task, especially\nin a language with a huge amount of characters like Chinese. Various deep\nlearning models have demonstrated the ability to efficiently generate new fonts\nwith a few reference characters of that style, but few models support\ncross-lingual font generation. This paper presents GAS-NeXt, a novel few-shot\ncross-lingual font generator based on AGIS-Net and Font Translator GAN, and\nimprove the performance metrics such as Fr\\'echet Inception Distance (FID),\nStructural Similarity Index Measure(SSIM), and Pixel-level Accuracy (pix-acc).\nOur approaches include replacing the original encoder and decoder with the idea\nof layer attention and context-aware attention from Font Translator GAN, while\nutilizing the shape, texture, and local discriminators of AGIS-Net. In our\nexperiment on English-to-Chinese font translation, we observed better results\nin fonts with distinct local features than conventional Chinese fonts compared\nto results obtained from Font Translator GAN. We also validate our method on\nmultiple languages and datasets.\n","authors":["Haoyang He","Xin Jin","Angela Chen"],"pdf_url":"https://arxiv.org/pdf/2212.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07611v1","updated":"2022-12-15T04:22:21Z","published":"2022-12-15T04:22:21Z","title":"Residual Policy Learning for Powertrain Control","summary":"  Eco-driving strategies have been shown to provide significant reductions in\nfuel consumption. This paper outlines an active driver assistance approach that\nuses a residual policy learning (RPL) agent trained to provide residual actions\nto default power train controllers while balancing fuel consumption against\nother driver-accommodation objectives. Using previous experiences, our RPL\nagent learns improved traction torque and gear shifting residual policies to\nadapt the operation of the powertrain to variations and uncertainties in the\nenvironment. For comparison, we consider a traditional reinforcement learning\n(RL) agent trained from scratch. Both agents employ the off-policy Maximum A\nPosteriori Policy Optimization algorithm with an actor-critic architecture. By\nimplementing on a simulated commercial vehicle in various car-following\nscenarios, we find that the RPL agent quickly learns significantly improved\npolicies compared to a baseline source policy but in some measures not as good\nas those eventually possible with the RL agent trained from scratch.\n","authors":["Lindsey Kerbel","Beshah Ayalew","Andrej Ivanco","Keith Loiselle"],"pdf_url":"https://arxiv.org/pdf/2212.07611v1.pdf","comment":"10th IFAC Symposium on Advances in Automotive Control AAC 2022"},{"id":"http://arxiv.org/abs/2212.07608v1","updated":"2022-12-15T04:05:39Z","published":"2022-12-15T04:05:39Z","title":"Output-Dependent Gaussian Process State-Space Model","summary":"  Gaussian process state-space model (GPSSM) is a fully probabilistic\nstate-space model that has attracted much attention over the past decade.\nHowever, the outputs of the transition function in the existing GPSSMs are\nassumed to be independent, meaning that the GPSSMs cannot exploit the inductive\nbiases between different outputs and lose certain model capacities. To address\nthis issue, this paper proposes an output-dependent and more realistic GPSSM by\nutilizing the well-known, simple yet practical linear model of\ncoregionalization (LMC) framework to represent the output dependency. To\njointly learn the output-dependent GPSSM and infer the latent states, we\npropose a variational sparse GP-based learning method that only gently\nincreases the computational complexity. Experiments on both synthetic and real\ndatasets demonstrate the superiority of the output-dependent GPSSM in terms of\nlearning and inference performance.\n","authors":["Zhidi Lin","Lei Cheng","Feng Yin","Lexi Xu","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2212.07608v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2210.08585v3","updated":"2022-12-15T02:56:29Z","published":"2022-10-16T17:10:52Z","title":"A new trigonometric kernel function for support vector machine","summary":"  In the last few years, various types of machine learning algorithms, such as\nSupport Vector Machine (SVM), Support Vector Regression (SVR), and Non-negative\nMatrix Factorization (NMF) have been introduced. The kernel approach is an\neffective method for increasing the classification accuracy of machine learning\nalgorithms. This paper introduces a family of one-parameter kernel functions\nfor improving the accuracy of SVM classification. The proposed kernel function\nconsists of a trigonometric term and differs from all existing kernel\nfunctions. We show this function is a positive definite kernel function.\nFinally, we evaluate the SVM method based on the new trigonometric kernel, the\nGaussian kernel, the polynomial kernel, and a convex combination of the new\nkernel function and the Gaussian kernel function on various types of datasets.\nEmpirical results show that the SVM based on the new trigonometric kernel\nfunction and the mixed kernel function achieve the best classification\naccuracy. Moreover, some numerical results of performing the SVR based on the\nnew trigonometric kernel function and the mixed kernel function are presented.\n","authors":["Sajad Fathi Hafshejani","Zahra Moberfard"],"pdf_url":"https://arxiv.org/pdf/2210.08585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05970v2","updated":"2022-12-15T02:52:34Z","published":"2022-12-09T03:29:38Z","title":"Decomposing a Recurrent Neural Network into Modules for Enabling\n  Reusability and Replacement","summary":"  Can we take a recurrent neural network (RNN) trained to translate between\nlanguages and augment it to support a new natural language without retraining\nthe model from scratch? Can we fix the faulty behavior of the RNN by replacing\nportions associated with the faulty behavior? Recent works on decomposing a\nfully connected neural network (FCNN) and convolutional neural network (CNN)\ninto modules have shown the value of engineering deep models in this manner,\nwhich is standard in traditional SE but foreign for deep learning models.\nHowever, prior works focus on the image-based multiclass classification\nproblems and cannot be applied to RNN due to (a) different layer structures,\n(b) loop structures, (c) different types of input-output architectures, and (d)\nusage of both nonlinear and logistic activation functions. In this work, we\npropose the first approach to decompose an RNN into modules. We study different\ntypes of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN\nmodules can be reused and replaced in various scenarios. We evaluate our\napproach against 5 canonical datasets (i.e., Math QA, Brown Corpus,\nWiki-toxicity, Clinc OOS, and Tatoeba) and 4 model variants for each dataset.\nWe found that decomposing a trained model has a small cost (Accuracy: -0.6%,\nBLEU score: +0.10%). Also, the decomposed modules can be reused and replaced\nwithout needing to retrain.\n","authors":["Sayem Mohammad Imtiaz","Fraol Batole","Astha Singh","Rangeet Pan","Breno Dantas Cruz","Hridesh Rajan"],"pdf_url":"https://arxiv.org/pdf/2212.05970v2.pdf","comment":"Accepted at 45th international conference on software engineering\n  (ICSE'2023)"},{"id":"http://arxiv.org/abs/2212.07594v1","updated":"2022-12-15T02:52:07Z","published":"2022-12-15T02:52:07Z","title":"Driver Assistance Eco-driving and Transmission Control with Deep\n  Reinforcement Learning","summary":"  With the growing need to reduce energy consumption and greenhouse gas\nemissions, Eco-driving strategies provide a significant opportunity for\nadditional fuel savings on top of other technological solutions being pursued\nin the transportation sector. In this paper, a model-free deep reinforcement\nlearning (RL) control agent is proposed for active Eco-driving assistance that\ntrades-off fuel consumption against other driver-accommodation objectives, and\nlearns optimal traction torque and transmission shifting policies from\nexperience. The training scheme for the proposed RL agent uses an off-policy\nactor-critic architecture that iteratively does policy evaluation with a\nmulti-step return and policy improvement with the maximum posteriori policy\noptimization algorithm for hybrid action spaces. The proposed Eco-driving RL\nagent is implemented on a commercial vehicle in car following traffic. It shows\nsuperior performance in minimizing fuel consumption compared to a baseline\ncontroller that has full knowledge of fuel-efficiency tables.\n","authors":["Lindsey Kerbel","Beshah Ayalew","Andrej Ivanco","Keith Loiselle"],"pdf_url":"https://arxiv.org/pdf/2212.07594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07591v1","updated":"2022-12-15T02:43:51Z","published":"2022-12-15T02:43:51Z","title":"Dissecting Distribution Inference","summary":"  A distribution inference attack aims to infer statistical properties of data\nused to train machine learning models. These attacks are sometimes surprisingly\npotent, but the factors that impact distribution inference risk are not well\nunderstood and demonstrated attacks often rely on strong and unrealistic\nassumptions such as full knowledge of training environments even in supposedly\nblack-box threat scenarios. To improve understanding of distribution inference\nrisks, we develop a new black-box attack that even outperforms the best known\nwhite-box attack in most settings. Using this new attack, we evaluate\ndistribution inference risk while relaxing a variety of assumptions about the\nadversary's knowledge under black-box access, like known model architectures\nand label-only access. Finally, we evaluate the effectiveness of previously\nproposed defenses and introduce new defenses. We find that although noise-based\ndefenses appear to be ineffective, a simple re-sampling defense can be highly\neffective. Code is available at\nhttps://github.com/iamgroot42/dissecting_distribution_inference\n","authors":["Anshuman Suri","Yifu Lu","Yanjin Chen","David Evans"],"pdf_url":"https://arxiv.org/pdf/2212.07591v1.pdf","comment":"Accepted at SaTML 2023"},{"id":"http://arxiv.org/abs/2212.07588v1","updated":"2022-12-15T02:40:57Z","published":"2022-12-15T02:40:57Z","title":"DeepJoin: Joinable Table Discovery with Pre-trained Language Models","summary":"  Due to the usefulness in data enrichment for data analysis tasks, joinable\ntable discovery has become an important operation in data lake management.\nExisting approaches target equi-joins, the most common way of combining tables\nfor creating a unified view, or semantic joins, which tolerate misspellings and\ndifferent formats to deliver more join results. They are either exact solutions\nwhose running time is linear in the sizes of query column and target table\nrepository or approximate solutions lacking precision. In this paper, we\npropose Deepjoin, a deep learning model for accurate and efficient joinable\ntable discovery. Our solution is an embedding-based retrieval, which employs a\npre-trained language model (PLM) and is designed as one framework serving both\nequi- and semantic joins. We propose a set of contextualization options to\ntransform column contents to a text sequence. The PLM reads the sequence and is\nfine-tuned to embed columns to vectors such that columns are expected to be\njoinable if they are close to each other in the vector space. Since the output\nof the PLM is fixed in length, the subsequent search procedure becomes\nindependent of the column size. With a state-of-the-art approximate nearest\nneighbor search algorithm, the search time is logarithmic in the repository\nsize. To train the model, we devise the techniques for preparing training data\nas well as data augmentation. The experiments on real datasets demonstrate that\nby training on a small subset of a corpus, Deepjoin generalizes to large\ndatasets and its precision consistently outperforms other approximate\nsolutions'. Deepjoin is even more accurate than an exact solution to semantic\njoins when evaluated with labels from experts. Moreover, when equipped with a\nGPU, Deepjoin is up to two orders of magnitude faster than existing solutions.\n","authors":["Yuyang Dong","Chuan Xiao","Takuma Nozawa","Masafumi Enomoto","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2212.07588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04993v4","updated":"2022-12-15T02:33:40Z","published":"2022-10-10T19:58:23Z","title":"Continual Learning with Evolving Class Ontologies","summary":"  Lifelong learners must recognize concept vocabularies that evolve over time.\nA common yet underexplored scenario is learning with class labels that\ncontinually refine/expand old classes. For example, humans learn to recognize\n${\\tt dog}$ before dog breeds. In practical settings, dataset\n$\\textit{versioning}$ often introduces refinement to ontologies, such as\nautonomous vehicle benchmarks that refine a previous ${\\tt vehicle}$ class into\n${\\tt school-bus}$ as autonomous operations expand to new cities. This paper\nformalizes a protocol for studying the problem of $\\textit{Learning with\nEvolving Class Ontology}$ (LECO). LECO requires learning classifiers in\ndistinct time periods (TPs); each TP introduces a new ontology of \"fine\" labels\nthat refines old ontologies of \"coarse\" labels (e.g., dog breeds that refine\nthe previous ${\\tt dog}$). LECO explores such questions as whether to annotate\nnew data or relabel the old, how to leverage coarse labels, and whether to\nfinetune the previous TP's model or train from scratch. To answer these\nquestions, we leverage insights from related problems such as class-incremental\nlearning. We validate them under the LECO protocol through the lens of image\nclassification (CIFAR and iNaturalist) and semantic segmentation (Mapillary).\nOur experiments lead to surprising conclusions; while the current status quo is\nto relabel existing datasets with new ontologies (such as COCO-to-LVIS or\nMapillary1.2-to-2.0), LECO demonstrates that a far better strategy is to\nannotate $\\textit{new}$ data with the new ontology. However, this produces an\naggregate dataset with inconsistent old-vs-new labels, complicating learning.\nTo address this challenge, we adopt methods from semi-supervised and\npartial-label learning. Such strategies can surprisingly be made near-optimal,\napproaching an \"oracle\" that learns on the aggregate dataset exhaustively\nlabeled with the newest ontology.\n","authors":["Zhiqiu Lin","Deepak Pathak","Yu-Xiong Wang","Deva Ramanan","Shu Kong"],"pdf_url":"https://arxiv.org/pdf/2210.04993v4.pdf","comment":"NeurIPS 2022; Website: https://linzhiqiu.github.io/papers/leco/"},{"id":"http://arxiv.org/abs/2212.07585v1","updated":"2022-12-15T02:25:22Z","published":"2022-12-15T02:25:22Z","title":"Co-Learning with Pre-Trained Networks Improves Source-Free Domain\n  Adaptation","summary":"  Source-free domain adaptation aims to adapt a source model trained on\nfully-labeled source domain data to a target domain with unlabeled target\ndomain data. Source data is assumed inaccessible due to proprietary or privacy\nreasons. Existing works use the source model to pseudolabel target data, but\nthe pseudolabels are unreliable due to data distribution shift between source\nand target domain. In this work, we propose to leverage an ImageNet pre-trained\nfeature extractor in a new co-learning framework to improve target pseudolabel\nquality for finetuning the source model. Benefits of the ImageNet feature\nextractor include that it is not source-biased and it provides an alternate\nview of features and classification decisions different from the source model.\nSuch pre-trained feature extractors are also publicly available, which allows\nus to readily leverage modern network architectures that have strong\nrepresentation learning ability. After co-learning, we sharpen predictions of\nnon-pseudolabeled samples by entropy minimization. Evaluation on 3 benchmark\ndatasets show that our proposed method can outperform existing source-free\ndomain adaptation methods, as well as unsupervised domain adaptation methods\nwhich assume joint access to source and target data.\n","authors":["Wenyu Zhang","Li Shen","Chuan-Sheng Foo"],"pdf_url":"https://arxiv.org/pdf/2212.07585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.03509v2","updated":"2022-12-15T02:03:57Z","published":"2021-04-08T04:52:21Z","title":"Py-Feat: Python Facial Expression Analysis Toolbox","summary":"  Studying facial expressions is a notoriously difficult endeavor. Recent\nadvances in the field of affective computing have yielded impressive progress\nin automatically detecting facial expressions from pictures and videos.\nHowever, much of this work has yet to be widely disseminated in social science\ndomains such as psychology. Current state of the art models require\nconsiderable domain expertise that is not traditionally incorporated into\nsocial science training programs. Furthermore, there is a notable absence of\nuser-friendly and open-source software that provides a comprehensive set of\ntools and functions that support facial expression research. In this paper, we\nintroduce Py-Feat, an open-source Python toolbox that provides support for\ndetecting, preprocessing, analyzing, and visualizing facial expression data.\nPy-Feat makes it easy for domain experts to disseminate and benchmark computer\nvision models and also for end users to quickly process, analyze, and visualize\nface expression data. We hope this platform will facilitate increased use of\nfacial expression data in human behavior research.\n","authors":["Eshin Jolly","Jin Hyun Cheong","Tiankang Xie","Sophie Byrne","Matthew Kenny","Luke J. Chang"],"pdf_url":"https://arxiv.org/pdf/2104.03509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05024v2","updated":"2022-12-15T01:17:25Z","published":"2022-12-09T18:16:41Z","title":"Decomposable Sparse Tensor on Tensor Regression","summary":"  Most regularized tensor regression research focuses on tensors predictors\nwith scalars responses or vectors predictors to tensors responses. We consider\nthe sparse low rank tensor on tensor regression where predictors $\\mathcal{X}$\nand responses $\\mathcal{Y}$ are both high-dimensional tensors. By demonstrating\nthat the general inner product or the contracted product on a unit rank tensor\ncan be decomposed into standard inner products and outer products, the problem\ncan be simply transformed into a tensor to scalar regression followed by a\ntensor decomposition. So we propose a fast solution based on stagewise search\ncomposed by contraction part and generation part which are optimized\nalternatively. We successfully demonstrate our method can out perform current\nmethods in terms of accuracy and predictors selection by effectively\nincorporating the structural information.\n","authors":["Haiyi Mao","Jason Xiaotian Dou"],"pdf_url":"https://arxiv.org/pdf/2212.05024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07568v1","updated":"2022-12-15T01:01:11Z","published":"2022-12-15T01:01:11Z","title":"Man-recon: manifold learning for reconstruction with deep autoencoder\n  for smart seismic interpretation","summary":"  Deep learning can extract rich data representations if provided sufficient\nquantities of labeled training data. For many tasks however, annotating data\nhas significant costs in terms of time and money, owing to the high standards\nof subject matter expertise required, for example in medical and geophysical\nimage interpretation tasks. Active Learning can identify the most informative\ntraining examples for the interpreter to train, leading to higher efficiency.\nWe propose an Active learning method based on jointly learning representations\nfor supervised and unsupervised tasks. The learned manifold structure is later\nutilized to identify informative training samples most dissimilar from the\nlearned manifold from the error profiles on the unsupervised task. We verify\nthe efficiency of the proposed method on a seismic facies segmentation dataset\nfrom the Netherlands F3 block survey, significantly outperforming contemporary\nmethods to achieve the highest mean Intersection-Over-Union value of 0.773.\n","authors":["Ahmad Mustafa","Ghassan AlRegib"],"pdf_url":"https://arxiv.org/pdf/2212.07568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07564v1","updated":"2022-12-15T00:41:09Z","published":"2022-12-15T00:41:09Z","title":"AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for\n  Approximating Reynolds-Averaged Navier-Stokes Solutions","summary":"  Surrogate models are necessary to optimize meaningful quantities in physical\ndynamics as their recursive numerical resolutions are often prohibitively\nexpensive. It is mainly the case for fluid dynamics and the resolution of\nNavier-Stokes equations. However, despite the fast-growing field of data-driven\nmodels for physical systems, reference datasets representing real-world\nphenomena are lacking. In this work, we develop AirfRANS, a dataset for\nstudying the two-dimensional incompressible steady-state Reynolds-Averaged\nNavier-Stokes equations over airfoils at a subsonic regime and for different\nangles of attacks. We also introduce metrics on the stress forces at the\nsurface of geometries and visualization of boundary layers to assess the\ncapabilities of models to accurately predict the meaningful information of the\nproblem. Finally, we propose deep learning baselines on four machine learning\ntasks to study AirfRANS under different constraints for generalization\nconsiderations: big and scarce data regime, Reynolds number, and angle of\nattack extrapolation.\n","authors":["Florent Bonnet","Ahmed Jocelyn Mazari","Paola Cinnella","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2212.07564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07563v1","updated":"2022-12-15T00:38:14Z","published":"2022-12-15T00:38:14Z","title":"Explainable Machine Learning for Hydrocarbon Prospect Risking","summary":"  Hydrocarbon prospect risking is a critical application in geophysics\npredicting well outcomes from a variety of data including geological,\ngeophysical, and other information modalities. Traditional routines require\ninterpreters to go through a long process to arrive at the probability of\nsuccess of specific outcomes. AI has the capability to automate the process but\nits adoption has been limited thus far owing to a lack of transparency in the\nway complicated, black box models generate decisions. We demonstrate how LIME\n-- a model-agnostic explanation technique -- can be used to inject trust in\nmodel decisions by uncovering the model's reasoning process for individual\npredictions. It generates these explanations by fitting interpretable models in\nthe local neighborhood of specific datapoints being queried. On a dataset of\nwell outcomes and corresponding geophysical attribute data, we show how LIME\ncan induce trust in model's decisions by revealing the decision-making process\nto be aligned to domain knowledge. Further, it has the potential to debug\nmispredictions made due to anomalous patterns in the data or faulty training\ndatasets.\n","authors":["Ahmad Mustafa","Ghassan AlRegib"],"pdf_url":"https://arxiv.org/pdf/2212.07563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07562v1","updated":"2022-12-15T00:37:41Z","published":"2022-12-15T00:37:41Z","title":"Robustness Evaluation of Regression Tasks with Skewed Domain Preferences","summary":"  In natural phenomena, data distributions often deviate from normality. One\ncan think of cataclysms as a self-explanatory example: events that occur almost\nnever, and at the same time are many standard deviations away from the common\noutcome. In many scientific contexts it is exactly these tail events that\nresearchers are most interested in anticipating, so that adequate measures can\nbe taken to prevent or attenuate a major impact on society. Despite such\nefforts, we have yet to provide definite answers to crucial issues in\nevaluating predictive solutions in domains such as weather, pollution, health.\nIn this paper, we deal with two encapsulated problems simultaneously. First,\nassessing the performance of regression models when non-uniform preferences\napply - not all values are equally relevant concerning the accuracy of their\nprediction, and there's a particular interest in the most extreme values.\nSecond, assessing the robustness of models when dealing with uncertainty\nregarding the actual underlying distribution of values relevant for such\nproblems. We show how different levels of relevance associated with target\nvalues may impact experimental conclusions, and demonstrate the practical\nutility of the proposed methods.\n","authors":["Nuno Costa","Nuno Moniz"],"pdf_url":"https://arxiv.org/pdf/2212.07562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.13741v2","updated":"2022-12-15T00:19:16Z","published":"2022-05-27T03:09:55Z","title":"Generating multivariate time series with COmmon Source CoordInated GAN\n  (COSCI-GAN)","summary":"  Generating multivariate time series is a promising approach for sharing\nsensitive data in many medical, financial, and IoT applications. A common type\nof multivariate time series originates from a single source such as the\nbiometric measurements from a medical patient. This leads to complex dynamical\npatterns between individual time series that are hard to learn by typical\ngeneration models such as GANs. There is valuable information in those patterns\nthat machine learning models can use to better classify, predict or perform\nother downstream tasks. We propose a novel framework that takes time series'\ncommon origin into account and favors channel/feature relationships\npreservation. The two key points of our method are: 1) the individual time\nseries are generated from a common point in latent space and 2) a central\ndiscriminator favors the preservation of inter-channel/feature dynamics. We\ndemonstrate empirically that our method helps preserve channel/feature\ncorrelations and that our synthetic data performs very well in downstream tasks\nwith medical and financial data.\n","authors":["Ali Seyfi","Jean-Francois Rajotte","Raymond T. Ng"],"pdf_url":"https://arxiv.org/pdf/2205.13741v2.pdf","comment":"19 pages, 16 figures"},{"id":"http://arxiv.org/abs/2212.07558v1","updated":"2022-12-15T00:08:05Z","published":"2022-12-15T00:08:05Z","title":"DOC-NAD: A Hybrid Deep One-class Classifier for Network Anomaly\n  Detection","summary":"  Machine Learning (ML) approaches have been used to enhance the detection\ncapabilities of Network Intrusion Detection Systems (NIDSs). Recent work has\nachieved near-perfect performance by following binary- and multi-class network\nanomaly detection tasks. Such systems depend on the availability of both\n(benign and malicious) network data classes during the training phase. However,\nattack data samples are often challenging to collect in most organisations due\nto security controls preventing the penetration of known malicious traffic to\ntheir networks. Therefore, this paper proposes a Deep One-Class (DOC)\nclassifier for network intrusion detection by only training on benign network\ndata samples. The novel one-class classification architecture consists of a\nhistogram-based deep feed-forward classifier to extract useful network data\nfeatures and use efficient outlier detection. The DOC classifier has been\nextensively evaluated using two benchmark NIDS datasets. The results\ndemonstrate its superiority over current state-of-the-art one-class classifiers\nin terms of detection and false positive rates.\n","authors":["Mohanad Sarhan","Gayan Kulatilleke","Wai Weng Lo","Siamak Layeghy","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2212.07558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08199v1","updated":"2022-12-15T23:55:01Z","published":"2022-12-15T23:55:01Z","title":"Asymptotic Analysis of Deep Residual Networks","summary":"  We investigate the asymptotic properties of deep Residual networks (ResNets)\nas the number of layers increases. We first show the existence of scaling\nregimes for trained weights markedly different from those implicitly assumed in\nthe neural ODE literature. We study the convergence of the hidden state\ndynamics in these scaling regimes, showing that one may obtain an ODE, a\nstochastic differential equation (SDE) or neither of these. In particular, our\nfindings point to the existence of a diffusive regime in which the deep network\nlimit is described by a class of stochastic differential equations (SDEs).\nFinally, we derive the corresponding scaling limits for the backpropagation\ndynamics.\n","authors":["Rama Cont","Alain Rossier","Renyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2212.08199v1.pdf","comment":"49 pages, 12 figures. arXiv admin note: substantial text overlap with\n  arXiv:2105.12245"},{"id":"http://arxiv.org/abs/2210.10047v3","updated":"2022-12-15T23:47:39Z","published":"2022-10-18T17:59:55Z","title":"From Play to Policy: Conditional Behavior Generation from Uncurated\n  Robot Data","summary":"  While large-scale sequence modeling from offline data has led to impressive\nperformance gains in natural language and image generation, directly\ntranslating such ideas to robotics has been challenging. One critical reason\nfor this is that uncurated robot demonstration data, i.e. play data, collected\nfrom non-expert human demonstrators are often noisy, diverse, and\ndistributionally multi-modal. This makes extracting useful, task-centric\nbehaviors from such data a difficult generative modeling problem. In this work,\nwe present Conditional Behavior Transformers (C-BeT), a method that combines\nthe multi-modal generation ability of Behavior Transformer with\nfuture-conditioned goal specification. On a suite of simulated benchmark tasks,\nwe find that C-BeT improves upon prior state-of-the-art work in learning from\nplay data by an average of 45.7%. Further, we demonstrate for the first time\nthat useful task-centric behaviors can be learned on a real-world robot purely\nfrom play data without any task labels or reward information. Robot videos are\nbest viewed on our project website: https://play-to-policy.github.io\n","authors":["Zichen Jeff Cui","Yibin Wang","Nur Muhammad Mahi Shafiullah","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2210.10047v3.pdf","comment":"Code and data available at: https://play-to-policy.github.io; (fixed\n  metadata author name format)"},{"id":"http://arxiv.org/abs/2009.04709v4","updated":"2022-12-15T23:35:23Z","published":"2020-09-10T07:48:42Z","title":"Quantifying the Preferential Direction of the Model Gradient in\n  Adversarial Training With Projected Gradient Descent","summary":"  Adversarial training, especially projected gradient descent (PGD), has proven\nto be a successful approach for improving robustness against adversarial\nattacks. After adversarial training, gradients of models with respect to their\ninputs have a preferential direction. However, the direction of alignment is\nnot mathematically well established, making it difficult to evaluate\nquantitatively. We propose a novel definition of this direction as the\ndirection of the vector pointing toward the closest point of the support of the\nclosest inaccurate class in decision space. To evaluate the alignment with this\ndirection after adversarial training, we apply a metric that uses generative\nadversarial networks to produce the smallest residual needed to change the\nclass present in the image. We show that PGD-trained models have a higher\nalignment than the baseline according to our definition, that our metric\npresents higher alignment values than a competing metric formulation, and that\nenforcing this alignment increases the robustness of models.\n","authors":["Ricardo Bigolin Lanfredi","Joyce D. Schroeder","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2009.04709v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08192v1","updated":"2022-12-15T23:26:54Z","published":"2022-12-15T23:26:54Z","title":"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources\n  in Natural Language Understanding Systems","summary":"  Many state-of-the-art natural language understanding (NLU) models are based\non pretrained neural language models. These models often make inferences using\ninformation from multiple sources. An important class of such inferences are\nthose that require both background knowledge, presumably contained in a model's\npretrained parameters, and instance-specific information that is supplied at\ninference time. However, the integration and reasoning abilities of NLU models\nin the presence of multiple knowledge sources have been largely understudied.\nIn this work, we propose a test suite of coreference resolution tasks that\nrequire reasoning over multiple facts. Our dataset is organized into subtasks\nthat differ in terms of which knowledge sources contain relevant facts. We\nevaluate state-of-the-art coreference resolution models on our dataset. Our\nresults indicate that several models struggle to reason on-the-fly over\nknowledge observed both at pretrain time and at inference time. However, with\ntask-specific training, a subset of models demonstrates the ability to\nintegrate certain knowledge types from multiple sources.\n","authors":["Akshatha Arodi","Martin Pömsl","Kaheer Suleman","Adam Trischler","Alexandra Olteanu","Jackie Chi Kit Cheung"],"pdf_url":"https://arxiv.org/pdf/2212.08192v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2212.08189v1","updated":"2022-12-15T23:21:49Z","published":"2022-12-15T23:21:49Z","title":"Multi-Resolution Online Deterministic Annealing: A Hierarchical and\n  Progressive Learning Architecture","summary":"  Hierarchical learning algorithms that gradually approximate a solution to a\ndata-driven optimization problem are essential to decision-making systems,\nespecially under limitations on time and computational resources. In this\nstudy, we introduce a general-purpose hierarchical learning architecture that\nis based on the progressive partitioning of a possibly multi-resolution data\nspace. The optimal partition is gradually approximated by solving a sequence of\noptimization sub-problems that yield a sequence of partitions with increasing\nnumber of subsets. We show that the solution of each optimization problem can\nbe estimated online using gradient-free stochastic approximation updates. As a\nconsequence, a function approximation problem can be defined within each subset\nof the partition and solved using the theory of two-timescale stochastic\napproximation algorithms. This simulates an annealing process and defines a\nrobust and interpretable heuristic method to gradually increase the complexity\nof the learning architecture in a task-agnostic manner, giving emphasis to\nregions of the data space that are considered more important according to a\npredefined criterion. Finally, by imposing a tree structure in the progression\nof the partitions, we provide a means to incorporate potential multi-resolution\nstructure of the data space into this approach, significantly reducing its\ncomplexity, while introducing hierarchical feature extraction properties\nsimilar to certain classes of deep learning architectures. Asymptotic\nconvergence analysis and experimental results are provided for clustering,\nclassification, and regression problems.\n","authors":["Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08187v1","updated":"2022-12-15T23:20:13Z","published":"2022-12-15T23:20:13Z","title":"Dual Moving Average Pseudo-Labeling for Source-Free Inductive Domain\n  Adaptation","summary":"  Unsupervised domain adaptation reduces the reliance on data annotation in\ndeep learning by adapting knowledge from a source to a target domain. For\nprivacy and efficiency concerns, source-free domain adaptation extends\nunsupervised domain adaptation by adapting a pre-trained source model to an\nunlabeled target domain without accessing the source data. However, most\nexisting source-free domain adaptation methods to date focus on the\ntransductive setting, where the target training set is also the testing set. In\nthis paper, we address source-free domain adaptation in the more realistic\ninductive setting, where the target training and testing sets are mutually\nexclusive. We propose a new semi-supervised fine-tuning method named Dual\nMoving Average Pseudo-Labeling (DMAPL) for source-free inductive domain\nadaptation. We first split the unlabeled training set in the target domain into\na pseudo-labeled confident subset and an unlabeled less-confident subset\naccording to the prediction confidence scores from the pre-trained source\nmodel. Then we propose a soft-label moving-average updating strategy for the\nunlabeled subset based on a moving-average prototypical classifier, which\ngradually adapts the source model towards the target domain. Experiments show\nthat our proposed method achieves state-of-the-art performance and outperforms\nprevious methods by large margins.\n","authors":["Hao Yan","Yuhong Guo"],"pdf_url":"https://arxiv.org/pdf/2212.08187v1.pdf","comment":"BMVC 2022"},{"id":"http://arxiv.org/abs/2210.01959v2","updated":"2022-12-15T23:16:30Z","published":"2022-10-04T23:33:52Z","title":"Detect, Retrieve, Comprehend: A Flexible Framework for Zero-Shot\n  Document-Level Question Answering","summary":"  Researchers produce thousands of scholarly documents containing valuable\ntechnical knowledge. The community faces the laborious task of reading these\ndocuments to identify, extract, and synthesize information. To automate\ninformation gathering, document-level question answering (QA) offers a flexible\nframework where human-posed questions can be adapted to extract diverse\nknowledge. Finetuning QA systems requires access to labeled data (tuples of\ncontext, question and answer). However, data curation for document QA is\nuniquely challenging because the context (i.e. answer evidence passage) needs\nto be retrieved from potentially long, ill-formatted documents. Existing QA\ndatasets sidestep this challenge by providing short, well-defined contexts that\nare unrealistic in real-world applications. We present a three-stage document\nQA approach: (1) text extraction from PDF; (2) evidence retrieval from\nextracted texts to form well-posed contexts; (3) QA to extract knowledge from\ncontexts to return high-quality answers -- extractive, abstractive, or Boolean.\nUsing QASPER for evaluation, our detect-retrieve-comprehend (DRC) system\nachieves a +7.19 improvement in Answer-F1 over existing baselines while\ndelivering superior context selection. Our results demonstrate that DRC holds\ntremendous promise as a flexible framework for practical scientific document\nQA.\n","authors":["Tavish McDonald","Brian Tsan","Amar Saini","Juanita Ordonez","Luis Gutierrez","Phan Nguyen","Blake Mason","Brenda Ng"],"pdf_url":"https://arxiv.org/pdf/2210.01959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08186v1","updated":"2022-12-15T23:12:53Z","published":"2022-12-15T23:12:53Z","title":"Learning Sparsity and Randomness for Data-driven Low Rank Approximation","summary":"  Learning-based low rank approximation algorithms can significantly improve\nthe performance of randomized low rank approximation with sketch matrix. With\nthe learned value and fixed non-zero positions for sketch matrices from\nlearning-based algorithms, these matrices can reduce the test error of low rank\napproximation significantly. However, there is still no good method to learn\nnon-zero positions as well as overcome the out-of-distribution performance\nloss. In this work, we introduce two new methods Learning Sparsity and Learning\nRandomness which try to learn a better sparsity patterns and add randomness to\nthe value of sketch matrix. These two methods can be applied with any\nlearning-based algorithms which use sketch matrix directly. Our experiments\nshow that these two methods can improve the performance of previous\nlearning-based algorithm for both test error and out-of-distribution test error\nwithout adding too much complexity.\n","authors":["Tiejin Chen","Yicheng Tao"],"pdf_url":"https://arxiv.org/pdf/2212.08186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15792v2","updated":"2022-12-15T23:02:07Z","published":"2022-11-28T21:59:58Z","title":"Provably Efficient Model-free RL in Leader-Follower MDP with Linear\n  Function Approximation","summary":"  We consider a multi-agent episodic MDP setup where an agent (leader) takes\naction at each step of the episode followed by another agent (follower). The\nstate evolution and rewards depend on the joint action pair of the leader and\nthe follower. Such type of interactions can find applications in many domains\nsuch as smart grids, mechanism design, security, and policymaking. We are\ninterested in how to learn policies for both the players with provable\nperformance guarantee under a bandit feedback setting. We focus on a setup\nwhere both the leader and followers are {\\em non-myopic}, i.e., they both seek\nto maximize their rewards over the entire episode and consider a linear MDP\nwhich can model continuous state-space which is very common in many RL\napplications. We propose a {\\em model-free} RL algorithm and show that\n$\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret bounds can be achieved for both\nthe leader and the follower, where $d$ is the dimension of the feature mapping,\n$H$ is the length of the episode, and $T$ is the total number of steps under\nthe bandit feedback information setup. Thus, our result holds even when the\nnumber of states becomes infinite. The algorithm relies on {\\em novel}\nadaptation of the LSVI-UCB algorithm. Specifically, we replace the standard\ngreedy policy (as the best response) with the soft-max policy for both the\nleader and the follower. This turns out to be key in establishing uniform\nconcentration bound for the value functions. To the best of our knowledge, this\nis the first sub-linear regret bound guarantee for the Markov games with\nnon-myopic followers with function approximation.\n","authors":["Arnob Ghosh"],"pdf_url":"https://arxiv.org/pdf/2211.15792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08184v1","updated":"2022-12-15T23:00:33Z","published":"2022-12-15T23:00:33Z","title":"NBC-Softmax : Darkweb Author fingerprinting and migration tracking","summary":"  Metric learning aims to learn distances from the data, which enhances the\nperformance of similarity-based algorithms. An author style detection task is a\nmetric learning problem, where learning style features with small intra-class\nvariations and larger inter-class differences is of great importance to achieve\nbetter performance. Recently, metric learning based on softmax loss has been\nused successfully for style detection. While softmax loss can produce separable\nrepresentations, its discriminative power is relatively poor. In this work, we\npropose NBC-Softmax, a contrastive loss based clustering technique for softmax\nloss, which is more intuitive and able to achieve superior performance. Our\ntechnique meets the criterion for larger number of samples, thus achieving\nblock contrastiveness, which is proven to outperform pair-wise losses. It uses\nmini-batch sampling effectively and is scalable. Experiments on 4 darkweb\nsocial forums, with NBCSAuthor that uses the proposed NBC-Softmax for author\nand sybil detection, shows that our negative block contrastive approach\nconstantly outperforms state-of-the-art methods using the same network\narchitecture.\n  Our code is publicly available at : https://github.com/gayanku/NBC-Softmax\n","authors":["Gayan K. Kulatilleke","Shekhar S. Chandra","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2212.08184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09718v2","updated":"2022-12-15T22:45:46Z","published":"2022-11-02T00:58:02Z","title":"Numerical Optimizations for Weighted Low-rank Estimation on Language\n  Model","summary":"  Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.\n","authors":["Ting Hua","Yen-Chang Hsu","Felicity Wang","Qian Lou","Yilin Shen","Hongxia Jin"],"pdf_url":"https://arxiv.org/pdf/2211.09718v2.pdf","comment":"long paper EMNLP 2022"},{"id":"http://arxiv.org/abs/2202.05159v3","updated":"2022-12-15T22:33:19Z","published":"2022-02-09T09:36:31Z","title":"Dimensional criterion for forecasting nonlinear systems by reservoir\n  computing","summary":"  Reservoir computers (RC) have proven useful as surrogate models in\nforecasting and replicating systems of chaotic dynamics. The quality of\nsurrogate models based on RCs is crucially dependent on their optimal\nimplementation that involves selecting optimal reservoir topology and\nhyperparameters. By systematically applying Bayesian hyperparameter\noptimization and using ensembles of reservoirs of various topology we show that\nconnectednes of reservoirs is of significance only in forecasting and\nreplication of chaotic system of sufficient complexity. By applying RCs of\ndifferent topology in forecasting and replicating the Lorenz system, a coupled\nWilson-Cowan system, and the Kuramoto-Sivashinsky system, we show that simple\nreservoirs of unconnected nodes (RUN) outperform reservoirs of connected nodes\nfor target systems whose estimated fractal dimension dimension is $d \\lesssim\n5.5$ and that linked reservoirs are better for systems with $d > 5.5$. This\nfinding is highly important for evaluation of reservoir computing methods and\non selecting a method for prediction of signals measured on nonlinear systems.\n","authors":["Pauliina Kärkkäinen","Riku Linna"],"pdf_url":"https://arxiv.org/pdf/2202.05159v3.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2206.00129v3","updated":"2022-12-15T22:32:26Z","published":"2022-05-31T22:16:44Z","title":"Fairness Transferability Subject to Bounded Distribution Shift","summary":"  Given an algorithmic predictor that is \"fair\" on some source distribution,\nwill it still be fair on an unknown target distribution that differs from the\nsource within some bound? In this paper, we study the transferability of\nstatistical group fairness for machine learning predictors (i.e., classifiers\nor regressors) subject to bounded distribution shifts. Such shifts may be\nintroduced by initial training data uncertainties, user adaptation to a\ndeployed predictor, dynamic environments, or the use of pre-trained models in\nnew settings. Herein, we develop a bound that characterizes such\ntransferability, flagging potentially inappropriate deployments of machine\nlearning for socially consequential tasks. We first develop a framework for\nbounding violations of statistical fairness subject to distribution shift,\nformulating a generic upper bound for transferred fairness violations as our\nprimary result. We then develop bounds for specific worked examples, focusing\non two commonly used fairness definitions (i.e., demographic parity and\nequalized odds) and two classes of distribution shift (i.e., covariate shift\nand label shift). Finally, we compare our theoretical bounds to deterministic\nmodels of distribution shift and against real-world data, finding that we are\nable to estimate fairness violation bounds in practice, even when simplifying\nassumptions are only approximately satisfied.\n","authors":["Yatong Chen","Reilly Raab","Jialu Wang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2206.00129v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08174v1","updated":"2022-12-15T22:29:29Z","published":"2022-12-15T22:29:29Z","title":"Non-IID Transfer Learning on Graphs","summary":"  Transfer learning refers to the transfer of knowledge or information from a\nrelevant source domain to a target domain. However, most existing transfer\nlearning theories and algorithms focus on IID tasks, where the source/target\nsamples are assumed to be independent and identically distributed. Very little\neffort is devoted to theoretically studying the knowledge transferability on\nnon-IID tasks, e.g., cross-network mining. To bridge the gap, in this paper, we\npropose rigorous generalization bounds and algorithms for cross-network\ntransfer learning from a source graph to a target graph. The crucial idea is to\ncharacterize the cross-network knowledge transferability from the perspective\nof the Weisfeiler-Lehman graph isomorphism test. To this end, we propose a\nnovel Graph Subtree Discrepancy to measure the graph distribution shift between\nsource and target graphs. Then the generalization error bounds on cross-network\ntransfer learning, including both cross-network node classification and link\nprediction tasks, can be derived in terms of the source knowledge and the Graph\nSubtree Discrepancy across domains. This thereby motivates us to propose a\ngeneric graph adaptive network (GRADE) to minimize the distribution shift\nbetween source and target graphs for cross-network transfer learning.\nExperimental results verify the effectiveness and efficiency of our GRADE\nframework on both cross-network node classification and cross-domain\nrecommendation tasks.\n","authors":["Jun Wu","Jingrui He","Elizabeth Ainsworth"],"pdf_url":"https://arxiv.org/pdf/2212.08174v1.pdf","comment":"Accepted by AAAI-23"},{"id":"http://arxiv.org/abs/2206.07144v2","updated":"2022-12-15T22:18:44Z","published":"2022-06-14T20:09:04Z","title":"Flatten the Curve: Efficiently Training Low-Curvature Neural Networks","summary":"  The highly non-linear nature of deep neural networks causes them to be\nsusceptible to adversarial examples and have unstable gradients which hinders\ninterpretability. However, existing methods to solve these issues, such as\nadversarial training, are expensive and often sacrifice predictive accuracy.\n  In this work, we consider curvature, which is a mathematical quantity which\nencodes the degree of non-linearity. Using this, we demonstrate low-curvature\nneural networks (LCNNs) that obtain drastically lower curvature than standard\nmodels while exhibiting similar predictive performance, which leads to improved\nrobustness and stable gradients, with only a marginally increased training\ntime. To achieve this, we minimize a data-independent upper bound on the\ncurvature of a neural network, which decomposes overall curvature in terms of\ncurvatures and slopes of its constituent layers. To efficiently minimize this\nbound, we introduce two novel architectural components: first, a non-linearity\ncalled centered-softplus that is a stable variant of the softplus\nnon-linearity, and second, a Lipschitz-constrained batch normalization layer.\n  Our experiments show that LCNNs have lower curvature, more stable gradients\nand increased off-the-shelf adversarial robustness when compared to their\nstandard high-curvature counterparts, all without affecting predictive\nperformance. Our approach is easy to use and can be readily incorporated into\nexisting neural network models.\n","authors":["Suraj Srinivas","Kyle Matoba","Himabindu Lakkaraju","Francois Fleuret"],"pdf_url":"https://arxiv.org/pdf/2206.07144v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08172v1","updated":"2022-12-15T22:15:11Z","published":"2022-12-15T22:15:11Z","title":"Reliable Measures of Spread in High Dimensional Latent Spaces","summary":"  Understanding geometric properties of natural language processing models'\nlatent spaces allows the manipulation of these properties for improved\nperformance on downstream tasks. One such property is the amount of data spread\nin a model's latent space, or how fully the available latent space is being\nused. In this work, we define data spread and demonstrate that the commonly\nused measures of data spread, Average Cosine Similarity and a partition\nfunction min/max ratio I(V), do not provide reliable metrics to compare the use\nof latent space across models. We propose and examine eight alternative\nmeasures of data spread, all but one of which improve over these current\nmetrics when applied to seven synthetic data distributions. Of our proposed\nmeasures, we recommend one principal component-based measure and one\nentropy-based measure that provide reliable, relative measures of spread and\ncan be used to compare models of different sizes and dimensionalities.\n","authors":["Anna C. Marbut","Katy McKinney-Bock","Travis J. Wheeler"],"pdf_url":"https://arxiv.org/pdf/2212.08172v1.pdf","comment":"24 pages, 11 figures, 13 tables"},{"id":"http://arxiv.org/abs/2212.08171v1","updated":"2022-12-15T22:11:34Z","published":"2022-12-15T22:11:34Z","title":"Graphon Pooling for Reducing Dimensionality of Signals and Convolutional\n  Operators on Graphs","summary":"  In this paper we propose a pooling approach for convolutional information\nprocessing on graphs relying on the theory of graphons and limits of dense\ngraph sequences. We present three methods that exploit the induced graphon\nrepresentation of graphs and graph signals on partitions of [0, 1]2 in the\ngraphon space. As a result we derive low dimensional representations of the\nconvolutional operators, while a dimensionality reduction of the signals is\nachieved by simple local interpolation of functions in L2([0, 1]). We prove\nthat those low dimensional representations constitute a convergent sequence of\ngraphs and graph signals, respectively. The methods proposed and the\ntheoretical guarantees that we provide show that the reduced graphs and signals\ninherit spectral-structural properties of the original quantities. We evaluate\nour approach with a set of numerical experiments performed on graph neural\nnetworks (GNNs) that rely on graphon pooling. We observe that graphon pooling\nperforms significantly better than other approaches proposed in the literature\nwhen dimensionality reduction ratios between layers are large. We also observe\nthat when graphon pooling is used we have, in general, less overfitting and\nlower computational cost.\n","authors":["Alejandro Parada-Mayorga","Zhiyang Wang","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2212.08171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08170v1","updated":"2022-12-15T22:10:11Z","published":"2022-12-15T22:10:11Z","title":"BNSynth: Bounded Boolean Functional Synthesis","summary":"  The automated synthesis of correct-by-construction Boolean functions from\nlogical specifications is known as the Boolean Functional Synthesis (BFS)\nproblem. BFS has many application areas that range from software engineering to\ncircuit design. In this paper, we introduce a tool BNSynth, that is the first\nto solve the BFS problem under a given bound on the solution space. Bounding\nthe solution space induces the synthesis of smaller functions that benefit\nresource constrained areas such as circuit design. BNSynth uses a\ncounter-example guided, neural approach to solve the bounded BFS problem.\nInitial results show promise in synthesizing smaller solutions; we observe at\nleast \\textbf{3.2X} (and up to \\textbf{24X}) improvement in the reduction of\nsolution size on average, as compared to state of the art tools on our\nbenchmarks. BNSynth is available on GitHub under an open source license.\n","authors":["Ravi Raja","Stanly Samuel","Chiranjib Bhattacharyya","Deepak D'Souza","Aditya Kanade"],"pdf_url":"https://arxiv.org/pdf/2212.08170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07231v2","updated":"2022-12-15T21:58:01Z","published":"2022-12-14T14:06:27Z","title":"Cutting Plane Selection with Analytic Centers and Multiregression","summary":"  Cutting planes are a crucial component of state-of-the-art mixed-integer\nprogramming solvers, with the choice of which subset of cuts to add being vital\nfor solver performance. We propose new distance-based measures to qualify the\nvalue of a cut by quantifying the extent to which it separates relevant parts\nof the relaxed feasible set. For this purpose, we use the analytic centers of\nthe relaxation polytope or of its optimal face, as well as alternative optimal\nsolutions of the linear programming relaxation. We assess the impact of the\nchoice of distance measure on root node performance and throughout the whole\nbranch-and-bound tree, comparing our measures against those prevalent in the\nliterature. Finally, by a multi-output regression, we predict the relative\nperformance of each measure, using static features readily available before the\nseparation process. Our results indicate that analytic center-based methods\nhelp to significantly reduce the number of branch-and-bound nodes needed to\nexplore the search space and that our multiregression approach can further\nimprove on any individual method.\n","authors":["Mark Turner","Timo Berthold","Mathieu Besançon","Thorsten Koch"],"pdf_url":"https://arxiv.org/pdf/2212.07231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08162v1","updated":"2022-12-15T21:50:54Z","published":"2022-12-15T21:50:54Z","title":"Huber-energy measure quantization","summary":"  We describe a measure quantization procedure i.e., an algorithm which finds\nthe best approximation of a target probability law (and more generally signed\nfinite variation measure) by a sum of Q Dirac masses (Q being the quantization\nparameter). The procedure is implemented by minimizing the statistical distance\nbetween the original measure and its quantized version; the distance is built\nfrom a negative definite kernel and, if necessary, can be computed on the fly\nand feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We\ninvestigate theoretically the fundamental questions of existence of the optimal\nmeasure quantizer and identify what are the required kernel properties that\nguarantee suitable behavior. We test the procedure, called HEMQ, on several\ndatabases: multi-dimensional Gaussian mixtures, Wiener space cubature, Italian\nwine cultivars and the MNIST image database. The results indicate that the HEMQ\nalgorithm is robust and versatile and, for the class of Huber-energy kernels,\nit matches the expected intuitive behavior.\n","authors":["Gabriel Turinici"],"pdf_url":"https://arxiv.org/pdf/2212.08162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08153v1","updated":"2022-12-15T21:35:46Z","published":"2022-12-15T21:35:46Z","title":"FiDO: Fusion-in-Decoder optimized for stronger performance and faster\n  inference","summary":"  Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that\nsets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD\nsuffers from very expensive inference. We show that the majority of inference\ntime results from memory bandwidth constraints in the decoder, and propose two\nsimple changes to the FiD architecture to speed up inference by 7x. The faster\ndecoder inference then allows for a much larger decoder. We denote FiD with the\nabove modifications as FiDO, and show that it strongly improves performance\nover existing FiD models for a wide range of inference budgets. For example,\nFiDO-Large-XXL performs faster inference than FiD-Base and achieves better\nperformance than FiD-Large.\n","authors":["Michiel de Jong","Yury Zemlyanskiy","Joshua Ainslie","Nicholas FitzGerald","Sumit Sanghai","Fei Sha","William Cohen"],"pdf_url":"https://arxiv.org/pdf/2212.08153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08151v1","updated":"2022-12-15T21:34:19Z","published":"2022-12-15T21:34:19Z","title":"First De-Trend then Attend: Rethinking Attention for Time-Series\n  Forecasting","summary":"  Transformer-based models have gained large popularity and demonstrated\npromising results in long-term time-series forecasting in recent years. In\naddition to learning attention in time domain, recent works also explore\nlearning attention in frequency domains (e.g., Fourier domain, wavelet domain),\ngiven that seasonal patterns can be better captured in these domains. In this\nwork, we seek to understand the relationships between attention models in\ndifferent time and frequency domains. Theoretically, we show that attention\nmodels in different domains are equivalent under linear conditions (i.e.,\nlinear kernel to attention scores). Empirically, we analyze how attention\nmodels of different domains show different behaviors through various synthetic\nexperiments with seasonality, trend and noise, with emphasis on the role of\nsoftmax operation therein. Both these theoretical and empirical analyses\nmotivate us to propose a new method: TDformer (Trend Decomposition\nTransformer), that first applies seasonal-trend decomposition, and then\nadditively combines an MLP which predicts the trend component with Fourier\nattention which predicts the seasonal component to obtain the final prediction.\nExtensive experiments on benchmark time-series forecasting datasets demonstrate\nthat TDformer achieves state-of-the-art performance against existing\nattention-based models.\n","authors":["Xiyuan Zhang","Xiaoyong Jin","Karthick Gopalswamy","Gaurav Gupta","Youngsuk Park","Xingjian Shi","Hao Wang","Danielle C. Maddix","Yuyang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.08151v1.pdf","comment":"NeurIPS 2022 All Things Attention Workshop"},{"id":"http://arxiv.org/abs/2205.11495v3","updated":"2022-12-15T20:57:59Z","published":"2022-05-23T17:51:48Z","title":"Flexible Diffusion Modeling of Long Videos","summary":"  We present a framework for video modeling based on denoising diffusion\nprobabilistic models that produces long-duration video completions in a variety\nof realistic environments. We introduce a generative model that can at\ntest-time sample any arbitrary subset of video frames conditioned on any other\nsubset and present an architecture adapted for this purpose. Doing so allows us\nto efficiently compare and optimize a variety of schedules for the order in\nwhich frames in a long video are sampled and use selective sparse and\nlong-range conditioning on previously sampled frames. We demonstrate improved\nvideo modeling over prior work on a number of datasets and sample temporally\ncoherent videos over 25 minutes in length. We additionally release a new video\nmodeling dataset and semantically meaningful metrics based on videos generated\nin the CARLA autonomous driving simulator.\n","authors":["William Harvey","Saeid Naderiparizi","Vaden Masrani","Christian Weilbach","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2205.11495v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08136v1","updated":"2022-12-15T20:51:27Z","published":"2022-12-15T20:51:27Z","title":"Efficient Long Sequence Modeling via State Space Augmented Transformer","summary":"  Transformer models have achieved superior performance in various natural\nlanguage processing tasks. However, the quadratic computational cost of the\nattention mechanism limits its practicality for long sequences. There are\nexisting attention variants that improve the computational efficiency, but they\nhave limited ability to effectively compute global information. In parallel to\nTransformer models, state space models (SSMs) are tailored for long sequences,\nbut they are not flexible enough to capture complicated local information. We\npropose SPADE, short for $\\underline{\\textbf{S}}$tate\ns$\\underline{\\textbf{P}}$ace\n$\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$\nTransform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the\nbottom layer of SPADE, and we employ efficient local attention methods for the\nother layers. The SSM augments global information, which complements the lack\nof long-range dependency issue in local attention methods. Experimental results\non the Long Range Arena benchmark and language modeling tasks demonstrate the\neffectiveness of the proposed method. To further demonstrate the scalability of\nSPADE, we pre-train large encoder-decoder models and present fine-tuning\nresults on natural language understanding and natural language generation\ntasks.\n","authors":["Simiao Zuo","Xiaodong Liu","Jian Jiao","Denis Charles","Eren Manavoglu","Tuo Zhao","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2212.08136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.13290v2","updated":"2022-12-15T20:42:56Z","published":"2022-08-28T21:10:56Z","title":"Domain Adaptation Principal Component Analysis: base linear method for\n  learning with out-of-distribution data","summary":"  Domain adaptation is a popular paradigm in modern machine learning which aims\nat tackling the problem of divergence (or shift) between the labeled training\nand validation datasets (source domain) and a potentially large unlabeled\ndataset (target domain). The task is to embed both datasets red into a common\nspace in which the source dataset is informative for training while the\ndivergence between source and target is minimized. The most popular domain\nadaptation solutions are based on training neural networks that combine\nclassification and adversarial learning modules, frequently making them both\ndata-hungry and difficult to train. We present a method called Domain\nAdaptation Principal Component Analysis (DAPCA) that identifies a linear\nreduced data representation useful for solving the domain adaptation task.\nDAPCA algorithm introduces positive and negative weights between pairs of data\npoints, and generalizes the supervised extension of principal component\nanalysis. DAPCA is an iterative algorithm that solves a simple quadratic\noptimization problem at each iteration. The convergence of the algorithm is\nguaranteed, and the number of iterations is small in practice. We validate the\nsuggested algorithm on previously proposed benchmarks for solving the domain\nadaptation task. We also show the benefit of using DAPCA in analyzing the\nsingle-cell omics datasets in biomedical applications. Overall, DAPCA can serve\nas a practical preprocessing step in many machine learning applications leading\nto reduced dataset representations, taking into account possible divergence\nbetween source and target domains.\n","authors":["Evgeny M Mirkes","Jonathan Bac","Aziz Fouché","Sergey V. Stasenko","Andrei Zinovyev","Alexander N. Gorban"],"pdf_url":"https://arxiv.org/pdf/2208.13290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08131v1","updated":"2022-12-15T20:36:10Z","published":"2022-12-15T20:36:10Z","title":"Bridging the Gap Between Offline and Online Reinforcement Learning\n  Evaluation Methodologies","summary":"  Reinforcement learning (RL) has shown great promise with algorithms learning\nin environments with large state and action spaces purely from scalar reward\nsignals. A crucial challenge for current deep RL algorithms is that they\nrequire a tremendous amount of environment interactions for learning. This can\nbe infeasible in situations where such interactions are expensive; such as in\nrobotics. Offline RL algorithms try to address this issue by bootstrapping the\nlearning process from existing logged data without needing to interact with the\nenvironment from the very beginning. While online RL algorithms are typically\nevaluated as a function of the number of environment interactions, there exists\nno single established protocol for evaluating offline RL methods.In this paper,\nwe propose a sequential approach to evaluate offline RL algorithms as a\nfunction of the training set size and thus by their data efficiency. Sequential\nevaluation provides valuable insights into the data efficiency of the learning\nprocess and the robustness of algorithms to distribution changes in the dataset\nwhile also harmonizing the visualization of the offline and online learning\nphases. Our approach is generally applicable and easy to implement. We compare\nseveral existing offline RL algorithms using this approach and present insights\nfrom a variety of tasks and offline datasets.\n","authors":["Shivakanth Sujit","Pedro H. M. Braga","Jorg Bornschein","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2212.08131v1.pdf","comment":"Offline RL Workshop, NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08130v1","updated":"2022-12-15T20:35:48Z","published":"2022-12-15T20:35:48Z","title":"On Evaluating Adversarial Robustness of Chest X-ray Classification:\n  Pitfalls and Best Practices","summary":"  Vulnerability to adversarial attacks is a well-known weakness of Deep Neural\nNetworks. While most of the studies focus on natural images with standardized\nbenchmarks like ImageNet and CIFAR, little research has considered real world\napplications, in particular in the medical domain. Our research shows that,\ncontrary to previous claims, robustness of chest x-ray classification is much\nharder to evaluate and leads to very different assessments based on the\ndataset, the architecture and robustness metric. We argue that previous studies\ndid not take into account the peculiarity of medical diagnosis, like the\nco-occurrence of diseases, the disagreement of labellers (domain experts), the\nthreat model of the attacks and the risk implications for each successful\nattack.\n  In this paper, we discuss the methodological foundations, review the pitfalls\nand best practices, and suggest new methodological considerations for\nevaluating the robustness of chest xray classification models. Our evaluation\non 3 datasets, 7 models, and 18 diseases is the largest evaluation of\nrobustness of chest x-ray classification models.\n","authors":["Salah Ghamizi","Maxime Cordy","Michail Papadakis","Yves Le Traon"],"pdf_url":"https://arxiv.org/pdf/2212.08130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.11270v3","updated":"2022-12-15T20:26:36Z","published":"2021-02-22T18:56:26Z","title":"Softmax Policy Gradient Methods Can Take Exponential Time to Converge","summary":"  The softmax policy gradient (PG) method, which performs gradient ascent under\nsoftmax policy parameterization, is arguably one of the de facto\nimplementations of policy optimization in modern reinforcement learning. For\n$\\gamma$-discounted infinite-horizon tabular Markov decision processes (MDPs),\nremarkable progress has recently been achieved towards establishing global\nconvergence of softmax PG methods in finding a near-optimal policy. However,\nprior results fall short of delineating clear dependencies of convergence rates\non salient parameters such as the cardinality of the state space $\\mathcal{S}$\nand the effective horizon $\\frac{1}{1-\\gamma}$, both of which could be\nexcessively large. In this paper, we deliver a pessimistic message regarding\nthe iteration complexity of softmax PG methods, despite assuming access to\nexact gradient computation. Specifically, we demonstrate that the softmax PG\nmethod with stepsize $\\eta$ can take \\[\n  \\frac{1}{\\eta} |\\mathcal{S}|^{2^{\\Omega\\big(\\frac{1}{1-\\gamma}\\big)}}\n~\\text{iterations} \\] to converge, even in the presence of a benign policy\ninitialization and an initial state distribution amenable to exploration (so\nthat the distribution mismatch coefficient is not exceedingly large). This is\naccomplished by characterizing the algorithmic dynamics over a\ncarefully-constructed MDP containing only three actions. Our exponential lower\nbound hints at the necessity of carefully adjusting update rules or enforcing\nproper regularization in accelerating PG methods.\n","authors":["Gen Li","Yuting Wei","Yuejie Chi","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2102.11270v3.pdf","comment":"accepted to Mathematical Programming (Series A); also presented in\n  part in Conference on Learning Theory (COLT) 2021"},{"id":"http://arxiv.org/abs/2212.08123v1","updated":"2022-12-15T20:23:09Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For CIFAR, the stochastic ensembles are\nquantitatively compared to published Hamiltonian Monte Carlo results for a\nResNet-20 architecture. We also test the quality of the posteriors directly\nagainst Hamiltonian Monte Carlo simulations in a simplified toy model. Our\nresults show that in a number of settings, stochastic ensembles provide more\naccurate posterior estimates than regular deep ensembles.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2212.08109v1","updated":"2022-12-15T19:49:34Z","published":"2022-12-15T19:49:34Z","title":"An Empirical Study of Deep Learning Models for Vulnerability Detection","summary":"  Deep learning (DL) models of code have recently reported great progress for\nvulnerability detection. In some cases, DL-based models have outperformed\nstatic analysis tools. Although many great models have been proposed, we do not\nyet have a good understanding of these models. This limits the further\nadvancement of model robustness, debugging, and deployment for the\nvulnerability detection. In this paper, we surveyed and reproduced 9\nstate-of-the-art (SOTA) deep learning models on 2 widely used vulnerability\ndetection datasets: Devign and MSR. We investigated 6 research questions in\nthree areas, namely model capabilities, training data, and model\ninterpretation. We experimentally demonstrated the variability between\ndifferent runs of a model and the low agreement among different models'\noutputs. We investigated models trained for specific types of vulnerabilities\ncompared to a model that is trained on all the vulnerabilities at once. We\nexplored the types of programs DL may consider \"hard\" to handle. We\ninvestigated the relations of training data sizes and training data composition\nwith model performance. Finally, we studied model interpretations and analyzed\nimportant features that the models used to make predictions. We believe that\nour findings can help better understand model results, provide guidance on\npreparing training data, and improve the robustness of the models. All of our\ndatasets, code, and results are available at\nhttps://figshare.com/s/284abfba67dba448fdc2.\n","authors":["Benjamin Steenhoek","Md Mahbubur Rahman","Richard Jiles","Wei Le"],"pdf_url":"https://arxiv.org/pdf/2212.08109v1.pdf","comment":"11 pages, 14 figures. Accepted at ICSE 2023 (not camera-ready\n  version)"},{"id":"http://arxiv.org/abs/2212.08108v1","updated":"2022-12-15T19:49:27Z","published":"2022-12-15T19:49:27Z","title":"DeepDFA: Dataflow Analysis-Guided Efficient Graph Learning for\n  Vulnerability Detection","summary":"  Deep learning-based vulnerability detection models have recently been shown\nto be effective and, in some cases, outperform static analysis tools. However,\nthe highest-performing approaches use token-based transformer models, which do\nnot leverage domain knowledge. Classical program analysis techniques such as\ndataflow analysis can detect many types of bugs and are the most commonly used\nmethods in practice. Motivated by the causal relationship between bugs and\ndataflow analysis, we present DeepDFA, a dataflow analysis-guided graph\nlearning framework and embedding that uses program semantic features for\nvulnerability detection. We show that DeepDFA is performant and efficient.\nDeepDFA ranked first in recall, first in generalizing over unseen projects, and\nsecond in F1 among all the state-of-the-art models we experimented with. It is\nalso the smallest model in terms of the number of parameters, and was trained\nin 9 minutes, 69x faster than the highest-performing baseline. DeepDFA can be\nused with other models. By integrating LineVul and DeepDFA, we achieved the\nbest vulnerability detection performance of 96.4 F1 score, 98.69 precision, and\n94.22 recall.\n","authors":["Benjamin Steenhoek","Wei Le","Hongyang Gao"],"pdf_url":"https://arxiv.org/pdf/2212.08108v1.pdf","comment":"10 pages, 8 figures. Under review as a conference paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2209.09024v2","updated":"2022-12-15T19:34:57Z","published":"2022-09-16T15:39:06Z","title":"Dataset Inference for Self-Supervised Models","summary":"  Self-supervised models are increasingly prevalent in machine learning (ML)\nsince they reduce the need for expensively labeled data. Because of their\nversatility in downstream applications, they are increasingly used as a service\nexposed via public APIs. At the same time, these encoder models are\nparticularly vulnerable to model stealing attacks due to the high\ndimensionality of vector representations they output. Yet, encoders remain\nundefended: existing mitigation strategies for stealing attacks focus on\nsupervised learning. We introduce a new dataset inference defense, which uses\nthe private training set of the victim encoder model to attribute its ownership\nin the event of stealing. The intuition is that the log-likelihood of an\nencoder's output representations is higher on the victim's training data than\non test data if it is stolen from the victim, but not if it is independently\ntrained. We compute this log-likelihood using density estimation models. As\npart of our evaluation, we also propose measuring the fidelity of stolen\nencoders and quantifying the effectiveness of the theft detection without\ninvolving downstream tasks; instead, we leverage mutual information and\ndistance measurements. Our extensive empirical results in the vision domain\ndemonstrate that dataset inference is a promising direction for defending\nself-supervised models against model stealing.\n","authors":["Adam Dziedzic","Haonan Duan","Muhammad Ahmad Kaleem","Nikita Dhawan","Jonas Guan","Yannis Cattan","Franziska Boenisch","Nicolas Papernot"],"pdf_url":"https://arxiv.org/pdf/2209.09024v2.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08101v1","updated":"2022-12-15T19:33:54Z","published":"2022-12-15T19:33:54Z","title":"Learning to repeatedly solve routing problems","summary":"  In the last years, there has been a great interest in machine-learning-based\nheuristics for solving NP-hard combinatorial optimization problems. The\ndeveloped methods have shown potential on many optimization problems. In this\npaper, we present a learned heuristic for the reoptimization of a problem after\na minor change in its data. We focus on the case of the capacited vehicle\nrouting problem with static clients (i.e., same client locations) and changed\ndemands. Given the edges of an original solution, the goal is to predict and\nfix the ones that have a high chance of remaining in an optimal solution after\na change of client demands. This partial prediction of the solution reduces the\ncomplexity of the problem and speeds up its resolution, while yielding a good\nquality solution. The proposed approach resulted in solutions with an\noptimality gap ranging from 0\\% to 1.7\\% on different benchmark instances\nwithin a reasonable computing time.\n","authors":["Mouad Morabit","Guy Desaulniers","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2212.08101v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2212.08071v1","updated":"2022-12-15T18:59:59Z","published":"2022-12-15T18:59:59Z","title":"MAViL: Masked Audio-Video Learners","summary":"  We present Masked Audio-Video Learners (MAViL) to train audio-visual\nrepresentations. Our approach learns with three complementary forms of\nself-supervision: (1) reconstruction of masked audio and video input data, (2)\nintra- and inter-modal contrastive learning with masking, and (3) self-training\nby reconstructing joint audio-video contextualized features learned from the\nfirst two objectives. Pre-training with MAViL not only enables the model to\nperform well in audio-visual classification and retrieval tasks but also\nimproves representations of each modality in isolation, without using\ninformation from the other modality for fine-tuning or inference. Empirically,\nMAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%\naccuracy). For the first time, a self-supervised audio-visual model outperforms\nones that use external supervision on these benchmarks. Code will be available\nsoon.\n","authors":["Po-Yao Huang","Vasu Sharma","Hu Xu","Chaitanya Ryali","Haoqi Fan","Yanghao Li","Shang-Wen Li","Gargi Ghosh","Jitendra Malik","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2212.08071v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2212.07835v1","updated":"2022-12-15T13:44:28Z","published":"2022-12-15T13:44:28Z","title":"You were saying? -- Spoken Language in the V3C Dataset","summary":"  This paper presents an analysis of the distribution of spoken language in the\nV3C video retrieval benchmark dataset based on automatically generated\ntranscripts. It finds that a large portion of the dataset is covered by spoken\nlanguage. Since language transcripts can be quickly and accurately described,\nthis has implications for retrieval tasks such as known-item search.\n","authors":["Luca Rossetto"],"pdf_url":"https://arxiv.org/pdf/2212.07835v1.pdf","comment":null}]},"2022-12-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.08635v1","updated":"2022-12-16T18:23:43Z","published":"2022-12-16T18:23:43Z","title":"Self-Prompting Large Language Models for Open-Domain QA","summary":"  Open-Domain Question Answering (ODQA) requires models to answer factoid\nquestions with no context given. The common way for this task is to train\nmodels on a large-scale annotated dataset to retrieve related documents and\ngenerate answers based on these documents. In this paper, we show that the ODQA\narchitecture can be dramatically simplified by treating Large Language Models\n(LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to\nperform ODQA so as to eliminate the need for training data and external\nknowledge corpus. Concretely, we firstly generate multiple pseudo QA pairs with\nbackground passages and one-sentence explanations for these QAs by prompting\nLLMs step by step and then leverage the generated QA pairs for in-context\nlearning. Experimental results show our method surpasses previous\nstate-of-the-art methods by +8.8 EM averagely on three widely-used ODQA\ndatasets, and even achieves comparable performance with several\nretrieval-augmented fine-tuned models.\n","authors":["Junlong Li","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2212.08635v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2212.08632v1","updated":"2022-12-16T18:12:04Z","published":"2022-12-16T18:12:04Z","title":"Enhancing Multi-modal and Multi-hop Question Answering via Structured\n  Knowledge and Unified Retrieval-Generation","summary":"  Multi-modal and multi-hop question answering aims to answer a question based\non multiple input sources from different modalities. Previous methods retrieve\nthe evidence separately and feed the retrieved evidence to a language model to\ngenerate the corresponding answer. However, these methods fail to build\nconnections between candidates and thus cannot model the inter-dependent\nrelation during retrieval. Moreover, the reasoning process over multi-modality\ncandidates can be unbalanced without building alignments between different\nmodalities. To address this limitation, we propose a Structured Knowledge and\nUnified Retrieval Generation based method (SKURG). We align the sources from\ndifferent modalities via the shared entities and map them into a shared\nsemantic space via structured knowledge. Then, we utilize a unified\nretrieval-generation decoder to integrate intermediate retrieval results for\nanswer generation and adaptively determine the number of retrieval steps. We\nperform experiments on two multi-modal and multi-hop datasets: WebQA and\nMultimodalQA. The results demonstrate that SKURG achieves state-of-the-art\nperformance on both retrieval and answer generation.\n","authors":["Qian Yang","Qian Chen","Wen Wang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.08632v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2212.08620v1","updated":"2022-12-16T17:57:41Z","published":"2022-12-16T17:57:41Z","title":"POTATO: The Portable Text Annotation Tool","summary":"  We present POTATO, the Portable text annotation tool, a free, fully\nopen-sourced annotation system that 1) supports labeling many types of text and\nmultimodal data; 2) offers easy-to-configure features to maximize the\nproductivity of both deployers and annotators (convenient templates for common\nML/NLP tasks, active learning, keypress shortcuts, keyword highlights,\ntooltips); and 3) supports a high degree of customization (editable UI,\ninserting pre-screening questions, attention and qualification tests).\nExperiments over two annotation tasks suggest that POTATO improves labeling\nspeed through its specially-designed productivity features, especially for long\ndocuments and complex tasks. POTATO is available at\nhttps://github.com/davidjurgens/potato and will continue to be updated.\n","authors":["Jiaxin Pei","Aparna Ananthasubramaniam","Xingyao Wang","Naitian Zhou","Jackson Sargent","Apostolos Dedeloudis","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2212.08620v1.pdf","comment":"EMNLP 2022 DEMO"},{"id":"http://arxiv.org/abs/2212.08619v1","updated":"2022-12-16T17:57:14Z","published":"2022-12-16T17:57:14Z","title":"Planting and Mitigating Memorized Content in Predictive-Text Language\n  Models","summary":"  Language models are widely deployed to provide automatic text completion\nservices in user products. However, recent research has revealed that language\nmodels (especially large ones) bear considerable risk of memorizing private\ntraining data, which is then vulnerable to leakage and extraction by\nadversaries. In this study, we test the efficacy of a range of\nprivacy-preserving techniques to mitigate unintended memorization of sensitive\nuser text, while varying other factors such as model size and adversarial\nconditions. We test both \"heuristic\" mitigations (those without formal privacy\nguarantees) and Differentially Private training, which provides provable levels\nof privacy at the cost of some model performance. Our experiments show that\n(with the exception of L2 regularization), heuristic mitigations are largely\nineffective in preventing memorization in our test suite, possibly because they\nmake too strong of assumptions about the characteristics that define\n\"sensitive\" or \"private\" text. In contrast, Differential Privacy reliably\nprevents memorization in our experiments, despite its computational and\nmodel-performance costs.\n","authors":["C. M. Downey","Wei Dai","Huseyin A. Inan","Kim Laine","Saurabh Naik","Tomasz Religa"],"pdf_url":"https://arxiv.org/pdf/2212.08619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08607v1","updated":"2022-12-16T17:36:23Z","published":"2022-12-16T17:36:23Z","title":"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text\n  Generation","summary":"  Prompting large language models has enabled significant recent progress in\nmulti-step reasoning over text. However, when applied to text generation from\nsemi-structured data (e.g., graphs or tables), these methods typically suffer\nfrom low semantic coverage, hallucination, and logical inconsistency. We\npropose MURMUR, a neuro-symbolic modular approach to text generation from\nsemi-structured data with multi-step reasoning. MURMUR is a best-first search\nmethod that generates reasoning paths using: (1) neural and symbolic modules\nwith specific linguistic and logical skills, (2) a grammar whose production\nrules define valid compositions of modules, and (3) value functions that assess\nthe quality of each reasoning step. We conduct experiments on two diverse\ndata-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in\ntheir data representations (graphs and tables) and span multiple linguistic and\nlogical skills. MURMUR obtains significant improvements over recent few-shot\nbaselines like direct prompting and chain-of-thought prompting, while also\nachieving comparable performance to fine-tuned GPT-2 on out-of-domain data.\nMoreover, human evaluation shows that MURMUR generates highly faithful and\ncorrect reasoning paths that lead to 26% more logically consistent summaries on\nLogicNLG, compared to direct prompting.\n","authors":["Swarnadeep Saha","Xinyan Velocity Yu","Mohit Bansal","Ramakanth Pasunuru","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2212.08607v1.pdf","comment":"22 pages (9 figures, 18 tables)"},{"id":"http://arxiv.org/abs/2212.08597v1","updated":"2022-12-16T17:24:49Z","published":"2022-12-16T17:24:49Z","title":"Detecting and Mitigating Hallucinations in Machine Translation: Model\n  Internal Workings Alone Do Well, Sentence Similarity Even Better","summary":"  While the problem of hallucinations in neural machine translation has long\nbeen recognized, so far the progress on its alleviation is very little. Indeed,\nrecently it turned out that without artificially encouraging models to\nhallucinate, previously existing methods fall short and even the standard\nsequence log-probability is more informative. It means that characteristics\ninternal to the model can give much more information than we expect, and before\nusing external models and measures, we first need to ask: how far can we go if\nwe use nothing but the translation model itself ? We propose to use a method\nthat evaluates the percentage of the source contribution to a generated\ntranslation. Intuitively, hallucinations are translations \"detached\" from the\nsource, hence they can be identified by low source contribution. This method\nimproves detection accuracy for the most severe hallucinations by a factor of 2\nand is able to alleviate hallucinations at test time on par with the previous\nbest approach that relies on external models. Next, if we move away from\ninternal model characteristics and allow external tools, we show that using\nsentence similarity from cross-lingual embeddings further improves these\nresults.\n","authors":["David Dale","Elena Voita","Loïc Barrault","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2212.08597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2012.14309v2","updated":"2022-12-16T17:17:57Z","published":"2020-12-28T15:46:19Z","title":"General Mechanism of Evolution Shared by Proteins and Words","summary":"  Complex systems, such as life and languages, are governed by principles of\nevolution. The analogy and comparison between biology and\nlinguistics\\cite{alphafold2, RoseTTAFold, lang_virus, cell language, faculty1,\nlanguage of gene, Protein linguistics, dictionary, Grammar of pro_dom,\ncomplexity, genomics_nlp, InterPro, language modeling, Protein language\nmodeling} provide a computational foundation for characterizing and analyzing\nprotein sequences, human corpora, and their evolution. However, no general\nmathematical formula has been proposed so far to illuminate the origin of\nquantitative hallmarks shared by life and language. Here we show several new\nstatistical relationships shared by proteins and words, which inspire us to\nestablish a general mechanism of evolution with explicit formulations that can\nincorporate both old and new characteristics. We found natural selection can be\nquantified via the entropic formulation by the principle of least effort to\ndetermine the sequence variation that survives in evolution. Besides, the\norigin of power law behavior and how changes in the environment stimulate the\nemergence of new proteins and words can also be explained via the introduction\nof function connection network. Our results demonstrate not only the\ncorrespondence between genetics and linguistics over their different\nhierarchies but also new fundamental physical properties for the evolution of\ncomplex adaptive systems. We anticipate our statistical tests can function as\nquantitative criteria to examine whether an evolution theory of sequence is\nconsistent with the regularity of real data. In the meantime, their\ncorrespondence broadens the bridge to exchange existing knowledge, spurs new\ninterpretations, and opens Pandora's box to release several potentially\nrevolutionary challenges. For example, does linguistic arbitrariness conflict\nwith the dogma that structure determines function?\n","authors":["Li-Min Wang","Hsing-Yi Lai","Sun-Ting Tsai","Chen Siang Ng","Shan-Jyun Wu","Meng-Xue Tsai","Yi-Ching Su","Daw-Wei Wang","Tzay-Ming Hong"],"pdf_url":"https://arxiv.org/pdf/2012.14309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08550v1","updated":"2022-12-16T16:00:19Z","published":"2022-12-16T16:00:19Z","title":"Fine-grained Czech News Article Dataset: An Interdisciplinary Approach\n  to Trustworthiness Analysis","summary":"  We present the Verifee Dataset: a novel dataset of news articles with\nfine-grained trustworthiness annotations. We develop a detailed methodology\nthat assesses the texts based on their parameters encompassing editorial\ntransparency, journalist conventions, and objective reporting while penalizing\nmanipulative techniques. We bring aboard a diverse set of researchers from\nsocial, media, and computer sciences to overcome barriers and limited framing\nof this interdisciplinary problem. We collect over $10,000$ unique articles\nfrom almost $60$ Czech online news sources. These are categorized into one of\nthe $4$ classes across the credibility spectrum we propose, raging from\nentirely trustworthy articles all the way to the manipulative ones. We produce\ndetailed statistics and study trends emerging throughout the set. Lastly, we\nfine-tune multiple popular sequence-to-sequence language models using our\ndataset on the trustworthiness classification task and report the best testing\nF-1 score of $0.52$. We open-source the dataset, annotation methodology, and\nannotators' instructions in full length at https://verifee.ai/research to\nenable easy build-up work. We believe similar methods can help prevent\ndisinformation and educate in the realm of media literacy.\n","authors":["Matyáš Boháček","Michal Bravanský","Filip Trhlík","Václav Moravec"],"pdf_url":"https://arxiv.org/pdf/2212.08550v1.pdf","comment":"13 pages, 3 figures; to be published at the Second Workshop on\n  Multimodal Fact-Checking and Hate Speech Detection (DEFACTIFY 2023) at the\n  AAAI 2023 Conference, February 14, 2023, Washington, D.C"},{"id":"http://arxiv.org/abs/2212.08542v1","updated":"2022-12-16T15:46:15Z","published":"2022-12-16T15:46:15Z","title":"Context-aware Fine-tuning of Self-supervised Speech Models","summary":"  Self-supervised pre-trained transformers have improved the state of the art\non a variety of speech tasks. Due to the quadratic time and space complexity of\nself-attention, they usually operate at the level of relatively short (e.g.,\nutterance) segments. In this paper, we study the use of context, i.e.,\nsurrounding segments, during fine-tuning and propose a new approach called\ncontext-aware fine-tuning. We attach a context module on top of the last layer\nof a pre-trained model to encode the whole segment into a context embedding\nvector which is then used as an additional feature for the final prediction.\nDuring the fine-tuning stage, we introduce an auxiliary loss that encourages\nthis context embedding vector to be similar to context vectors of surrounding\nsegments. This allows the model to make predictions without access to these\nsurrounding segments at inference time and requires only a tiny overhead\ncompared to standard fine-tuned models. We evaluate the proposed approach using\nthe SLUE and Librilight benchmarks for several downstream tasks: Automatic\nspeech recognition (ASR), named entity recognition (NER), and sentiment\nanalysis (SA). The results show that context-aware fine-tuning not only\noutperforms a standard fine-tuning baseline but also rivals a strong context\ninjection baseline that uses neighboring speech segments during inference.\n","authors":["Suwon Shon","Felix Wu","Kwangyoun Kim","Prashant Sridhar","Karen Livescu","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2212.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07428v2","updated":"2022-12-16T15:35:37Z","published":"2022-12-14T10:50:13Z","title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural\n  Language Inference","summary":"  We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n","authors":["Maren Pielka","Svetlana Schmidt","Lisa Pucknat","Rafet Sifa"],"pdf_url":"https://arxiv.org/pdf/2212.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08514v1","updated":"2022-12-16T14:54:56Z","published":"2022-12-16T14:54:56Z","title":"Check-worthy Claim Detection across Topics for Automated Fact-checking","summary":"  An important component of an automated fact-checking system is the claim\ncheck-worthiness detection system, which ranks sentences by prioritising them\nbased on their need to be checked. Despite a body of research tackling the\ntask, previous research has overlooked the challenging nature of identifying\ncheck-worthy claims across different topics. In this paper, we assess and\nquantify the challenge of detecting check-worthy claims for new, unseen topics.\nAfter highlighting the problem, we propose the AraCWA model to mitigate the\nperformance deterioration when detecting check-worthy claims across topics. The\nAraCWA model enables boosting the performance for new topics by incorporating\ntwo components for few-shot learning and data augmentation. Using a publicly\navailable dataset of Arabic tweets consisting of 14 different topics, we\ndemonstrate that our proposed data augmentation strategy achieves substantial\nimprovements across topics overall, where the extent of the improvement varies\nacross topics. Further, we analyse the semantic similarities between topics,\nsuggesting that the similarity metric could be used as a proxy to determine the\ndifficulty level of an unseen topic prior to undertaking the task of labelling\nthe underlying sentences.\n","authors":["Amani S. Abumansour","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2212.08514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08489v1","updated":"2022-12-16T14:01:42Z","published":"2022-12-16T14:01:42Z","title":"Effectiveness of Text, Acoustic, and Lattice-based representations in\n  Spoken Language Understanding tasks","summary":"  In this paper, we perform an exhaustive evaluation of different\nrepresentations to address the intent classification problem in a Spoken\nLanguage Understanding (SLU) setup. We benchmark three types of systems to\nperform the SLU intent detection task: 1) text-based, 2) lattice-based, and a\nnovel 3) multimodal approach. Our work provides a comprehensive analysis of\nwhat could be the achievable performance of different state-of-the-art SLU\nsystems under different circumstances, e.g., automatically- vs.\nmanually-generated transcripts. We evaluate the systems on the publicly\navailable SLURP spoken language resource corpus. Our results indicate that\nusing richer forms of Automatic Speech Recognition (ASR) outputs allows SLU\nsystems to improve in comparison to the 1-best setup (4% relative improvement).\nHowever, crossmodal approaches, i.e., learning from acoustic and text\nembeddings, obtains performance similar to the oracle setup, and a relative\nimprovement of 18% over the 1-best configuration. Thus, crossmodal\narchitectures represent a good alternative to overcome the limitations of\nworking purely automatically generated textual data.\n","authors":["Esaú Villatoro-Tello","Srikanth Madikeri","Juan Zuluaga-Gomez","Bidisha Sharma","Seyyed Saeed Sarfjoo","Iuliia Nigmatulina","Petr Motlicek","Alexei V. Ivanov","Aravind Ganapathiraju"],"pdf_url":"https://arxiv.org/pdf/2212.08489v1.pdf","comment":"Submitted to ICASSP 2023 (Under review)"},{"id":"http://arxiv.org/abs/2212.08486v1","updated":"2022-12-16T14:00:26Z","published":"2022-12-16T14:00:26Z","title":"BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric","summary":"  End-to-End speech-to-speech translation (S2ST) is generally evaluated with\ntext-based metrics. This means that generated speech has to be automatically\ntranscribed, making the evaluation dependent on the availability and quality of\nautomatic speech recognition (ASR) systems. In this paper, we propose a\ntext-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the\ndependency on ASR systems. BLASER leverages a multilingual multimodal encoder\nto directly encode the speech segments for source input, translation output and\nreference into a shared embedding space and computes a score of the translation\nquality that can be used as a proxy to human evaluation. To evaluate our\napproach, we construct training and evaluation sets from more than 40k human\nannotations covering seven language directions. The best results of BLASER are\nachieved by training with supervision from human rating scores. We show that\nwhen evaluated at the sentence level, BLASER correlates significantly better\nwith human judgment compared to ASR-dependent metrics including ASR-SENTBLEU in\nall translation directions and ASR-COMET in five of them. Our analysis shows\ncombining speech and text as inputs to BLASER does not increase the correlation\nwith human scores, but best correlations are achieved when using speech, which\nmotivates the goal of our research. Moreover, we show that using ASR for\nreferences is detrimental for text-based metrics.\n","authors":["Mingda Chen","Paul-Ambroise Duquenne","Pierre Andrews","Justine Kao","Alexandre Mourachko","Holger Schwenk","Marta R. Costa-jussà"],"pdf_url":"https://arxiv.org/pdf/2212.08486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08482v1","updated":"2022-12-16T13:55:22Z","published":"2022-12-16T13:55:22Z","title":"Implementation of general formal translators","summary":"  The general translator formalism and computing specific implementations are\nproposed. The implementation of specific elements necessary to process the\nsource and destination information within the translators are presented. Some\ncommon directives or instructions, such as classes and procedures, were unified\nand generalized in order to allow general translations implementations. In\norder to cover general cases, two levels of processing are required, related to\nthe source and destination information appropriate transformations, with the\nrelated control and processing instructions. The proposed general translator\nelements are useful for processing natural or artificial information described\nthrough any types of languages or systems.\n","authors":["Iosif Iulian Petrila"],"pdf_url":"https://arxiv.org/pdf/2212.08482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08459v1","updated":"2022-12-16T13:07:39Z","published":"2022-12-16T13:07:39Z","title":"Experiments on Generalizability of BERTopic on Multi-Domain Short Text","summary":"  Topic modeling is widely used for analytically evaluating large collections\nof textual data. One of the most popular topic techniques is Latent Dirichlet\nAllocation (LDA), which is flexible and adaptive, but not optimal for e.g.\nshort texts from various domains. We explore how the state-of-the-art BERTopic\nalgorithm performs on short multi-domain text and find that it generalizes\nbetter than LDA in terms of topic coherence and diversity. We further analyze\nthe performance of the HDBSCAN clustering algorithm utilized by BERTopic and\nfind that it classifies a majority of the documents as outliers. This crucial,\nyet overseen problem excludes too many documents from further analysis. When we\nreplace HDBSCAN with k-Means, we achieve similar performance, but without\noutliers.\n","authors":["Muriël de Groot","Mohammad Aliannejadi","Marcel R. Haas"],"pdf_url":"https://arxiv.org/pdf/2212.08459v1.pdf","comment":"Accepted poster presentation at WiNLP 2022, as a part of EMNLP 2022,\n  2 pages"},{"id":"http://arxiv.org/abs/2212.08458v1","updated":"2022-12-16T13:07:09Z","published":"2022-12-16T13:07:09Z","title":"Fast Rule-Based Decoding: Revisiting Syntactic Rules in Neural\n  Constituency Parsing","summary":"  Most recent studies on neural constituency parsing focus on encoder\nstructures, while few developments are devoted to decoders. Previous research\nhas demonstrated that probabilistic statistical methods based on syntactic\nrules are particularly effective in constituency parsing, whereas syntactic\nrules are not used during the training of neural models in prior work probably\ndue to their enormous computation requirements. In this paper, we first\nimplement a fast CKY decoding procedure harnessing GPU acceleration, based on\nwhich we further derive a syntactic rule-based (rule-constrained) CKY decoding.\nIn the experiments, our method obtains 95.89 and 92.52 F1 on the datasets of\nPTB and CTB respectively, which shows significant improvements compared with\nprevious approaches. Besides, our parser achieves strong and competitive\ncross-domain performance in zero-shot settings.\n","authors":["Tianyu Shi","Zhicheng Wang","Liyin Xiao","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07127v3","updated":"2022-12-16T12:52:19Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Philine Bouscasse","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08410v1","updated":"2022-12-16T11:24:42Z","published":"2022-12-16T11:24:42Z","title":"Teaching Small Language Models to Reason","summary":"  Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.\n","authors":["Lucie Charlotte Magister","Jonathan Mallinson","Jakub Adamek","Eric Malmi","Aliaksei Severyn"],"pdf_url":"https://arxiv.org/pdf/2212.08410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08408v1","updated":"2022-12-16T11:15:39Z","published":"2022-12-16T11:15:39Z","title":"Decoder Tuning: Efficient Language Understanding as Decoding","summary":"  With the evergrowing sizes of pre-trained models (PTMs), it has been an\nemerging practice to only provide the inference APIs for users, namely\nmodel-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,\nmost current approaches focus on the input side, seeking for powerful prompts\nto stimulate models for correct answers. However, we argue that input-side\nadaptation could be arduous due to the lack of gradient signals and they\nusually require thousands of API queries, resulting in high computation and\ntime costs. In light of this, we present Decoder Tuning (DecT), which in\ncontrast optimizes task-specific decoder networks on the output side.\nSpecifically, DecT first extracts prompt-stimulated output scores for initial\npredictions. On top of that, we train an additional decoder network on the\noutput representations to incorporate posterior data knowledge. By\ngradient-based optimization, DecT can be trained within several seconds and\nrequires only one PTM query per sample. Empirically, we conduct extensive\nnatural language understanding experiments and show that DecT significantly\noutperforms state-of-the-art algorithms with a $10^3\\times$ speed-up.\n","authors":["Ganqu Cui","Wentao Li","Ning Ding","Longtao Huang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2212.08408v1.pdf","comment":"Work in progress. 13 pages"},{"id":"http://arxiv.org/abs/2212.08407v1","updated":"2022-12-16T11:06:48Z","published":"2022-12-16T11:06:48Z","title":"Utilizing distilBert transformer model for sentiment classification of\n  COVID-19's Persian open-text responses","summary":"  The COVID-19 pandemic has caused drastic alternations in human life in all\naspects. The government's laws in this regard affected the lifestyle of all\npeople. Due to this fact studying the sentiment of individuals is essential to\nbe aware of the future impacts of the coming pandemics. To contribute to this\naim, we proposed an NLP (Natural Language Processing) model to analyze\nopen-text answers in a survey in Persian and detect positive and negative\nfeelings of the people in Iran. In this study, a distilBert transformer model\nwas applied to take on this task. We deployed three approaches to perform the\ncomparison, and our best model could gain accuracy: 0.824, Precision: 0.824,\nRecall: 0.798, and F1 score: 0.804.\n","authors":["Fatemeh Sadat Masoumi","Mohammad Bahrani"],"pdf_url":"https://arxiv.org/pdf/2212.08407v1.pdf","comment":"The paper is accepted at The 7th International Conference on Science\n  and Technology of Electrical, Computer, and Mechanical Engineering of Iran"},{"id":"http://arxiv.org/abs/2212.08395v1","updated":"2022-12-16T10:39:22Z","published":"2022-12-16T10:39:22Z","title":"Metaphorical Polysemy Detection: Conventional Metaphor meets Word Sense\n  Disambiguation","summary":"  Linguists distinguish between novel and conventional metaphor, a distinction\nwhich the metaphor detection task in NLP does not take into account. Instead,\nmetaphoricity is formulated as a property of a token in a sentence, regardless\nof metaphor type. In this paper, we investigate the limitations of treating\nconventional metaphors in this way, and advocate for an alternative which we\nname 'metaphorical polysemy detection' (MPD). In MPD, only conventional\nmetaphoricity is treated, and it is formulated as a property of word senses in\na lexicon. We develop the first MPD model, which learns to identify\nconventional metaphors in the English WordNet. To train it, we present a novel\ntraining procedure that combines metaphor detection with word sense\ndisambiguation (WSD). For evaluation, we manually annotate metaphor in two\nsubsets of WordNet. Our model significantly outperforms a strong baseline based\non a state-of-the-art metaphor detection model, attaining an ROC-AUC score of\n.78 (compared to .65) on one of the sets. Additionally, when paired with a WSD\nmodel, our approach outperforms a state-of-the-art metaphor detection model at\nidentifying conventional metaphors in text (.659 F1 compared to .626).\n","authors":["Rowan Hall Maudslay","Simone Teufel"],"pdf_url":"https://arxiv.org/pdf/2212.08395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08390v1","updated":"2022-12-16T10:33:38Z","published":"2022-12-16T10:33:38Z","title":"Lessons learned from the evaluation of Spanish Language Models","summary":"  Given the impact of language models on the field of Natural Language\nProcessing, a number of Spanish encoder-only masked language models (aka BERTs)\nhave been trained and released. These models were developed either within large\nprojects using very large private corpora or by means of smaller scale academic\nefforts leveraging freely available data. In this paper we present a\ncomprehensive head-to-head comparison of language models for Spanish with the\nfollowing results: (i) Previously ignored multilingual models from large\ncompanies fare better than monolingual models, substantially changing the\nevaluation landscape of language models in Spanish; (ii) Results across the\nmonolingual models are not conclusive, with supposedly smaller and inferior\nmodels performing competitively. Based on these empirical results, we argue for\nthe need of more research to understand the factors underlying them. In this\nsense, the effect of corpus size, quality and pre-training techniques need to\nbe further investigated to be able to obtain Spanish monolingual models\nsignificantly better than the multilingual ones released by large private\ncompanies, specially in the face of rapid ongoing progress in the field. The\nrecent activity in the development of language technology for Spanish is to be\nwelcomed, but our results show that building language models remains an open,\nresource-heavy problem which requires to marry resources (monetary and/or\ncomputational) with the best research expertise and practice.\n","authors":["Rodrigo Agerri","Eneko Agirre"],"pdf_url":"https://arxiv.org/pdf/2212.08390v1.pdf","comment":"10 pages, three tables"},{"id":"http://arxiv.org/abs/2212.08388v1","updated":"2022-12-16T10:23:26Z","published":"2022-12-16T10:23:26Z","title":"Homonymy Information for English WordNet","summary":"  A widely acknowledged shortcoming of WordNet is that it lacks a distinction\nbetween word meanings which are systematically related (polysemy), and those\nwhich are coincidental (homonymy). Several previous works have attempted to\nfill this gap, by inferring this information using computational methods. We\nrevisit this task, and exploit recent advances in language modelling to\nsynthesise homonymy annotation for Princeton WordNet. Previous approaches treat\nthe problem using clustering methods; by contrast, our method works by linking\nWordNet to the Oxford English Dictionary, which contains the information we\nneed. To perform this alignment, we pair definitions based on their proximity\nin an embedding space produced by a Transformer model. Despite the simplicity\nof this approach, our best model attains an F1 of .97 on an evaluation set that\nwe annotate. The outcome of our work is a high-quality homonymy annotation\nlayer for Princeton WordNet, which we release.\n","authors":["Rowan Hall Maudslay","Simone Teufel"],"pdf_url":"https://arxiv.org/pdf/2212.08388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04800v2","updated":"2022-12-16T09:47:43Z","published":"2022-12-09T12:06:15Z","title":"AUC Maximization for Low-Resource Named Entity Recognition","summary":"  Current work in named entity recognition (NER) uses either cross entropy (CE)\nor conditional random fields (CRF) as the objective/loss functions to optimize\nthe underlying NER model. Both of these traditional objective functions for the\nNER problem generally produce adequate performance when the data distribution\nis balanced and there are sufficient annotated training examples. But since NER\nis inherently an imbalanced tagging problem, the model performance under the\nlow-resource settings could suffer using these standard objective functions.\nBased on recent advances in area under the ROC curve (AUC) maximization, we\npropose to optimize the NER model by maximizing the AUC score. We give evidence\nthat by simply combining two binary-classifiers that maximize the AUC score,\nsignificant performance improvement over traditional loss functions is achieved\nunder low-resource NER settings. We also conduct extensive experiments to\ndemonstrate the advantages of our method under the low-resource and\nhighly-imbalanced data distribution settings. To the best of our knowledge,\nthis is the first work that brings AUC maximization to the NER setting.\nFurthermore, we show that our method is agnostic to different types of NER\nembeddings, models and domains. The code to replicate this work will be\nprovided upon request.\n","authors":["Ngoc Dang Nguyen","Wei Tan","Wray Buntine","Richard Beare","Changyou Chen","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2212.04800v2.pdf","comment":"10 pages, 4 figures, AAAI 2023 accepted paper"},{"id":"http://arxiv.org/abs/2212.08354v1","updated":"2022-12-16T09:01:56Z","published":"2022-12-16T09:01:56Z","title":"FewFedWeight: Few-shot Federated Learning Framework across Multiple NLP\n  Tasks","summary":"  Massively multi-task learning with large language models has recently made\nsubstantial progress on few-shot generalization. However, this is usually\nperformed in a centralized learning fashion, ignoring the privacy sensitivity\nissue of (annotated) data used in multiple tasks. To mitigate this issue, we\npropose FewFedWeight, a few-shot federated learning framework across multiple\ntasks, to achieve the best of both worlds: privacy preservation and cross-task\ngeneralization. FewFedWeight trains client models in isolated devices without\nsharing data. It broadcasts the global model in the server to each client and\nproduces pseudo data for clients so that knowledge from the global model can be\nexplored to enhance few-shot learning of each client model. An energy-based\nalgorithm is further proposed to weight pseudo samples in order to reduce the\nnegative impact of noise from the generated pseudo data. Adaptive model weights\nof client models are also tuned according to their performance. We use these\nmodel weights to dynamically aggregate client models to update the global\nmodel. Experiments on 118 NLP tasks show that FewFedWeight can significantly\nimprove the performance of client models on 61% tasks with an average\nperformance improvement rate of 30.5% over the baseline and substantially\noutperform FedAvg and other decentralized learning methods.\n","authors":["Weilong Dong","Xinwei Wu","Junzhuo Li","Shuangzhi Wu","Chao Bian","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2212.08354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08353v1","updated":"2022-12-16T09:01:19Z","published":"2022-12-16T09:01:19Z","title":"How to disagree well: Investigating the dispute tactics used on\n  Wikipedia","summary":"  Disagreements are frequently studied from the perspective of either detecting\ntoxicity or analysing argument structure. We propose a framework of dispute\ntactics that unifies these two perspectives, as well as other dialogue acts\nwhich play a role in resolving disputes, such as asking questions and providing\nclarification. This framework includes a preferential ordering among\nrebuttal-type tactics, ranging from ad hominem attacks to refuting the central\nargument. Using this framework, we annotate 213 disagreements (3,865\nutterances) from Wikipedia Talk pages. This allows us to investigate research\nquestions around the tactics used in disagreements; for instance, we provide\nempirical validation of the approach to disagreement recommended by Wikipedia.\nWe develop models for multilabel prediction of dispute tactics in an utterance,\nachieving the best performance with a transformer-based label powerset model.\nAdding an auxiliary task to incorporate the ordering of rebuttal tactics\nfurther yields a statistically significant increase. Finally, we show that\nthese annotations can be used to provide useful additional signals to improve\nperformance on the task of predicting escalation.\n","authors":["Christine de Kock","Tom Stafford","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2212.08353v1.pdf","comment":"Accepted to EMNLP 2022 (Long paper)"},{"id":"http://arxiv.org/abs/2212.08335v1","updated":"2022-12-16T08:26:32Z","published":"2022-12-16T08:26:32Z","title":"Law to Binary Tree -- An Formal Interpretation of Legal Natural Language","summary":"  Knowledge representation and reasoning in law are essential to facilitate the\nautomation of legal analysis and decision-making tasks. In this paper, we\npropose a new approach based on legal science, specifically legal taxonomy, for\nrepresenting and reasoning with legal documents. Our approach interprets the\nregulations in legal documents as binary trees, which facilitates legal\nreasoning systems to make decisions and resolve logical contradictions. The\nadvantages of this approach are twofold. First, legal reasoning can be\nperformed on the basis of the binary tree representation of the regulations.\nSecond, the binary tree representation of the regulations is more\nunderstandable than the existing sentence-based representations. We provide an\nexample of how our approach can be used to interpret the regulations in a legal\ndocument.\n","authors":["Ha-Thanh Nguyen","Vu Tran","Ngoc-Cam Le","Thi-Thuy Le","Quang-Huy Nguyen","Le-Minh Nguyen","Ken Satoh"],"pdf_url":"https://arxiv.org/pdf/2212.08335v1.pdf","comment":"LN2FR 2022"},{"id":"http://arxiv.org/abs/2212.08329v1","updated":"2022-12-16T08:14:04Z","published":"2022-12-16T08:14:04Z","title":"Text-to-speech synthesis based on latent variable conversion using\n  diffusion probabilistic model and variational autoencoder","summary":"  Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of\nthe factors that have been driving TTS are the advancements of probabilistic\nmodels and latent representation learning. We propose a TTS method based on\nlatent variable conversion using a diffusion probabilistic model and the\nvariational autoencoder (VAE). In our TTS method, we use a waveform model based\non VAE, a diffusion model that predicts the distribution of latent variables in\nthe waveform model from texts, and an alignment model that learns alignments\nbetween the text and speech latent sequences. Our method integrates diffusion\nwith VAE by modeling both mean and variance parameters with diffusion, where\nthe target distribution is determined by approximation from VAE. This latent\nvariable conversion framework potentially enables us to flexibly incorporate\nvarious latent feature extractors. Our experiments show that our method is\nrobust to linguistic labels with poor orthography and alignment errors.\n","authors":["Yusuke Yasuda","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2212.08329v1.pdf","comment":"Submitted to ICASSP 2023"},{"id":"http://arxiv.org/abs/2212.08330v1","updated":"2022-12-16T08:14:04Z","published":"2022-12-16T08:14:04Z","title":"Convolution-enhanced Evolving Attention Networks","summary":"  Attention-based neural networks, such as Transformers, have become ubiquitous\nin numerous applications, including computer vision, natural language\nprocessing, and time-series analysis. In all kinds of attention networks, the\nattention maps are crucial as they encode semantic dependencies between input\ntokens. However, most existing attention networks perform modeling or reasoning\nbased on representations, wherein the attention maps of different layers are\nlearned separately without explicit interactions. In this paper, we propose a\nnovel and generic evolving attention mechanism, which directly models the\nevolution of inter-token relationships through a chain of residual\nconvolutional modules. The major motivations are twofold. On the one hand, the\nattention maps in different layers share transferable knowledge, thus adding a\nresidual connection can facilitate the information flow of inter-token\nrelationships across layers. On the other hand, there is naturally an\nevolutionary trend among attention maps at different abstraction levels, so it\nis beneficial to exploit a dedicated convolution-based module to capture this\nprocess. Equipped with the proposed mechanism, the convolution-enhanced\nevolving attention networks achieve superior performance in various\napplications, including time-series representation, natural language\nunderstanding, machine translation, and image classification. Especially on\ntime-series representation tasks, Evolving Attention-enhanced Dilated\nConvolutional (EA-DC-) Transformer outperforms state-of-the-art models\nsignificantly, achieving an average of 17% improvement compared to the best\nSOTA. To the best of our knowledge, this is the first work that explicitly\nmodels the layer-wise evolution of attention maps. Our implementation is\navailable at https://github.com/pkuyym/EvolvingAttention\n","authors":["Yujing Wang","Yaming Yang","Zhuo Li","Jiangang Bai","Mingliang Zhang","Xiangtai Li","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2212.08330v1.pdf","comment":"Extension of the previous work (arXiv:2102.12895). arXiv admin note:\n  text overlap with arXiv:2102.12895"},{"id":"http://arxiv.org/abs/2212.08322v1","updated":"2022-12-16T07:48:02Z","published":"2022-12-16T07:48:02Z","title":"ReCo: Reliable Causal Chain Reasoning via Structural Causal Recurrent\n  Neural Networks","summary":"  Causal chain reasoning (CCR) is an essential ability for many decision-making\nAI systems, which requires the model to build reliable causal chains by\nconnecting causal pairs. However, CCR suffers from two main transitive\nproblems: threshold effect and scene drift. In other words, the causal pairs to\nbe spliced may have a conflicting threshold boundary or scenario. To address\nthese issues, we propose a novel Reliable Causal chain reasoning\nframework~(ReCo), which introduces exogenous variables to represent the\nthreshold and scene factors of each causal pair within the causal chain, and\nestimates the threshold and scene contradictions across exogenous variables via\nstructural causal recurrent neural networks~(SRNN). Experiments show that ReCo\noutperforms a series of strong baselines on both Chinese and English CCR\ndatasets. Moreover, by injecting reliable causal chain knowledge distilled by\nReCo, BERT can achieve better performances on four downstream causal-related\ntasks than BERT models enhanced by other kinds of knowledge.\n","authors":["Kai Xiong","Xiao Ding","Zhongyang Li","Li Du","Bing Qin","Yi Zheng","Baoxing Huai"],"pdf_url":"https://arxiv.org/pdf/2212.08322v1.pdf","comment":"Accepted by EMNLP 2022"},{"id":"http://arxiv.org/abs/2212.08321v1","updated":"2022-12-16T07:47:03Z","published":"2022-12-16T07:47:03Z","title":"Investigation of Japanese PnG BERT language model in text-to-speech\n  synthesis for pitch accent language","summary":"  End-to-end text-to-speech synthesis (TTS) can generate highly natural\nsynthetic speech from raw text. However, rendering the correct pitch accents is\nstill a challenging problem for end-to-end TTS. To tackle the challenge of\nrendering correct pitch accent in Japanese end-to-end TTS, we adopt PnG~BERT, a\nself-supervised pretrained model in the character and phoneme domain for TTS.\nWe investigate the effects of features captured by PnG~BERT on Japanese TTS by\nmodifying the fine-tuning condition to determine the conditions helpful\ninferring pitch accents. We manipulate content of PnG~BERT features from being\ntext-oriented to speech-oriented by changing the number of fine-tuned layers\nduring TTS. In addition, we teach PnG~BERT pitch accent information by\nfine-tuning with tone prediction as an additional downstream task. Our\nexperimental results show that the features of PnG~BERT captured by pretraining\ncontain information helpful inferring pitch accent, and PnG~BERT outperforms\nbaseline Tacotron on accent correctness in a listening test.\n","authors":["Yusuke Yasuda","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2212.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08307v1","updated":"2022-12-16T07:11:18Z","published":"2022-12-16T07:11:18Z","title":"Controllable Text Generation via Probability Density Estimation in the\n  Latent Space","summary":"  Previous work on controllable text generation has explored the idea of\ncontrol from the latent space, such as optimizing a representation with\nattribute-related classifiers or sampling a representation from relevant\ndiscrete samples. However, they are not effective enough in modeling both the\nlatent space and the control, leaving controlled text with low quality and\ndiversity. In this work, we propose a novel control framework using probability\ndensity estimation in the latent space. Our method utilizes an invertible\ntransformation function, the Normalizing Flow, that maps the complex\ndistributions in the latent space to simple Gaussian distributions in the prior\nspace. Thus, we can perform sophisticated and flexible control in the prior\nspace and feed the control effects back into the latent space owing to the\none-one-mapping property of invertible transformations. Experiments on\nsingle-attribute controls and multi-attribute control reveal that our method\noutperforms several strong baselines on attribute relevance and text quality\nand achieves the SOTA. Further analysis of control strength adjustment\ndemonstrates the flexibility of our control strategy.\n","authors":["Yuxuan Gu","Xiaocheng Feng","Sicheng Ma","Lingyuan Zhang","Heng Gong","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2212.08307v1.pdf","comment":"13 pages, 6 figures, Work in progress"},{"id":"http://arxiv.org/abs/2212.08287v1","updated":"2022-12-16T05:17:59Z","published":"2022-12-16T05:17:59Z","title":"Rich Event Modeling for Script Event Prediction","summary":"  Script is a kind of structured knowledge extracted from texts, which contains\na sequence of events. Based on such knowledge, script event prediction aims to\npredict the subsequent event. To do so, two aspects should be considered for\nevents, namely, event description (i.e., what the events should contain) and\nevent encoding (i.e., how they should be encoded). Most existing methods\ndescribe an event by a verb together with only a few core arguments (i.e.,\nsubject, object, and indirect object), which are not precise. In addition,\nexisting event encoders are limited to a fixed number of arguments, which are\nnot flexible to deal with extra information. Thus, in this paper, we propose\nthe Rich Event Prediction (REP) framework for script event prediction.\nFundamentally, it is based on the proposed rich event description, which\nenriches the existing ones with three kinds of important information, namely,\nthe senses of verbs, extra semantic roles, and types of participants. REP\ncontains an event extractor to extract such information from texts. Based on\nthe extracted rich information, a predictor then selects the most probable\nsubsequent event. The core component of the predictor is a transformer-based\nevent encoder to flexibly deal with an arbitrary number of arguments.\nExperimental results on the widely used Gigaword Corpus show the effectiveness\nof the proposed framework.\n","authors":["Long Bai","Saiping Guan","Zixuan Li","Jiafeng Guo","Xiaolong Jin","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.08287v1.pdf","comment":"AAAI 2023 (main conference)"},{"id":"http://arxiv.org/abs/2212.08286v1","updated":"2022-12-16T05:15:41Z","published":"2022-12-16T05:15:41Z","title":"ALERT: Adapting Language Models to Reasoning Tasks","summary":"  Current large language models can perform reasonably well on complex tasks\nthat require step-by-step reasoning with few-shot learning. Are these models\napplying reasoning skills they have learnt during pre-training and reason\noutside of their training context, or are they simply memorizing their training\ncorpus at finer granularity and have learnt to better understand their context?\nTo tease apart these possibilities, we introduce ALERT, a benchmark and suite\nof analyses for assessing language models' reasoning ability comparing\npre-trained and finetuned models on complex tasks that require reasoning skills\nto solve. ALERT provides a test bed to asses any language model on fine-grained\nreasoning skills, which spans over 20 datasets and covers 10 different\nreasoning skills. We leverage ALERT to further investigate the role of\nfinetuning. With extensive empirical analysis we find that language models\nlearn more reasoning skills such as textual entailment, abductive reasoning,\nand analogical reasoning during finetuning stage compared to pretraining state.\nWe also find that when language models are finetuned they tend to overfit to\nthe prompt template, which hurts the robustness of models causing\ngeneralization problems.\n","authors":["Ping Yu","Tianlu Wang","Olga Golovneva","Badr Alkhamissy","Gargi Ghosh","Mona Diab","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2212.08286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08283v1","updated":"2022-12-16T05:10:09Z","published":"2022-12-16T05:10:09Z","title":"SceneGATE: Scene-Graph based co-Attention networks for TExt visual\n  question answering","summary":"  Most TextVQA approaches focus on the integration of objects, scene texts and\nquestion words by a simple transformer encoder. But this fails to capture the\nsemantic relations between different modalities. The paper proposes a Scene\nGraph based co-Attention Network (SceneGATE) for TextVQA, which reveals the\nsemantic relations among the objects, Optical Character Recognition (OCR)\ntokens and the question words. It is achieved by a TextVQA-based scene graph\nthat discovers the underlying semantics of an image. We created a\nguided-attention module to capture the intra-modal interplay between the\nlanguage and the vision as a guidance for inter-modal interactions. To make\nexplicit teaching of the relations between the two modalities, we proposed and\nintegrated two attention modules, namely a scene graph-based semantic\nrelation-aware attention and a positional relation-aware attention. We\nconducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.\nIt is shown that our SceneGATE method outperformed existing ones because of the\nscene graph and its attention modules.\n","authors":["Siwen Luo","Feiqi Cao","Felipe Nunez","Zean Wen","Josiah Poon","Caren Han"],"pdf_url":"https://arxiv.org/pdf/2212.08283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08279v1","updated":"2022-12-16T04:52:53Z","published":"2022-12-16T04:52:53Z","title":"Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion\n  Behaviors in Social Deduction Games","summary":"  Persuasion modeling is a key building block for conversational agents.\nExisting works in this direction are limited to analyzing textual dialogue\ncorpus. We argue that visual signals also play an important role in\nunderstanding human persuasive behaviors. In this paper, we introduce the first\nmultimodal dataset for modeling persuasion behaviors. Our dataset includes 199\ndialogue transcriptions and videos captured in a multi-player social deduction\ngame setting, 26,647 utterance level annotations of persuasion strategy, and\ngame level annotations of deduction game outcomes. We provide extensive\nexperiments to show how dialogue context and visual signals benefit persuasion\nstrategy prediction. We also explore the generalization ability of language\nmodels for persuasion modeling and the role of persuasion strategies in\npredicting social deduction game outcomes. Our dataset, code, and models can be\nfound at https://persuasion-deductiongame.socialai-data.org.\n","authors":["Bolin Lai","Hongxin Zhang","Miao Liu","Aryan Pariani","Fiona Ryan","Wenqi Jia","Shirley Anugrah Hayati","James M. Rehg","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08279v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2212.08216v1","updated":"2022-12-16T01:10:41Z","published":"2022-12-16T01:10:41Z","title":"Azimuth: Systematic Error Analysis for Text Classification","summary":"  We present Azimuth, an open-source and easy-to-use tool to perform error\nanalysis for text classification. Compared to other stages of the ML\ndevelopment cycle, such as model training and hyper-parameter tuning, the\nprocess and tooling for the error analysis stage are less mature. However, this\nstage is critical for the development of reliable and trustworthy AI systems.\nTo make error analysis more systematic, we propose an approach comprising\ndataset analysis and model quality assessment, which Azimuth facilitates. We\naim to help AI practitioners discover and address areas where the model does\nnot generalize by leveraging and integrating a range of ML techniques, such as\nsaliency maps, similarity, uncertainty, and behavioral analyses, all in one\ntool. Our code and documentation are available at\ngithub.com/servicenow/azimuth.\n","authors":["Gabrielle Gauthier-Melançon","Orlando Marquez Ayala","Lindsay Brin","Chris Tyler","Frédéric Branchaud-Charron","Joseph Marinier","Karine Grande","Di Le"],"pdf_url":"https://arxiv.org/pdf/2212.08216v1.pdf","comment":"To be published in Proceedings of the 2022 Conference on Empirical\n  Methods in Natural Language Processing: System Demonstrations. 13 pages and\n  14 figures"},{"id":"http://arxiv.org/abs/2212.08206v1","updated":"2022-12-16T00:21:30Z","published":"2022-12-16T00:21:30Z","title":"Meeting Summarization: A Survey of the State of the Art","summary":"  Information overloading requires the need for summarizers to extract salient\ninformation from the text. Currently, there is an overload of dialogue data due\nto the rise of virtual communication platforms. The rise of Covid-19 has led\npeople to rely on online communication platforms like Zoom, Slack, Microsoft\nTeams, Discord, etc. to conduct their company meetings. Instead of going\nthrough the entire meeting transcripts, people can use meeting summarizers to\nselect useful data. Nevertheless, there is a lack of comprehensive surveys in\nthe field of meeting summarizers. In this survey, we aim to cover recent\nmeeting summarization techniques. Our survey offers a general overview of text\nsummarization along with datasets and evaluation metrics for meeting\nsummarization. We also provide the performance of each summarizer on a\nleaderboard. We conclude our survey with different challenges in this domain\nand potential research opportunities for future researchers.\n","authors":["Lakshmi Prasanna Kumar","Arman Kabiri"],"pdf_url":"https://arxiv.org/pdf/2212.08206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08205v1","updated":"2022-12-16T00:15:45Z","published":"2022-12-16T00:15:45Z","title":"A unified information-theoretic model of EEG signatures of human\n  language processing","summary":"  We advance an information-theoretic model of human language processing in the\nbrain, in which incoming linguistic input is processed at two levels, in terms\nof a heuristic interpretation and in terms of error correction. We propose that\nthese two kinds of information processing have distinct electroencephalographic\nsignatures, corresponding to the well-documented N400 and P600 components of\nlanguage-related event-related potentials (ERPs). Formally, we show that the\ninformation content (surprisal) of a word in context can be decomposed into two\nquantities: (A) heuristic surprise, which signals processing difficulty of word\ngiven its inferred context, and corresponds with the N400 signal; and (B)\ndiscrepancy signal, which reflects divergence between the true context and the\ninferred context, and corresponds to the P600 signal. Both of these quantities\ncan be estimated using modern NLP techniques. We validate our theory by\nsuccessfully simulating ERP patterns elicited by a variety of linguistic\nmanipulations in previously-reported experimental data from Ryskin et al.\n(2021). Our theory is in principle compatible with traditional cognitive\ntheories assuming a `good-enough' heuristic interpretation stage, but with\nprecise information-theoretic formulation.\n","authors":["Jiaxuan Li","Richard Futrell"],"pdf_url":"https://arxiv.org/pdf/2212.08205v1.pdf","comment":"4 pages, 3 figures, accepted InfoCog workshop at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08204v1","updated":"2022-12-16T00:15:14Z","published":"2022-12-16T00:15:14Z","title":"LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text\n  Comprehension","summary":"  The application of Natural Language Processing (NLP) to specialized domains,\nsuch as the law, has recently received a surge of interest. As many legal\nservices rely on processing and analyzing large collections of documents,\nautomating such tasks with NLP tools emerges as a key challenge. Many popular\nlanguage models, such as BERT or RoBERTa, are general-purpose models, which\nhave limitations on processing specialized legal terminology and syntax. In\naddition, legal documents may contain specialized vocabulary from other\ndomains, such as medical terminology in personal injury text. Here, we propose\nLegalRelectra, a legal-domain language model that is trained on mixed-domain\nlegal and medical corpora. We show that our model improves over general-domain\nand single-domain medical and legal language models when processing\nmixed-domain (personal injury) text. Our training architecture implements the\nElectra framework, but utilizes Reformer instead of BERT for its generator and\ndiscriminator. We show that this improves the model's performance on processing\nlong passages and results in better long-range text comprehension.\n","authors":["Wenyue Hua","Yuchen Zhang","Zhe Chen","Josie Li","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2212.08204v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2212.08653v1","updated":"2022-12-16T18:59:12Z","published":"2022-12-16T18:59:12Z","title":"Attentive Mask CLIP","summary":"  Image token removal is an efficient augmentation strategy for reducing the\ncost of computing image features. However, this efficient augmentation strategy\nhas been found to adversely affect the accuracy of CLIP-based training. We\nhypothesize that removing a large portion of image tokens may improperly\ndiscard the semantic content associated with a given text description, thus\nconstituting an incorrect pairing target in CLIP training. To address this\nissue, we propose an attentive token removal approach for CLIP training, which\nretains tokens with a high semantic correlation to the text description. The\ncorrelation scores are computed in an online fashion using the EMA version of\nthe visual encoder. Our experiments show that the proposed attentive masking\napproach performs better than the previous method of random token removal for\nCLIP training. The approach also makes it efficient to apply multiple\naugmentation views to the image, as well as introducing instance contrastive\nlearning tasks between these views into the CLIP framework. Compared to other\nCLIP improvements that combine different pre-training targets such as SLIP and\nMaskCLIP, our method is not only more effective, but also much more efficient.\nSpecifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\\%$\ntop-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$\nand $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are\n$+1.1\\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being\n$2.30\\times$ faster. An efficient version of our approach running $1.16\\times$\nfaster than the plain CLIP model achieves significant gains of $+5.3\\%$,\n$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.\n","authors":["Yifan Yang","Weiquan Huang","Yixuan Wei","Houwen Peng","Xinyang Jiang","Huiqiang Jiang","Fangyun Wei","Yin Wang","Han Hu","Lili Qiu","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08650v1","updated":"2022-12-16T18:51:41Z","published":"2022-12-16T18:51:41Z","title":"On Human Visual Contrast Sensitivity and Machine Vision Robustness: A\n  Comparative Study","summary":"  It is well established in neuroscience that color vision plays an essential\npart in the human visual perception system. Meanwhile, many novel designs for\ncomputer vision inspired by human vision have achieved success in a wide range\nof tasks and applications. Nonetheless, how color differences affect machine\nvision has not been well explored. Our work tries to bridge this gap between\nthe human color vision aspect of visual recognition and that of the machine. To\nachieve this, we curate two datasets: CIFAR10-F and CIFAR100-F, which are based\non the foreground colors of the popular CIFAR datasets. Together with CIFAR10-B\nand CIFAR100-B, the existing counterpart datasets with information on the\nbackground colors of CIFAR test sets, we assign each image based on its color\ncontrast level per its foreground and background color labels and use this as a\nproxy to study how color contrast affects machine vision. We first conduct a\nproof-of-concept study, showing the effect of color difference and validate our\ndatasets. Furthermore, on a broader level, an important characteristic of human\nvision is its robustness against ambient changes; therefore, drawing\ninspirations from ophthalmology and the robustness literature, we analogize\ncontrast sensitivity from the human visual aspect to machine vision and\ncomplement the current robustness study using corrupted images with our\nCIFAR-CoCo datasets. In summary, motivated by neuroscience and equipped with\nthe datasets we curate, we devise a new framework in two dimensions to perform\nextensive analyses on the effect of color contrast and corrupted images: (1)\nmodel architecture, (2) model size, to measure the perception ability of\nmachine vision beyond total accuracy. We also explore how task complexity and\ndata augmentation play a role in this setup. Our results call attention to new\nevaluation approaches for human-like machine perception.\n","authors":["Ming-Chang Chiu","Yingfei Wang","Derrick Eui Gyu Kim","Pin-Yu Chen","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2212.08650v1.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2212.08649v1","updated":"2022-12-16T18:51:10Z","published":"2022-12-16T18:51:10Z","title":"Better May Not Be Fairer: Can Data Augmentation Mitigate Subgroup\n  Degradation?","summary":"  It is no secret that deep learning models exhibit undesirable behaviors such\nas learning spurious correlations instead of learning correct relationships\nbetween input/output pairs. Prior works on robustness study datasets that mix\nlow-level features to quantify how spurious correlations affect predictions\ninstead of considering natural semantic factors due to limitations in accessing\nrealistic datasets for comprehensive evaluation. To bridge this gap, in this\npaper we first investigate how natural background colors play a role as\nspurious features in image classification tasks by manually splitting the test\nsets of CIFAR10 and CIFAR100 into subgroups based on the background color of\neach image. We name our datasets CIFAR10-B and CIFAR100-B. We find that while\nstandard CNNs achieve human-level accuracy, the subgroup performances are not\nconsistent, and the phenomenon remains even after data augmentation (DA). To\nalleviate this issue, we propose FlowAug, a semantic DA method that leverages\nthe decoupled semantic representations captured by a pre-trained generative\nflow. Experimental results show that FlowAug achieves more consistent results\nacross subgroups than other types of DA methods on CIFAR10 and CIFAR100.\nAdditionally, it shows better generalization performance. Furthermore, we\npropose a generic metric for studying model robustness to spurious\ncorrelations, where we take a macro average on the weighted standard deviations\nacross different classes. Per our metric, FlowAug demonstrates less reliance on\nspurious correlations. Although this metric is proposed to study our curated\ndatasets, it applies to all datasets that have subgroups or subclasses. Lastly,\naside from less dependence on spurious correlations and better generalization\non in-distribution test sets, we also show superior out-of-distribution results\non CIFAR10.1 and competitive performances on CIFAR10-C and CIFAR100-C.\n","authors":["Ming-Chang Chiu","Pin-Yu Chen","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2212.08649v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.14821v2","updated":"2022-12-16T18:47:57Z","published":"2022-11-27T13:24:28Z","title":"Towards Realistic Underwater Dataset Generation and Color Restoration","summary":"  Recovery of true color from underwater images is an ill-posed problem. This\nis because the wide-band attenuation coefficients for the RGB color channels\ndepend on object range, reflectance, etc. which are difficult to model. Also,\nthere is backscattering due to suspended particles in water. Thus, most\nexisting deep-learning based color restoration methods, which are trained on\nsynthetic underwater datasets, do not perform well on real underwater data.\nThis can be attributed to the fact that synthetic data cannot accurately\nrepresent real conditions. To address this issue, we use an image to image\ntranslation network to bridge the gap between the synthetic and real domains by\ntranslating images from synthetic underwater domain to real underwater domain.\nUsing this multimodal domain adaptation technique, we create a dataset that can\ncapture a diverse array of underwater conditions. We then train a simple but\neffective CNN based network on our domain adapted dataset to perform color\nrestoration. Code and pre-trained models can be accessed at\nhttps://github.com/nehamjain10/TRUDGCR\n","authors":["Neham Jain","Gopi Matta","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2211.14821v2.pdf","comment":"Published at The Indian Conference on Computer Vision, Graphics and\n  Image Processing (ICVGIP) 2022"},{"id":"http://arxiv.org/abs/2101.11878v3","updated":"2022-12-16T18:45:15Z","published":"2021-01-28T09:16:21Z","title":"CORL: Compositional Representation Learning for Few-Shot Classification","summary":"  Few-shot image classification consists of two consecutive learning processes:\n1) In the meta-learning stage, the model acquires a knowledge base from a set\nof training classes. 2) During meta-testing, the acquired knowledge is used to\nrecognize unseen classes from very few examples. Inspired by the compositional\nrepresentation of objects in humans, we train a neural network architecture\nthat explicitly represents objects as a dictionary of shared components and\ntheir spatial composition. In particular, during meta-learning, we train a\nknowledge base that consists of a dictionary of component representations and a\ndictionary of component activation maps that encode common spatial activation\npatterns of components. The elements of both dictionaries are shared among the\ntraining classes. During meta-testing, the representation of unseen classes is\nlearned using the component representations and the component activation maps\nfrom the knowledge base. Finally, an attention mechanism is used to strengthen\nthose components that are most important for each category. We demonstrate the\nvalue of our interpretable compositional learning framework for a few-shot\nclassification using miniImageNet, tieredImageNet, CIFAR-FS, and FC100, where\nwe achieve comparable performance.\n","authors":["Ju He","Adam Kortylewski","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2101.11878v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07555v2","updated":"2022-12-16T18:39:09Z","published":"2022-12-14T23:59:24Z","title":"IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object\n  Interactions","summary":"  Can we make virtual characters in a scene interact with their surrounding\nobjects through simple instructions? Is it possible to synthesize such motion\nplausibly with a diverse set of objects and instructions? Inspired by these\nquestions, we present the first framework to synthesize the full-body motion of\nvirtual human characters performing specified actions with 3D objects placed\nwithin their reach. Our system takes as input textual instructions specifying\nthe objects and the associated intentions of the virtual characters and outputs\ndiverse sequences of full-body motions. This is in contrast to existing work,\nwhere full-body action synthesis methods generally do not consider object\ninteractions, and human-object interaction methods focus mainly on synthesizing\nhand or finger movements for grasping objects. We accomplish our objective by\ndesigning an intent-driven full-body motion generator, which uses a pair of\ndecoupled conditional variational autoencoders (CVAE) to learn the motion of\nthe body parts in an autoregressive manner. We also optimize for the positions\nof the objects with six degrees of freedom (6DoF) such that they plausibly fit\nwithin the hands of the synthesized characters. We compare our proposed method\nwith the existing methods of motion synthesis and establish a new and stronger\nstate-of-the-art for the task of intent-driven motion synthesis. Through a user\nstudy, we further show that our synthesized full-body motions appear more\nrealistic to the participants in more than 80% of scenarios compared to the\ncurrent state-of-the-art methods, and are perceived to be as good as the ground\ntruth on several occasions.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2212.07555v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.08641v1","updated":"2022-12-16T18:31:48Z","published":"2022-12-16T18:31:48Z","title":"GFPose: Learning 3D Human Pose Prior with Gradient Fields","summary":"  Learning 3D human pose prior is essential to human-centered AI. Here, we\npresent GFPose, a versatile framework to model plausible 3D human poses for\nvarious applications. At the core of GFPose is a time-dependent score network,\nwhich estimates the gradient on each body joint and progressively denoises the\nperturbed 3D human pose to match a given task specification. During the\ndenoising process, GFPose implicitly incorporates pose priors in gradients and\nunifies various discriminative and generative tasks in an elegant framework.\nDespite the simplicity, GFPose demonstrates great potential in several\ndownstream tasks. Our experiments empirically show that 1) as a\nmulti-hypothesis pose estimator, GFPose outperforms existing SOTAs by 20% on\nHuman3.6M dataset. 2) as a single-hypothesis pose estimator, GFPose achieves\ncomparable results to deterministic SOTAs, even with a vanilla backbone. 3)\nGFPose is able to produce diverse and realistic samples in pose denoising,\ncompletion and generation tasks. Project page\nhttps://sites.google.com/view/gfpose/\n","authors":["Hai Ci","Mingdong Wu","Wentao Zhu","Xiaoxuan Ma","Hao Dong","Fangwei Zhong","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2212.08641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08639v1","updated":"2022-12-16T18:31:23Z","published":"2022-12-16T18:31:23Z","title":"An annotated instance segmentation XXL-CT dataset from a historic\n  airplane","summary":"  The Me 163 was a Second World War fighter airplane and a result of the German\nair force secret developments. One of these airplanes is currently owned and\ndisplayed in the historic aircraft exhibition of the Deutsches Museum in\nMunich, Germany. To gain insights with respect to its history, design and state\nof preservation, a complete CT scan was obtained using an industrial\nXXL-computer tomography scanner.\n  Using the CT data from the Me 163, all its details can visually be examined\nat various levels, ranging from the complete hull down to single sprockets and\nrivets. However, while a trained human observer can identify and interpret the\nvolumetric data with all its parts and connections, a virtual dissection of the\nairplane and all its different parts would be quite desirable. Nevertheless,\nthis means, that an instance segmentation of all components and objects of\ninterest into disjoint entities from the CT data is necessary.\n  As of currently, no adequate computer-assisted tools for automated or\nsemi-automated segmentation of such XXL-airplane data are available, in a first\nstep, an interactive data annotation and object labeling process has been\nestablished. So far, seven 512 x 512 x 512 voxel sub-volumes from the Me 163\nairplane have been annotated and labeled, whose results can potentially be used\nfor various new applications in the field of digital heritage, non-destructive\ntesting, or machine-learning.\n  This work describes the data acquisition process of the airplane using an\nindustrial XXL-CT scanner, outlines the interactive segmentation and labeling\nscheme to annotate sub-volumes of the airplane's CT data, describes and\ndiscusses various challenges with respect to interpreting and handling the\nannotated and labeled data.\n","authors":["Roland Gruber","Nils Reims","Andreas Hempfer","Stefan Gerth","Michael Salamon","Thomas Wittenberg"],"pdf_url":"https://arxiv.org/pdf/2212.08639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05153v2","updated":"2022-12-16T18:18:59Z","published":"2022-12-10T00:18:05Z","title":"Algorithmic progress in computer vision","summary":"  We investigate algorithmic progress in image classification on ImageNet,\nperhaps the most well-known test bed for computer vision. We estimate a model,\ninformed by work on neural scaling laws, and infer a decomposition of progress\ninto the scaling of compute, data, and algorithms. Using Shapley values to\nattribute performance improvements, we find that algorithmic improvements have\nbeen roughly as important as the scaling of compute for progress computer\nvision. Our estimates indicate that algorithmic innovations mostly take the\nform of compute-augmenting algorithmic advances (which enable researchers to\nget better performance from less compute), not data-augmenting algorithmic\nadvances. We find that compute-augmenting algorithmic advances are made at a\npace more than twice as fast as the rate usually associated with Moore's law.\nIn particular, we estimate that compute-augmenting innovations halve compute\nrequirements every nine months (95\\% confidence interval: 4 to 25 months).\n","authors":["Ege Erdil","Tamay Besiroglu"],"pdf_url":"https://arxiv.org/pdf/2212.05153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08632v1","updated":"2022-12-16T18:12:04Z","published":"2022-12-16T18:12:04Z","title":"Enhancing Multi-modal and Multi-hop Question Answering via Structured\n  Knowledge and Unified Retrieval-Generation","summary":"  Multi-modal and multi-hop question answering aims to answer a question based\non multiple input sources from different modalities. Previous methods retrieve\nthe evidence separately and feed the retrieved evidence to a language model to\ngenerate the corresponding answer. However, these methods fail to build\nconnections between candidates and thus cannot model the inter-dependent\nrelation during retrieval. Moreover, the reasoning process over multi-modality\ncandidates can be unbalanced without building alignments between different\nmodalities. To address this limitation, we propose a Structured Knowledge and\nUnified Retrieval Generation based method (SKURG). We align the sources from\ndifferent modalities via the shared entities and map them into a shared\nsemantic space via structured knowledge. Then, we utilize a unified\nretrieval-generation decoder to integrate intermediate retrieval results for\nanswer generation and adaptively determine the number of retrieval steps. We\nperform experiments on two multi-modal and multi-hop datasets: WebQA and\nMultimodalQA. The results demonstrate that SKURG achieves state-of-the-art\nperformance on both retrieval and answer generation.\n","authors":["Qian Yang","Qian Chen","Wen Wang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.08632v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2212.08624v1","updated":"2022-12-16T17:59:40Z","published":"2022-12-16T17:59:40Z","title":"Development of A Real-time POCUS Image Quality Assessment and\n  Acquisition Guidance System","summary":"  Point-of-care ultrasound (POCUS) is one of the most commonly applied tools\nfor cardiac function imaging in the clinical routine of the emergency\ndepartment and pediatric intensive care unit. The prior studies demonstrate\nthat AI-assisted software can guide nurses or novices without prior sonography\nexperience to acquire POCUS by recognizing the interest region, assessing image\nquality, and providing instructions. However, these AI algorithms cannot simply\nreplace the role of skilled sonographers in acquiring diagnostic-quality POCUS.\nUnlike chest X-ray, CT, and MRI, which have standardized imaging protocols,\nPOCUS can be acquired with high inter-observer variability. Though being with\nvariability, they are usually all clinically acceptable and interpretable. In\nchallenging clinical environments, sonographers employ novel heuristics to\nacquire POCUS in complex scenarios. To help novice learners to expedite the\ntraining process while reducing the dependency on experienced sonographers in\nthe curriculum implementation, We will develop a framework to perform real-time\nAI-assisted quality assessment and probe position guidance to provide training\nprocess for novice learners with less manual intervention.\n","authors":["Zhenge Jia","Yiyu Shi","Jingtong Hu","Lei Yang","Benjamin Nti"],"pdf_url":"https://arxiv.org/pdf/2212.08624v1.pdf","comment":"4 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2212.08613v1","updated":"2022-12-16T17:42:38Z","published":"2022-12-16T17:42:38Z","title":"Atrous Space Bender U-Net (ASBU-Net/LogiNet)","summary":"  $ $With recent advances in CNNs, exceptional improvements have been made in\nsemantic segmentation of high resolution images in terms of accuracy and\nlatency. However, challenges still remain in detecting objects in crowded\nscenes, large scale variations, partial occlusion, and distortions, while still\nmaintaining mobility and latency. We introduce a fast and efficient\nconvolutional neural network, ASBU-Net, for semantic segmentation of high\nresolution images that addresses these problems and uses no novelty layers for\nease of quantization and embedded hardware support. ASBU-Net is based on a new\nfeature extraction module, atrous space bender layer (ASBL), which is efficient\nin terms of computation and memory. The ASB layers form a building block that\nis used to make ASBNet. Since this network does not use any special layers it\ncan be easily implemented, quantized and deployed on FPGAs and other hardware\nwith limited memory. We present experiments on resource and accuracy trade-offs\nand show strong performance compared to other popular models.\n","authors":["Anurag Bansal","Oleg Ostap","Miguel Maestre Trueba","Kristopher Perry"],"pdf_url":"https://arxiv.org/pdf/2212.08613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08610v1","updated":"2022-12-16T17:39:32Z","published":"2022-12-16T17:39:32Z","title":"Huruf: An Application for Arabic Handwritten Character Recognition Using\n  Deep Learning","summary":"  Handwriting Recognition has been a field of great interest in the Artificial\nIntelligence domain. Due to its broad use cases in real life, research has been\nconducted widely on it. Prominent work has been done in this field focusing\nmainly on Latin characters. However, the domain of Arabic handwritten character\nrecognition is still relatively unexplored. The inherent cursive nature of the\nArabic characters and variations in writing styles across individuals makes the\ntask even more challenging. We identified some probable reasons behind this and\nproposed a lightweight Convolutional Neural Network-based architecture for\nrecognizing Arabic characters and digits. The proposed pipeline consists of a\ntotal of 18 layers containing four layers each for convolution, pooling, batch\nnormalization, dropout, and finally one Global average pooling and a Dense\nlayer. Furthermore, we thoroughly investigated the different choices of\nhyperparameters such as the choice of the optimizer, kernel initializer,\nactivation function, etc. Evaluating the proposed architecture on the publicly\navailable 'Arabic Handwritten Character Dataset (AHCD)' and 'Modified Arabic\nhandwritten digits Database (MadBase)' datasets, the proposed model\nrespectively achieved an accuracy of 96.93% and 99.35% which is comparable to\nthe state-of-the-art and makes it a suitable solution for real-life end-level\napplications.\n","authors":["Minhaz Kamal","Fairuz Shaiara","Chowdhury Mohammad Abdullah","Sabbir Ahmed","Tasnim Ahmed","Md. Hasanul Kabir"],"pdf_url":"https://arxiv.org/pdf/2212.08610v1.pdf","comment":"Accepted in 25th ICCIT (6 pages, 4 tables, 4 figures)"},{"id":"http://arxiv.org/abs/2212.08596v1","updated":"2022-12-16T17:22:51Z","published":"2022-12-16T17:22:51Z","title":"De-risking Carbon Capture and Sequestration with Explainable CO2 Leakage\n  Detection in Time-lapse Seismic Monitoring Images","summary":"  With the growing global deployment of carbon capture and sequestration\ntechnology to combat climate change, monitoring and detection of potential CO2\nleakage through existing or storage induced faults are critical to the safe and\nlong-term viability of the technology. Recent work on time-lapse seismic\nmonitoring of CO2 storage has shown promising results in its ability to monitor\nthe growth of the CO2 plume from surface recorded seismic data. However, due to\nthe low sensitivity of seismic imaging to CO2 concentration, additional\ndevelopments are required to efficiently interpret the seismic images for\nleakage. In this work, we introduce a binary classification of time-lapse\nseismic images to delineate CO2 plumes (leakage) using state-of-the-art deep\nlearning models. Additionally, we localize the leakage region of CO2 plumes by\nleveraging Class Activation Mapping methods.\n","authors":["Huseyin Tuna Erdinc","Abhinav Prakash Gahlot","Ziyi Yin","Mathias Louboutin","Felix J. Herrmann"],"pdf_url":"https://arxiv.org/pdf/2212.08596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.13240v2","updated":"2022-12-16T17:15:54Z","published":"2022-07-27T01:49:26Z","title":"Contrastive Image Synthesis and Self-supervised Feature Adaptation for\n  Cross-Modality Biomedical Image Segmentation","summary":"  This work presents a novel framework CISFA (Contrastive Image synthesis and\nSelf-supervised Feature Adaptation)that builds on image domain translation and\nunsupervised feature adaptation for cross-modality biomedical image\nsegmentation. Different from existing works, we use a one-sided generative\nmodel and add a weighted patch-wise contrastive loss between sampled patches of\nthe input image and the corresponding synthetic image, which serves as shape\nconstraints. Moreover, we notice that the generated images and input images\nshare similar structural information but are in different modalities. As such,\nwe enforce contrastive losses on the generated images and the input images to\ntrain the encoder of a segmentation model to minimize the discrepancy between\npaired images in the learned embedding space. Compared with existing works that\nrely on adversarial learning for feature adaptation, such a method enables the\nencoder to learn domain-independent features in a more explicit way. We\nextensively evaluate our methods on segmentation tasks containing CT and MRI\nimages for abdominal cavities and whole hearts. Experimental results show that\nthe proposed framework not only outputs synthetic images with less distortion\nof organ shapes, but also outperforms state-of-the-art domain adaptation\nmethods by a large margin.\n","authors":["Xinrong Hu","Corey Wang","Yiyu Shi"],"pdf_url":"https://arxiv.org/pdf/2207.13240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08586v1","updated":"2022-12-16T17:06:28Z","published":"2022-12-16T17:06:28Z","title":"Rethinking Cooking State Recognition with Vision Transformers","summary":"  To ensure proper knowledge representation of the kitchen environment, it is\nvital for kitchen robots to recognize the states of the food items that are\nbeing cooked. Although the domain of object detection and recognition has been\nextensively studied, the task of object state classification has remained\nrelatively unexplored. The high intra-class similarity of ingredients during\ndifferent states of cooking makes the task even more challenging. Researchers\nhave proposed adopting Deep Learning based strategies in recent times, however,\nthey are yet to achieve high performance. In this study, we utilized the\nself-attention mechanism of the Vision Transformer (ViT) architecture for the\nCooking State Recognition task. The proposed approach encapsulates the globally\nsalient features from images, while also exploiting the weights learned from a\nlarger dataset. This global attention allows the model to withstand the\nsimilarities between samples of different cooking objects, while the employment\nof transfer learning helps to overcome the lack of inductive bias by utilizing\npretrained weights. To improve recognition accuracy, several augmentation\ntechniques have been employed as well. Evaluation of our proposed framework on\nthe `Cooking State Recognition Challenge Dataset' has achieved an accuracy of\n94.3%, which significantly outperforms the state-of-the-art.\n","authors":["Akib Mohammed Khan","Alif Ashrafee","Reeshoon Sayera","Shahriar Ivan","Sabbir Ahmed"],"pdf_url":"https://arxiv.org/pdf/2212.08586v1.pdf","comment":"Accepted in 25th ICCIT (6 pages, 5 Figures, 5 Tables)"},{"id":"http://arxiv.org/abs/2212.08583v1","updated":"2022-12-16T17:02:55Z","published":"2022-12-16T17:02:55Z","title":"Semi-Siamese Network for Robust Change Detection Across Different\n  Domains with Applications to 3D Printing","summary":"  Automatic defect detection for 3D printing processes, which shares many\ncharacteristics with change detection problems, is a vital step for quality\ncontrol of 3D printed products. However, there are some critical challenges in\nthe current state of practice. First, existing methods for computer\nvision-based process monitoring typically work well only under specific camera\nviewpoints and lighting situations, requiring expensive pre-processing,\nalignment, and camera setups. Second, many defect detection techniques are\nspecific to pre-defined defect patterns and/or print schematics. In this work,\nwe approach the automatic defect detection problem differently using a novel\nSemi-Siamese deep learning model that directly compares a reference schematic\nof the desired print and a camera image of the achieved print. The model then\nsolves an image segmentation problem, identifying the locations of defects with\nrespect to the reference frame. Unlike most change detection problems, our\nmodel is specially developed to handle images coming from different domains and\nis robust against perturbations in the imaging setup such as camera angle and\nillumination. Defect localization predictions were made in 2.75 seconds per\nlayer using a standard MacBookPro, which is comparable to the typical tens of\nseconds or less for printing a single layer on an inkjet-based 3D printer,\nwhile achieving an F1-score of more than 0.9.\n","authors":["Yushuo Niu","Ethan Chadwick","Anson W. K. Ma","Qian Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12065v2","updated":"2022-12-16T16:46:04Z","published":"2022-07-25T11:18:48Z","title":"Dynamic Channel Selection in Self-Supervised Learning","summary":"  Whilst computer vision models built using self-supervised approaches are now\ncommonplace, some important questions remain. Do self-supervised models learn\nhighly redundant channel features? What if a self-supervised network could\ndynamically select the important channels and get rid of the unnecessary ones?\nCurrently, convnets pre-trained with self-supervision have obtained comparable\nperformance on downstream tasks in comparison to their supervised counterparts\nin computer vision. However, there are drawbacks to self-supervised models\nincluding their large numbers of parameters, computationally expensive training\nstrategies and a clear need for faster inference on downstream tasks. In this\nwork, our goal is to address the latter by studying how a standard channel\nselection method developed for supervised learning can be applied to networks\ntrained with self-supervision. We validate our findings on a range of target\nbudgets $t_{d}$ for channel computation on image classification task across\ndifferent datasets, specifically CIFAR-10, CIFAR-100, and ImageNet-100,\nobtaining comparable performance to that of the original network when selecting\nall channels but at a significant reduction in computation reported in terms of\nFLOPs.\n","authors":["Tarun Krishna","Ayush K. Rai","Yasser A. D. Djilali","Alan F. Smeaton","Kevin McGuinness","Noel E. O'Connor"],"pdf_url":"https://arxiv.org/pdf/2207.12065v2.pdf","comment":"Accepted in Irish Machine Vision and Image Processing Conference 2022"},{"id":"http://arxiv.org/abs/2212.08568v1","updated":"2022-12-16T16:44:46Z","published":"2022-12-16T16:44:46Z","title":"Biomedical image analysis competitions: The state of current\n  participation practice","summary":"  The number of international benchmarking competitions is steadily increasing\nin various fields of machine learning (ML) research and practice. So far,\nhowever, little is known about the common practice as well as bottlenecks faced\nby the community in tackling the research questions posed. To shed light on the\nstatus quo of algorithm development in the specific field of biomedical imaging\nanalysis, we designed an international survey that was issued to all\nparticipants of challenges conducted in conjunction with the IEEE ISBI 2021 and\nMICCAI 2021 conferences (80 competitions in total). The survey covered\nparticipants' expertise and working environments, their chosen strategies, as\nwell as algorithm characteristics. A median of 72% challenge participants took\npart in the survey. According to our results, knowledge exchange was the\nprimary incentive (70%) for participation, while the reception of prize money\nplayed only a minor role (16%). While a median of 80 working hours was spent on\nmethod development, a large portion of participants stated that they did not\nhave enough time for method development (32%). 25% perceived the infrastructure\nto be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of\nthese, 84% were based on standard architectures. 43% of the respondents\nreported that the data samples (e.g., images) were too large to be processed at\nonce. This was most commonly addressed by patch-based training (69%),\ndownsampling (37%), and solving 3D analysis tasks as a series of 2D tasks.\nK-fold cross-validation on the training set was performed by only 37% of the\nparticipants and only 50% of the participants performed ensembling based on\nmultiple identical models (61%) or heterogeneous models (39%). 48% of the\nrespondents applied postprocessing steps.\n","authors":["Matthias Eisenmann","Annika Reinke","Vivienn Weru","Minu Dietlinde Tizabi","Fabian Isensee","Tim J. Adler","Patrick Godau","Veronika Cheplygina","Michal Kozubek","Sharib Ali","Anubha Gupta","Jan Kybic","Alison Noble","Carlos Ortiz de Solórzano","Samiksha Pachade","Caroline Petitjean","Daniel Sage","Donglai Wei","Elizabeth Wilden","Deepak Alapatt","Vincent Andrearczyk","Ujjwal Baid","Spyridon Bakas","Niranjan Balu","Sophia Bano","Vivek Singh Bawa","Jorge Bernal","Sebastian Bodenstedt","Alessandro Casella","Jinwook Choi","Olivier Commowick","Marie Daum","Adrien Depeursinge","Reuben Dorent","Jan Egger","Hannah Eichhorn","Sandy Engelhardt","Melanie Ganz","Gabriel Girard","Lasse Hansen","Mattias Heinrich","Nicholas Heller","Alessa Hering","Arnaud Huaulmé","Hyunjeong Kim","Bennett Landman","Hongwei Bran Li","Jianning Li","Jun Ma","Anne Martel","Carlos Martín-Isla","Bjoern Menze","Chinedu Innocent Nwoye","Valentin Oreiller","Nicolas Padoy","Sarthak Pati","Kelly Payette","Carole Sudre","Kimberlin van Wijnen","Armine Vardazaryan","Tom Vercauteren","Martin Wagner","Chuanbo Wang","Moi Hoon Yap","Zeyun Yu","Chun Yuan","Maximilian Zenk","Aneeq Zia","David Zimmerer","Rina Bao","Chanyeol Choi","Andrew Cohen","Oleh Dzyubachyk","Adrian Galdran","Tianyuan Gan","Tianqi Guo","Pradyumna Gupta","Mahmood Haithami","Edward Ho","Ikbeom Jang","Zhili Li","Zhengbo Luo","Filip Lux","Sokratis Makrogiannis","Dominik Müller","Young-tack Oh","Subeen Pang","Constantin Pape","Gorkem Polat","Charlotte Rosalie Reed","Kanghyun Ryu","Tim Scherr","Vajira Thambawita","Haoyu Wang","Xinliang Wang","Kele Xu","Hung Yeh","Doyeob Yeo","Yixuan Yuan","Yan Zeng","Xin Zhao","Julian Abbing","Jannes Adam","Nagesh Adluru","Niklas Agethen","Salman Ahmed","Yasmina Al Khalil","Mireia Alenyà","Esa Alhoniemi","Chengyang An","Talha Anwar","Tewodros Weldebirhan Arega","Netanell Avisdris","Dogu Baran Aydogan","Yingbin Bai","Maria Baldeon Calisto","Berke Doga Basaran","Marcel Beetz","Cheng Bian","Hao Bian","Kevin Blansit","Louise Bloch","Robert Bohnsack","Sara Bosticardo","Jack Breen","Mikael Brudfors","Raphael Brüngel","Mariano Cabezas","Alberto Cacciola","Zhiwei Chen","Yucong Chen","Daniel Tianming Chen","Minjeong Cho","Min-Kook Choi","Chuantao Xie Chuantao Xie","Dana Cobzas","Julien Cohen-Adad","Jorge Corral Acero","Sujit Kumar Das","Marcela de Oliveira","Hanqiu Deng","Guiming Dong","Lars Doorenbos","Cory Efird","Di Fan","Mehdi Fatan Serj","Alexandre Fenneteau","Lucas Fidon","Patryk Filipiak","René Finzel","Nuno R. Freitas","Christoph M. Friedrich","Mitchell Fulton","Finn Gaida","Francesco Galati","Christoforos Galazis","Chang Hee Gan","Zheyao Gao","Shengbo Gao","Matej Gazda","Beerend Gerats","Neil Getty","Adam Gibicar","Ryan Gifford","Sajan Gohil","Maria Grammatikopoulou","Daniel Grzech","Orhun Güley","Timo Günnemann","Chunxu Guo","Sylvain Guy","Heonjin Ha","Luyi Han","Il Song Han","Ali Hatamizadeh","Tian He","Jimin Heo","Sebastian Hitziger","SeulGi Hong","SeungBum Hong","Rian Huang","Ziyan Huang","Markus Huellebrand","Stephan Huschauer","Mustaffa Hussain","Tomoo Inubushi","Ece Isik Polat","Mojtaba Jafaritadi","SeongHun Jeong","Bailiang Jian","Yuanhong Jiang","Zhifan Jiang","Yueming Jin","Smriti Joshi","Abdolrahim Kadkhodamohammadi","Reda Abdellah Kamraoui","Inha Kang","Junghwa Kang","Davood Karimi","April Khademi","Muhammad Irfan Khan","Suleiman A. Khan","Rishab Khantwal","Kwang-Ju Kim","Timothy Kline","Satoshi Kondo","Elina Kontio","Adrian Krenzer","Artem Kroviakov","Hugo Kuijf","Satyadwyoom Kumar","Francesco La Rosa","Abhi Lad","Doohee Lee","Minho Lee","Chiara Lena","Hao Li","Ling Li","Xingyu Li","Fuyuan Liao","KuanLun Liao","Arlindo Limede Oliveira","Chaonan Lin","Shan Lin","Akis Linardos","Marius George Linguraru","Han Liu","Tao Liu","Di Liu","Yanling Liu","João Lourenço-Silva","Jingpei Lu","Jiangshan Lu","Imanol Luengo","Christina B. Lund","Huan Minh Luu","Yi Lv","Yi Lv","Uzay Macar","Leon Maechler","Sina Mansour L.","Kenji Marshall","Moona Mazher","Richard McKinley","Alfonso Medela","Felix Meissen","Mingyuan Meng","Dylan Miller","Seyed Hossein Mirjahanmardi","Arnab Mishra","Samir Mitha","Hassan Mohy-ud-Din","Tony Chi Wing Mok","Gowtham Krishnan Murugesan","Enamundram Naga Karthik","Sahil Nalawade","Jakub Nalepa","Mohamed Naser","Ramin Nateghi","Hammad Naveed","Quang-Minh Nguyen","Cuong Nguyen Quoc","Brennan Nichyporuk","Bruno Oliveira","David Owen","Jimut Bahan Pal","Junwen Pan","Wentao Pan","Winnie Pang","Bogyu Park","Vivek Pawar","Kamlesh Pawar","Michael Peven","Lena Philipp","Tomasz Pieciak","Szymon Plotka","Marcel Plutat","Fattaneh Pourakpour","Domen Preložnik","Kumaradevan Punithakumar","Abdul Qayyum","Sandro Queirós","Arman Rahmim","Salar Razavi","Jintao Ren","Mina Rezaei","Jonathan Adam Rico","ZunHyan Rieu","Markus Rink","Johannes Roth","Yusely Ruiz-Gonzalez","Numan Saeed","Anindo Saha","Mostafa Salem","Ricardo Sanchez-Matilla","Kurt Schilling","Wei Shao","Zhiqiang Shen","Ruize Shi","Pengcheng Shi","Daniel Sobotka","Théodore Soulier","Bella Specktor Fadida","Danail Stoyanov","Timothy Sum Hon Mun","Xiaowu Sun","Rong Tao","Franz Thaler","Antoine Théberge","Felix Thielke","Helena Torres","Kareem A. Wahid","Jiacheng Wang","YiFei Wang","Wei Wang","Xiong Wang","Jianhui Wen","Ning Wen","Marek Wodzinski","Ye Wu","Fangfang Xia","Tianqi Xiang","Chen Xiaofei","Lizhan Xu","Tingting Xue","Yuxuan Yang","Lin Yang","Kai Yao","Huifeng Yao","Amirsaeed Yazdani","Michael Yip","Hwanseung Yoo","Fereshteh Yousefirizi","Shunkai Yu","Lei Yu","Jonathan Zamora","Ramy Ashraf Zeineldin","Dewen Zeng","Jianpeng Zhang","Bokai Zhang","Jiapeng Zhang","Fan Zhang","Huahong Zhang","Zhongchen Zhao","Zixuan Zhao","Jiachen Zhao","Can Zhao","Qingshuo Zheng","Yuheng Zhi","Ziqi Zhou","Baosheng Zou","Klaus Maier-Hein","Paul F. Jäger","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2212.08568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08558v1","updated":"2022-12-16T16:25:36Z","published":"2022-12-16T16:25:36Z","title":"Simulating Road Spray Effects in Automotive Lidar Sensor Models","summary":"  Modeling perception sensors is key for simulation based testing of automated\ndriving functions. Beyond weather conditions themselves, sensors are also\nsubjected to object dependent environmental influences like tire spray caused\nby vehicles moving on wet pavement. In this work, a novel modeling approach for\nspray in lidar data is introduced. The model conforms to the Open Simulation\nInterface (OSI) standard and is based on the formation of detection clusters\nwithin a spray plume. The detections are rendered with a simple custom ray\ncasting algorithm without the need of a fluid dynamics simulation or physics\nengine. The model is subsequently used to generate training data for object\ndetection algorithms. It is shown that the model helps to improve detection in\nreal-world spray scenarios significantly. Furthermore, a systematic real-world\ndata set is recorded and published for analysis, model calibration and\nvalidation of spray effects in active perception sensors. Experiments are\nconducted on a test track by driving over artificially watered pavement with\nvarying vehicle speeds, vehicle types and levels of pavement wetness. All\nmodels and data of this work are available open source.\n","authors":["Clemens Linnhoff","Dominik Scheuble","Mario Bijelic","Lukas Elster","Philipp Rosenberger","Werner Ritter","Dengxin Dai","Hermann Winner"],"pdf_url":"https://arxiv.org/pdf/2212.08558v1.pdf","comment":"Submitted to IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2110.03905v3","updated":"2022-12-16T15:45:18Z","published":"2021-10-08T05:57:30Z","title":"COVID-19 Monitoring System using Social Distancing and Face Mask\n  Detection on Surveillance video datasets","summary":"  In the current times, the fear and danger of COVID-19 virus still stands\nlarge. Manual monitoring of social distancing norms is impractical with a large\npopulation moving about and with insufficient task force and resources to\nadminister them. There is a need for a lightweight, robust and 24X7\nvideo-monitoring system that automates this process. This paper proposes a\ncomprehensive and effective solution to perform person detection, social\ndistancing violation detection, face detection and face mask classification\nusing object detection, clustering and Convolution Neural Network (CNN) based\nbinary classifier. For this, YOLOv3, Density-based spatial clustering of\napplications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and\nMobileNetV2 based binary classifier have been employed on surveillance video\ndatasets. This paper also provides a comparative study of different face\ndetection and face mask classification models. Finally, a video dataset\nlabelling method is proposed along with the labelled video dataset to\ncompensate for the lack of dataset in the community and is used for evaluation\nof the system. The system performance is evaluated in terms of accuracy, F1\nscore as well as the prediction time, which has to be low for practical\napplicability. The system performs with an accuracy of 91.2% and F1 score of\n90.79% on the labelled video dataset and has an average prediction time of 7.12\nseconds for 78 frames of a video.\n","authors":["Sahana Srinivasan","Rujula Singh R","Ruchita R Biradar","Revathi SA"],"pdf_url":"https://arxiv.org/pdf/2110.03905v3.pdf","comment":"I, Rujula Singh R, would like to apologize to the research community\n  for the confusion caused by the inconsistency in author lists between\n  multiple versions of this paper. I take full responsibility for this error\n  and will be more diligent in the future to ensure the accuracy and\n  consistency of our research publications"},{"id":"http://arxiv.org/abs/2212.08536v1","updated":"2022-12-16T15:35:34Z","published":"2022-12-16T15:35:34Z","title":"Detection-aware multi-object tracking evaluation","summary":"  How would you fairly evaluate two multi-object tracking algorithms (i.e.\ntrackers), each one employing a different object detector? Detectors keep\nimproving, thus trackers can make less effort to estimate object states over\ntime. Is it then fair to compare a new tracker employing a new detector with\nanother tracker using an old detector? In this paper, we propose a novel\nperformance measure, named Tracking Effort Measure (TEM), to evaluate trackers\nthat use different detectors. TEM estimates the improvement that the tracker\ndoes with respect to its input data (i.e. detections) at frame level\n(intra-frame complexity) and sequence level (inter-frame complexity). We\nevaluate TEM over well-known datasets, four trackers and eight detection sets.\nResults show that, unlike conventional tracking evaluation measures, TEM can\nquantify the effort done by the tracker with a reduced correlation on the input\ndetections. Its implementation is publicly available online at\nhttps://github.com/vpulab/MOT-evaluation.\n","authors":["Juan C. SanMiguel","Jorge Muñoz","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2212.08536v1.pdf","comment":"This paper was accepted at IEEE International Conference on Advanced\n  Video and Signal Based Surveillance (AVSS)"},{"id":"http://arxiv.org/abs/2212.08526v1","updated":"2022-12-16T15:15:34Z","published":"2022-12-16T15:15:34Z","title":"Unifying Human Motion Synthesis and Style Transfer with Denoising\n  Diffusion Probabilistic Models","summary":"  Generating realistic motions for digital humans is a core but challenging\npart of computer animations and games, as human motions are both diverse in\ncontent and rich in styles. While the latest deep learning approaches have made\nsignificant advancements in this domain, they mostly consider motion synthesis\nand style manipulation as two separate problems. This is mainly due to the\nchallenge of learning both motion contents that account for the inter-class\nbehaviour and styles that account for the intra-class behaviour effectively in\na common representation. To tackle this challenge, we propose a denoising\ndiffusion probabilistic model solution for styled motion synthesis. As\ndiffusion models have a high capacity brought by the injection of\nstochasticity, we can represent both inter-class motion content and intra-class\nstyle behaviour in the same latent. This results in an integrated, end-to-end\ntrained pipeline that facilitates the generation of optimal motion and\nexploration of content-style coupled latent space. To achieve high-quality\nresults, we design a multi-task architecture of diffusion model that\nstrategically generates aspects of human motions for local guidance. We also\ndesign adversarial and physical regulations for global guidance. We demonstrate\nsuperior performance with quantitative and qualitative results and validate the\neffectiveness of our multi-task architecture.\n","authors":["Ziyi Chang","Edmund J. C. Findlay","Haozheng Zhang","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2212.08526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16161v2","updated":"2022-12-16T14:52:08Z","published":"2022-11-29T12:44:45Z","title":"Artifact Removal in Histopathology Images","summary":"  In the clinical setting of histopathology, whole-slide image (WSI) artifacts\nfrequently arise, distorting regions of interest, and having a pernicious\nimpact on WSI analysis. Image-to-image translation networks such as CycleGANs\nare in principle capable of learning an artifact removal function from unpaired\ndata. However, we identify a surjection problem with artifact removal, and\npropose an weakly-supervised extension to CycleGAN to address this. We assemble\na pan-cancer dataset comprising artifact and clean tiles from the TCGA\ndatabase. Promising results highlight the soundness of our method.\n","authors":["Cameron Dahan","Stergios Christodoulidis","Maria Vakalopoulou","Joseph Boyd"],"pdf_url":"https://arxiv.org/pdf/2211.16161v2.pdf","comment":"Corrected typos, small modification of Figure 1 (+ reflected in\n  Section 2.1), results unchanged"},{"id":"http://arxiv.org/abs/2212.08511v1","updated":"2022-12-16T14:49:27Z","published":"2022-12-16T14:49:27Z","title":"Road Detection in Snowy Forest Environment using RGB Camera","summary":"  Automated driving technology has gained a lot of momentum in the last few\nyears. For the exploration field, navigation is the important key for\nautonomous operation. In difficult scenarios such as snowy environment, the\nroad is covered with snow and road detection is impossible in this situation\nusing only basic techniques. This paper introduces detection of snowy road in\nforest environment using RGB camera. The method combines noise filtering\ntechnique with morphological operation to classify the image component. By\nusing the assumption that all road is covered by snow and the snow part is\ndefined as road area. From the perspective image of road, the vanishing point\nof road is one of factor to scope the region of road. This vanishing point is\nfound with fitting triangle technique. The performance of algorithm is\nevaluated by two error value: False Negative Rate and False Positive Rate. The\nerror shows that the method has high efficiency for detect road with straight\nroad but low performance for curved road. This road region will be applied with\ndepth information from camera to detect for obstacle in the future work.\n","authors":["Sirawich Vachmanus","Takanori Emaru","Ankit A. Ravankar","Yukinori Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2212.08511v1.pdf","comment":"5 pages, 9 figures, conference proceeding"},{"id":"http://arxiv.org/abs/2212.08506v1","updated":"2022-12-16T14:38:30Z","published":"2022-12-16T14:38:30Z","title":"Weakly Supervised Video Anomaly Detection Based on Cross-Batch\n  Clustering Guidance","summary":"  Weakly supervised video anomaly detection (WSVAD) is a challenging task since\nonly video-level labels are available for training. In previous studies, the\ndiscriminative power of the learned features is not strong enough, and the data\nimbalance resulting from the mini-batch training strategy is ignored. To\naddress these two issues, we propose a novel WSVAD method based on cross-batch\nclustering guidance. To enhance the discriminative power of features, we\npropose a batch clustering based loss to encourage a clustering branch to\ngenerate distinct normal and abnormal clusters based on a batch of data.\nMeanwhile, we design a cross-batch learning strategy by introducing clustering\nresults from previous mini-batches to reduce the impact of data imbalance. In\naddition, we propose to generate more accurate segment-level anomaly scores\nbased on batch clustering guidance further improving the performance of WSVAD.\nExtensive experiments on two public datasets demonstrate the effectiveness of\nour approach.\n","authors":["Congqi Cao","Xin Zhang","Shizhou Zhang","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.08506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10521v2","updated":"2022-12-16T14:05:11Z","published":"2022-08-22T18:01:49Z","title":"Estimation Contracts for Outlier-Robust Geometric Perception","summary":"  Outlier-robust estimation is a fundamental problem and has been extensively\ninvestigated by statisticians and practitioners. The last few years have seen a\nconvergence across research fields towards \"algorithmic robust statistics\",\nwhich focuses on developing tractable outlier-robust techniques for\nhigh-dimensional estimation problems. Despite this convergence, research\nefforts across fields have been mostly disconnected from one another. This\nmonograph bridges recent work on certifiable outlier-robust estimation for\ngeometric perception in robotics and computer vision with parallel work in\nrobust statistics. In particular, we adapt and extend recent results on robust\nlinear regression (applicable to the low-outlier regime with << 50% outliers)\nand list-decodable regression (applicable to the high-outlier regime with >>\n50% outliers) to the setup commonly found in robotics and vision, where (i)\nvariables (e.g., rotations, poses) belong to a non-convex domain, (ii)\nmeasurements are vector-valued, and (iii) the number of outliers is not known a\npriori. The emphasis here is on performance guarantees: rather than proposing\nradically new algorithms, we provide conditions on the input measurements under\nwhich modern estimation algorithms (possibly after small modifications) are\nguaranteed to recover an estimate close to the ground truth in the presence of\noutliers. These conditions are what we call an \"estimation contract\". Besides\nthe proposed extensions of existing results, we believe the main contributions\nof this monograph are (i) to unify parallel research lines by pointing out\ncommonalities and differences, (ii) to introduce advanced material (e.g.,\nsum-of-squares proofs) in an accessible and self-contained presentation for the\npractitioner, and (iii) to point out a few immediate opportunities and open\nquestions in outlier-robust geometric perception.\n","authors":["Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2208.10521v2.pdf","comment":"95 pages, 12 figures"},{"id":"http://arxiv.org/abs/2212.08490v1","updated":"2022-12-16T14:02:12Z","published":"2022-12-16T14:02:12Z","title":"LEDCNet: A Lightweight and Efficient Semantic Segmentation Algorithm\n  Using Dual Context Module for Extracting Ground Objects from UAV Aerial\n  Remote Sensing Images","summary":"  Semantic segmentation of UAV aerial remote sensing images provides a more\nefficient and convenient surveying and mapping method for traditional surveying\nand mapping. In order to make the model lightweight and improve a certain\naccuracy, this research developed a new lightweight and efficient network for\nthe extraction of ground features from UAV aerial remote sensing images, called\nLDMCNet. Meanwhile, this research develops a powerful lightweight backbone\nnetwork for the proposed semantic segmentation model. It is called LDCNet, and\nit is hoped that it can become the backbone network of a new generation of\nlightweight semantic segmentation algorithms. The proposed model uses dual\nmulti-scale context modules, namely the Atrous Space Pyramid Pooling module\n(ASPP) and the Object Context Representation module (OCR). In addition, this\nresearch constructs a private dataset for semantic segmentation of aerial\nremote sensing images from drones. This data set contains 2431 training sets,\n945 validation sets, and 475 test sets. The proposed model performs well on\nthis dataset, with only 1.4M parameters and 5.48G floating-point operations\n(FLOPs), achieving an average intersection-over-union ratio (mIoU) of 71.12%.\n7.88% higher than the baseline model. In order to verify the effectiveness of\nthe proposed model, training on the public datasets \"LoveDA\" and \"CITY-OSM\"\nalso achieved excellent results, achieving mIoU of 65.27% and 74.39%,\nrespectively.\n","authors":["Xiaoxiang Han","Yiman Liu","Gang Liu","Qiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08490v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2212.08479v1","updated":"2022-12-16T13:46:17Z","published":"2022-12-16T13:46:17Z","title":"Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR\n  Imaging","summary":"  In this work, we propose a novel image reconstruction framework that directly\nlearns a neural implicit representation in k-space for ECG-triggered\nnon-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods\nbin acquired data from neighboring time points to reconstruct one phase of the\ncardiac motion, our framework allows for a continuous, binning-free, and\nsubject-specific k-space representation.We assign a unique coordinate that\nconsists of time, coil index, and frequency domain location to each sampled\nk-space point. We then learn the subject-specific mapping from these unique\ncoordinates to k-space intensities using a multi-layer perceptron with\nfrequency domain regularization. During inference, we obtain a complete k-space\nfor Cartesian coordinates and an arbitrary temporal resolution. A simple\ninverse Fourier transform recovers the image, eliminating the need for density\ncompensation and costly non-uniform Fourier transforms for non-Cartesian data.\nThis novel imaging framework was tested on 42 radially sampled datasets from 6\nsubjects. The proposed method outperforms other techniques qualitatively and\nquantitatively using data from four and one heartbeat(s) and 30 cardiac phases.\nOur results for one heartbeat reconstruction of 50 cardiac phases show improved\nartifact removal and spatio-temporal resolution, leveraging the potential for\nreal-time CMR.\n","authors":["Wenqi Huang","Hongwei Li","Gastao Cruz","Jiazhen Pan","Daniel Rueckert","Kerstin Hammernik"],"pdf_url":"https://arxiv.org/pdf/2212.08479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02541v3","updated":"2022-12-16T13:42:24Z","published":"2022-08-04T09:17:30Z","title":"MVSFormer: Multi-View Stereo by Learning Robust Image Features and\n  Temperature-based Depth","summary":"  Feature representation learning is the key recipe for learning-based\nMulti-View Stereo (MVS). As the common feature extractor of learning-based MVS,\nvanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature\nrepresentations for reflection and texture-less areas, which limits the\ngeneralization of MVS. Even FPNs worked with pre-trained Convolutional Neural\nNetworks (CNNs) fail to tackle these issues. On the other hand, Vision\nTransformers (ViTs) have achieved prominent success in many 2D vision tasks.\nThus we ask whether ViTs can facilitate feature learning in MVS? In this paper,\nwe propose a pre-trained ViT enhanced MVS network called MVSFormer, which can\nlearn more reliable feature representations benefited by informative priors\nfrom ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention\nmechanisms can achieve prominent improvement based on FPNs. Besides, the\nalternative MVSFormer with frozen ViT weights is further proposed. This largely\nalleviates the training cost with competitive performance strengthened by the\nattention map from the self-distillation pre-training. MVSFormer can be\ngeneralized to various input resolutions with efficient multi-scale training\nstrengthened by gradient accumulation. Moreover, we discuss the merits and\ndrawbacks of classification and regression-based MVS methods, and further\npropose to unify them with a temperature-based strategy. MVSFormer achieves\nstate-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks\nas Top-1 on both intermediate and advanced sets of the highly competitive\nTanks-and-Temples leaderboard.\n","authors":["Chenjie Cao","Xinlin Ren","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2208.02541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08472v1","updated":"2022-12-16T13:37:23Z","published":"2022-12-16T13:37:23Z","title":"One-Stage Cascade Refinement Networks for Infrared Small Target\n  Detection","summary":"  Single-frame InfraRed Small Target (SIRST) detection has been a challenging\ntask due to a lack of inherent characteristics, imprecise bounding box\nregression, a scarcity of real-world datasets, and sensitive localization\nevaluation. In this paper, we propose a comprehensive solution to these\nchallenges. First, we find that the existing anchor-free label assignment\nmethod is prone to mislabeling small targets as background, leading to their\nomission by detectors. To overcome this issue, we propose an all-scale\npseudo-box-based label assignment scheme that relaxes the constraints on scale\nand decouples the spatial assignment from the size of the ground-truth target.\nSecond, motivated by the structured prior of feature pyramids, we introduce the\none-stage cascade refinement network (OSCAR), which uses the high-level head as\nsoft proposals for the low-level refinement head. This allows OSCAR to process\nthe same target in a cascade coarse-to-fine manner. Finally, we present a new\nresearch benchmark for infrared small target detection, consisting of the\nSIRST-V2 dataset of real-world, high-resolution single-frame targets, the\nnormalized contrast evaluation metric, and the DeepInfrared toolkit for\ndetection. We conduct extensive ablation studies to evaluate the components of\nOSCAR and compare its performance to state-of-the-art model-driven and\ndata-driven methods on the SIRST-V2 benchmark. Our results demonstrate that a\ntop-down cascade refinement framework can improve the accuracy of infrared\nsmall target detection without sacrificing efficiency. The DeepInfrared\ntoolkit, dataset, and trained models are available at\nhttps://github.com/YimianDai/open-deepinfrared to advance further research in\nthis field.\n","authors":["Yimian Dai","Xiang Li","Fei Zhou","Yulei Qian","Yaohong Chen","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08472v1.pdf","comment":"Submitted to TGRS, Major Revision"},{"id":"http://arxiv.org/abs/2212.05729v2","updated":"2022-12-16T13:33:27Z","published":"2022-12-12T06:38:35Z","title":"ROIFormer: Semantic-Aware Region of Interest Transformer for Efficient\n  Self-Supervised Monocular Depth Estimation","summary":"  The exploration of mutual-benefit cross-domains has shown great potential\ntoward accurate self-supervised depth estimation. In this work, we revisit\nfeature fusion between depth and semantic information and propose an efficient\nlocal adaptive attention method for geometric aware representation enhancement.\nInstead of building global connections or deforming attention across the\nfeature space without restraint, we bound the spatial interaction within a\nlearnable region of interest. In particular, we leverage geometric cues from\nsemantic information to learn local adaptive bounding boxes to guide\nunsupervised feature aggregation. The local areas preclude most irrelevant\nreference points from attention space, yielding more selective feature learning\nand faster convergence. We naturally extend the paradigm into a multi-head and\nhierarchic way to enable the information distillation in different semantic\nlevels and improve the feature discriminative ability for fine-grained depth\nestimation. Extensive experiments on the KITTI dataset show that our proposed\nmethod establishes a new state-of-the-art in self-supervised monocular depth\nestimation task, demonstrating the effectiveness of our approach over former\nTransformer variants.\n","authors":["Daitao Xing","Jinglin Shen","Chiuman Ho","Anthony Tzes"],"pdf_url":"https://arxiv.org/pdf/2212.05729v2.pdf","comment":"9 Pages, AAAI 2023"},{"id":"http://arxiv.org/abs/2212.08464v1","updated":"2022-12-16T13:20:31Z","published":"2022-12-16T13:20:31Z","title":"Free-form 3D Scene Inpainting with Dual-stream GAN","summary":"  Nowadays, the need for user editing in a 3D scene has rapidly increased due\nto the development of AR and VR technology. However, the existing 3D scene\ncompletion task (and datasets) cannot suit the need because the missing regions\nin scenes are generated by the sensor limitation or object occlusion. Thus, we\npresent a novel task named free-form 3D scene inpainting. Unlike scenes in\nprevious 3D completion datasets preserving most of the main structures and\nhints of detailed shapes around missing regions, the proposed inpainting\ndataset, FF-Matterport, contains large and diverse missing regions formed by\nour free-form 3D mask generation algorithm that can mimic human drawing\ntrajectories in 3D space. Moreover, prior 3D completion methods cannot perform\nwell on this challenging yet practical task, simply interpolating nearby\ngeometry and color context. Thus, a tailored dual-stream GAN method is\nproposed. First, our dual-stream generator, fusing both geometry and color\ninformation, produces distinct semantic boundaries and solves the interpolation\nissue. To further enhance the details, our lightweight dual-stream\ndiscriminator regularizes the geometry and color edges of the predicted scenes\nto be realistic and sharp. We conducted experiments with the proposed\nFF-Matterport dataset. Qualitative and quantitative results validate the\nsuperiority of our approach over existing scene completion methods and the\nefficacy of all proposed components.\n","authors":["Ru-Fen Jheng","Tsung-Han Wu","Jia-Fong Yeh","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2212.08464v1.pdf","comment":"BMVC 2022"},{"id":"http://arxiv.org/abs/2212.08448v1","updated":"2022-12-16T12:46:21Z","published":"2022-12-16T12:46:21Z","title":"From Xception to NEXcepTion: New Design Decisions and Neural\n  Architecture Search","summary":"  In this paper, we present a modified Xception architecture, the NEXcepTion\nnetwork. Our network has significantly better performance than the original\nXception, achieving top-1 accuracy of 81.5% on the ImageNet validation dataset\n(an improvement of 2.5%) as well as a 28% higher throughput. Another variant of\nour model, NEXcepTion-TP, reaches 81.8% top-1 accuracy, similar to ConvNeXt\n(82.1%), while having a 27% higher throughput. Our model is the result of\napplying improved training procedures and new design decisions combined with an\napplication of Neural Architecture Search (NAS) on a smaller dataset. These\nfindings call for revisiting older architectures and reassessing their\npotential when combined with the latest enhancements.\n","authors":["Hadar Shavit","Filip Jatelnicki","Pol Mor-Puigventós","Wojtek Kowalczyk"],"pdf_url":"https://arxiv.org/pdf/2212.08448v1.pdf","comment":"Accepted at ICPRAM 2023"},{"id":"http://arxiv.org/abs/2212.08423v1","updated":"2022-12-16T11:52:15Z","published":"2022-12-16T11:52:15Z","title":"Context Label Learning: Improving Background Class Representations in\n  Semantic Segmentation","summary":"  Background samples provide key contextual information for segmenting regions\nof interest (ROIs). However, they always cover a diverse set of structures,\ncausing difficulties for the segmentation model to learn good decision\nboundaries with high sensitivity and precision. The issue concerns the highly\nheterogeneous nature of the background class, resulting in multi-modal\ndistributions. Empirically, we find that neural networks trained with\nheterogeneous background struggle to map the corresponding contextual samples\nto compact clusters in feature space. As a result, the distribution over\nbackground logit activations may shift across the decision boundary, leading to\nsystematic over-segmentation across different datasets and tasks. In this\nstudy, we propose context label learning (CoLab) to improve the context\nrepresentations by decomposing the background class into several subclasses.\nSpecifically, we train an auxiliary network as a task generator, along with the\nprimary segmentation model, to automatically generate context labels that\npositively affect the ROI segmentation accuracy. Extensive experiments are\nconducted on several challenging segmentation tasks and datasets. The results\ndemonstrate that CoLab can guide the segmentation model to map the logits of\nbackground samples away from the decision boundary, resulting in significantly\nimproved segmentation accuracy. Code is available.\n","authors":["Zeju Li","Konstantinos Kamnitsas","Cheng Ouyang","Chen Chen","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2212.08423v1.pdf","comment":"Provisionally accepted to IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2212.08420v1","updated":"2022-12-16T11:44:01Z","published":"2022-12-16T11:44:01Z","title":"Fake it till you make it: Learning(s) from a synthetic ImageNet clone","summary":"  Recent large-scale image generation models such as Stable Diffusion have\nexhibited an impressive ability to generate fairly realistic images starting\nfrom a very simple text prompt. Could such models render real images obsolete\nfor training image prediction models? In this paper, we answer part of this\nprovocative question by questioning the need for real images when training\nmodels for ImageNet classification. More precisely, provided only with the\nclass names that have been used to build the dataset, we explore the ability of\nStable Diffusion to generate synthetic clones of ImageNet and measure how\nuseful they are for training classification models from scratch. We show that\nwith minimal and class-agnostic prompt engineering those ImageNet clones we\ndenote as ImageNet-SD are able to close a large part of the gap between models\nproduced by synthetic images and models trained with real images for the\nseveral standard classification benchmarks that we consider in this study. More\nimportantly, we show that models trained on synthetic images exhibit strong\ngeneralization properties and perform on par with models trained on real data.\n","authors":["Mert Bulent Sariyildiz","Karteek Alahari","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2212.08420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12592v2","updated":"2022-12-16T11:33:17Z","published":"2022-01-29T13:58:03Z","title":"Exact Decomposition of Joint Low Rankness and Local Smoothness Plus\n  Sparse Matrices","summary":"  It is known that the decomposition in low-rank and sparse matrices\n(\\textbf{L+S} for short) can be achieved by several Robust PCA techniques.\nBesides the low rankness, the local smoothness (\\textbf{LSS}) is a vitally\nessential prior for many real-world matrix data such as hyperspectral images\nand surveillance videos, which makes such matrices have low-rankness and local\nsmoothness properties at the same time. This poses an interesting question: Can\nwe make a matrix decomposition in terms of \\textbf{L\\&LSS +S } form exactly? To\naddress this issue, we propose in this paper a new RPCA model based on\nthree-dimensional correlated total variation regularization (3DCTV-RPCA for\nshort) by fully exploiting and encoding the prior expression underlying such\njoint low-rank and local smoothness matrices. Specifically, using a\nmodification of Golfing scheme, we prove that under some mild assumptions, the\nproposed 3DCTV-RPCA model can decompose both components exactly, which should\nbe the first theoretical guarantee among all such related methods combining low\nrankness and local smoothness. In addition, by utilizing Fast Fourier Transform\n(FFT), we propose an efficient ADMM algorithm with a solid convergence\nguarantee for solving the resulting optimization problem. Finally, a series of\nexperiments on both simulations and real applications are carried out to\ndemonstrate the general validity of the proposed 3DCTV-RPCA model.\n","authors":["Jiangjun Peng","Yao Wang","Hongying Zhang","Jianjun Wang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2201.12592v2.pdf","comment":"15 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2212.08415v1","updated":"2022-12-16T11:27:50Z","published":"2022-12-16T11:27:50Z","title":"Person Detection Using an Ultra Low-resolution Thermal Imager on a\n  Low-cost MCU","summary":"  Detecting persons in images or video with neural networks is a well-studied\nsubject in literature. However, such works usually assume the availability of a\ncamera of decent resolution and a high-performance processor or GPU to run the\ndetection algorithm, which significantly increases the cost of a complete\ndetection system. However, many applications require low-cost solutions,\ncomposed of cheap sensors and simple microcontrollers. In this paper, we\ndemonstrate that even on such hardware we are not condemned to simple classic\nimage processing techniques. We propose a novel ultra-lightweight CNN-based\nperson detector that processes thermal video from a low-cost 32x24 pixel static\nimager. Trained and compressed on our own recorded dataset, our model achieves\nup to 91.62% accuracy (F1-score), has less than 10k parameters, and runs as\nfast as 87ms and 46ms on low-cost microcontrollers STM32F407 and STM32F746,\nrespectively.\n","authors":["Maarten Vandersteegen","Wouter Reusen","Kristof Van Beeck","Toon Goedemé"],"pdf_url":"https://arxiv.org/pdf/2212.08415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08414v1","updated":"2022-12-16T11:27:44Z","published":"2022-12-16T11:27:44Z","title":"Deep Learning Methods for Calibrated Photometric Stereo and Beyond: A\n  Survey","summary":"  Photometric stereo recovers the surface normals of an object from multiple\nimages with varying shading cues, i.e., modeling the relationship between\nsurface orientation and intensity at each pixel. Photometric stereo prevails in\nsuperior per-pixel resolution and fine reconstruction details. However, it is a\ncomplicated problem because of the non-linear relationship caused by\nnon-Lambertian surface reflectance. Recently, various deep learning methods\nhave shown a powerful ability in the context of photometric stereo against\nnon-Lambertian surfaces. This paper provides a comprehensive review of existing\ndeep learning-based calibrated photometric stereo methods. We first analyze\nthese methods from different perspectives, including input processing,\nsupervision, and network architecture. We summarize the performance of deep\nlearning photometric stereo models on the most widely-used benchmark data set.\nThis demonstrates the advanced performance of deep learning-based photometric\nstereo methods. Finally, we give suggestions and propose future research trends\nbased on the limitations of existing models.\n","authors":["Yakun Ju","Kin-Man Lam","Wuyuan Xie","Huiyu Zhou","Junyu Dong","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2212.08414v1.pdf","comment":"16 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2207.12080v3","updated":"2022-12-16T10:47:34Z","published":"2022-07-25T11:57:01Z","title":"Intention-Conditioned Long-Term Human Egocentric Action Forecasting","summary":"  To anticipate how a human would act in the future, it is essential to\nunderstand the human intention since it guides the human towards a certain\ngoal. In this paper, we propose a hierarchical architecture which assumes a\nsequence of human action (low-level) can be driven from the human intention\n(high-level). Based on this, we deal with Long-Term Action Anticipation task in\negocentric videos. Our framework first extracts two level of human information\nover the N observed videos human actions through a Hierarchical Multi-task MLP\nMixer (H3M). Then, we condition the uncertainty of the future through an\nIntention-Conditioned Variational Auto-Encoder (I-CVAE) that generates K stable\npredictions of the next Z=20 actions that the observed human might perform. By\nleveraging human intention as high-level information, we claim that our model\nis able to anticipate more time-consistent actions in the long-term, thus\nimproving the results over baseline methods in EGO4D Challenge. This work\nranked first in both CVPR@2022 and ECVV@2022 EGO4D LTA Challenge by providing\nmore plausible anticipated sequences, improving the anticipation of nouns and\noverall actions. The code is available at\nhttps://github.com/Evm7/ego4dlta-icvae.\n","authors":["Esteve Valls Mascaro","Hyemin Ahn","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2207.12080v3.pdf","comment":"Validation report Winner of CVPR@2022 and ECCV@2022 EGO4D LTA\n  Challenge Accepted in IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV), 2023 More info\n  https://sites.google.com/view/estevevallsmascaro/publications/wacv2023"},{"id":"http://arxiv.org/abs/2212.08387v1","updated":"2022-12-16T10:21:29Z","published":"2022-12-16T10:21:29Z","title":"Traffic sign detection and recognition using event camera image\n  reconstruction","summary":"  This paper presents a method for detection and recognition of traffic signs\nbased on information extracted from an event camera. The solution used a\nFireNet deep convolutional neural network to reconstruct events into greyscale\nframes. Two YOLOv4 network models were trained, one based on greyscale images\nand the other on colour images. The best result was achieved for the model\ntrained on the basis of greyscale images, achieving an efficiency of 87.03%.\n","authors":["Kamil Jeziorek","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2212.08387v1.pdf","comment":"Paper accepted for publication in: Zeszyty Studenckiego Towarzystwa\n  Naukowego, 59. Hutnicza Konferencja Studenckich Kol Naukowych AGH,ISSN\n  1732-0925, 2022 nr 38, pp. 127-134. (original manuscript in Polish)"},{"id":"http://arxiv.org/abs/2212.08384v1","updated":"2022-12-16T10:16:42Z","published":"2022-12-16T10:16:42Z","title":"Fast-moving object counting with an event camera","summary":"  This paper proposes the use of an event camera as a component of a vision\nsystem that enables counting of fast-moving objects - in this case, falling\ncorn grains. These type of cameras transmit information about the change in\nbrightness of individual pixels and are characterised by low latency, no motion\nblur, correct operation in different lighting conditions, as well as very low\npower consumption. The proposed counting algorithm processes events in real\ntime. The operation of the solution was demonstrated on a stand consisting of a\nchute with a vibrating feeder, which allowed the number of grains falling to be\nadjusted. The objective of the control system with a PID controller was to\nmaintain a constant average number of falling objects. The proposed solution\nwas subjected to a series of tests to determine the correctness of the\ndeveloped method operation. On their basis, the validity of using an event\ncamera to count small, fast-moving objects and the associated wide range of\npotential industrial applications can be confirmed.\n","authors":["Kamil Bialik","Marcin Kowalczyk","Krzysztof Blachut","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2212.08384v1.pdf","comment":"Paper accepted for the Automation 2023 (7-9 March 2023, Warsaw,\n  Poland) conference and PAR journal (original manuscript in Polish)"},{"id":"http://arxiv.org/abs/2212.08380v1","updated":"2022-12-16T10:13:25Z","published":"2022-12-16T10:13:25Z","title":"Instance-specific Label Distribution Regularization for Learning with\n  Label Noise","summary":"  Modeling noise transition matrix is a kind of promising method for learning\nwith label noise. Based on the estimated noise transition matrix and the noisy\nposterior probabilities, the clean posterior probabilities, which are jointly\ncalled Label Distribution (LD) in this paper, can be calculated as the\nsupervision. To reliably estimate the noise transition matrix, some methods\nassume that anchor points are available during training. Nonetheless, if anchor\npoints are invalid, the noise transition matrix might be poorly learned,\nresulting in poor performance. Consequently, other methods treat reliable data\npoints, extracted from training data, as pseudo anchor points. However, from a\nstatistical point of view, the noise transition matrix can be inferred from\ndata with noisy labels under the clean-label-domination assumption. Therefore,\nwe aim to estimate the noise transition matrix without (pseudo) anchor points.\nThere is evidence showing that samples are more likely to be mislabeled as\nother similar class labels, which means the mislabeling probability is highly\ncorrelated with the inter-class correlation. Inspired by this observation, we\npropose an instance-specific Label Distribution Regularization (LDR), in which\nthe instance-specific LD is estimated as the supervision, to prevent DCNNs from\nmemorizing noisy labels. Specifically, we estimate the noisy posterior under\nthe supervision of noisy labels, and approximate the batch-level noise\ntransition matrix by estimating the inter-class correlation matrix with neither\nanchor points nor pseudo anchor points. Experimental results on two synthetic\nnoisy datasets and two real-world noisy datasets demonstrate that our LDR\noutperforms existing methods.\n","authors":["Zehui Liao","Shishuai Hu","Yutong Xie","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2212.08380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08378v1","updated":"2022-12-16T10:08:38Z","published":"2022-12-16T10:08:38Z","title":"Feature Dropout: Revisiting the Role of Augmentations in Contrastive\n  Learning","summary":"  What role do augmentations play in contrastive learning? Recent work suggests\nthat good augmentations are label-preserving with respect to a specific\ndownstream task. We complicate this picture by showing that label-destroying\naugmentations can be useful in the foundation model setting, where the goal is\nto learn diverse, general-purpose representations for multiple downstream\ntasks. We perform contrastive learning experiments on a range of image and\naudio datasets with multiple downstream tasks (e.g. for digits superimposed on\nphotographs, predicting the class of one vs. the other). We find that Viewmaker\nNetworks, a recently proposed model for learning augmentations for contrastive\nlearning, produce label-destroying augmentations that stochastically destroy\nfeatures needed for different downstream tasks. These augmentations are\ninterpretable (e.g. altering shapes, digits, or letters added to images) and\nsurprisingly often result in better performance compared to expert-designed\naugmentations, despite not preserving label information. To support our\nempirical results, we theoretically analyze a simple contrastive learning\nsetting with a linear model. In this setting, label-destroying augmentations\nare crucial for preventing one set of features from suppressing the learning of\nfeatures useful for another downstream task. Our results highlight the need for\nanalyzing the interaction between multiple downstream tasks when trying to\nexplain the success of foundation models.\n","authors":["Alex Tamkin","Margalit Glasgow","Xiluo He","Noah Goodman"],"pdf_url":"https://arxiv.org/pdf/2212.08378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08377v1","updated":"2022-12-16T10:05:31Z","published":"2022-12-16T10:05:31Z","title":"PointAvatar: Deformable Point-based Head Avatars from Videos","summary":"  The ability to create realistic, animatable and relightable head avatars from\ncasual video sequences would open up wide ranging applications in communication\nand entertainment. Current methods either build on explicit 3D morphable meshes\n(3DMM) or exploit neural implicit representations. The former are limited by\nfixed topology, while the latter are non-trivial to deform and inefficient to\nrender. Furthermore, existing approaches entangle lighting in the color\nestimation, thus they are limited in re-rendering the avatar in new\nenvironments. In contrast, we propose PointAvatar, a deformable point-based\nrepresentation that disentangles the source color into intrinsic albedo and\nnormal-dependent shading. We demonstrate that PointAvatar bridges the gap\nbetween existing mesh- and implicit representations, combining high-quality\ngeometry and appearance with topological flexibility, ease of deformation and\nrendering efficiency. We show that our method is able to generate animatable 3D\navatars using monocular videos from multiple sources including hand-held\nsmartphones, laptop webcams and internet videos, achieving state-of-the-art\nquality in challenging cases where previous methods fail, e.g., thin hair\nstrands, while being significantly more efficient in training than competing\nmethods.\n","authors":["Yufeng Zheng","Wang Yifan","Gordon Wetzstein","Michael J. Black","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2212.08377v1.pdf","comment":"Project page: https://zhengyuf.github.io/pointavatar/"},{"id":"http://arxiv.org/abs/2211.06108v2","updated":"2022-12-16T09:40:57Z","published":"2022-11-11T10:24:42Z","title":"RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object\n  Detection System","summary":"  In autonomous driving systems, LiDAR and radar play important roles in the\nperception of the surrounding environment.LiDAR provides accurate 3D spatial\nsensing information but cannot work in adverse weather like fog. On the other\nhand, the radar signal can be diffracted when encountering raindrops or mist\nparticles thanks to its wavelength, but it suffers from large noise. Recent\nstate-of-the-art works reveal that fusion of radar and LiDAR can lead to robust\ndetection in adverse weather. The existing works adopt convolutional neural\nnetwork architecture to extract features from each sensor data stream, then\nalign and aggregate the two branch features to predict object detection\nresults. However, these methods have low accuracy of bounding box estimations\ndue to a simple design of label assignment and fusion strategies. In this\npaper, we propose a bird's-eye view fusion learning-based anchor box-free\nobject detection system, which fuses the feature derived from the radar\nrange-azimuth heatmap and the LiDAR point cloud to estimate the possible\nobjects. Different label assignment strategies have been designed to facilitate\nthe consistency between the classification of foreground or background anchor\npoints and the corresponding bounding box regressions. In addition, the\nperformance of the proposed object detector is further enhanced by employing a\nnovel interactive transformer module. The superior performance of the proposed\nmethods in this paper has been demonstrated using the recently published Oxford\nradar robotCar dataset, showing that the average precision of our system\nsignificantly outperforms the best state-of-the-art method by 14.4% and 20.5%\nat IoU equals 0.8 in clear and foggy weather testing, respectively.\n","authors":["Yanlong Yang","Jianan Liu","Tao Huang","Qing-Long Han","Gang Ma","Bing Zhu"],"pdf_url":"https://arxiv.org/pdf/2211.06108v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.08365v1","updated":"2022-12-16T09:33:31Z","published":"2022-12-16T09:33:31Z","title":"Geometric Rectification of Creased Document Images based on Isometric\n  Mapping","summary":"  Geometric rectification of images of distorted documents finds wide\napplications in document digitization and Optical Character Recognition (OCR).\nAlthough smoothly curved deformations have been widely investigated by many\nworks, the most challenging distortions, e.g. complex creases and large\nfoldings, have not been studied in particular. The performance of existing\napproaches, when applied to largely creased or folded documents, is far from\nsatisfying, leaving substantial room for improvement. To tackle this task,\nknowledge about document rectification should be incorporated into the\ncomputation, among which the developability of 3D document models and\nparticular textural features in the images, such as straight lines, are the\nmost essential ones. For this purpose, we propose a general framework of\ndocument image rectification in which a computational isometric mapping model\nis utilized for expressing a 3D document model and its flattening in the plane.\nBased on this framework, both model developability and textural features are\nconsidered in the computation. The experiments and comparisons to the\nstate-of-the-art approaches demonstrated the effectiveness and outstanding\nperformance of the proposed method. Our method is also flexible in that the\nrectification results can be enhanced by any other methods that extract\nhigh-quality feature lines in the images.\n","authors":["Dong Luo","Pengbo Bo"],"pdf_url":"https://arxiv.org/pdf/2212.08365v1.pdf","comment":"23 pages,17 figures"},{"id":"http://arxiv.org/abs/2212.08363v1","updated":"2022-12-16T09:31:15Z","published":"2022-12-16T09:31:15Z","title":"Fast Learning of Dynamic Hand Gesture Recognition with Few-Shot Learning\n  Models","summary":"  We develop Few-Shot Learning models trained to recognize five or ten\ndifferent dynamic hand gestures, respectively, which are arbitrarily\ninterchangeable by providing the model with one, two, or five examples per hand\ngesture. All models were built in the Few-Shot Learning architecture of the\nRelation Network (RN), in which Long-Short-Term Memory cells form the backbone.\nThe models use hand reference points extracted from RGB-video sequences of the\nJester dataset which was modified to contain 190 different types of hand\ngestures. Result show accuracy of up to 88.8% for recognition of five and up to\n81.2% for ten dynamic hand gestures. The research also sheds light on the\npotential effort savings of using a Few-Shot Learning approach instead of a\ntraditional Deep Learning approach to detect dynamic hand gestures. Savings\nwere defined as the number of additional observations required when a Deep\nLearning model is trained on new hand gestures instead of a Few Shot Learning\nmodel. The difference with respect to the total number of observations required\nto achieve approximately the same accuracy indicates potential savings of up to\n630 observations for five and up to 1260 observations for ten hand gestures to\nbe recognized. Since labeling video recordings of hand gestures implies\nsignificant effort, these savings can be considered substantial.\n","authors":["Niels Schlüsener","Michael Bücker"],"pdf_url":"https://arxiv.org/pdf/2212.08363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.12570v3","updated":"2022-12-16T09:29:56Z","published":"2021-06-23T17:54:35Z","title":"Learning Multimodal VAEs through Mutual Supervision","summary":"  Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.\n","authors":["Tom Joy","Yuge Shi","Philip H. S. Torr","Tom Rainforth","Sebastian M. Schmon","N. Siddharth"],"pdf_url":"https://arxiv.org/pdf/2106.12570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10779v4","updated":"2022-12-16T09:04:36Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively.\n","authors":["Xunguang Wang","Yinqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08356v1","updated":"2022-12-16T09:02:01Z","published":"2022-12-16T09:02:01Z","title":"CD-TTA: Compound Domain Test-time Adaptation for Semantic Segmentation","summary":"  Test-time adaptation (TTA) has attracted significant attention due to its\npractical properties which enable the adaptation of a pre-trained model to a\nnew domain with only target dataset during the inference stage. Prior works on\nTTA assume that the target dataset comes from the same distribution and thus\nconstitutes a single homogeneous domain. In practice, however, the target\ndomain can contain multiple homogeneous domains which are sufficiently\ndistinctive from each other and those multiple domains might occur cyclically.\nOur preliminary investigation shows that domain-specific TTA outperforms\nvanilla TTA treating compound domain (CD) as a single one. However, domain\nlabels are not available for CD, which makes domain-specific TTA not\npracticable. To this end, we propose an online clustering algorithm for finding\npseudo-domain labels to obtain similar benefits as domain-specific\nconfiguration and accumulating knowledge of cyclic domains effectively.\nMoreover, we observe that there is a significant discrepancy in terms of\nprediction quality among samples, especially in the CD context. This further\nmotivates us to boost its performance with gradient denoising by considering\nthe image-wise similarity with the source distribution. Overall, the key\ncontribution of our work lies in proposing a highly significant new task\ncompound domain test-time adaptation (CD-TTA) on semantic segmentation as well\nas providing a strong baseline to facilitate future works to benchmark.\n","authors":["Junha Song","Kwanyong Park","Inkyu Shin","Sanghyun Woo","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2212.08356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08355v1","updated":"2022-12-16T09:01:57Z","published":"2022-12-16T09:01:57Z","title":"Learning Classifiers of Prototypes and Reciprocal Points for Universal\n  Domain Adaptation","summary":"  Universal Domain Adaptation aims to transfer the knowledge between the\ndatasets by handling two shifts: domain-shift and category-shift. The main\nchallenge is correctly distinguishing the unknown target samples while adapting\nthe distribution of known class knowledge from source to target. Most existing\nmethods approach this problem by first training the target adapted known\nclassifier and then relying on the single threshold to distinguish unknown\ntarget samples. However, this simple threshold-based approach prevents the\nmodel from considering the underlying complexities existing between the known\nand unknown samples in the high-dimensional feature space. In this paper, we\npropose a new approach in which we use two sets of feature points, namely dual\nClassifiers for Prototypes and Reciprocals (CPR). Our key idea is to associate\neach prototype with corresponding known class features while pushing the\nreciprocals apart from these prototypes to locate them in the potential unknown\nfeature space. The target samples are then classified as unknown if they fall\nnear any reciprocals at test time. To successfully train our framework, we\ncollect the partial, confident target samples that are classified as known or\nunknown through on our proposed multi-criteria selection. We then additionally\napply the entropy loss regularization to them. For further adaptation, we also\napply standard consistency regularization that matches the predictions of two\ndifferent views of the input to make more compact target feature space. We\nevaluate our proposal, CPR, on three standard benchmarks and achieve comparable\nor new state-of-the-art results. We also provide extensive ablation experiments\nto verify our main design choices in our framework.\n","authors":["Sungsu Hur","Inkyu Shin","Kwanyong Park","Sanghyun Woo","In So Kweon"],"pdf_url":"https://arxiv.org/pdf/2212.08355v1.pdf","comment":"Accepted at WACV 2023"},{"id":"http://arxiv.org/abs/2210.16810v3","updated":"2022-12-16T08:57:41Z","published":"2022-10-30T11:08:25Z","title":"SL3D: Self-supervised-Self-labeled 3D Recognition","summary":"  Deep learning has attained remarkable success in many 3D visual recognition\ntasks, including shape classification, object detection, and semantic\nsegmentation. However, many of these results rely on manually collecting\ndensely annotated real-world 3D data, which is highly time-consuming and\nexpensive to obtain, limiting the scalability of 3D recognition tasks. Thus, we\nstudy unsupervised 3D recognition and propose a Self-supervised-Self-Labeled 3D\nRecognition (SL3D) framework. SL3D simultaneously solves two coupled\nobjectives, i.e., clustering and learning feature representation to generate\npseudo-labeled data for unsupervised 3D recognition. SL3D is a generic\nframework and can be applied to solve different 3D recognition tasks, including\nclassification, object detection, and semantic segmentation. Extensive\nexperiments demonstrate its effectiveness. Code is available at\nhttps://github.com/fcendra/sl3d.\n","authors":["Fernando Julio Cendra","Lan Ma","Jiajun Shen","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2210.16810v3.pdf","comment":"This paper has already been accepted by Neural Information Processing\n  Systems (NeurIPS 2022) Workshop on Self-Supervised Learning: Theory and\n  Practice"},{"id":"http://arxiv.org/abs/2212.07648v2","updated":"2022-12-16T08:51:53Z","published":"2022-12-15T08:06:03Z","title":"Relightable Neural Human Assets from Multi-view Gradient Illuminations","summary":"  Human modeling and relighting are two fundamental problems in computer vision\nand graphics, where high-quality datasets can largely facilitate related\nresearch. However, most existing human datasets only provide multi-view human\nimages captured under the same illumination. Although valuable for modeling\ntasks, they are not readily used in relighting problems. To promote research in\nboth fields, in this paper, we present UltraStage, a new 3D human dataset that\ncontains more than 2K high-quality human assets captured under both multi-view\nand multi-illumination settings. Specifically, for each example, we provide 32\nsurrounding views illuminated with one white light and two gradient\nilluminations. In addition to regular multi-view images, gradient illuminations\nhelp recover detailed surface normal and spatially-varying material maps,\nenabling various relighting applications. Inspired by recent advances in neural\nrepresentation, we further interpret each example into a neural human asset\nwhich allows novel view synthesis under arbitrary lighting conditions. We show\nour neural human assets can achieve extremely high capture performance and are\ncapable of representing fine details such as facial wrinkles and cloth folds.\nWe also validate UltraStage in single image relighting tasks, training neural\nnetworks with virtual relighted data from neural assets and demonstrating\nrealistic rendering improvements over prior arts. UltraStage will be publicly\navailable to the community to stimulate significant future developments in\nvarious human modeling and rendering tasks.\n","authors":["Taotao Zhou","Kai He","Di Wu","Teng Xu","Qixuan Zhang","Kuixiang Shao","Wenzheng Chen","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2212.07648v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.08341v1","updated":"2022-12-16T08:35:21Z","published":"2022-12-16T08:35:21Z","title":"Adversarial Example Defense via Perturbation Grading Strategy","summary":"  Deep Neural Networks have been widely used in many fields. However, studies\nhave shown that DNNs are easily attacked by adversarial examples, which have\ntiny perturbations and greatly mislead the correct judgment of DNNs.\nFurthermore, even if malicious attackers cannot obtain all the underlying model\nparameters, they can use adversarial examples to attack various DNN-based task\nsystems. Researchers have proposed various defense methods to protect DNNs,\nsuch as reducing the aggressiveness of adversarial examples by preprocessing or\nimproving the robustness of the model by adding modules. However, some defense\nmethods are only effective for small-scale examples or small perturbations but\nhave limited defense effects for adversarial examples with large perturbations.\nThis paper assigns different defense strategies to adversarial perturbations of\ndifferent strengths by grading the perturbations on the input examples.\nExperimental results show that the proposed method effectively improves defense\nperformance. In addition, the proposed method does not modify any task model,\nwhich can be used as a preprocessing module, which significantly reduces the\ndeployment cost in practical applications.\n","authors":["Shaowei Zhu","Wanli Lyu","Bin Li","Zhaoxia Yin","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2212.08341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08340v1","updated":"2022-12-16T08:31:07Z","published":"2022-12-16T08:31:07Z","title":"Neural Enhanced Belief Propagation for Multiobject Tracking","summary":"  Algorithmic solutions for multi-object tracking (MOT) are a key enabler for\napplications in autonomous navigation and applied ocean sciences.\nState-of-the-art MOT methods fully rely on a statistical model and typically\nuse preprocessed sensor data as measurements. In particular, measurements are\nproduced by a detector that extracts potential object locations from the raw\nsensor data collected for a discrete time step. This preparatory processing\nstep reduces data flow and computational complexity but may result in a loss of\ninformation. State-of-the-art Bayesian MOT methods that are based on belief\npropagation (BP) systematically exploit graph structures of the statistical\nmodel to reduce computational complexity and improve scalability. However, as a\nfully model-based approach, BP can only provide suboptimal estimates when there\nis a mismatch between the statistical model and the true data-generating\nprocess. Existing BP-based MOT methods can further only make use of\npreprocessed measurements. In this paper, we introduce a variant of BP that\ncombines model-based with data-driven MOT. The proposed neural enhanced belief\npropagation (NEBP) method complements the statistical model of BP by\ninformation learned from raw sensor data. This approach conjectures that the\nlearned information can reduce model mismatch and thus improve data association\nand false alarm rejection. Our NEBP method improves tracking performance\ncompared to model-based methods. At the same time, it inherits the advantages\nof BP-based MOT, i.e., it scales only quadratically in the number of objects,\nand it can thus generate and maintain a large number of object tracks. We\nevaluate the performance of our NEBP approach for MOT on the nuScenes\nautonomous driving dataset and demonstrate that it has state-of-the-art\nperformance.\n","authors":["Mingchao Liang","Florian Meyer"],"pdf_url":"https://arxiv.org/pdf/2212.08340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08334v1","updated":"2022-12-16T08:22:55Z","published":"2022-12-16T08:22:55Z","title":"Lightweight integration of 3D features to improve 2D image segmentation","summary":"  Scene understanding is a major challenge of today's computer vision. Center\nto this task is image segmentation, since scenes are often provided as a set of\npictures. Nowadays, many such datasets also provide 3D geometry information\ngiven as a 3D point cloud acquired by a laser scanner or a depth camera. To\nexploit this geometric information, many current approaches rely on both a 2D\nloss and 3D loss, requiring not only 2D per pixel labels but also 3D per point\nlabels. However obtaining a 3D groundtruth is challenging, time-consuming and\nerror-prone. In this paper, we show that image segmentation can benefit from 3D\ngeometric information without requiring any 3D groundtruth, by training the\ngeometric feature extraction with a 2D segmentation loss in an end-to-end\nfashion. Our method starts by extracting a map of 3D features directly from the\npoint cloud by using a lightweight and simple 3D encoder neural network. The 3D\nfeature map is then used as an additional input to a classical image\nsegmentation network. During training, the 3D features extraction is optimized\nfor the segmentation task by back-propagation through the entire pipeline. Our\nmethod exhibits state-of-the-art performance with much lighter input dataset\nrequirements, since no 3D groundtruth is required.\n","authors":["Olivier Pradelle","Raphaelle Chaine","David Wendland","Julie Digne"],"pdf_url":"https://arxiv.org/pdf/2212.08334v1.pdf","comment":"main : 7 pages, 4 figures; supplementary : 4 pages, 3 figures;\n  submitted to CVIU"},{"id":"http://arxiv.org/abs/2212.08330v1","updated":"2022-12-16T08:14:04Z","published":"2022-12-16T08:14:04Z","title":"Convolution-enhanced Evolving Attention Networks","summary":"  Attention-based neural networks, such as Transformers, have become ubiquitous\nin numerous applications, including computer vision, natural language\nprocessing, and time-series analysis. In all kinds of attention networks, the\nattention maps are crucial as they encode semantic dependencies between input\ntokens. However, most existing attention networks perform modeling or reasoning\nbased on representations, wherein the attention maps of different layers are\nlearned separately without explicit interactions. In this paper, we propose a\nnovel and generic evolving attention mechanism, which directly models the\nevolution of inter-token relationships through a chain of residual\nconvolutional modules. The major motivations are twofold. On the one hand, the\nattention maps in different layers share transferable knowledge, thus adding a\nresidual connection can facilitate the information flow of inter-token\nrelationships across layers. On the other hand, there is naturally an\nevolutionary trend among attention maps at different abstraction levels, so it\nis beneficial to exploit a dedicated convolution-based module to capture this\nprocess. Equipped with the proposed mechanism, the convolution-enhanced\nevolving attention networks achieve superior performance in various\napplications, including time-series representation, natural language\nunderstanding, machine translation, and image classification. Especially on\ntime-series representation tasks, Evolving Attention-enhanced Dilated\nConvolutional (EA-DC-) Transformer outperforms state-of-the-art models\nsignificantly, achieving an average of 17% improvement compared to the best\nSOTA. To the best of our knowledge, this is the first work that explicitly\nmodels the layer-wise evolution of attention maps. Our implementation is\navailable at https://github.com/pkuyym/EvolvingAttention\n","authors":["Yujing Wang","Yaming Yang","Zhuo Li","Jiangang Bai","Mingliang Zhang","Xiangtai Li","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2212.08330v1.pdf","comment":"Extension of the previous work (arXiv:2102.12895). arXiv admin note:\n  text overlap with arXiv:2102.12895"},{"id":"http://arxiv.org/abs/2212.08328v1","updated":"2022-12-16T08:04:56Z","published":"2022-12-16T08:04:56Z","title":"MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance\n  Fields","summary":"  Hinged on the representation power of neural networks, neural radiance fields\n(NeRF) have recently emerged as one of the promising and widely applicable\nmethods for 3D object and scene representation. However, NeRF faces challenges\nin practical applications, such as large-scale scenes and edge devices with a\nlimited amount of memory, where data needs to be processed sequentially. Under\nsuch incremental learning scenarios, neural networks are known to suffer\ncatastrophic forgetting: easily forgetting previously seen data after training\nwith new data. We observe that previous incremental learning algorithms are\nlimited by either low performance or memory scalability issues. As such, we\ndevelop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).\nMEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve\nas a memory that provides the pixel RGB values, given rays as queries. Upon the\nmotivation, our framework learns which rays to query NeRF to extract previous\npixel values. The extracted pixel values are then used to train NeRF in a\nself-distillation manner to prevent catastrophic forgetting. As a result,\nMEIL-NeRF demonstrates constant memory consumption and competitive performance.\n","authors":["Jaeyoung Chung","Kanggeon Lee","Sungyong Baik","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2212.08328v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2212.08327v1","updated":"2022-12-16T08:00:54Z","published":"2022-12-16T08:00:54Z","title":"WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement","summary":"  Image enhancement is a technique that frequently utilized in digital image\nprocessing. In recent years, the popularity of learning-based techniques for\nenhancing the aesthetic performance of photographs has increased. However, the\nmajority of current works do not optimize an image from different frequency\ndomains and typically focus on either pixel-level or global-level enhancements.\nIn this paper, we propose a transformer-based model in the wavelet domain to\nrefine different frequency bands of an image. Our method focuses both on local\ndetails and high-level features for enhancement, which can generate superior\nresults. On the basis of comprehensive benchmark evaluations, our method\noutperforms the state-of-the-art methods.\n","authors":["Zinuo Li","Xuhang Chen","Chi-Man Pun","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.08327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.11291v2","updated":"2022-12-16T07:48:13Z","published":"2022-10-20T14:23:40Z","title":"Cyclical Self-Supervision for Semi-Supervised Ejection Fraction\n  Prediction from Echocardiogram Videos","summary":"  Left-ventricular ejection fraction (LVEF) is an important indicator of heart\nfailure. Existing methods for LVEF estimation from video require large amounts\nof annotated data to achieve high performance, e.g. using 10,030 labeled\nechocardiogram videos to achieve mean absolute error (MAE) of 4.10. Labeling\nthese videos is time-consuming however and limits potential downstream\napplications to other heart diseases. This paper presents the first\nsemi-supervised approach for LVEF prediction. Unlike general video prediction\ntasks, LVEF prediction is specifically related to changes in the left ventricle\n(LV) in echocardiogram videos. By incorporating knowledge learned from\npredicting LV segmentations into LVEF regression, we can provide additional\ncontext to the model for better predictions. To this end, we propose a novel\nCyclical Self-Supervision (CSS) method for learning video-based LV\nsegmentation, which is motivated by the observation that the heartbeat is a\ncyclical process with temporal repetition. Prediction masks from our\nsegmentation model can then be used as additional input for LVEF regression to\nprovide spatial context for the LV region. We also introduce teacher-student\ndistillation to distill the information from LV segmentation masks into an\nend-to-end LVEF regression model that only requires video inputs. Results show\nour method outperforms alternative semi-supervised methods and can achieve MAE\nof 4.17, which is competitive with state-of-the-art supervised performance,\nusing half the number of labels. Validation on an external dataset also shows\nimproved generalization ability from using our method. Our code is available at\nhttps://github.com/xmed-lab/CSS-SemiVideo.\n","authors":["Weihang Dai","Xiaomeng Li","Xinpeng Ding","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2210.11291v2.pdf","comment":"Accepted in IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2212.08320v1","updated":"2022-12-16T07:46:53Z","published":"2022-12-16T07:46:53Z","title":"Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image\n  Transformers Help 3D Representation Learning?","summary":"  The success of deep learning heavily relies on large-scale data with\ncomprehensive labels, which is more expensive and time-consuming to fetch in 3D\ncompared to 2D images or natural languages. This promotes the potential of\nutilizing models pretrained with data more than 3D as teachers for cross-modal\nknowledge transferring. In this paper, we revisit masked modeling in a unified\nfashion of knowledge distillation, and we show that foundational Transformers\npretrained with 2D images or natural languages can help self-supervised 3D\nrepresentation learning through training Autoencoders as Cross-Modal Teachers\n(ACT). The pretrained Transformers are transferred as cross-modal 3D teachers\nusing discrete variational autoencoding self-supervision, during which the\nTransformers are frozen with prompt tuning for better knowledge inheritance.\nThe latent features encoded by the 3D teachers are used as the target of masked\npoint modeling, wherein the dark knowledge is distilled to the 3D Transformer\nstudents as foundational geometry understanding. Our ACT pretrained 3D learner\nachieves state-of-the-art generalization capacity across various downstream\nbenchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes will be\nreleased at https://github.com/RunpeiDong/ACT.\n","authors":["Runpei Dong","Zekun Qi","Linfeng Zhang","Junbo Zhang","Jianjian Sun","Zheng Ge","Li Yi","Kaisheng Ma"],"pdf_url":"https://arxiv.org/pdf/2212.08320v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2205.04948v2","updated":"2022-12-16T07:22:56Z","published":"2022-05-10T15:03:00Z","title":"Transformer-based Cross-Modal Recipe Embeddings with Large Batch\n  Training","summary":"  In this paper, we present a cross-modal recipe retrieval framework,\nTransformer-based Network for Large Batch Training (TNLBT), which is inspired\nby ACME~(Adversarial Cross-Modal Embedding) and H-T~(Hierarchical Transformer).\nTNLBT aims to accomplish retrieval tasks while generating images from recipe\nembeddings. We apply the Hierarchical Transformer-based recipe text encoder,\nthe Vision Transformer~(ViT)-based recipe image encoder, and an adversarial\nnetwork architecture to enable better cross-modal embedding learning for recipe\ntexts and images. In addition, we use self-supervised learning to exploit the\nrich information in the recipe texts having no corresponding images. Since\ncontrastive learning could benefit from a larger batch size according to the\nrecent literature on self-supervised learning, we adopt a large batch size\nduring training and have validated its effectiveness. In the experiments, the\nproposed framework significantly outperformed the current state-of-the-art\nframeworks in both cross-modal recipe retrieval and image generation tasks on\nthe benchmark Recipe1M. This is the first work which confirmed the\neffectiveness of large batch training on cross-modal recipe embeddings.\n","authors":["Jing Yang","Junwen Chen","Keiji Yanai"],"pdf_url":"https://arxiv.org/pdf/2205.04948v2.pdf","comment":"Accepted at MMM2023"},{"id":"http://arxiv.org/abs/2212.08311v1","updated":"2022-12-16T07:20:28Z","published":"2022-12-16T07:20:28Z","title":"Can We Find Strong Lottery Tickets in Generative Models?","summary":"  Yes. In this paper, we investigate strong lottery tickets in generative\nmodels, the subnetworks that achieve good generative performance without any\nweight update. Neural network pruning is considered the main cornerstone of\nmodel compression for reducing the costs of computation and memory.\nUnfortunately, pruning a generative model has not been extensively explored,\nand all existing pruning algorithms suffer from excessive weight-training\ncosts, performance degradation, limited generalizability, or complicated\ntraining. To address these problems, we propose to find a strong lottery ticket\nvia moment-matching scores. Our experimental results show that the discovered\nsubnetwork can perform similarly or better than the trained dense model even\nwhen only 10% of the weights remain. To the best of our knowledge, we are the\nfirst to show the existence of strong lottery tickets in generative models and\nprovide an algorithm to find it stably. Our code and supplementary materials\nare publicly available.\n","authors":["Sangyeop Yeo","Yoojin Jang","Jy-yong Sohn","Dongyoon Han","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2212.08311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12268v2","updated":"2022-12-16T06:43:23Z","published":"2022-11-22T13:37:34Z","title":"Out-of-Candidate Rectification for Weakly Supervised Semantic\n  Segmentation","summary":"  Weakly supervised semantic segmentation is typically inspired by class\nactivation maps, which serve as pseudo masks with class-discriminative regions\nhighlighted. Although tremendous efforts have been made to recall precise and\ncomplete locations for each class, existing methods still commonly suffer from\nthe unsolicited Out-of-Candidate (OC) error predictions that not belongs to the\nlabel candidates, which could be avoidable since the contradiction with\nimage-level class tags is easy to be detected. In this paper, we develop a\ngroup ranking-based Out-of-Candidate Rectification (OCR) mechanism in a\nplug-and-play fashion. Firstly, we adaptively split the semantic categories\ninto In-Candidate (IC) and OC groups for each OC pixel according to their prior\nannotation correlation and posterior prediction correlation. Then, we derive a\ndifferentiable rectification loss to force OC pixels to shift to the IC group.\nIncorporating our OCR with seminal baselines (e.g., AffinityNet, SEAM,\nMCTformer), we can achieve remarkable performance gains on both Pascal VOC\n(+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with\nnegligible extra training overhead, which justifies the effectiveness and\ngenerality of our OCR.\n","authors":["Zesen Cheng","Pengchong Qiao","Kehan Li","Siheng Li","Pengxu Wei","Xiangyang Ji","Li Yuan","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2211.12268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.12194v3","updated":"2022-12-16T06:41:05Z","published":"2022-07-25T13:33:53Z","title":"Domain Decorrelation with Potential Energy Ranking","summary":"  Machine learning systems, especially the methods based on deep learning,\nenjoy great success in modern computer vision tasks under experimental\nsettings. Generally, these classic deep learning methods are built on the\n\\emph{i.i.d.} assumption, supposing the training and test data are drawn from a\nsimilar distribution independently and identically. However, the aforementioned\n\\emph{i.i.d.} assumption is in general unavailable in the real-world scenario,\nand as a result, leads to sharp performance decay of deep learning algorithms.\nBehind this, domain shift is one of the primary factors to be blamed. In order\nto tackle this problem, we propose using \\textbf{Po}tential \\textbf{E}nergy\n\\textbf{R}anking (PoER) to decouple the object feature and the domain feature\n(\\emph{i.e.,} appearance feature) in given images, promoting the learning of\nlabel-discriminative features while filtering out the irrelevant correlations\nbetween the objects and the background. PoER helps the neural networks to\ncapture label-related features which contain the domain information first in\nshallow layers and then distills the label-discriminative representations out\nprogressively, enforcing the neural networks to be aware of the characteristic\nof objects and background which is vital to the generation of domain-invariant\nfeatures. PoER reports superior performance on domain generalization\nbenchmarks, improving the average top-1 accuracy by at least 1.20\\% compared to\nthe existing methods. Moreover, we use PoER in the ECCV 2022 NICO\nChallenge\\footnote{https://nicochallenge.com}, achieving top place with only a\nvanilla ResNet-18. The code has been made available at\nhttps://github.com/ForeverPs/PoER.\n","authors":["Sen Pei","Jiaxi Sun","Richard Yi Da Xu","Shiming Xiang","Gaofeng Meng"],"pdf_url":"https://arxiv.org/pdf/2207.12194v3.pdf","comment":"2022 ECCV jury award, accepted by AAAI 2023"},{"id":"http://arxiv.org/abs/2209.05166v3","updated":"2022-12-16T06:30:21Z","published":"2022-09-12T11:51:08Z","title":"Global Prototype Encoding for Incremental Video Highlights Detection","summary":"  Video highlights detection has been long researched as a topic in computer\nvision tasks, digging the user-appealing clips out given unexposed raw video\ninputs. However, in most case, the mainstream methods in this line of research\nare built on the closed world assumption, where a fixed number of highlight\ncategories is defined properly in advance and need all training data to be\navailable at the same time, and as a result, leads to poor scalability with\nrespect to both the highlight categories and the size of the dataset. To tackle\nthe problem mentioned above, we propose a video highlights detector that is\nable to learn incrementally, namely \\textbf{G}lobal \\textbf{P}rototype\n\\textbf{E}ncoding (GPE), capturing newly defined video highlights in the\nextended dataset via their corresponding prototypes. Alongside, we present a\nwell annotated and costly dataset termed \\emph{ByteFood}, including more than\n5.1k gourmet videos belongs to four different domains which are \\emph{cooking},\n\\emph{eating}, \\emph{food material}, and \\emph{presentation} respectively. To\nthe best of our knowledge, this is the first time the incremental learning\nsettings are introduced to video highlights detection, which in turn relieves\nthe burden of training video inputs and promotes the scalability of\nconventional neural networks in proportion to both the size of the dataset and\nthe quantity of domains. Moreover, the proposed GPE surpasses current\nincremental learning methods on \\emph{ByteFood}, reporting an improvement of\n1.57\\% mAP at least. The code and dataset will be made available sooner.\n","authors":["Sen Pei","Shixiong Xu","Ye Yuan","Jiashi Feng","Xiaohui Shen","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2209.05166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08296v1","updated":"2022-12-16T06:23:58Z","published":"2022-12-16T06:23:58Z","title":"DQnet: Cross-Model Detail Querying for Camouflaged Object Detection","summary":"  Camouflaged objects are seamlessly blended in with their surroundings, which\nbrings a challenging detection task in computer vision. Optimizing a\nconvolutional neural network (CNN) for camouflaged object detection (COD) tends\nto activate local discriminative regions while ignoring complete object extent,\ncausing the partial activation issue which inevitably leads to missing or\nredundant regions of objects. In this paper, we argue that partial activation\nis caused by the intrinsic characteristics of CNN, where the convolution\noperations produce local receptive fields and experience difficulty to capture\nlong-range feature dependency among image regions. In order to obtain feature\nmaps that could activate full object extent, keeping the segmental results from\nbeing overwhelmed by noisy features, a novel framework termed Cross-Model\nDetail Querying network (DQnet) is proposed. It reasons the relations between\nlong-range-aware representations and multi-scale local details to make the\nenhanced representation fully highlight the object regions and eliminate noise\non non-object regions. Specifically, a vanilla ViT pretrained with\nself-supervised learning (SSL) is employed to model long-range dependencies\namong image regions. A ResNet is employed to enable learning fine-grained\nspatial local details in multiple scales. Then, to effectively retrieve\nobject-related details, a Relation-Based Querying (RBQ) module is proposed to\nexplore window-based interactions between the global representations and the\nmulti-scale local details. Extensive experiments are conducted on the widely\nused COD datasets and show that our DQnet outperforms the current\nstate-of-the-arts.\n","authors":["Wei Sun","Chengao Liu","Linyan Zhang","Yu Li","Pengxu Wei","Chang Liu","Jialing Zou","Jianbin Jiao","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2212.08296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08290v1","updated":"2022-12-16T05:51:52Z","published":"2022-12-16T05:51:52Z","title":"Robust Learning Protocol for Federated Tumor Segmentation Challenge","summary":"  In this work, we devise robust and efficient learning protocols for\norchestrating a Federated Learning (FL) process for the Federated Tumor\nSegmentation Challenge (FeTS 2022). Enabling FL for FeTS setup is challenging\nmainly due to data heterogeneity among collaborators and communication cost of\ntraining. To tackle these challenges, we propose Robust Learning Protocol\n(RoLePRO) which is a combination of server-side adaptive optimisation (e.g.,\nserver-side Adam) and judicious parameter (weights) aggregation schemes (e.g.,\nadaptive weighted aggregation). RoLePRO takes a two-phase approach, where the\nfirst phase consists of vanilla Federated Averaging, while the second phase\nconsists of a judicious aggregation scheme that uses a sophisticated\nreweighting, all in the presence of an adaptive optimisation algorithm at the\nserver. We draw insights from extensive experimentation to tune learning rates\nfor the two phases.\n","authors":["Ambrish Rawat","Giulio Zizzo","Swanand Kadhe","Jonathan P. Epperlein","Stefano Braghin"],"pdf_url":"https://arxiv.org/pdf/2212.08290v1.pdf","comment":"14 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2212.08283v1","updated":"2022-12-16T05:10:09Z","published":"2022-12-16T05:10:09Z","title":"SceneGATE: Scene-Graph based co-Attention networks for TExt visual\n  question answering","summary":"  Most TextVQA approaches focus on the integration of objects, scene texts and\nquestion words by a simple transformer encoder. But this fails to capture the\nsemantic relations between different modalities. The paper proposes a Scene\nGraph based co-Attention Network (SceneGATE) for TextVQA, which reveals the\nsemantic relations among the objects, Optical Character Recognition (OCR)\ntokens and the question words. It is achieved by a TextVQA-based scene graph\nthat discovers the underlying semantics of an image. We created a\nguided-attention module to capture the intra-modal interplay between the\nlanguage and the vision as a guidance for inter-modal interactions. To make\nexplicit teaching of the relations between the two modalities, we proposed and\nintegrated two attention modules, namely a scene graph-based semantic\nrelation-aware attention and a positional relation-aware attention. We\nconducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA.\nIt is shown that our SceneGATE method outperformed existing ones because of the\nscene graph and its attention modules.\n","authors":["Siwen Luo","Feiqi Cao","Felipe Nunez","Zean Wen","Josiah Poon","Caren Han"],"pdf_url":"https://arxiv.org/pdf/2212.08283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08281v1","updated":"2022-12-16T05:08:52Z","published":"2022-12-16T05:08:52Z","title":"HGAN: Hierarchical Graph Alignment Network for Image-Text Retrieval","summary":"  Image-text retrieval (ITR) is a challenging task in the field of multimodal\ninformation processing due to the semantic gap between different modalities. In\nrecent years, researchers have made great progress in exploring the accurate\nalignment between image and text. However, existing works mainly focus on the\nfine-grained alignment between image regions and sentence fragments, which\nignores the guiding significance of context background information. Actually,\nintegrating the local fine-grained information and global context background\ninformation can provide more semantic clues for retrieval. In this paper, we\npropose a novel Hierarchical Graph Alignment Network (HGAN) for image-text\nretrieval. First, to capture the comprehensive multimodal features, we\nconstruct the feature graphs for the image and text modality respectively.\nThen, a multi-granularity shared space is established with a designed\nMulti-granularity Feature Aggregation and Rearrangement (MFAR) module, which\nenhances the semantic corresponding relations between the local and global\ninformation, and obtains more accurate feature representations for the image\nand text modalities. Finally, the ultimate image and text features are further\nrefined through three-level similarity functions to achieve the hierarchical\nalignment. To justify the proposed model, we perform extensive experiments on\nMS-COCO and Flickr30K datasets. Experimental results show that the proposed\nHGAN outperforms the state-of-the-art methods on both datasets, which\ndemonstrates the effectiveness and superiority of our model.\n","authors":["Jie Guo","Meiting Wang","Yan Zhou","Bin Song","Yuhao Chi","Wei Fan","Jianglong Chang"],"pdf_url":"https://arxiv.org/pdf/2212.08281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08279v1","updated":"2022-12-16T04:52:53Z","published":"2022-12-16T04:52:53Z","title":"Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion\n  Behaviors in Social Deduction Games","summary":"  Persuasion modeling is a key building block for conversational agents.\nExisting works in this direction are limited to analyzing textual dialogue\ncorpus. We argue that visual signals also play an important role in\nunderstanding human persuasive behaviors. In this paper, we introduce the first\nmultimodal dataset for modeling persuasion behaviors. Our dataset includes 199\ndialogue transcriptions and videos captured in a multi-player social deduction\ngame setting, 26,647 utterance level annotations of persuasion strategy, and\ngame level annotations of deduction game outcomes. We provide extensive\nexperiments to show how dialogue context and visual signals benefit persuasion\nstrategy prediction. We also explore the generalization ability of language\nmodels for persuasion modeling and the role of persuasion strategies in\npredicting social deduction game outcomes. Our dataset, code, and models can be\nfound at https://persuasion-deductiongame.socialai-data.org.\n","authors":["Bolin Lai","Hongxin Zhang","Miao Liu","Aryan Pariani","Fiona Ryan","Wenqi Jia","Shirley Anugrah Hayati","James M. Rehg","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08279v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2212.08277v1","updated":"2022-12-16T04:25:43Z","published":"2022-12-16T04:25:43Z","title":"Improving self-supervised representation learning via sequential\n  adversarial masking","summary":"  Recent methods in self-supervised learning have demonstrated that\nmasking-based pretext tasks extend beyond NLP, serving as useful pretraining\nobjectives in computer vision. However, existing approaches apply random or ad\nhoc masking strategies that limit the difficulty of the reconstruction task\nand, consequently, the strength of the learnt representations. We improve upon\ncurrent state-of-the-art work in learning adversarial masks by proposing a new\nframework that generates masks in a sequential fashion with different\nconstraints on the adversary. This leads to improvements in performance on\nvarious downstream tasks, such as classification on ImageNet100, STL10, and\nCIFAR10/100 and segmentation on Pascal VOC. Our results further demonstrate the\npromising capabilities of masking-based approaches for SSL in computer vision.\n","authors":["Dylan Sam","Min Bai","Tristan McKinney","Li Erran Li"],"pdf_url":"https://arxiv.org/pdf/2212.08277v1.pdf","comment":"9 pages, 2 figures, Presented at NeurIPS 2022 SSL: Theory and\n  Practice Workshop"},{"id":"http://arxiv.org/abs/2212.08273v1","updated":"2022-12-16T04:18:47Z","published":"2022-12-16T04:18:47Z","title":"Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy\n  Communication","summary":"  Deep learning has been widely used in the perception (e.g., 3D object\ndetection) of intelligent vehicle driving. Due to the beneficial\nVehicle-to-Vehicle (V2V) communication, the deep learning based features from\nother agents can be shared to the ego vehicle so as to improve the perception\nof the ego vehicle. It is named as Cooperative Perception in the V2V research,\nwhose algorithms have been dramatically advanced recently. However, all the\nexisting cooperative perception algorithms assume the ideal V2V communication\nwithout considering the possible lossy shared features because of the Lossy\nCommunication (LC) which is common in the complex real-world driving scenarios.\nIn this paper, we first study the side effect (e.g., detection performance\ndrop) by the lossy communication in the V2V Cooperative Perception, and then we\npropose a novel intermediate LC-aware feature fusion method to relieve the side\neffect of lossy communication by a LC-aware Repair Network (LCRN) and enhance\nthe interaction between the ego vehicle and other vehicles by a specially\ndesigned V2V Attention Module (V2VAM) including intra-vehicle attention of ego\nvehicle and uncertainty-aware inter-vehicle attention. The extensive experiment\non the public cooperative perception dataset OPV2V (based on digital-twin CARLA\nsimulator) demonstrates that the proposed method is quite effective for the\ncooperative point cloud based 3D object detection under lossy V2V\ncommunication.\n","authors":["Jinlong Li","Runsheng Xu","Xinyu Liu","Jin Ma","Zicheng Chi","Jiaqi Ma","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2212.08273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02062v2","updated":"2022-12-16T03:51:02Z","published":"2022-07-05T14:13:22Z","title":"Image Amodal Completion: A Survey","summary":"  Existing computer vision systems can compete with humans in understanding the\nvisible parts of objects, but still fall far short of humans when it comes to\ndepicting the invisible parts of partially occluded objects. Image amodal\ncompletion aims to equip computers with human-like amodal completion functions\nto understand an intact object despite it being partially occluded. The main\npurpose of this survey is to provide an intuitive understanding of the research\nhotspots, key technologies and future trends in the field of image amodal\ncompletion. Firstly, we present a comprehensive review of the latest literature\nin this emerging field, exploring three key tasks in image amodal completion,\nincluding amodal shape completion, amodal appearance completion, and order\nperception. Then we examine popular datasets related to image amodal completion\nalong with their common data collection methods and evaluation metrics.\nFinally, we discuss real-world applications and future research directions for\nimage amodal completion, facilitating the reader's understanding of the\nchallenges of existing technologies and upcoming research trends.\n","authors":["Jiayang Ao","Qiuhong Ke","Krista A. Ehinger"],"pdf_url":"https://arxiv.org/pdf/2207.02062v2.pdf","comment":"The manuscript is under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2212.08254v1","updated":"2022-12-16T02:52:37Z","published":"2022-12-16T02:52:37Z","title":"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of\n  Vision Transformers","summary":"  Post-training quantization (PTQ), which only requires a tiny dataset for\ncalibration without end-to-end retraining, is a light and practical model\ncompression technique. Recently, several PTQ schemes for vision transformers\n(ViTs) have been presented; unfortunately, they typically suffer from\nnon-trivial accuracy degradation, especially in low-bit cases. In this paper,\nwe propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale\nreparameterization, to address the above issues. RepQ-ViT decouples the\nquantization and inference processes, where the former employs complex\nquantizers and the latter employs scale-reparameterized simplified quantizers.\nThis ensures both accurate quantization and efficient inference, which\ndistinguishes it from existing approaches that sacrifice quantization\nperformance to meet the target hardware. More specifically, we focus on two\ncomponents with extreme distributions: post-LayerNorm activations with severe\ninter-channel variation and post-Softmax activations with power-law features,\nand initially apply channel-wise quantization and log$\\sqrt{2}$ quantization,\nrespectively. Then, we reparameterize the scales to hardware-friendly\nlayer-wise quantization and log2 quantization for inference, with only slight\naccuracy or computational costs. Extensive experiments are conducted on\nmultiple vision tasks with different model variants, proving that RepQ-ViT,\nwithout hyperparameters and expensive reconstruction procedures, can outperform\nexisting strong baselines and encouragingly improve the accuracy of 4-bit PTQ\nof ViTs to a usable level.\n","authors":["Zhikai Li","Junrui Xiao","Lianwei Yang","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2212.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08251v1","updated":"2022-12-16T02:43:52Z","published":"2022-12-16T02:43:52Z","title":"Robust Saliency Guidance for Data-free Class Incremental Learning","summary":"  Data-Free Class Incremental Learning (DFCIL) aims to sequentially learn tasks\nwith access only to data from the current one. DFCIL is of interest because it\nmitigates concerns about privacy and long-term storage of data, while at the\nsame time alleviating the problem of catastrophic forgetting in incremental\nlearning. In this work, we introduce robust saliency guidance for DFCIL and\npropose a new framework, which we call RObust Saliency Supervision (ROSS), for\nmitigating the negative effect of saliency drift. Firstly, we use a\nteacher-student architecture leveraging low-level tasks to supervise the model\nwith global saliency. We also apply boundary-guided saliency to protect it from\ndrifting across object boundaries at intermediate layers. Finally, we introduce\na module for injecting and recovering saliency noise to increase robustness of\nsaliency preservation. Our experiments demonstrate that our method can retain\nbetter saliency maps across tasks and achieve state-of-the-art results on the\nCIFAR-100, Tiny-ImageNet and ImageNet-Subset DFCIL benchmarks. Code will be\nmade publicly available.\n","authors":["Xialei Liu","Jiang-Tian Zhai","Andrew D. Bagdanov","Ke Li","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2212.08251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08244v1","updated":"2022-12-16T02:23:50Z","published":"2022-12-16T02:23:50Z","title":"Offline Reinforcement Learning for Visual Navigation","summary":"  Reinforcement learning can enable robots to navigate to distant goals while\noptimizing user-specified reward functions, including preferences for following\nlanes, staying on paved paths, or avoiding freshly mowed grass. However, online\nlearning from trial-and-error for real-world robots is logistically\nchallenging, and methods that instead can utilize existing datasets of robotic\nnavigation data could be significantly more scalable and enable broader\ngeneralization. In this paper, we present ReViND, the first offline RL system\nfor robotic navigation that can leverage previously collected data to optimize\nuser-specified reward functions in the real-world. We evaluate our system for\noff-road navigation without any additional data collection or fine-tuning, and\nshow that it can navigate to distant goals using only offline training from\nthis dataset, and exhibit behaviors that qualitatively differ based on the\nuser-specified reward function.\n","authors":["Dhruv Shah","Arjun Bhorkar","Hrish Leen","Ilya Kostrikov","Nick Rhinehart","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2212.08244v1.pdf","comment":"Project page https://sites.google.com/view/revind/home"},{"id":"http://arxiv.org/abs/2212.08228v1","updated":"2022-12-16T01:35:27Z","published":"2022-12-16T01:35:27Z","title":"SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image\n  Generation","summary":"  Human organs constantly undergo anatomical changes due to a complex mix of\nshort-term (e.g., heartbeat) and long-term (e.g., aging) factors. Evidently,\nprior knowledge of these factors will be beneficial when modeling their future\nstate, i.e., via image generation. However, most of the medical image\ngeneration tasks only rely on the input from a single image, thus ignoring the\nsequential dependency even when longitudinal data is available. Sequence-aware\ndeep generative models, where model input is a sequence of ordered and\ntimestamped images, are still underexplored in the medical imaging domain that\nis featured by several unique challenges: 1) Sequences with various lengths; 2)\nMissing data or frame, and 3) High dimensionality. To this end, we propose a\nsequence-aware diffusion model (SADM) for the generation of longitudinal\nmedical images. Recently, diffusion models have shown promising results on\nhigh-fidelity image generation. Our method extends this new technique by\nintroducing a sequence-aware transformer as the conditional module in a\ndiffusion model. The novel design enables learning longitudinal dependency even\nwith missing data during training and allows autoregressive generation of a\nsequence of images during inference. Our extensive experiments on 3D\nlongitudinal medical images demonstrate the effectiveness of SADM compared with\nbaselines and alternative methods.\n","authors":["Jee Seok Yoon","Chenghao Zhang","Heung-Il Suk","Jia Guo","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2212.08228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14670v3","updated":"2022-12-16T01:23:13Z","published":"2022-10-26T12:47:29Z","title":"Boosting Semi-Supervised Semantic Segmentation with Probabilistic\n  Representations","summary":"  Recent breakthroughs in semi-supervised semantic segmentation have been\ndeveloped through contrastive learning. In prevalent pixel-wise contrastive\nlearning solutions, the model maps pixels to deterministic representations and\nregularizes them in the latent space. However, there exist inaccurate\npseudo-labels which map the ambiguous representations of pixels to the wrong\nclasses due to the limited cognitive ability of the model. In this paper, we\ndefine pixel-wise representations from a new perspective of probability theory\nand propose a Probabilistic Representation Contrastive Learning (PRCL)\nframework that improves representation quality by taking its probability into\nconsideration. Through modelling the mapping from pixels to representations as\nthe probability via multivariate Gaussian distributions, we can tune the\ncontribution of the ambiguous representations to tolerate the risk of\ninaccurate pseudo-labels. Furthermore, we define prototypes in the form of\ndistributions, which indicates the confidence of a class, while the point\nprototype cannot. Moreover, we propose to regularize the distribution variance\nto enhance the reliability of representations. Taking advantage of these\nbenefits, high-quality feature representations can be derived in the latent\nspace, thereby the performance of semantic segmentation can be further\nimproved. We conduct sufficient experiment to evaluate PRCL on Pascal VOC and\nCityScapes to demonstrate its superiority. The code is available at\nhttps://github.com/Haoyu-Xie/PRCL.\n","authors":["Haoyu Xie","Changqi Wang","Mingkai Zheng","Minjing Dong","Shan You","Chong Fu","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2210.14670v3.pdf","comment":"Accepted to AAAI 2023"},{"id":"http://arxiv.org/abs/2212.08208v1","updated":"2022-12-16T00:32:38Z","published":"2022-12-16T00:32:38Z","title":"Location-aware Adaptive Denormalization: A Deep Learning Approach For\n  Wildfire Danger Forecasting","summary":"  Climate change is expected to intensify and increase extreme events in the\nweather cycle. Since this has a significant impact on various sectors of our\nlife, recent works are concerned with identifying and predicting such extreme\nevents from Earth observations. This paper proposes a 2D/3D two-branch\nconvolutional neural network (CNN) for wildfire danger forecasting. To use a\nunified framework, previous approaches duplicate static variables along the\ntime dimension and neglect the intrinsic differences between static and dynamic\nvariables. Furthermore, most existing multi-branch architectures lose the\ninterconnections between the branches during the feature learning stage. To\naddress these issues, we propose a two-branch architecture with a\nLocation-aware Adaptive Denormalization layer (LOADE). Using LOADE as a\nbuilding block, we can modulate the dynamic features conditional on their\ngeographical location. Thus, our approach considers feature properties as a\nunified yet compound 2D/3D model. Besides, we propose using an absolute\ntemporal encoding for time-related forecasting problems. Our experimental\nresults show a better performance of our approach than other baselines on the\nchallenging FireCube dataset.\n","authors":["Mohamad Hakam Shams Eddin","Ribana Roscher","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2212.08208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02879v2","updated":"2022-12-16T00:31:44Z","published":"2022-08-04T20:31:46Z","title":"PointConvFormer: Revenge of the Point-based Convolution","summary":"  We introduce PointConvFormer, a novel building block for point cloud based\ndeep network architectures. Inspired by generalization theory, PointConvFormer\ncombines ideas from point convolution, where filter weights are only based on\nrelative position, and Transformers which utilize feature-based attention. In\nPointConvFormer, attention computed from feature difference between neighboring\npoints is used to modify the convolutional weights at each point. Hence,\ninvariances from point convolution are preserved, whereas attention helps to\nselect relevant points in the neighborhood. PointConvFormer is suitable for\nmultiple tasks that require details at the point level, such as segmentation\nand scene flow estimation tasks. We experiment on both tasks with multiple\ndatasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our\nresults show that PointConvFormer substantially outperforms classic\nconvolutions, regular transformers, and voxelized sparse convolution approaches\nwith much smaller and faster networks. Visualizations show that PointConvFormer\nperforms similarly to convolution on flat areas, whereas the neighborhood\nselection effect is stronger on object boundaries, showing that it has got the\nbest of both worlds.\n","authors":["Wenxuan Wu","Qi Shan","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2208.02879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.06997v4","updated":"2022-12-16T00:14:00Z","published":"2022-02-14T19:29:08Z","title":"Cross-Modality Neuroimage Synthesis: A Survey","summary":"  The existence of completely aligned and paired multi-modal neuroimaging data\nhas proved its effectiveness in diagnosis of brain diseases. However,\ncollecting the full set of well-aligned and paired data is expensive or even\nimpractical, since the practical difficulties may include high cost, long time\nacquisition, image corruption, and privacy issues. A realistic solution is to\nexplore either an unsupervised learning or a semi-supervised learning to\nsynthesize the absent neuroimaging data. In this paper, we are the first one to\ncomprehensively approach cross-modality neuroimage synthesis task from\ndifferent perspectives, which include the level of the supervision (especially\nfor weakly-supervised and unsupervised), loss function, evaluation metrics, the\nrange of modality synthesis, datasets (aligned, private and public) and the\nsynthesis-based downstream tasks. To begin with, we highlight several opening\nchallenges for cross-modality neuroimage sysnthesis. Then we summarize the\narchitecture of cross-modality synthesis under various of supervision level. In\naddition, we provide in-depth analysis of how cross-modality neuroimage\nsynthesis can improve the performance of different downstream tasks. Finally,\nwe re-evaluate the open challenges and point out the future directions for the\nremaining challenges. All resources are available at\nhttps://github.com/M-3LAB/awesome-multimodal-brain-image-systhesis\n","authors":["Guoyang Xie","Jinbao Wang","Yawen Huang","Jiayi Lyu","Feng Zheng","Yefeng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2202.06997v4.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2212.08262v1","updated":"2022-12-16T03:13:43Z","published":"2022-12-16T03:13:43Z","title":"Uniform Sequence Better: Time Interval Aware Data Augmentation for\n  Sequential Recommendation","summary":"  Sequential recommendation is an important task to predict the next-item to\naccess based on a sequence of interacted items. Most existing works learn user\npreference as the transition pattern from the previous item to the next one,\nignoring the time interval between these two items. However, we observe that\nthe time interval in a sequence may vary significantly different, and thus\nresult in the ineffectiveness of user modeling due to the issue of\n\\emph{preference drift}. In fact, we conducted an empirical study to validate\nthis observation, and found that a sequence with uniformly distributed time\ninterval (denoted as uniform sequence) is more beneficial for performance\nimprovement than that with greatly varying time interval. Therefore, we propose\nto augment sequence data from the perspective of time interval, which is not\nstudied in the literature. Specifically, we design five operators (Ti-Crop,\nTi-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original\nnon-uniform sequence to uniform sequence with the consideration of variance of\ntime intervals. Then, we devise a control strategy to execute data augmentation\non item sequences in different lengths. Finally, we implement these\nimprovements on a state-of-the-art model CoSeRec and validate our approach on\nfour real datasets. The experimental results show that our approach reaches\nsignificantly better performance than the other 11 competing methods. Our\nimplementation is available: https://github.com/KingGugu/TiCoSeRec.\n","authors":["Yizhou Dang","Enneng Yang","Guibing Guo","Linying Jiang","Xingwei Wang","Xiaoxiao Xu","Qinghui Sun","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08262v1.pdf","comment":"9 pages, 4 figures, AAAI-2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2209.00626v3","updated":"2022-12-16T18:50:31Z","published":"2022-08-30T02:12:47Z","title":"The alignment problem from a deep learning perspective","summary":"  Within the coming decades, artificial general intelligence (AGI) may surpass\nhuman capabilities at a wide range of important tasks. We outline a case for\nexpecting that, without substantial effort to prevent it, AGIs could learn to\npursue goals which are very undesirable (in other words, misaligned) from a\nhuman perspective. We argue that AGIs trained in similar ways as today's most\ncapable models could learn to act deceptively to receive higher reward; learn\ninternally-represented goals which generalize beyond their training\ndistributions; and pursue those goals using power-seeking strategies. We\noutline how the deployment of misaligned AGIs might irreversibly undermine\nhuman control over the world, and briefly review research directions aimed at\npreventing these problems.\n","authors":["Richard Ngo","Lawrence Chan","Sören Mindermann"],"pdf_url":"https://arxiv.org/pdf/2209.00626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08648v1","updated":"2022-12-16T18:48:54Z","published":"2022-12-16T18:48:54Z","title":"Connecting Permutation Equivariant Neural Networks and Partition\n  Diagrams","summary":"  We show how the Schur-Weyl duality that exists between the partition algebra\nand the symmetric group results in a stronger theoretical foundation for\ncharacterising all of the possible permutation equivariant neural networks\nwhose layers are some tensor power of the permutation representation $M_n$ of\nthe symmetric group $S_n$. In doing so, we unify two separate bodies of\nliterature, and we correct some of the major results that are now widely quoted\nby the machine learning community. In particular, we find a basis of matrices\nfor the learnable, linear, permutation equivariant layer functions between such\ntensor power spaces in the standard basis of $M_n$ by using an elegant\ngraphical representation of a basis of set partitions for the partition algebra\nand its related vector spaces. Also, we show how we can calculate the number of\nweights that must appear in these layer functions by looking at certain paths\nthrough the McKay quiver for $M_n$. Finally, we describe how our approach\ngeneralises to the construction of neural networks that are equivariant to\nlocal symmetries.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2212.08648v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2212.08645v1","updated":"2022-12-16T18:39:32Z","published":"2022-12-16T18:39:32Z","title":"Efficient Conditionally Invariant Representation Learning","summary":"  We introduce the Conditional Independence Regression CovariancE (CIRCE), a\nmeasure of conditional independence for multivariate continuous-valued\nvariables. CIRCE applies as a regularizer in settings where we wish to learn\nneural features $\\varphi(X)$ of data $X$ to estimate a target $Y$, while being\nconditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are\nassumed to be continuous-valued but relatively low dimensional, whereas $X$ and\nits features may be complex and high dimensional. Relevant settings include\ndomain-invariant learning, fairness, and causal learning. The procedure\nrequires just a single ridge regression from $Y$ to kernelized features of $Z$,\nwhich can be done in advance. It is then only necessary to enforce independence\nof $\\varphi(X)$ from residuals of this regression, which is possible with\nattractive estimation properties and consistency guarantees. By contrast,\nearlier measures of conditional feature dependence require multiple regressions\nfor each step of feature learning, resulting in more severe bias and variance,\nand greater computational cost. When sufficiently rich features are used, we\nestablish that CIRCE is zero if and only if $\\varphi(X) \\perp \\!\\!\\! \\perp Z\n\\mid Y$. In experiments, we show superior performance to previous methods on\nchallenging benchmarks, including learning conditionally invariant image\nfeatures.\n","authors":["Roman Pogodin","Namrata Deka","Yazhe Li","Danica J. Sutherland","Victor Veitch","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2212.08645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07555v2","updated":"2022-12-16T18:39:09Z","published":"2022-12-14T23:59:24Z","title":"IMoS: Intent-Driven Full-Body Motion Synthesis for Human-Object\n  Interactions","summary":"  Can we make virtual characters in a scene interact with their surrounding\nobjects through simple instructions? Is it possible to synthesize such motion\nplausibly with a diverse set of objects and instructions? Inspired by these\nquestions, we present the first framework to synthesize the full-body motion of\nvirtual human characters performing specified actions with 3D objects placed\nwithin their reach. Our system takes as input textual instructions specifying\nthe objects and the associated intentions of the virtual characters and outputs\ndiverse sequences of full-body motions. This is in contrast to existing work,\nwhere full-body action synthesis methods generally do not consider object\ninteractions, and human-object interaction methods focus mainly on synthesizing\nhand or finger movements for grasping objects. We accomplish our objective by\ndesigning an intent-driven full-body motion generator, which uses a pair of\ndecoupled conditional variational autoencoders (CVAE) to learn the motion of\nthe body parts in an autoregressive manner. We also optimize for the positions\nof the objects with six degrees of freedom (6DoF) such that they plausibly fit\nwithin the hands of the synthesized characters. We compare our proposed method\nwith the existing methods of motion synthesis and establish a new and stronger\nstate-of-the-art for the task of intent-driven motion synthesis. Through a user\nstudy, we further show that our synthesized full-body motions appear more\nrealistic to the participants in more than 80% of scenarios compared to the\ncurrent state-of-the-art methods, and are perceived to be as good as the ground\ntruth on several occasions.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2212.07555v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2212.01688v2","updated":"2022-12-16T18:21:16Z","published":"2022-12-03T20:55:10Z","title":"LDL: A Defense for Label-Based Membership Inference Attacks","summary":"  The data used to train deep neural network (DNN) models in applications such\nas healthcare and finance typically contain sensitive information. A DNN model\nmay suffer from overfitting. Overfitted models have been shown to be\nsusceptible to query-based attacks such as membership inference attacks (MIAs).\nMIAs aim to determine whether a sample belongs to the dataset used to train a\nclassifier (members) or not (nonmembers). Recently, a new class of label based\nMIAs (LAB MIAs) was proposed, where an adversary was only required to have\nknowledge of predicted labels of samples. Developing a defense against an\nadversary carrying out a LAB MIA on DNN models that cannot be retrained remains\nan open problem. We present LDL, a light weight defense against LAB MIAs. LDL\nworks by constructing a high-dimensional sphere around queried samples such\nthat the model decision is unchanged for (noisy) variants of the sample within\nthe sphere. This sphere of label-invariance creates ambiguity and prevents a\nquerying adversary from correctly determining whether a sample is a member or a\nnonmember. We analytically characterize the success rate of an adversary\ncarrying out a LAB MIA when LDL is deployed, and show that the formulation is\nconsistent with experimental observations. We evaluate LDL on seven datasets --\nCIFAR-10, CIFAR-100, GTSRB, Face, Purchase, Location, and Texas -- with varying\nsizes of training data. All of these datasets have been used by SOTA LAB MIAs.\nOur experiments demonstrate that LDL reduces the success rate of an adversary\ncarrying out a LAB MIA in each case. We empirically compare LDL with defenses\nagainst LAB MIAs that require retraining of DNN models, and show that LDL\nperforms favorably despite not needing to retrain the DNNs.\n","authors":["Arezoo Rajabi","Dinuka Sahabandu","Luyao Niu","Bhaskar Ramasubramanian","Radha Poovendran"],"pdf_url":"https://arxiv.org/pdf/2212.01688v2.pdf","comment":"to appear in ACM ASIA Conference on Computer and Communications\n  Security (ACM ASIACCS 2023)"},{"id":"http://arxiv.org/abs/2212.05153v2","updated":"2022-12-16T18:18:59Z","published":"2022-12-10T00:18:05Z","title":"Algorithmic progress in computer vision","summary":"  We investigate algorithmic progress in image classification on ImageNet,\nperhaps the most well-known test bed for computer vision. We estimate a model,\ninformed by work on neural scaling laws, and infer a decomposition of progress\ninto the scaling of compute, data, and algorithms. Using Shapley values to\nattribute performance improvements, we find that algorithmic improvements have\nbeen roughly as important as the scaling of compute for progress computer\nvision. Our estimates indicate that algorithmic innovations mostly take the\nform of compute-augmenting algorithmic advances (which enable researchers to\nget better performance from less compute), not data-augmenting algorithmic\nadvances. We find that compute-augmenting algorithmic advances are made at a\npace more than twice as fast as the rate usually associated with Moore's law.\nIn particular, we estimate that compute-augmenting innovations halve compute\nrequirements every nine months (95\\% confidence interval: 4 to 25 months).\n","authors":["Ege Erdil","Tamay Besiroglu"],"pdf_url":"https://arxiv.org/pdf/2212.05153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08630v1","updated":"2022-12-16T18:08:51Z","published":"2022-12-16T18:08:51Z","title":"Brauer's Group Equivariant Neural Networks","summary":"  We provide a full characterisation of all of the possible group equivariant\nneural networks whose layers are some tensor power of $\\mathbb{R}^{n}$ for\nthree symmetry groups that are missing from the machine learning literature:\n$O(n)$, the orthogonal group; $SO(n)$, the special orthogonal group; and\n$Sp(n)$, the symplectic group. In particular, we find a spanning set of\nmatrices for the learnable, linear, equivariant layer functions between such\ntensor power spaces in the standard basis of $\\mathbb{R}^{n}$ when the group is\n$O(n)$ or $SO(n)$, and in the symplectic basis of $\\mathbb{R}^{n}$ when the\ngroup is $Sp(n)$. The neural networks that we characterise are simple to\nimplement since our method circumvents the typical requirement when building\ngroup equivariant neural networks of having to decompose the tensor power\nspaces of $\\mathbb{R}^{n}$ into irreducible representations. We also describe\nhow our approach generalises to the construction of neural networks that are\nequivariant to local symmetries.\n  The theoretical background for our results comes from the Schur-Weyl\ndualities that were established by Brauer in his 1937 paper \"On Algebras Which\nare Connected with the Semisimple Continuous Groups\" for each of the three\ngroups in question. We suggest that Schur-Weyl duality is a powerful\nmathematical concept that could be used to understand the structure of neural\nnetworks that are equivariant to groups beyond those considered in this paper.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2212.08630v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2212.08624v1","updated":"2022-12-16T17:59:40Z","published":"2022-12-16T17:59:40Z","title":"Development of A Real-time POCUS Image Quality Assessment and\n  Acquisition Guidance System","summary":"  Point-of-care ultrasound (POCUS) is one of the most commonly applied tools\nfor cardiac function imaging in the clinical routine of the emergency\ndepartment and pediatric intensive care unit. The prior studies demonstrate\nthat AI-assisted software can guide nurses or novices without prior sonography\nexperience to acquire POCUS by recognizing the interest region, assessing image\nquality, and providing instructions. However, these AI algorithms cannot simply\nreplace the role of skilled sonographers in acquiring diagnostic-quality POCUS.\nUnlike chest X-ray, CT, and MRI, which have standardized imaging protocols,\nPOCUS can be acquired with high inter-observer variability. Though being with\nvariability, they are usually all clinically acceptable and interpretable. In\nchallenging clinical environments, sonographers employ novel heuristics to\nacquire POCUS in complex scenarios. To help novice learners to expedite the\ntraining process while reducing the dependency on experienced sonographers in\nthe curriculum implementation, We will develop a framework to perform real-time\nAI-assisted quality assessment and probe position guidance to provide training\nprocess for novice learners with less manual intervention.\n","authors":["Zhenge Jia","Yiyu Shi","Jingtong Hu","Lei Yang","Benjamin Nti"],"pdf_url":"https://arxiv.org/pdf/2212.08624v1.pdf","comment":"4 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2212.08620v1","updated":"2022-12-16T17:57:41Z","published":"2022-12-16T17:57:41Z","title":"POTATO: The Portable Text Annotation Tool","summary":"  We present POTATO, the Portable text annotation tool, a free, fully\nopen-sourced annotation system that 1) supports labeling many types of text and\nmultimodal data; 2) offers easy-to-configure features to maximize the\nproductivity of both deployers and annotators (convenient templates for common\nML/NLP tasks, active learning, keypress shortcuts, keyword highlights,\ntooltips); and 3) supports a high degree of customization (editable UI,\ninserting pre-screening questions, attention and qualification tests).\nExperiments over two annotation tasks suggest that POTATO improves labeling\nspeed through its specially-designed productivity features, especially for long\ndocuments and complex tasks. POTATO is available at\nhttps://github.com/davidjurgens/potato and will continue to be updated.\n","authors":["Jiaxin Pei","Aparna Ananthasubramaniam","Xingyao Wang","Naitian Zhou","Jackson Sargent","Apostolos Dedeloudis","David Jurgens"],"pdf_url":"https://arxiv.org/pdf/2212.08620v1.pdf","comment":"EMNLP 2022 DEMO"},{"id":"http://arxiv.org/abs/2212.08607v1","updated":"2022-12-16T17:36:23Z","published":"2022-12-16T17:36:23Z","title":"MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text\n  Generation","summary":"  Prompting large language models has enabled significant recent progress in\nmulti-step reasoning over text. However, when applied to text generation from\nsemi-structured data (e.g., graphs or tables), these methods typically suffer\nfrom low semantic coverage, hallucination, and logical inconsistency. We\npropose MURMUR, a neuro-symbolic modular approach to text generation from\nsemi-structured data with multi-step reasoning. MURMUR is a best-first search\nmethod that generates reasoning paths using: (1) neural and symbolic modules\nwith specific linguistic and logical skills, (2) a grammar whose production\nrules define valid compositions of modules, and (3) value functions that assess\nthe quality of each reasoning step. We conduct experiments on two diverse\ndata-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in\ntheir data representations (graphs and tables) and span multiple linguistic and\nlogical skills. MURMUR obtains significant improvements over recent few-shot\nbaselines like direct prompting and chain-of-thought prompting, while also\nachieving comparable performance to fine-tuned GPT-2 on out-of-domain data.\nMoreover, human evaluation shows that MURMUR generates highly faithful and\ncorrect reasoning paths that lead to 26% more logically consistent summaries on\nLogicNLG, compared to direct prompting.\n","authors":["Swarnadeep Saha","Xinyan Velocity Yu","Mohit Bansal","Ramakanth Pasunuru","Asli Celikyilmaz"],"pdf_url":"https://arxiv.org/pdf/2212.08607v1.pdf","comment":"22 pages (9 figures, 18 tables)"},{"id":"http://arxiv.org/abs/2212.08604v1","updated":"2022-12-16T17:32:56Z","published":"2022-12-16T17:32:56Z","title":"Planning Visual-Tactile Precision Grasps via Complementary Use of Vision\n  and Touch","summary":"  Reliably planning fingertip grasps for multi-fingered hands lies as a key\nchallenge for many tasks including tool use, insertion, and dexterous in-hand\nmanipulation. This task becomes even more difficult when the robot lacks an\naccurate model of the object to be grasped. Tactile sensing offers a promising\napproach to account for uncertainties in object shape. However, current robotic\nhands tend to lack full tactile coverage. As such, a problem arises of how to\nplan and execute grasps for multi-fingered hands such that contact is made with\nthe area covered by the tactile sensors. To address this issue, we propose an\napproach to grasp planning that explicitly reasons about where the fingertips\nshould contact the estimated object surface while maximizing the probability of\ngrasp success. Key to our method's success is the use of visual surface\nestimation for initial planning to encode the contact constraint. The robot\nthen executes this plan using a tactile-feedback controller that enables the\nrobot to adapt to online estimates of the object's surface to correct for\nerrors in the initial plan. Importantly, the robot never explicitly integrates\nobject pose or surface estimates between visual and tactile sensing, instead it\nuses the two modalities in complementary ways. Vision guides the robots motion\nprior to contact; touch updates the plan when contact occurs differently than\npredicted from vision. We show that our method successfully synthesises and\nexecutes precision grasps for previously unseen objects using surface estimates\nfrom a single camera view. Further, our approach outperforms a state of the art\nmulti-fingered grasp planner, while also beating several baselines we propose.\n","authors":["Martin Matak","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2212.08604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12584v2","updated":"2022-12-16T17:08:46Z","published":"2022-01-29T13:13:57Z","title":"Convolutional Filtering in Simplicial Complexes","summary":"  This paper proposes convolutional filtering for data whose structure can be\nmodeled by a simplicial complex (SC). SCs are mathematical tools that not only\ncapture pairwise relationships as graphs but account also for higher-order\nnetwork structures. These filters are built by following the shift-and-sum\nprinciple of the convolution operation and rely on the Hodge-Laplacians to\nshift the signal within the simplex. But since in SCs we have also\ninter-simplex coupling, we use the incidence matrices to transfer the signal in\nadjacent simplices and build a filter bank to jointly filter signals from\ndifferent levels. We prove some interesting properties for the proposed filter\nbank, including permutation and orientation equivariance, a computational\ncomplexity that is linear in the SC dimension, and a spectral interpretation\nusing the simplicial Fourier transform. We illustrate the proposed approach\nwith numerical experiments.\n","authors":["Elvin Isufi","Maosheng Yang"],"pdf_url":"https://arxiv.org/pdf/2201.12584v2.pdf","comment":"5 pages, 2 figures, accepted in ICASSP 2022 (The first version has\n  some errors and we fixed them in the second version)"},{"id":"http://arxiv.org/abs/1910.02519v4","updated":"2022-12-16T17:01:59Z","published":"2019-10-06T20:34:52Z","title":"FIS-GAN: GAN with Flow-based Importance Sampling","summary":"  Generative Adversarial Networks (GAN) training process, in most cases, apply\nUniform or Gaussian sampling methods in the latent space, which probably spends\nmost of the computation on examples that can be properly handled and easy to\ngenerate. Theoretically, importance sampling speeds up stochastic optimization\nin supervised learning by prioritizing training examples. In this paper, we\nexplore the possibility of adapting importance sampling into adversarial\nlearning. We use importance sampling to replace Uniform and Gaussian sampling\nmethods in the latent space and employ normalizing flow to approximate latent\nspace posterior distribution by density estimation. Empirically, results on\nMNIST and Fashion-MNIST demonstrate that our method significantly accelerates\nGAN's optimization while retaining visual fidelity in generated samples.\n","authors":["Shiyu Yi","Donglin Zhan","Wenqing Zhang","Denglin Jiang","Kang An","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/1910.02519v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08578v1","updated":"2022-12-16T16:54:37Z","published":"2022-12-16T16:54:37Z","title":"Provable Fairness for Neural Network Models using Formal Verification","summary":"  Machine learning models are increasingly deployed for critical\ndecision-making tasks, making it important to verify that they do not contain\ngender or racial biases picked up from training data. Typical approaches to\nachieve fairness revolve around efforts to clean or curate training data, with\npost-hoc statistical evaluation of the fairness of the model on evaluation\ndata. In contrast, we propose techniques to \\emph{prove} fairness using\nrecently developed formal methods that verify properties of neural network\nmodels.Beyond the strength of guarantee implied by a formal proof, our methods\nhave the advantage that we do not need explicit training or evaluation data\n(which is often proprietary) in order to analyze a given trained model. In\nexperiments on two familiar datasets in the fairness literature (COMPAS and\nADULTS), we show that through proper training, we can reduce unfairness by an\naverage of 65.4\\% at a cost of less than 1\\% in AUC score.\n","authors":["Giorgian Borca-Tasciuc","Xingzhi Guo","Stanley Bak","Steven Skiena"],"pdf_url":"https://arxiv.org/pdf/2212.08578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08568v1","updated":"2022-12-16T16:44:46Z","published":"2022-12-16T16:44:46Z","title":"Biomedical image analysis competitions: The state of current\n  participation practice","summary":"  The number of international benchmarking competitions is steadily increasing\nin various fields of machine learning (ML) research and practice. So far,\nhowever, little is known about the common practice as well as bottlenecks faced\nby the community in tackling the research questions posed. To shed light on the\nstatus quo of algorithm development in the specific field of biomedical imaging\nanalysis, we designed an international survey that was issued to all\nparticipants of challenges conducted in conjunction with the IEEE ISBI 2021 and\nMICCAI 2021 conferences (80 competitions in total). The survey covered\nparticipants' expertise and working environments, their chosen strategies, as\nwell as algorithm characteristics. A median of 72% challenge participants took\npart in the survey. According to our results, knowledge exchange was the\nprimary incentive (70%) for participation, while the reception of prize money\nplayed only a minor role (16%). While a median of 80 working hours was spent on\nmethod development, a large portion of participants stated that they did not\nhave enough time for method development (32%). 25% perceived the infrastructure\nto be a bottleneck. Overall, 94% of all solutions were deep learning-based. Of\nthese, 84% were based on standard architectures. 43% of the respondents\nreported that the data samples (e.g., images) were too large to be processed at\nonce. This was most commonly addressed by patch-based training (69%),\ndownsampling (37%), and solving 3D analysis tasks as a series of 2D tasks.\nK-fold cross-validation on the training set was performed by only 37% of the\nparticipants and only 50% of the participants performed ensembling based on\nmultiple identical models (61%) or heterogeneous models (39%). 48% of the\nrespondents applied postprocessing steps.\n","authors":["Matthias Eisenmann","Annika Reinke","Vivienn Weru","Minu Dietlinde Tizabi","Fabian Isensee","Tim J. Adler","Patrick Godau","Veronika Cheplygina","Michal Kozubek","Sharib Ali","Anubha Gupta","Jan Kybic","Alison Noble","Carlos Ortiz de Solórzano","Samiksha Pachade","Caroline Petitjean","Daniel Sage","Donglai Wei","Elizabeth Wilden","Deepak Alapatt","Vincent Andrearczyk","Ujjwal Baid","Spyridon Bakas","Niranjan Balu","Sophia Bano","Vivek Singh Bawa","Jorge Bernal","Sebastian Bodenstedt","Alessandro Casella","Jinwook Choi","Olivier Commowick","Marie Daum","Adrien Depeursinge","Reuben Dorent","Jan Egger","Hannah Eichhorn","Sandy Engelhardt","Melanie Ganz","Gabriel Girard","Lasse Hansen","Mattias Heinrich","Nicholas Heller","Alessa Hering","Arnaud Huaulmé","Hyunjeong Kim","Bennett Landman","Hongwei Bran Li","Jianning Li","Jun Ma","Anne Martel","Carlos Martín-Isla","Bjoern Menze","Chinedu Innocent Nwoye","Valentin Oreiller","Nicolas Padoy","Sarthak Pati","Kelly Payette","Carole Sudre","Kimberlin van Wijnen","Armine Vardazaryan","Tom Vercauteren","Martin Wagner","Chuanbo Wang","Moi Hoon Yap","Zeyun Yu","Chun Yuan","Maximilian Zenk","Aneeq Zia","David Zimmerer","Rina Bao","Chanyeol Choi","Andrew Cohen","Oleh Dzyubachyk","Adrian Galdran","Tianyuan Gan","Tianqi Guo","Pradyumna Gupta","Mahmood Haithami","Edward Ho","Ikbeom Jang","Zhili Li","Zhengbo Luo","Filip Lux","Sokratis Makrogiannis","Dominik Müller","Young-tack Oh","Subeen Pang","Constantin Pape","Gorkem Polat","Charlotte Rosalie Reed","Kanghyun Ryu","Tim Scherr","Vajira Thambawita","Haoyu Wang","Xinliang Wang","Kele Xu","Hung Yeh","Doyeob Yeo","Yixuan Yuan","Yan Zeng","Xin Zhao","Julian Abbing","Jannes Adam","Nagesh Adluru","Niklas Agethen","Salman Ahmed","Yasmina Al Khalil","Mireia Alenyà","Esa Alhoniemi","Chengyang An","Talha Anwar","Tewodros Weldebirhan Arega","Netanell Avisdris","Dogu Baran Aydogan","Yingbin Bai","Maria Baldeon Calisto","Berke Doga Basaran","Marcel Beetz","Cheng Bian","Hao Bian","Kevin Blansit","Louise Bloch","Robert Bohnsack","Sara Bosticardo","Jack Breen","Mikael Brudfors","Raphael Brüngel","Mariano Cabezas","Alberto Cacciola","Zhiwei Chen","Yucong Chen","Daniel Tianming Chen","Minjeong Cho","Min-Kook Choi","Chuantao Xie Chuantao Xie","Dana Cobzas","Julien Cohen-Adad","Jorge Corral Acero","Sujit Kumar Das","Marcela de Oliveira","Hanqiu Deng","Guiming Dong","Lars Doorenbos","Cory Efird","Di Fan","Mehdi Fatan Serj","Alexandre Fenneteau","Lucas Fidon","Patryk Filipiak","René Finzel","Nuno R. Freitas","Christoph M. Friedrich","Mitchell Fulton","Finn Gaida","Francesco Galati","Christoforos Galazis","Chang Hee Gan","Zheyao Gao","Shengbo Gao","Matej Gazda","Beerend Gerats","Neil Getty","Adam Gibicar","Ryan Gifford","Sajan Gohil","Maria Grammatikopoulou","Daniel Grzech","Orhun Güley","Timo Günnemann","Chunxu Guo","Sylvain Guy","Heonjin Ha","Luyi Han","Il Song Han","Ali Hatamizadeh","Tian He","Jimin Heo","Sebastian Hitziger","SeulGi Hong","SeungBum Hong","Rian Huang","Ziyan Huang","Markus Huellebrand","Stephan Huschauer","Mustaffa Hussain","Tomoo Inubushi","Ece Isik Polat","Mojtaba Jafaritadi","SeongHun Jeong","Bailiang Jian","Yuanhong Jiang","Zhifan Jiang","Yueming Jin","Smriti Joshi","Abdolrahim Kadkhodamohammadi","Reda Abdellah Kamraoui","Inha Kang","Junghwa Kang","Davood Karimi","April Khademi","Muhammad Irfan Khan","Suleiman A. Khan","Rishab Khantwal","Kwang-Ju Kim","Timothy Kline","Satoshi Kondo","Elina Kontio","Adrian Krenzer","Artem Kroviakov","Hugo Kuijf","Satyadwyoom Kumar","Francesco La Rosa","Abhi Lad","Doohee Lee","Minho Lee","Chiara Lena","Hao Li","Ling Li","Xingyu Li","Fuyuan Liao","KuanLun Liao","Arlindo Limede Oliveira","Chaonan Lin","Shan Lin","Akis Linardos","Marius George Linguraru","Han Liu","Tao Liu","Di Liu","Yanling Liu","João Lourenço-Silva","Jingpei Lu","Jiangshan Lu","Imanol Luengo","Christina B. Lund","Huan Minh Luu","Yi Lv","Yi Lv","Uzay Macar","Leon Maechler","Sina Mansour L.","Kenji Marshall","Moona Mazher","Richard McKinley","Alfonso Medela","Felix Meissen","Mingyuan Meng","Dylan Miller","Seyed Hossein Mirjahanmardi","Arnab Mishra","Samir Mitha","Hassan Mohy-ud-Din","Tony Chi Wing Mok","Gowtham Krishnan Murugesan","Enamundram Naga Karthik","Sahil Nalawade","Jakub Nalepa","Mohamed Naser","Ramin Nateghi","Hammad Naveed","Quang-Minh Nguyen","Cuong Nguyen Quoc","Brennan Nichyporuk","Bruno Oliveira","David Owen","Jimut Bahan Pal","Junwen Pan","Wentao Pan","Winnie Pang","Bogyu Park","Vivek Pawar","Kamlesh Pawar","Michael Peven","Lena Philipp","Tomasz Pieciak","Szymon Plotka","Marcel Plutat","Fattaneh Pourakpour","Domen Preložnik","Kumaradevan Punithakumar","Abdul Qayyum","Sandro Queirós","Arman Rahmim","Salar Razavi","Jintao Ren","Mina Rezaei","Jonathan Adam Rico","ZunHyan Rieu","Markus Rink","Johannes Roth","Yusely Ruiz-Gonzalez","Numan Saeed","Anindo Saha","Mostafa Salem","Ricardo Sanchez-Matilla","Kurt Schilling","Wei Shao","Zhiqiang Shen","Ruize Shi","Pengcheng Shi","Daniel Sobotka","Théodore Soulier","Bella Specktor Fadida","Danail Stoyanov","Timothy Sum Hon Mun","Xiaowu Sun","Rong Tao","Franz Thaler","Antoine Théberge","Felix Thielke","Helena Torres","Kareem A. Wahid","Jiacheng Wang","YiFei Wang","Wei Wang","Xiong Wang","Jianhui Wen","Ning Wen","Marek Wodzinski","Ye Wu","Fangfang Xia","Tianqi Xiang","Chen Xiaofei","Lizhan Xu","Tingting Xue","Yuxuan Yang","Lin Yang","Kai Yao","Huifeng Yao","Amirsaeed Yazdani","Michael Yip","Hwanseung Yoo","Fereshteh Yousefirizi","Shunkai Yu","Lei Yu","Jonathan Zamora","Ramy Ashraf Zeineldin","Dewen Zeng","Jianpeng Zhang","Bokai Zhang","Jiapeng Zhang","Fan Zhang","Huahong Zhang","Zhongchen Zhao","Zixuan Zhao","Jiachen Zhao","Can Zhao","Qingshuo Zheng","Yuheng Zhi","Ziqi Zhou","Baosheng Zou","Klaus Maier-Hein","Paul F. Jäger","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2212.08568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08554v1","updated":"2022-12-16T16:12:51Z","published":"2022-12-16T16:12:51Z","title":"An automated parameter domain decomposition approach for gravitational\n  wave surrogates using hp-greedy refinement","summary":"  We introduce hp-greedy, a refinement approach for building gravitational wave\nsurrogates as an extension of the standard reduced basis framework. Our\nproposal is data-driven, with a domain decomposition of the parameter space,\nlocal reduced basis, and a binary tree as the resulting structure, which are\nobtained in an automated way. When compared to the standard global reduced\nbasis approach, the numerical simulations of our proposal show three salient\nfeatures: i) representations of lower dimension with no loss of accuracy, ii) a\nsignificantly higher accuracy for a fixed maximum dimensionality of the basis,\nin some cases by orders of magnitude, and iii) results that depend on the\nreduced basis seed choice used by the refinement algorithm. We first illustrate\nthe key parts of our approach with a toy model and then present a more\nrealistic use case of gravitational waves emitted by the collision of two\nspinning, non-precessing black holes. We discuss performance aspects of\nhp-greedy, such as overfitting with respect to the depth of the tree structure,\nand other hyperparameter dependences. As two direct applications of the\nproposed hp-greedy refinement, we envision: i) a further acceleration of\nstatistical inference, which might be complementary to focused reduced-order\nquadratures, and ii) the search of gravitational waves through clustering and\nnearest neighbors.\n","authors":["Franco Cerino","J. Andrés Diaz-Pace","Manuel Tiglio"],"pdf_url":"https://arxiv.org/pdf/2212.08554v1.pdf","comment":"16 pages, code available from authors upon request"},{"id":"http://arxiv.org/abs/2212.08546v1","updated":"2022-12-16T15:55:16Z","published":"2022-12-16T15:55:16Z","title":"Estimating truncation effects of quantum bosonic systems using sampling\n  algorithms","summary":"  To simulate bosons on a qubit- or qudit-based quantum computer, one has to\nregularize the theory by truncating infinite-dimensional local Hilbert spaces\nto finite dimensions. In the search for practical quantum applications, it is\nimportant to know how big the truncation errors can be. In general, it is not\neasy to estimate errors unless we have a good quantum computer. In this paper\nwe show that traditional sampling methods on classical devices, specifically\nMarkov Chain Monte Carlo, can address this issue with a reasonable amount of\ncomputational resources available today. As a demonstration, we apply this idea\nto the scalar field theory on a two-dimensional lattice, with a size that goes\nbeyond what is achievable using exact diagonalization methods. This method can\nbe used to estimate the resources needed for realistic quantum simulations of\nbosonic theories, and also, to check the validity of the results of the\ncorresponding quantum simulations.\n","authors":["Masanori Hanada","Junyu Liu","Enrico Rinaldi","Masaki Tezuka"],"pdf_url":"https://arxiv.org/pdf/2212.08546v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2110.03905v3","updated":"2022-12-16T15:45:18Z","published":"2021-10-08T05:57:30Z","title":"COVID-19 Monitoring System using Social Distancing and Face Mask\n  Detection on Surveillance video datasets","summary":"  In the current times, the fear and danger of COVID-19 virus still stands\nlarge. Manual monitoring of social distancing norms is impractical with a large\npopulation moving about and with insufficient task force and resources to\nadminister them. There is a need for a lightweight, robust and 24X7\nvideo-monitoring system that automates this process. This paper proposes a\ncomprehensive and effective solution to perform person detection, social\ndistancing violation detection, face detection and face mask classification\nusing object detection, clustering and Convolution Neural Network (CNN) based\nbinary classifier. For this, YOLOv3, Density-based spatial clustering of\napplications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and\nMobileNetV2 based binary classifier have been employed on surveillance video\ndatasets. This paper also provides a comparative study of different face\ndetection and face mask classification models. Finally, a video dataset\nlabelling method is proposed along with the labelled video dataset to\ncompensate for the lack of dataset in the community and is used for evaluation\nof the system. The system performance is evaluated in terms of accuracy, F1\nscore as well as the prediction time, which has to be low for practical\napplicability. The system performs with an accuracy of 91.2% and F1 score of\n90.79% on the labelled video dataset and has an average prediction time of 7.12\nseconds for 78 frames of a video.\n","authors":["Sahana Srinivasan","Rujula Singh R","Ruchita R Biradar","Revathi SA"],"pdf_url":"https://arxiv.org/pdf/2110.03905v3.pdf","comment":"I, Rujula Singh R, would like to apologize to the research community\n  for the confusion caused by the inconsistency in author lists between\n  multiple versions of this paper. I take full responsibility for this error\n  and will be more diligent in the future to ensure the accuracy and\n  consistency of our research publications"},{"id":"http://arxiv.org/abs/2212.08541v1","updated":"2022-12-16T15:43:41Z","published":"2022-12-16T15:43:41Z","title":"Learnable Commutative Monoids for Graph Neural Networks","summary":"  Graph neural networks (GNNs) have been shown to be highly sensitive to the\nchoice of aggregation function. While summing over a node's neighbours can\napproximate any permutation-invariant function over discrete inputs,\nCohen-Karlik et al. [2020] proved there are set-aggregation problems for which\nsumming cannot generalise to unbounded inputs, proposing recurrent neural\nnetworks regularised towards permutation-invariance as a more expressive\naggregator. We show that these results carry over to the graph domain: GNNs\nequipped with recurrent aggregators are competitive with state-of-the-art\npermutation-invariant aggregators, on both synthetic benchmarks and real-world\nproblems. However, despite the benefits of recurrent aggregators, their $O(V)$\ndepth makes them both difficult to parallelise and harder to train on large\ngraphs. Inspired by the observation that a well-behaved aggregator for a GNN is\na commutative monoid over its latent space, we propose a framework for\nconstructing learnable, commutative, associative binary operators. And with\nthis, we construct an aggregator of $O(\\log V)$ depth, yielding exponential\nimprovements for both parallelism and dependency length while achieving\nperformance competitive with recurrent aggregators. Based on our empirical\nobservations, our proposed learnable commutative monoid (LCM) aggregator\nrepresents a favourable tradeoff between efficient and expressive aggregators.\n","authors":["Euan Ong","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2212.08541v1.pdf","comment":"Accepted to the proceedings of the First Learning on Graphs\n  Conference (LoG 2022)"},{"id":"http://arxiv.org/abs/2212.07428v2","updated":"2022-12-16T15:35:37Z","published":"2022-12-14T10:50:13Z","title":"Towards Linguistically Informed Multi-Objective Pre-Training for Natural\n  Language Inference","summary":"  We introduce a linguistically enhanced combination of pre-training methods\nfor transformers. The pre-training objectives include POS-tagging, synset\nprediction based on semantic knowledge graphs, and parent prediction based on\ndependency parse trees. Our approach achieves competitive results on the\nNatural Language Inference task, compared to the state of the art. Specifically\nfor smaller models, the method results in a significant performance boost,\nemphasizing the fact that intelligent pre-training can make up for fewer\nparameters and help building more efficient models. Combining POS-tagging and\nsynset prediction yields the overall best results.\n","authors":["Maren Pielka","Svetlana Schmidt","Lisa Pucknat","Rafet Sifa"],"pdf_url":"https://arxiv.org/pdf/2212.07428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08532v1","updated":"2022-12-16T15:32:49Z","published":"2022-12-16T15:32:49Z","title":"Do Not Trust a Model Because It is Confident: Uncovering and\n  Characterizing Unknown Unknowns to Student Success Predictors in Online-Based\n  Learning","summary":"  Student success models might be prone to develop weak spots, i.e., examples\nhard to accurately classify due to insufficient representation during model\ncreation. This weakness is one of the main factors undermining users' trust,\nsince model predictions could for instance lead an instructor to not intervene\non a student in need. In this paper, we unveil the need of detecting and\ncharacterizing unknown unknowns in student success prediction in order to\nbetter understand when models may fail. Unknown unknowns include the students\nfor which the model is highly confident in its predictions, but is actually\nwrong. Therefore, we cannot solely rely on the model's confidence when\nevaluating the predictions quality. We first introduce a framework for the\nidentification and characterization of unknown unknowns. We then assess its\ninformativeness on log data collected from flipped courses and online courses\nusing quantitative analyses and interviews with instructors. Our results show\nthat unknown unknowns are a critical issue in this domain and that our\nframework can be applied to support their detection. The source code is\navailable at https://github.com/epfl-ml4ed/unknown-unknowns.\n","authors":["Roberta Galici","Tanja Käser","Gianni Fenu","Mirko Marras"],"pdf_url":"https://arxiv.org/pdf/2212.08532v1.pdf","comment":"Accepted as a full paper at the International Conference on Learning\n  Analytics & Knowledge (LAK23)"},{"id":"http://arxiv.org/abs/2206.08871v2","updated":"2022-12-16T15:18:22Z","published":"2022-06-17T16:18:28Z","title":"How Robust is Unsupervised Representation Learning to Distribution\n  Shift?","summary":"  The robustness of machine learning algorithms to distributions shift is\nprimarily discussed in the context of supervised learning (SL). As such, there\nis a lack of insight on the robustness of the representations learned from\nunsupervised methods, such as self-supervised learning (SSL) and auto-encoder\nbased algorithms (AE), to distribution shift. We posit that the input-driven\nobjectives of unsupervised algorithms lead to representations that are more\nrobust to distribution shift than the target-driven objective of SL. We verify\nthis by extensively evaluating the performance of SSL and AE on both synthetic\nand realistic distribution shift datasets. Following observations that the\nlinear layer used for classification itself can be susceptible to spurious\ncorrelations, we evaluate the representations using a linear head trained on a\nsmall amount of out-of-distribution (OOD) data, to isolate the robustness of\nthe learned representations from that of the linear head. We also develop\n\"controllable\" versions of existing realistic domain generalisation datasets\nwith adjustable degrees of distribution shifts. This allows us to study the\nrobustness of different learning algorithms under versatile yet realistic\ndistribution shift conditions. Our experiments show that representations\nlearned from unsupervised learning algorithms generalise better than SL under a\nwide variety of extreme as well as realistic distribution shifts.\n","authors":["Yuge Shi","Imant Daunhawer","Julia E. Vogt","Philip H. S. Torr","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2206.08871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07282v2","updated":"2022-12-16T15:11:16Z","published":"2022-12-14T15:30:56Z","title":"Directional Direct Feedback Alignment: Estimating Backpropagation Paths\n  for Efficient Learning on Neural Processors","summary":"  The error Backpropagation algorithm (BP) is a key method for training deep\nneural networks. While performant, it is also resource-demanding in terms of\ncomputation, memory usage and energy. This makes it unsuitable for online\nlearning on edge devices that require a high processing rate and low energy\nconsumption. More importantly, BP does not take advantage of the parallelism\nand local characteristics offered by dedicated neural processors. There is\ntherefore a demand for alternative algorithms to BP that could improve the\nlatency, memory requirements, and energy footprint of neural networks on\nhardware. In this work, we propose a novel method based on Direct Feedback\nAlignment (DFA) which uses Forward-Mode Automatic Differentiation to estimate\nbackpropagation paths and learn feedback connections in an online manner. We\nexperimentally show that Directional DFA achieves performances that are closer\nto BP than other feedback methods on several benchmark datasets and\narchitectures while benefiting from the locality and parallelization\ncharacteristics of DFA. Moreover, we show that, unlike other feedback learning\nalgorithms, our method provides stable learning for convolution layers.\n","authors":["Florian Bacho","Dominique Chu"],"pdf_url":"https://arxiv.org/pdf/2212.07282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.05957v2","updated":"2022-12-16T15:03:15Z","published":"2022-09-13T12:46:57Z","title":"Adversarial Inter-Group Link Injection Degrades the Fairness of Graph\n  Neural Networks","summary":"  We present evidence for the existence and effectiveness of adversarial\nattacks on graph neural networks (GNNs) that aim to degrade fairness. These\nattacks can disadvantage a particular subgroup of nodes in GNN-based node\nclassification, where nodes of the underlying network have sensitive\nattributes, such as race or gender. We conduct qualitative and experimental\nanalyses explaining how adversarial link injection impairs the fairness of GNN\npredictions. For example, an attacker can compromise the fairness of GNN-based\nnode classification by injecting adversarial links between nodes belonging to\nopposite subgroups and opposite class labels. Our experiments on empirical\ndatasets demonstrate that adversarial fairness attacks can significantly\ndegrade the fairness of GNN predictions (attacks are effective) with a low\nperturbation rate (attacks are efficient) and without a significant drop in\naccuracy (attacks are deceptive). This work demonstrates the vulnerability of\nGNN models to adversarial fairness attacks. We hope our findings raise\nawareness about this issue in our community and lay a foundation for the future\ndevelopment of GNN models that are more robust to such attacks.\n","authors":["Hussain Hussain","Meng Cao","Sandipan Sikdar","Denis Helic","Elisabeth Lex","Markus Strohmaier","Roman Kern"],"pdf_url":"https://arxiv.org/pdf/2209.05957v2.pdf","comment":"A shorter version of this work has been accepted by IEEE ICDM 2022"},{"id":"http://arxiv.org/abs/2209.15154v2","updated":"2022-12-16T15:01:34Z","published":"2022-09-30T00:49:31Z","title":"Variable-Based Calibration for Machine Learning Classifiers","summary":"  The deployment of machine learning classifiers in high-stakes domains\nrequires well-calibrated confidence scores for model predictions. In this paper\nwe introduce the notion of variable-based calibration to characterize\ncalibration properties of a model with respect to a variable of interest,\ngeneralizing traditional score-based calibration and metrics such as expected\ncalibration error (ECE). In particular, we find that models with near-perfect\nECE can exhibit significant variable-based calibration error as a function of\nfeatures of the data. We demonstrate this phenomenon both theoretically and in\npractice on multiple well-known datasets, and show that it can persist after\nthe application of existing recalibration methods. To mitigate this issue, we\npropose strategies for detection, visualization, and quantification of\nvariable-based calibration error. We then examine the limitations of current\nscore-based recalibration methods and explore potential modifications. Finally,\nwe discuss the implications of these findings, emphasizing that an\nunderstanding of calibration beyond simple aggregate measures is crucial for\nendeavors such as fairness and model interpretability.\n","authors":["Markelle Kelly","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2209.15154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.07011v2","updated":"2022-12-16T14:59:17Z","published":"2022-10-13T13:19:51Z","title":"Variational Graph Generator for Multi-View Graph Clustering","summary":"  Multi-view graph clustering (MGC) methods are increasingly being studied due\nto the explosion of multi-view data with graph structural information. The\ncritical point of MGC is to better utilize the view-specific and view-common\ninformation in features and graphs of multiple views. However, existing works\nhave an inherent limitation that they are unable to concurrently utilize the\nconsensus graph information across multiple graphs and the view-specific\nfeature information. To address this issue, we propose Variational Graph\nGenerator for Multi-View Graph Clustering (VGMGC). Specifically, a novel\nvariational graph generator is proposed to extract common information among\nmultiple graphs. This generator infers a reliable variational consensus graph\nbased on a priori assumption over multiple graphs. Then a simple yet effective\ngraph encoder in conjunction with the multi-view clustering objective is\npresented to learn the desired graph embeddings for clustering, which embeds\nthe inferred view-common graph and view-specific graphs together with features.\nFinally, theoretical results illustrate the rationality of VGMGC by analyzing\nthe uncertainty of the inferred consensus graph with information bottleneck\nprinciple. Extensive experiments demonstrate the superior performance of our\nVGMGC over SOTAs.\n","authors":["Jianpeng Chen","Yawen Ling","Jie Xu","Yazhou Ren","Shudong Huang","Xiaorong Pu","Zhifeng Hao","Philip S. Yu","Lifang He"],"pdf_url":"https://arxiv.org/pdf/2210.07011v2.pdf","comment":"submitted to TNNLS"},{"id":"http://arxiv.org/abs/2212.08507v1","updated":"2022-12-16T14:40:25Z","published":"2022-12-16T14:40:25Z","title":"Robust Explanation Constraints for Neural Networks","summary":"  Post-hoc explanation methods are used with the intent of providing insights\nabout neural networks and are sometimes said to help engender trust in their\noutputs. However, popular explanations methods have been found to be fragile to\nminor perturbations of input features or model parameters. Relying on\nconstraint relaxation techniques from non-convex optimization, we develop a\nmethod that upper-bounds the largest change an adversary can make to a\ngradient-based explanation via bounded manipulation of either the input\nfeatures or model parameters. By propagating a compact input or parameter set\nas symbolic intervals through the forwards and backwards computations of the\nneural network we can formally certify the robustness of gradient-based\nexplanations. Our bounds are differentiable, hence we can incorporate provable\nexplanation robustness into neural network training. Empirically, our method\nsurpasses the robustness provided by previous heuristic approaches. We find\nthat our training method is the only method able to learn neural networks with\ncertificates of explanation robustness across all six datasets tested.\n","authors":["Matthew Wicker","Juyeon Heo","Luca Costabello","Adrian Weller"],"pdf_url":"https://arxiv.org/pdf/2212.08507v1.pdf","comment":"23 pages, 12 figures"},{"id":"http://arxiv.org/abs/2212.08496v1","updated":"2022-12-16T14:21:29Z","published":"2022-12-16T14:21:29Z","title":"Federated Learning with Flexible Control","summary":"  Federated learning (FL) enables distributed model training from local data\ncollected by users. In distributed systems with constrained resources and\npotentially high dynamics, e.g., mobile edge networks, the efficiency of FL is\nan important problem. Existing works have separately considered different\nconfigurations to make FL more efficient, such as infrequent transmission of\nmodel updates, client subsampling, and compression of update vectors. However,\nan important open problem is how to jointly apply and tune these control knobs\nin a single FL algorithm, to achieve the best performance by allowing a high\ndegree of freedom in control decisions. In this paper, we address this problem\nand propose FlexFL - an FL algorithm with multiple options that can be adjusted\nflexibly. Our FlexFL algorithm allows both arbitrary rates of local computation\nat clients and arbitrary amounts of communication between clients and the\nserver, making both the computation and communication resource consumption\nadjustable. We prove a convergence upper bound of this algorithm. Based on this\nresult, we further propose a stochastic optimization formulation and algorithm\nto determine the control decisions that (approximately) minimize the\nconvergence bound, while conforming to constraints related to resource\nconsumption. The advantage of our approach is also verified using experiments.\n","authors":["Shiqiang Wang","Jake Perazzone","Mingyue Ji","Kevin S. Chan"],"pdf_url":"https://arxiv.org/pdf/2212.08496v1.pdf","comment":"Accepted to IEEE INFOCOM 2023"},{"id":"http://arxiv.org/abs/2212.01199v3","updated":"2022-12-16T14:12:08Z","published":"2022-12-02T14:25:58Z","title":"Gibbs-Helmholtz Graph Neural Network: capturing the temperature\n  dependency of activity coefficients at infinite dilution","summary":"  The accurate prediction of physicochemical properties of chemical compounds\nin mixtures (such as the activity coefficient at infinite dilution\n$\\gamma_{ij}^\\infty$) is essential for developing novel and more sustainable\nchemical processes. In this work, we analyze the performance of\npreviously-proposed GNN-based models for the prediction of\n$\\gamma_{ij}^\\infty$, and compare them with several mechanistic models in a\nseries of 9 isothermal studies. Moreover, we develop the Gibbs-Helmholtz Graph\nNeural Network (GH-GNN) model for predicting $\\ln \\gamma_{ij}^\\infty$ of\nmolecular systems at different temperatures. Our method combines the simplicity\nof a Gibbs-Helmholtz-derived expression with a series of graph neural networks\nthat incorporate explicit molecular and intermolecular descriptors for\ncapturing dispersion and hydrogen bonding effects. We have trained this model\nusing experimentally determined $\\ln \\gamma_{ij}^\\infty$ data of 40,219\nbinary-systems involving 1032 solutes and 866 solvents, overall showing\nsuperior performance compared to the popular UNIFAC-Dortmund model. We analyze\nthe performance of GH-GNN for continuous and discrete inter/extrapolation and\ngive indications for the model's applicability domain and expected accuracy. In\ngeneral, GH-GNN is able to produce accurate predictions for extrapolated\nbinary-systems if at least 25 systems with the same combination of\nsolute-solvent chemical classes are contained in the training set and a\nsimilarity indicator above 0.35 is also present. This model and its\napplicability domain recommendations have been made open-source at\nhttps://github.com/edgarsmdn/GH-GNN.\n","authors":["Edgar Ivan Sanchez Medina","Steffen Linke","Martin Stoll","Kai Sundmacher"],"pdf_url":"https://arxiv.org/pdf/2212.01199v3.pdf","comment":"Code available at: https://github.com/edgarsmdn/GH-GNN"},{"id":"http://arxiv.org/abs/2208.10521v2","updated":"2022-12-16T14:05:11Z","published":"2022-08-22T18:01:49Z","title":"Estimation Contracts for Outlier-Robust Geometric Perception","summary":"  Outlier-robust estimation is a fundamental problem and has been extensively\ninvestigated by statisticians and practitioners. The last few years have seen a\nconvergence across research fields towards \"algorithmic robust statistics\",\nwhich focuses on developing tractable outlier-robust techniques for\nhigh-dimensional estimation problems. Despite this convergence, research\nefforts across fields have been mostly disconnected from one another. This\nmonograph bridges recent work on certifiable outlier-robust estimation for\ngeometric perception in robotics and computer vision with parallel work in\nrobust statistics. In particular, we adapt and extend recent results on robust\nlinear regression (applicable to the low-outlier regime with << 50% outliers)\nand list-decodable regression (applicable to the high-outlier regime with >>\n50% outliers) to the setup commonly found in robotics and vision, where (i)\nvariables (e.g., rotations, poses) belong to a non-convex domain, (ii)\nmeasurements are vector-valued, and (iii) the number of outliers is not known a\npriori. The emphasis here is on performance guarantees: rather than proposing\nradically new algorithms, we provide conditions on the input measurements under\nwhich modern estimation algorithms (possibly after small modifications) are\nguaranteed to recover an estimate close to the ground truth in the presence of\noutliers. These conditions are what we call an \"estimation contract\". Besides\nthe proposed extensions of existing results, we believe the main contributions\nof this monograph are (i) to unify parallel research lines by pointing out\ncommonalities and differences, (ii) to introduce advanced material (e.g.,\nsum-of-squares proofs) in an accessible and self-contained presentation for the\npractitioner, and (iii) to point out a few immediate opportunities and open\nquestions in outlier-robust geometric perception.\n","authors":["Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2208.10521v2.pdf","comment":"95 pages, 12 figures"},{"id":"http://arxiv.org/abs/2212.08479v1","updated":"2022-12-16T13:46:17Z","published":"2022-12-16T13:46:17Z","title":"Neural Implicit k-Space for Binning-free Non-Cartesian Cardiac MR\n  Imaging","summary":"  In this work, we propose a novel image reconstruction framework that directly\nlearns a neural implicit representation in k-space for ECG-triggered\nnon-Cartesian Cardiac Magnetic Resonance Imaging (CMR). While existing methods\nbin acquired data from neighboring time points to reconstruct one phase of the\ncardiac motion, our framework allows for a continuous, binning-free, and\nsubject-specific k-space representation.We assign a unique coordinate that\nconsists of time, coil index, and frequency domain location to each sampled\nk-space point. We then learn the subject-specific mapping from these unique\ncoordinates to k-space intensities using a multi-layer perceptron with\nfrequency domain regularization. During inference, we obtain a complete k-space\nfor Cartesian coordinates and an arbitrary temporal resolution. A simple\ninverse Fourier transform recovers the image, eliminating the need for density\ncompensation and costly non-uniform Fourier transforms for non-Cartesian data.\nThis novel imaging framework was tested on 42 radially sampled datasets from 6\nsubjects. The proposed method outperforms other techniques qualitatively and\nquantitatively using data from four and one heartbeat(s) and 30 cardiac phases.\nOur results for one heartbeat reconstruction of 50 cardiac phases show improved\nartifact removal and spatio-temporal resolution, leveraging the potential for\nreal-time CMR.\n","authors":["Wenqi Huang","Hongwei Li","Gastao Cruz","Jiazhen Pan","Daniel Rueckert","Kerstin Hammernik"],"pdf_url":"https://arxiv.org/pdf/2212.08479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08458v1","updated":"2022-12-16T13:07:09Z","published":"2022-12-16T13:07:09Z","title":"Fast Rule-Based Decoding: Revisiting Syntactic Rules in Neural\n  Constituency Parsing","summary":"  Most recent studies on neural constituency parsing focus on encoder\nstructures, while few developments are devoted to decoders. Previous research\nhas demonstrated that probabilistic statistical methods based on syntactic\nrules are particularly effective in constituency parsing, whereas syntactic\nrules are not used during the training of neural models in prior work probably\ndue to their enormous computation requirements. In this paper, we first\nimplement a fast CKY decoding procedure harnessing GPU acceleration, based on\nwhich we further derive a syntactic rule-based (rule-constrained) CKY decoding.\nIn the experiments, our method obtains 95.89 and 92.52 F1 on the datasets of\nPTB and CTB respectively, which shows significant improvements compared with\nprevious approaches. Besides, our parser achieves strong and competitive\ncross-domain performance in zero-shot settings.\n","authors":["Tianyu Shi","Zhicheng Wang","Liyin Xiao","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.07127v3","updated":"2022-12-16T12:52:19Z","published":"2022-12-14T09:26:07Z","title":"Towards mapping the contemporary art world with ArtLM: an art-specific\n  NLP model","summary":"  With an increasing amount of data in the art world, discovering artists and\nartworks suitable to collectors' tastes becomes a challenge. It is no longer\nenough to use visual information, as contextual information about the artist\nhas become just as important in contemporary art. In this work, we present a\ngeneric Natural Language Processing framework (called ArtLM) to discover the\nconnections among contemporary artists based on their biographies. In this\napproach, we first continue to pre-train the existing general English language\nmodels with a large amount of unlabelled art-related data. We then fine-tune\nthis new pre-trained model with our biography pair dataset manually annotated\nby a team of professionals in the art industry. With extensive experiments, we\ndemonstrate that our ArtLM achieves 85.6% accuracy and 84.0% F1 score and\noutperforms other baseline models. We also provide a visualisation and a\nqualitative analysis of the artist network built from ArtLM's outputs.\n","authors":["Qinkai Chen","Mohamed El-Mennaoui","Antoine Fosset","Amine Rebei","Haoyang Cao","Philine Bouscasse","Christy Eóin O'Beirne","Sasha Shevchenko","Mathieu Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2212.07127v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14990v2","updated":"2022-12-16T12:52:07Z","published":"2022-09-29T17:51:51Z","title":"Partially Observable RL with B-Stability: Unified Structural Condition\n  and Sharp Sample-Efficient Algorithms","summary":"  Partial Observability -- where agents can only observe partial information\nabout the true underlying state of the system -- is ubiquitous in real-world\napplications of Reinforcement Learning (RL). Theoretically, learning a\nnear-optimal policy under partial observability is known to be hard in the\nworst case due to an exponential sample complexity lower bound. Recent work has\nidentified several tractable subclasses that are learnable with polynomial\nsamples, such as Partially Observable Markov Decision Processes (POMDPs) with\ncertain revealing or decodability conditions. However, this line of research is\nstill in its infancy, where (1) unified structural conditions enabling\nsample-efficient learning are lacking; (2) existing sample complexities for\nknown tractable subclasses are far from sharp; and (3) fewer sample-efficient\nalgorithms are available than in fully observable RL.\n  This paper advances all three aspects above for Partially Observable RL in\nthe general setting of Predictive State Representations (PSRs). First, we\npropose a natural and unified structural condition for PSRs called\n\\emph{B-stability}. B-stable PSRs encompasses the vast majority of known\ntractable subclasses such as weakly revealing POMDPs, low-rank\nfuture-sufficient POMDPs, decodable POMDPs, and regular PSRs. Next, we show\nthat any B-stable PSR can be learned with polynomial samples in relevant\nproblem parameters. When instantiated in the aforementioned subclasses, our\nsample complexities improve substantially over the current best ones. Finally,\nour results are achieved by three algorithms simultaneously: Optimistic Maximum\nLikelihood Estimation, Estimation-to-Decisions, and Model-Based Optimistic\nPosterior Sampling. The latter two algorithms are new for sample-efficient\nlearning of POMDPs/PSRs.\n","authors":["Fan Chen","Yu Bai","Song Mei"],"pdf_url":"https://arxiv.org/pdf/2209.14990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08445v1","updated":"2022-12-16T12:45:16Z","published":"2022-12-16T12:45:16Z","title":"Conditional Generative Adversarial Network for keystroke presentation\n  attack","summary":"  Cybersecurity is a crucial step in data protection to ensure user security\nand personal data privacy. In this sense, many companies have started to\ncontrol and restrict access to their data using authentication systems.\nHowever, these traditional authentication methods, are not enough for ensuring\ndata protection, and for this reason, behavioral biometrics have gained\nimportance. Despite their promising results and the wide range of applications,\nbiometric systems have shown to be vulnerable to malicious attacks, such as\nPresentation Attacks. For this reason, in this work, we propose to study a new\napproach aiming to deploy a presentation attack towards a keystroke\nauthentication system. Our idea is to use Conditional Generative Adversarial\nNetworks (cGAN) for generating synthetic keystroke data that can be used for\nimpersonating an authorized user. These synthetic data are generated following\ntwo different real use cases, one in which the order of the typed words is\nknown (ordered dynamic) and the other in which this order is unknown\n(no-ordered dynamic). Finally, both keystroke dynamics (ordered and no-ordered)\nare validated using an external keystroke authentication system. Results\nindicate that the cGAN can effectively generate keystroke dynamics patterns\nthat can be used for deceiving keystroke authentication systems.\n","authors":["Idoia Eizaguirre-Peral","Lander Segurola-Gil","Francesco Zola"],"pdf_url":"https://arxiv.org/pdf/2212.08445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.07859v2","updated":"2022-12-16T12:44:36Z","published":"2022-07-16T07:23:45Z","title":"Deep Learning and Its Applications to WiFi Human Sensing: A Benchmark\n  and A Tutorial","summary":"  WiFi sensing has been evolving rapidly in recent years. Empowered by\npropagation models and deep learning methods, many challenging applications are\nrealized such as WiFi-based human activity recognition and gesture recognition.\nHowever, in contrast to deep learning for visual recognition and natural\nlanguage processing, no sufficiently comprehensive public benchmark exists. In\nthis paper, we highlight the recent progress on deep learning enabled WiFi\nsensing, and then propose a benchmark, SenseFi, to study the effectiveness of\nvarious deep learning models for WiFi sensing. These advanced models are\ncompared in terms of distinct sensing tasks, WiFi platforms, recognition\naccuracy, model size, computational complexity, feature transferability, and\nadaptability of unsupervised learning. It is also regarded as a tutorial for\ndeep learning based WiFi sensing, starting from CSI hardware platform to\nsensing algorithms. The extensive experiments provide us with experiences in\ndeep model design, learning strategy skills and training techniques for\nreal-world applications. To the best of our knowledge, this is the first\nbenchmark with an open-source library for deep learning in WiFi sensing\nresearch. The benchmark codes are available at\nhttps://github.com/CHENXINYAN-sg/WiFi-CSI-Sensing-Benchmark.\n","authors":["Jianfei Yang","Xinyan Chen","Dazhuo Wang","Han Zou","Chris Xiaoxuan Lu","Sumei Sun","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2207.07859v2.pdf","comment":"A benchmark and tutorial for WiFi CSI Human sensing based on deep\n  learning methods"},{"id":"http://arxiv.org/abs/2211.07915v4","updated":"2022-12-16T12:30:00Z","published":"2022-11-15T06:00:28Z","title":"Backdoor Attacks on Time Series: A Generative Approach","summary":"  Backdoor attacks have emerged as one of the major security threats to deep\nlearning models as they can easily control the model's test-time predictions by\npre-injecting a backdoor trigger into the model at training time. While\nbackdoor attacks have been extensively studied on images, few works have\ninvestigated the threat of backdoor attacks on time series data. To fill this\ngap, in this paper we present a novel generative approach for time series\nbackdoor attacks against deep learning based time series classifiers. Backdoor\nattacks have two main goals: high stealthiness and high attack success rate. We\nfind that, compared to images, it can be more challenging to achieve the two\ngoals on time series. This is because time series have fewer input dimensions\nand lower degrees of freedom, making it hard to achieve a high attack success\nrate without compromising stealthiness. Our generative approach addresses this\nchallenge by generating trigger patterns that are as realistic as real-time\nseries patterns while achieving a high attack success rate without causing a\nsignificant drop in clean accuracy. We also show that our proposed attack is\nresistant to potential backdoor defenses. Furthermore, we propose a novel\nuniversal generator that can poison any type of time series with a single\ngenerator that allows universal attacks without the need to fine-tune the\ngenerative model for new time series datasets.\n","authors":["Yujing Jiang","Xingjun Ma","Sarah Monazam Erfani","James Bailey"],"pdf_url":"https://arxiv.org/pdf/2211.07915v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05781v2","updated":"2022-12-16T12:06:16Z","published":"2022-12-12T09:07:37Z","title":"Robust Recurrent Neural Network to Identify Ship Motion in Open Water\n  with Performance Guarantees -- Technical Report","summary":"  Recurrent neural networks are capable of learning the dynamics of an unknown\nnonlinear system purely from input-output measurements. However, the resulting\nmodels do not provide any stability guarantees on the input-output mapping. In\nthis work, we represent a recurrent neural network as a linear time-invariant\nsystem with nonlinear disturbances. By introducing constraints on the\nparameters, we can guarantee finite gain stability and incremental finite gain\nstability. We apply this identification method to learn the motion of a\nfour-degrees-of-freedom ship that is moving in open water and compare it\nagainst other purely learning-based approaches with unconstrained parameters.\nOur analysis shows that the constrained recurrent neural network has a lower\nprediction accuracy on the test set, but it achieves comparable results on an\nout-of-distribution set and respects stability conditions.\n","authors":["Daniel Frank","Decky Aspandi Latif","Michael Muehlebach","Benjamin Unger","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2212.05781v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.01955v2","updated":"2022-12-16T11:55:00Z","published":"2021-12-03T15:01:52Z","title":"Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and\n  Distribution-Aware Criterion","summary":"  Various deep neural network (DNN) coverage criteria have been proposed to\nassess DNN test inputs and steer input mutations. The coverage is characterized\nvia neurons having certain outputs, or the discrepancy between neuron outputs.\nNevertheless, recent research indicates that neuron coverage criteria show\nlittle correlation with test suite quality.\n  In general, DNNs approximate distributions, by incorporating hierarchical\nlayers, to make predictions for inputs. Thus, we champion to deduce DNN\nbehaviors based on its approximated distributions from a layer perspective. A\ntest suite should be assessed using its induced layer output distributions.\nAccordingly, to fully examine DNN behaviors, input mutation should be directed\ntoward diversifying the approximated distributions.\n  This paper summarizes eight design requirements for DNN coverage criteria,\ntaking into account distribution properties and practical concerns. We then\npropose a new criterion, NeuraL Coverage (NLC), that satisfies all design\nrequirements. NLC treats a single DNN layer as the basic computational unit\n(rather than a single neuron) and captures four critical properties of neuron\noutput distributions. Thus, NLC accurately describes how DNNs comprehend inputs\nvia approximated distributions. We demonstrate that NLC is significantly\ncorrelated with the diversity of a test suite across a number of tasks\n(classification and generation) and data formats (image and text). Its capacity\nto discover DNN prediction errors is promising. Test input mutation guided by\nNLC results in a greater quality and diversity of exposed erroneous behaviors.\n","authors":["Yuanyuan Yuan","Qi Pang","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2112.01955v2.pdf","comment":"The extended version of a paper to appear in the Proceedings of the\n  45th IEEE/ACM International Conference on Software Engineering, 2023, (ICSE\n  '23), 14 pages"},{"id":"http://arxiv.org/abs/2212.08423v1","updated":"2022-12-16T11:52:15Z","published":"2022-12-16T11:52:15Z","title":"Context Label Learning: Improving Background Class Representations in\n  Semantic Segmentation","summary":"  Background samples provide key contextual information for segmenting regions\nof interest (ROIs). However, they always cover a diverse set of structures,\ncausing difficulties for the segmentation model to learn good decision\nboundaries with high sensitivity and precision. The issue concerns the highly\nheterogeneous nature of the background class, resulting in multi-modal\ndistributions. Empirically, we find that neural networks trained with\nheterogeneous background struggle to map the corresponding contextual samples\nto compact clusters in feature space. As a result, the distribution over\nbackground logit activations may shift across the decision boundary, leading to\nsystematic over-segmentation across different datasets and tasks. In this\nstudy, we propose context label learning (CoLab) to improve the context\nrepresentations by decomposing the background class into several subclasses.\nSpecifically, we train an auxiliary network as a task generator, along with the\nprimary segmentation model, to automatically generate context labels that\npositively affect the ROI segmentation accuracy. Extensive experiments are\nconducted on several challenging segmentation tasks and datasets. The results\ndemonstrate that CoLab can guide the segmentation model to map the logits of\nbackground samples away from the decision boundary, resulting in significantly\nimproved segmentation accuracy. Code is available.\n","authors":["Zeju Li","Konstantinos Kamnitsas","Cheng Ouyang","Chen Chen","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2212.08423v1.pdf","comment":"Provisionally accepted to IEEE Transactions on Medical Imaging"},{"id":"http://arxiv.org/abs/2212.08420v1","updated":"2022-12-16T11:44:01Z","published":"2022-12-16T11:44:01Z","title":"Fake it till you make it: Learning(s) from a synthetic ImageNet clone","summary":"  Recent large-scale image generation models such as Stable Diffusion have\nexhibited an impressive ability to generate fairly realistic images starting\nfrom a very simple text prompt. Could such models render real images obsolete\nfor training image prediction models? In this paper, we answer part of this\nprovocative question by questioning the need for real images when training\nmodels for ImageNet classification. More precisely, provided only with the\nclass names that have been used to build the dataset, we explore the ability of\nStable Diffusion to generate synthetic clones of ImageNet and measure how\nuseful they are for training classification models from scratch. We show that\nwith minimal and class-agnostic prompt engineering those ImageNet clones we\ndenote as ImageNet-SD are able to close a large part of the gap between models\nproduced by synthetic images and models trained with real images for the\nseveral standard classification benchmarks that we consider in this study. More\nimportantly, we show that models trained on synthetic images exhibit strong\ngeneralization properties and perform on par with models trained on real data.\n","authors":["Mert Bulent Sariyildiz","Karteek Alahari","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2212.08420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08415v1","updated":"2022-12-16T11:27:50Z","published":"2022-12-16T11:27:50Z","title":"Person Detection Using an Ultra Low-resolution Thermal Imager on a\n  Low-cost MCU","summary":"  Detecting persons in images or video with neural networks is a well-studied\nsubject in literature. However, such works usually assume the availability of a\ncamera of decent resolution and a high-performance processor or GPU to run the\ndetection algorithm, which significantly increases the cost of a complete\ndetection system. However, many applications require low-cost solutions,\ncomposed of cheap sensors and simple microcontrollers. In this paper, we\ndemonstrate that even on such hardware we are not condemned to simple classic\nimage processing techniques. We propose a novel ultra-lightweight CNN-based\nperson detector that processes thermal video from a low-cost 32x24 pixel static\nimager. Trained and compressed on our own recorded dataset, our model achieves\nup to 91.62% accuracy (F1-score), has less than 10k parameters, and runs as\nfast as 87ms and 46ms on low-cost microcontrollers STM32F407 and STM32F746,\nrespectively.\n","authors":["Maarten Vandersteegen","Wouter Reusen","Kristof Van Beeck","Toon Goedemé"],"pdf_url":"https://arxiv.org/pdf/2212.08415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08410v1","updated":"2022-12-16T11:24:42Z","published":"2022-12-16T11:24:42Z","title":"Teaching Small Language Models to Reason","summary":"  Chain of thought prompting successfully improves the reasoning capabilities\nof large language models, achieving state of the art results on a range of\ndatasets. However, these reasoning capabilities only appear to emerge in models\nwith a size of over 100 billion parameters. In this paper, we explore the\ntransfer of such reasoning capabilities to models with less than 100 billion\nparameters via knowledge distillation. Specifically, we finetune a student\nmodel on the chain of thought outputs generated by a larger teacher model. Our\nexperiments show that the proposed method improves task performance across\narithmetic, commonsense and symbolic reasoning datasets. For example, the\naccuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on\nPaLM-540B generated chains of thought.\n","authors":["Lucie Charlotte Magister","Jonathan Mallinson","Jakub Adamek","Eric Malmi","Aliaksei Severyn"],"pdf_url":"https://arxiv.org/pdf/2212.08410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02704v2","updated":"2022-12-16T11:24:24Z","published":"2022-04-06T10:00:33Z","title":"Fundamental limits to learning closed-form mathematical models from data","summary":"  Given a finite and noisy dataset generated with a closed-form mathematical\nmodel, when is it possible to learn the true generating model from the data\nalone? This is the question we investigate here. We show that this\nmodel-learning problem displays a transition from a low-noise phase in which\nthe true model can be learned, to a phase in which the observation noise is too\nhigh for the true model to be learned by any method. Both in the low-noise\nphase and in the high-noise phase, probabilistic model selection leads to\noptimal generalization to unseen data. This is in contrast to standard machine\nlearning approaches, including artificial neural networks, which in this\nparticular problem are limited, in the low-noise phase, by their ability to\ninterpolate. In the transition region between the learnable and unlearnable\nphases, generalization is hard for all approaches including probabilistic model\nselection.\n","authors":["Oscar Fajardo-Fontiveros","Ignasi Reichardt","Harry R. De Los Rios","Jordi Duch","Marta Sales-Pardo","Roger Guimera"],"pdf_url":"https://arxiv.org/pdf/2204.02704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08408v1","updated":"2022-12-16T11:15:39Z","published":"2022-12-16T11:15:39Z","title":"Decoder Tuning: Efficient Language Understanding as Decoding","summary":"  With the evergrowing sizes of pre-trained models (PTMs), it has been an\nemerging practice to only provide the inference APIs for users, namely\nmodel-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen,\nmost current approaches focus on the input side, seeking for powerful prompts\nto stimulate models for correct answers. However, we argue that input-side\nadaptation could be arduous due to the lack of gradient signals and they\nusually require thousands of API queries, resulting in high computation and\ntime costs. In light of this, we present Decoder Tuning (DecT), which in\ncontrast optimizes task-specific decoder networks on the output side.\nSpecifically, DecT first extracts prompt-stimulated output scores for initial\npredictions. On top of that, we train an additional decoder network on the\noutput representations to incorporate posterior data knowledge. By\ngradient-based optimization, DecT can be trained within several seconds and\nrequires only one PTM query per sample. Empirically, we conduct extensive\nnatural language understanding experiments and show that DecT significantly\noutperforms state-of-the-art algorithms with a $10^3\\times$ speed-up.\n","authors":["Ganqu Cui","Wentao Li","Ning Ding","Longtao Huang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2212.08408v1.pdf","comment":"Work in progress. 13 pages"},{"id":"http://arxiv.org/abs/2212.08403v1","updated":"2022-12-16T10:59:03Z","published":"2022-12-16T10:59:03Z","title":"LiFe-net: Data-driven Modelling of Time-dependent Temperatures and\n  Charging Statistics Of Tesla's LiFePo4 EV Battery","summary":"  Modelling the temperature of Electric Vehicle (EV) batteries is a fundamental\ntask of EV manufacturing. Extreme temperatures in the battery packs can affect\ntheir longevity and power output. Although theoretical models exist for\ndescribing heat transfer in battery packs, they are computationally expensive\nto simulate. Furthermore, it is difficult to acquire data measurements from\nwithin the battery cell. In this work, we propose a data-driven surrogate model\n(LiFe-net) that uses readily accessible driving diagnostics for battery\ntemperature estimation to overcome these limitations. This model incorporates\nNeural Operators with a traditional numerical integration scheme to estimate\nthe temperature evolution. Moreover, we propose two further variations of the\nbaseline model: LiFe-net trained with a regulariser and LiFe-net trained with\ntime stability loss. We compared these models in terms of generalization error\non test data. The results showed that LiFe-net trained with time stability loss\noutperforms the other two models and can estimate the temperature evolution on\nunseen data with a relative error of 2.77 % on average.\n","authors":["Jeyhun Rustamov","Luisa Fennert","Nico Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2212.08403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14311v3","updated":"2022-12-16T10:54:24Z","published":"2022-08-30T14:50:25Z","title":"Modeling Volatility and Dependence of European Carbon and Energy Prices","summary":"  We study the prices of European Emission Allowances (EUA), whereby we analyze\ntheir uncertainty and dependencies on related energy prices (natural gas, coal,\nand oil). We propose a probabilistic multivariate conditional time series model\nwith a VECM-Copula-GARCH structure which exploits key characteristics of the\ndata. Data are normalized with respect to inflation and carbon emissions to\nallow for proper cross-series evaluation. The forecasting performance is\nevaluated in an extensive rolling-window forecasting study, covering eight\nyears out-of-sample. We discuss our findings for both levels- and\nlog-transformed data, focusing on time-varying correlations, and in view of the\nRussian invasion of Ukraine.\n","authors":["Jonathan Berrisch","Sven Pappert","Florian Ziel","Antonia Arsova"],"pdf_url":"https://arxiv.org/pdf/2208.14311v3.pdf","comment":"Accepted for publication in Finance Research Letters"},{"id":"http://arxiv.org/abs/2212.08399v1","updated":"2022-12-16T10:46:20Z","published":"2022-12-16T10:46:20Z","title":"Reducing Sequence Length Learning Impacts on Transformer Models","summary":"  Classification algorithms using Transformer architectures can be affected by\nthe sequence length learning problem whenever observations from different\nclasses have a different length distribution. This problem brings models to use\nsequence length as a predictive feature instead of relying on important textual\ninformation. Even if most public datasets are not affected by this problem,\nprivately corpora for fields such as medicine and insurance may carry this data\nbias. This poses challenges throughout the value chain given their usage in a\nmachine learning application. In this paper, we empirically expose this problem\nand present approaches to minimize its impacts.\n","authors":["Jean-Thomas Baillargeon","Luc Lamontagne"],"pdf_url":"https://arxiv.org/pdf/2212.08399v1.pdf","comment":"10 pages, 8 content - 2 appendix, 2 figures"},{"id":"http://arxiv.org/abs/2212.08379v1","updated":"2022-12-16T10:12:54Z","published":"2022-12-16T10:12:54Z","title":"GeneFormer: Learned Gene Compression using Transformer-based Context\n  Modeling","summary":"  With the development of gene sequencing technology, an explosive growth of\ngene data has been witnessed. And the storage of gene data has become an\nimportant issue. Traditional gene data compression methods rely on general\nsoftware like G-zip, which fails to utilize the interrelation of nucleotide\nsequence. Recently, many researchers begin to investigate deep learning based\ngene data compression method. In this paper, we propose a transformer-based\ngene compression method named GeneFormer. Specifically, we first introduce a\nmodified transformer structure to fully explore the nucleotide sequence\ndependency. Then, we propose fixed-length parallel grouping to accelerate the\ndecoding speed of our autoregressive model. Experimental results on real-world\ndatasets show that our method saves 29.7% bit rate compared with the\nstate-of-the-art method, and the decoding speed is significantly faster than\nall existing learning-based gene compression methods.\n","authors":["Zhanbei Cui","Yu Liao","Tongda Xu","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2212.08379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.15701v3","updated":"2022-12-16T10:11:53Z","published":"2021-10-29T12:01:48Z","title":"Successor Feature Representations","summary":"  Transfer in Reinforcement Learning aims to improve learning performance on\ntarget tasks using knowledge from experienced source tasks. Successor\nRepresentations (SR) and their extension Successor Features (SF) are prominent\ntransfer mechanisms in domains where reward functions change between tasks.\nThey reevaluate the expected return of previously learned policies in a new\ntarget task to transfer their knowledge. The SF framework extended SR by\nlinearly decomposing rewards into successor features and a reward weight vector\nallowing their application in high-dimensional tasks. But this came with the\ncost of having a linear relationship between reward functions and successor\nfeatures, limiting its application to such tasks. We propose a novel\nformulation of SR based on learning the cumulative discounted probability of\nsuccessor features, called Successor Feature Representations (SFR). Crucially,\nSFR allows to reevaluate the expected return of policies for general reward\nfunctions. We introduce different SFR variations, prove its convergence, and\nprovide a guarantee on its transfer performance. Experimental evaluations based\non SFR with function approximation demonstrate its advantage over SF not only\nfor general reward functions but also in the case of linearly decomposable\nreward functions.\n","authors":["Chris Reinke","Xavier Alameda-Pineda"],"pdf_url":"https://arxiv.org/pdf/2110.15701v3.pdf","comment":"source code available at\n  https://gitlab.inria.fr/robotlearn/sfr_learning [v2] added experiments with\n  learned features [v3] renamed paper and changed scope"},{"id":"http://arxiv.org/abs/2212.08378v1","updated":"2022-12-16T10:08:38Z","published":"2022-12-16T10:08:38Z","title":"Feature Dropout: Revisiting the Role of Augmentations in Contrastive\n  Learning","summary":"  What role do augmentations play in contrastive learning? Recent work suggests\nthat good augmentations are label-preserving with respect to a specific\ndownstream task. We complicate this picture by showing that label-destroying\naugmentations can be useful in the foundation model setting, where the goal is\nto learn diverse, general-purpose representations for multiple downstream\ntasks. We perform contrastive learning experiments on a range of image and\naudio datasets with multiple downstream tasks (e.g. for digits superimposed on\nphotographs, predicting the class of one vs. the other). We find that Viewmaker\nNetworks, a recently proposed model for learning augmentations for contrastive\nlearning, produce label-destroying augmentations that stochastically destroy\nfeatures needed for different downstream tasks. These augmentations are\ninterpretable (e.g. altering shapes, digits, or letters added to images) and\nsurprisingly often result in better performance compared to expert-designed\naugmentations, despite not preserving label information. To support our\nempirical results, we theoretically analyze a simple contrastive learning\nsetting with a linear model. In this setting, label-destroying augmentations\nare crucial for preventing one set of features from suppressing the learning of\nfeatures useful for another downstream task. Our results highlight the need for\nanalyzing the interaction between multiple downstream tasks when trying to\nexplain the success of foundation models.\n","authors":["Alex Tamkin","Margalit Glasgow","Xiluo He","Noah Goodman"],"pdf_url":"https://arxiv.org/pdf/2212.08378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.10102v3","updated":"2022-12-16T09:52:47Z","published":"2020-06-17T18:54:03Z","title":"Capturing Label Characteristics in VAEs","summary":"  We present a principled approach to incorporating labels in VAEs that\ncaptures the rich characteristic information associated with those labels.\nWhile prior work has typically conflated these by learning latent variables\nthat directly correspond to label values, we argue this is contrary to the\nintended effect of supervision in VAEs-capturing rich label characteristics\nwith the latents. For example, we may want to capture the characteristics of a\nface that make it look young, rather than just the age of the person. To this\nend, we develop the CCVAE, a novel VAE model and concomitant variational\nobjective which captures label characteristics explicitly in the latent space,\neschewing direct correspondences between label values and latents. Through\njudicious structuring of mappings between such characteristic latents and\nlabels, we show that the CCVAE can effectively learn meaningful representations\nof the characteristics of interest across a variety of supervision schemes. In\nparticular, we show that the CCVAE allows for more effective and more general\ninterventions to be performed, such as smooth traversals within the\ncharacteristics for a given label, diverse conditional generation, and\ntransferring characteristics across datapoints.\n","authors":["Tom Joy","Sebastian M. Schmon","Philip H. S. Torr","N. Siddharth","Tom Rainforth"],"pdf_url":"https://arxiv.org/pdf/2006.10102v3.pdf","comment":"Accepted to ICLR 2021"},{"id":"http://arxiv.org/abs/2212.04800v2","updated":"2022-12-16T09:47:43Z","published":"2022-12-09T12:06:15Z","title":"AUC Maximization for Low-Resource Named Entity Recognition","summary":"  Current work in named entity recognition (NER) uses either cross entropy (CE)\nor conditional random fields (CRF) as the objective/loss functions to optimize\nthe underlying NER model. Both of these traditional objective functions for the\nNER problem generally produce adequate performance when the data distribution\nis balanced and there are sufficient annotated training examples. But since NER\nis inherently an imbalanced tagging problem, the model performance under the\nlow-resource settings could suffer using these standard objective functions.\nBased on recent advances in area under the ROC curve (AUC) maximization, we\npropose to optimize the NER model by maximizing the AUC score. We give evidence\nthat by simply combining two binary-classifiers that maximize the AUC score,\nsignificant performance improvement over traditional loss functions is achieved\nunder low-resource NER settings. We also conduct extensive experiments to\ndemonstrate the advantages of our method under the low-resource and\nhighly-imbalanced data distribution settings. To the best of our knowledge,\nthis is the first work that brings AUC maximization to the NER setting.\nFurthermore, we show that our method is agnostic to different types of NER\nembeddings, models and domains. The code to replicate this work will be\nprovided upon request.\n","authors":["Ngoc Dang Nguyen","Wei Tan","Wray Buntine","Richard Beare","Changyou Chen","Lan Du"],"pdf_url":"https://arxiv.org/pdf/2212.04800v2.pdf","comment":"10 pages, 4 figures, AAAI 2023 accepted paper"},{"id":"http://arxiv.org/abs/2212.08370v1","updated":"2022-12-16T09:45:22Z","published":"2022-12-16T09:45:22Z","title":"Shapley variable importance cloud for machine learning models","summary":"  Current practice in interpretable machine learning often focuses on\nexplaining the final model trained from data, e.g., by using the Shapley\nadditive explanations (SHAP) method. The recently developed Shapley variable\nimportance cloud (ShapleyVIC) extends the current practice to a group of\n\"nearly optimal models\" to provide comprehensive and robust variable importance\nassessments, with estimated uncertainty intervals for a more complete\nunderstanding of variable contributions to predictions. ShapleyVIC was\ninitially developed for applications with traditional regression models, and\nthe benefits of ShapleyVIC inference have been demonstrated in real-life\nprediction tasks using the logistic regression model. However, as a\nmodel-agnostic approach, ShapleyVIC application is not limited to such\nscenarios. In this work, we extend ShapleyVIC implementation for machine\nlearning models to enable wider applications, and propose it as a useful\ncomplement to the current SHAP analysis to enable more trustworthy applications\nof these black-box models.\n","authors":["Yilin Ning","Mingxuan Liu","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.06108v2","updated":"2022-12-16T09:40:57Z","published":"2022-11-11T10:24:42Z","title":"RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object\n  Detection System","summary":"  In autonomous driving systems, LiDAR and radar play important roles in the\nperception of the surrounding environment.LiDAR provides accurate 3D spatial\nsensing information but cannot work in adverse weather like fog. On the other\nhand, the radar signal can be diffracted when encountering raindrops or mist\nparticles thanks to its wavelength, but it suffers from large noise. Recent\nstate-of-the-art works reveal that fusion of radar and LiDAR can lead to robust\ndetection in adverse weather. The existing works adopt convolutional neural\nnetwork architecture to extract features from each sensor data stream, then\nalign and aggregate the two branch features to predict object detection\nresults. However, these methods have low accuracy of bounding box estimations\ndue to a simple design of label assignment and fusion strategies. In this\npaper, we propose a bird's-eye view fusion learning-based anchor box-free\nobject detection system, which fuses the feature derived from the radar\nrange-azimuth heatmap and the LiDAR point cloud to estimate the possible\nobjects. Different label assignment strategies have been designed to facilitate\nthe consistency between the classification of foreground or background anchor\npoints and the corresponding bounding box regressions. In addition, the\nperformance of the proposed object detector is further enhanced by employing a\nnovel interactive transformer module. The superior performance of the proposed\nmethods in this paper has been demonstrated using the recently published Oxford\nradar robotCar dataset, showing that the average precision of our system\nsignificantly outperforms the best state-of-the-art method by 14.4% and 20.5%\nat IoU equals 0.8 in clear and foggy weather testing, respectively.\n","authors":["Yanlong Yang","Jianan Liu","Tao Huang","Qing-Long Han","Gang Ma","Bing Zhu"],"pdf_url":"https://arxiv.org/pdf/2211.06108v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2106.12570v3","updated":"2022-12-16T09:29:56Z","published":"2021-06-23T17:54:35Z","title":"Learning Multimodal VAEs through Mutual Supervision","summary":"  Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.\n","authors":["Tom Joy","Yuge Shi","Philip H. S. Torr","Tom Rainforth","Sebastian M. Schmon","N. Siddharth"],"pdf_url":"https://arxiv.org/pdf/2106.12570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10779v4","updated":"2022-12-16T09:04:36Z","published":"2022-04-18T04:51:08Z","title":"CgAT: Center-Guided Adversarial Training for Deep Hashing-Based\n  Retrieval","summary":"  Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. However, deep hashing models are\nvulnerable to adversarial examples, making it essential to develop adversarial\ndefense methods for image retrieval. Existing solutions achieved limited\ndefense performance because of using weak adversarial samples for training and\nlacking discriminative optimization objectives to learn robust features. In\nthis paper, we present a min-max based Center-guided Adversarial Training,\nnamely CgAT, to improve the robustness of deep hashing networks through worst\nadversarial examples. Specifically, we first formulate the center code as a\nsemantically-discriminative representative of the input image content, which\npreserves the semantic similarity with positive samples and dissimilarity with\nnegative examples. We prove that a mathematical formula can calculate the\ncenter code immediately. After obtaining the center codes in each optimization\niteration of the deep hashing network, they are adopted to guide the\nadversarial training process. On the one hand, CgAT generates the worst\nadversarial examples as augmented data by maximizing the Hamming distance\nbetween the hash codes of the adversarial examples and the center codes. On the\nother hand, CgAT learns to mitigate the effects of adversarial samples by\nminimizing the Hamming distance to the center codes. Extensive experiments on\nthe benchmark datasets demonstrate the effectiveness of our adversarial\ntraining algorithm in defending against adversarial attacks for deep\nhashing-based retrieval. Compared with the current state-of-the-art defense\nmethod, we significantly improve the defense performance by an average of\n18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively.\n","authors":["Xunguang Wang","Yinqun Lin","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2204.10779v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08349v1","updated":"2022-12-16T08:57:18Z","published":"2022-12-16T08:57:18Z","title":"Swing Distillation: A Privacy-Preserving Knowledge Distillation\n  Framework","summary":"  Knowledge distillation (KD) has been widely used for model compression and\nknowledge transfer. Typically, a big teacher model trained on sufficient data\ntransfers knowledge to a small student model. However, despite the success of\nKD, little effort has been made to study whether KD leaks the training data of\nthe teacher model. In this paper, we experimentally reveal that KD suffers from\nthe risk of privacy leakage. To alleviate this issue, we propose a novel\nknowledge distillation method, swing distillation, which can effectively\nprotect the private information of the teacher model from flowing to the\nstudent model. In our framework, the temperature coefficient is dynamically and\nadaptively adjusted according to the degree of private information contained in\nthe data, rather than a predefined constant hyperparameter. It assigns\ndifferent temperatures to tokens according to the likelihood that a token in a\nposition contains private information. In addition, we inject noise into soft\ntargets provided to the student model, in order to avoid unshielded knowledge\ntransfer. Experiments on multiple datasets and tasks demonstrate that the\nproposed swing distillation can significantly reduce (by over 80% in terms of\ncanary exposure) the risk of privacy leakage in comparison to KD with\ncompetitive or better performance. Furthermore, swing distillation is robust\nagainst the increasing privacy budget.\n","authors":["Junzhuo Li","Xinwei Wu","Weilong Dong","Shuangzhi Wu","Chao Bian","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2212.08349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.05075v4","updated":"2022-12-16T08:49:08Z","published":"2021-08-11T07:37:03Z","title":"Jujutsu: A Two-stage Defense against Adversarial Patch Attacks on Deep\n  Neural Networks","summary":"  Adversarial patch attacks create adversarial examples by injecting arbitrary\ndistortions within a bounded region of the input to fool deep neural networks\n(DNNs). These attacks are robust (i.e., physically-realizable) and universally\nmalicious, and hence represent a severe security threat to real-world DNN-based\nsystems.\n  We propose Jujutsu, a two-stage technique to detect and mitigate robust and\nuniversal adversarial patch attacks. We first observe that adversarial patches\nare crafted as localized features that yield large influence on the prediction\noutput, and continue to dominate the prediction on any input. Jujutsu leverages\nthis observation for accurate attack detection with low false positives. Patch\nattacks corrupt only a localized region of the input, while the majority of the\ninput remains unperturbed. Therefore, Jujutsu leverages generative adversarial\nnetworks (GAN) to perform localized attack recovery by synthesizing the\nsemantic contents of the input that are corrupted by the attacks, and\nreconstructs a ``clean'' input for correct prediction.\n  We evaluate Jujutsu on four diverse datasets spanning 8 different DNN models,\nand find that it achieves superior performance and significantly outperforms\nfour existing defenses. We further evaluate Jujutsu against physical-world\nattacks, as well as adaptive attacks.\n","authors":["Zitao Chen","Pritam Dash","Karthik Pattabiraman"],"pdf_url":"https://arxiv.org/pdf/2108.05075v4.pdf","comment":"To appear in AsiaCCS'23"},{"id":"http://arxiv.org/abs/2210.08349v2","updated":"2022-12-16T08:41:45Z","published":"2022-10-15T17:57:43Z","title":"When to Update Your Model: Constrained Model-based Reinforcement\n  Learning","summary":"  Designing and analyzing model-based RL (MBRL) algorithms with guaranteed\nmonotonic improvement has been challenging, mainly due to the interdependence\nbetween policy optimization and model learning. Existing discrepancy bounds\ngenerally ignore the impacts of model shifts, and their corresponding\nalgorithms are prone to degrade performance by drastic model updating. In this\nwork, we first propose a novel and general theoretical scheme for a\nnon-decreasing performance guarantee of MBRL. Our follow-up derived bounds\nreveal the relationship between model shifts and performance improvement. These\ndiscoveries encourage us to formulate a constrained lower-bound optimization\nproblem to permit the monotonicity of MBRL. A further example demonstrates that\nlearning models from a dynamically-varying number of explorations benefit the\neventual returns. Motivated by these analyses, we design a simple but effective\nalgorithm CMLO (Constrained Model-shift Lower-bound Optimization), by\nintroducing an event-triggered mechanism that flexibly determines when to\nupdate the model. Experiments show that CMLO surpasses other state-of-the-art\nmethods and produces a boost when various policy optimization methods are\nemployed.\n","authors":["Tianying Ji","Yu Luo","Fuchun Sun","Mingxuan Jing","Fengxiang He","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2210.08349v2.pdf","comment":"NeurIPS 2022"},{"id":"http://arxiv.org/abs/2105.04187v3","updated":"2022-12-16T08:37:34Z","published":"2021-05-10T08:33:10Z","title":"A Rigorous Information-Theoretic Definition of Redundancy and Relevancy\n  in Feature Selection Based on (Partial) Information Decomposition","summary":"  Selecting a minimal feature set that is maximally informative about a target\nvariable is a central task in machine learning and statistics. Information\ntheory provides a powerful framework for formulating feature selection\nalgorithms -- yet, a rigorous, information-theoretic definition of feature\nrelevancy, which accounts for feature interactions such as redundant and\nsynergistic contributions, is still missing. We argue that this lack is\ninherent to classical information theory which does not provide measures to\ndecompose the information a set of variables provides about a target into\nunique, redundant, and synergistic contributions. Such a decomposition has been\nintroduced only recently by the partial information decomposition (PID)\nframework. Using PID, we clarify why feature selection is a conceptually\ndifficult problem when approached using information theory and provide a novel\ndefinition of feature relevancy and redundancy in PID terms. From this\ndefinition, we show that the conditional mutual information (CMI) maximizes\nrelevancy while minimizing redundancy and propose an iterative, CMI-based\nalgorithm for practical feature selection. We demonstrate the power of our\nCMI-based algorithm in comparison to the unconditional mutual information on\nbenchmark examples and provide corresponding PID estimates to highlight how PID\nallows to quantify information contribution of features and their interactions\nin feature-selection problems.\n","authors":["Patricia Wollstadt","Sebastian Schmitt","Michael Wibral"],"pdf_url":"https://arxiv.org/pdf/2105.04187v3.pdf","comment":"43 pages, 12 figures. Reorganization and shortening of manuscript,\n  added Appendix with theoretical guarantees, background information on the\n  algorithm used, and an additional example application on a larger problem"},{"id":"http://arxiv.org/abs/2212.08343v1","updated":"2022-12-16T08:37:24Z","published":"2022-12-16T08:37:24Z","title":"SplitGP: Achieving Both Generalization and Personalization in Federated\n  Learning","summary":"  A fundamental challenge to providing edge-AI services is the need for a\nmachine learning (ML) model that achieves personalization (i.e., to individual\nclients) and generalization (i.e., to unseen data) properties concurrently.\nExisting techniques in federated learning (FL) have encountered a steep\ntradeoff between these objectives and impose large computational requirements\non edge devices during training and inference. In this paper, we propose\nSplitGP, a new split learning solution that can simultaneously capture\ngeneralization and personalization capabilities for efficient inference across\nresource-constrained clients (e.g., mobile/IoT devices). Our key idea is to\nsplit the full ML model into client-side and server-side components, and impose\ndifferent roles to them: the client-side model is trained to have strong\npersonalization capability optimized to each client's main task, while the\nserver-side model is trained to have strong generalization capability for\nhandling all clients' out-of-distribution tasks. We analytically characterize\nthe convergence behavior of SplitGP, revealing that all client models approach\nstationary points asymptotically. Further, we analyze the inference time in\nSplitGP and provide bounds for determining model split ratios. Experimental\nresults show that SplitGP outperforms existing baselines by wide margins in\ninference time and test accuracy for varying amounts of out-of-distribution\nsamples.\n","authors":["Dong-Jun Han","Do-Yeon Kim","Minseok Choi","Christopher G. Brinton","Jaekyun Moon"],"pdf_url":"https://arxiv.org/pdf/2212.08343v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2209.12590v2","updated":"2022-12-16T08:37:14Z","published":"2022-09-26T11:21:19Z","title":"Learning to Drop Out: An Adversarial Approach to Training Sequence VAEs","summary":"  In principle, applying variational autoencoders (VAEs) to sequential data\noffers a method for controlled sequence generation, manipulation, and\nstructured representation learning. However, training sequence VAEs is\nchallenging: autoregressive decoders can often explain the data without\nutilizing the latent space, known as posterior collapse. To mitigate this,\nstate-of-the-art models weaken the powerful decoder by applying uniformly\nrandom dropout to the decoder input. We show theoretically that this removes\npointwise mutual information provided by the decoder input, which is\ncompensated for by utilizing the latent space. We then propose an adversarial\ntraining strategy to achieve information-based stochastic dropout. Compared to\nuniform dropout on standard text benchmark datasets, our targeted approach\nincreases both sequence modeling performance and the information captured in\nthe latent space.\n","authors":["Đorđe Miladinović","Kumar Shridhar","Kushal Jain","Max B. Paulus","Joachim M. Buhmann","Mrinmaya Sachan","Carl Allen"],"pdf_url":"https://arxiv.org/pdf/2209.12590v2.pdf","comment":"Accepted at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08341v1","updated":"2022-12-16T08:35:21Z","published":"2022-12-16T08:35:21Z","title":"Adversarial Example Defense via Perturbation Grading Strategy","summary":"  Deep Neural Networks have been widely used in many fields. However, studies\nhave shown that DNNs are easily attacked by adversarial examples, which have\ntiny perturbations and greatly mislead the correct judgment of DNNs.\nFurthermore, even if malicious attackers cannot obtain all the underlying model\nparameters, they can use adversarial examples to attack various DNN-based task\nsystems. Researchers have proposed various defense methods to protect DNNs,\nsuch as reducing the aggressiveness of adversarial examples by preprocessing or\nimproving the robustness of the model by adding modules. However, some defense\nmethods are only effective for small-scale examples or small perturbations but\nhave limited defense effects for adversarial examples with large perturbations.\nThis paper assigns different defense strategies to adversarial perturbations of\ndifferent strengths by grading the perturbations on the input examples.\nExperimental results show that the proposed method effectively improves defense\nperformance. In addition, the proposed method does not modify any task model,\nwhich can be used as a preprocessing module, which significantly reduces the\ndeployment cost in practical applications.\n","authors":["Shaowei Zhu","Wanli Lyu","Bin Li","Zhaoxia Yin","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2212.08341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08340v1","updated":"2022-12-16T08:31:07Z","published":"2022-12-16T08:31:07Z","title":"Neural Enhanced Belief Propagation for Multiobject Tracking","summary":"  Algorithmic solutions for multi-object tracking (MOT) are a key enabler for\napplications in autonomous navigation and applied ocean sciences.\nState-of-the-art MOT methods fully rely on a statistical model and typically\nuse preprocessed sensor data as measurements. In particular, measurements are\nproduced by a detector that extracts potential object locations from the raw\nsensor data collected for a discrete time step. This preparatory processing\nstep reduces data flow and computational complexity but may result in a loss of\ninformation. State-of-the-art Bayesian MOT methods that are based on belief\npropagation (BP) systematically exploit graph structures of the statistical\nmodel to reduce computational complexity and improve scalability. However, as a\nfully model-based approach, BP can only provide suboptimal estimates when there\nis a mismatch between the statistical model and the true data-generating\nprocess. Existing BP-based MOT methods can further only make use of\npreprocessed measurements. In this paper, we introduce a variant of BP that\ncombines model-based with data-driven MOT. The proposed neural enhanced belief\npropagation (NEBP) method complements the statistical model of BP by\ninformation learned from raw sensor data. This approach conjectures that the\nlearned information can reduce model mismatch and thus improve data association\nand false alarm rejection. Our NEBP method improves tracking performance\ncompared to model-based methods. At the same time, it inherits the advantages\nof BP-based MOT, i.e., it scales only quadratically in the number of objects,\nand it can thus generate and maintain a large number of object tracks. We\nevaluate the performance of our NEBP approach for MOT on the nuScenes\nautonomous driving dataset and demonstrate that it has state-of-the-art\nperformance.\n","authors":["Mingchao Liang","Florian Meyer"],"pdf_url":"https://arxiv.org/pdf/2212.08340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08339v1","updated":"2022-12-16T08:30:41Z","published":"2022-12-16T08:30:41Z","title":"Generalization Bounds for Inductive Matrix Completion in Low-noise\n  Settings","summary":"  We study inductive matrix completion (matrix completion with side\ninformation) under an i.i.d. subgaussian noise assumption at a low noise\nregime, with uniform sampling of the entries. We obtain for the first time\ngeneralization bounds with the following three properties: (1) they scale like\nthe standard deviation of the noise and in particular approach zero in the\nexact recovery case; (2) even in the presence of noise, they converge to zero\nwhen the sample size approaches infinity; and (3) for a fixed dimension of the\nside information, they only have a logarithmic dependence on the size of the\nmatrix. Differently from many works in approximate recovery, we present results\nboth for bounded Lipschitz losses and for the absolute loss, with the latter\nrelying on Talagrand-type inequalities. The proofs create a bridge between two\napproaches to the theoretical analysis of matrix completion, since they consist\nin a combination of techniques from both the exact recovery literature and the\napproximate recovery literature.\n","authors":["Antoine Ledent","Rodrigo Alves","Yunwen Lei","Yann Guermeur","Marius Kloft"],"pdf_url":"https://arxiv.org/pdf/2212.08339v1.pdf","comment":"30 Pages, 1 figure; Accepted for publication at AAAI 2023"},{"id":"http://arxiv.org/abs/2209.08037v2","updated":"2022-12-16T08:23:09Z","published":"2022-09-16T16:31:27Z","title":"DAGMA: Learning DAGs via M-matrices and a Log-Determinant Acyclicity\n  Characterization","summary":"  The combinatorial problem of learning directed acyclic graphs (DAGs) from\ndata was recently framed as a purely continuous optimization problem by\nleveraging a differentiable acyclicity characterization of DAGs based on the\ntrace of a matrix exponential function. Existing acyclicity characterizations\nare based on the idea that powers of an adjacency matrix contain information\nabout walks and cycles. In this work, we propose a new acyclicity\ncharacterization based on the log-determinant (log-det) function, which\nleverages the nilpotency property of DAGs. To deal with the inherent\nasymmetries of a DAG, we relate the domain of our log-det characterization to\nthe set of $\\textit{M-matrices}$, which is a key difference to the classical\nlog-det function defined over the cone of positive definite matrices. Similar\nto acyclicity functions previously proposed, our characterization is also exact\nand differentiable. However, when compared to existing characterizations, our\nlog-det function: (1) Is better at detecting large cycles; (2) Has\nbetter-behaved gradients; and (3) Its runtime is in practice about an order of\nmagnitude faster. From the optimization side, we drop the typically used\naugmented Lagrangian scheme and propose DAGMA ($\\textit{DAGs via M-matrices for\nAcyclicity}$), a method that resembles the central path for barrier methods.\nEach point in the central path of DAGMA is a solution to an unconstrained\nproblem regularized by our log-det function, then we show that at the limit of\nthe central path the solution is guaranteed to be a DAG. Finally, we provide\nextensive experiments for $\\textit{linear}$ and $\\textit{nonlinear}$ SEMs and\nshow that our approach can reach large speed-ups and smaller structural Hamming\ndistances against state-of-the-art methods. Code implementing the proposed\nmethod is open-source and publicly available at\nhttps://github.com/kevinsbello/dagma.\n","authors":["Kevin Bello","Bryon Aragam","Pradeep Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2209.08037v2.pdf","comment":"28 pages, 13 figures, published at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08330v1","updated":"2022-12-16T08:14:04Z","published":"2022-12-16T08:14:04Z","title":"Convolution-enhanced Evolving Attention Networks","summary":"  Attention-based neural networks, such as Transformers, have become ubiquitous\nin numerous applications, including computer vision, natural language\nprocessing, and time-series analysis. In all kinds of attention networks, the\nattention maps are crucial as they encode semantic dependencies between input\ntokens. However, most existing attention networks perform modeling or reasoning\nbased on representations, wherein the attention maps of different layers are\nlearned separately without explicit interactions. In this paper, we propose a\nnovel and generic evolving attention mechanism, which directly models the\nevolution of inter-token relationships through a chain of residual\nconvolutional modules. The major motivations are twofold. On the one hand, the\nattention maps in different layers share transferable knowledge, thus adding a\nresidual connection can facilitate the information flow of inter-token\nrelationships across layers. On the other hand, there is naturally an\nevolutionary trend among attention maps at different abstraction levels, so it\nis beneficial to exploit a dedicated convolution-based module to capture this\nprocess. Equipped with the proposed mechanism, the convolution-enhanced\nevolving attention networks achieve superior performance in various\napplications, including time-series representation, natural language\nunderstanding, machine translation, and image classification. Especially on\ntime-series representation tasks, Evolving Attention-enhanced Dilated\nConvolutional (EA-DC-) Transformer outperforms state-of-the-art models\nsignificantly, achieving an average of 17% improvement compared to the best\nSOTA. To the best of our knowledge, this is the first work that explicitly\nmodels the layer-wise evolution of attention maps. Our implementation is\navailable at https://github.com/pkuyym/EvolvingAttention\n","authors":["Yujing Wang","Yaming Yang","Zhuo Li","Jiangang Bai","Mingliang Zhang","Xiangtai Li","Jing Yu","Ce Zhang","Gao Huang","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2212.08330v1.pdf","comment":"Extension of the previous work (arXiv:2102.12895). arXiv admin note:\n  text overlap with arXiv:2102.12895"},{"id":"http://arxiv.org/abs/2212.08324v1","updated":"2022-12-16T07:53:05Z","published":"2022-12-16T07:53:05Z","title":"Mobile Augmented Reality with Federated Learning in the Metaverse","summary":"  The Metaverse is deemed the next evolution of the Internet and has received\nmuch attention recently. Metaverse applications via mobile augmented reality\n(MAR) require rapid and accurate object detection to mix digital data with the\nreal world. As mobile devices evolve, they become more potent in computing.\nHence, their computational resources can be leveraged to train machine learning\nmodels. In light of the increasing concerns of user privacy and data security,\nfederated learning (FL) has become a promising distributed learning framework\nfor privacy-preserving analytics. In this article, FL and MAR are brought\ntogether in the Metaverse. We discuss the necessity and rationality of the\ncombination of FL and MAR. The prospective technologies that power FL and MAR\nin the Metaverse are also identified. In addition, existing challenges that\nprevent the fulfilment of FL and MAR in the Metaverse and several application\nscenarios are presented. Finally, two case studies of Metaverse FL-MAR systems\nare demonstrated.\n","authors":["Xinyu Zhou","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2212.08324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08323v1","updated":"2022-12-16T07:49:15Z","published":"2022-12-16T07:49:15Z","title":"An ensemble neural network approach to forecast Dengue outbreak based on\n  climatic condition","summary":"  Dengue fever is a virulent disease spreading over 100 tropical and\nsubtropical countries in Africa, the Americas, and Asia. This arboviral disease\naffects around 400 million people globally, severely distressing the healthcare\nsystems. The unavailability of a specific drug and ready-to-use vaccine makes\nthe situation worse. Hence, policymakers must rely on early warning systems to\ncontrol intervention-related decisions. Forecasts routinely provide critical\ninformation for dangerous epidemic events. However, the available forecasting\nmodels (e.g., weather-driven mechanistic, statistical time series, and machine\nlearning models) lack a clear understanding of different components to improve\nprediction accuracy and often provide unstable and unreliable forecasts. This\nstudy proposes an ensemble wavelet neural network with exogenous factor(s)\n(XEWNet) model that can produce reliable estimates for dengue outbreak\nprediction for three geographical regions, namely San Juan, Iquitos, and\nAhmedabad. The proposed XEWNet model is flexible and can easily incorporate\nexogenous climate variable(s) confirmed by statistical causality tests in its\nscalable framework. The proposed model is an integrated approach that uses\nwavelet transformation into an ensemble neural network framework that helps in\ngenerating more reliable long-term forecasts. The proposed XEWNet allows\ncomplex non-linear relationships between the dengue incidence cases and\nrainfall; however, mathematically interpretable, fast in execution, and easily\ncomprehensible. The proposal's competitiveness is measured using computational\nexperiments based on various statistical metrics and several statistical\ncomparison tests. In comparison with statistical, machine learning, and deep\nlearning methods, our proposed XEWNet performs better in 75% of the cases for\nshort-term and long-term forecasting of dengue incidence.\n","authors":["Madhurima Panja","Tanujit Chakraborty","Sk Shahid Nadim","Indrajit Ghosh","Uttam Kumar","Nan Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08312v1","updated":"2022-12-16T07:24:58Z","published":"2022-12-16T07:24:58Z","title":"An Efficient Framework for Monitoring Subgroup Performance of Machine\n  Learning Systems","summary":"  Monitoring machine learning systems post deployment is critical to ensure the\nreliability of the systems. Particularly importance is the problem of\nmonitoring the performance of machine learning systems across all the data\nsubgroups (subpopulations). In practice, this process could be prohibitively\nexpensive as the number of data subgroups grows exponentially with the number\nof input features, and the process of labelling data to evaluate each\nsubgroup's performance is costly. In this paper, we propose an efficient\nframework for monitoring subgroup performance of machine learning systems.\nSpecifically, we aim to find the data subgroup with the worst performance using\na limited number of labeled data. We mathematically formulate this problem as\nan optimization problem with an expensive black-box objective function, and\nthen suggest to use Bayesian optimization to solve this problem. Our\nexperimental results on various real-world datasets and machine learning\nsystems show that our proposed framework can retrieve the worst-performing data\nsubgroup effectively and efficiently.\n","authors":["Huong Ha"],"pdf_url":"https://arxiv.org/pdf/2212.08312v1.pdf","comment":"Accepted to the ML Safety Workshop at NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08311v1","updated":"2022-12-16T07:20:28Z","published":"2022-12-16T07:20:28Z","title":"Can We Find Strong Lottery Tickets in Generative Models?","summary":"  Yes. In this paper, we investigate strong lottery tickets in generative\nmodels, the subnetworks that achieve good generative performance without any\nweight update. Neural network pruning is considered the main cornerstone of\nmodel compression for reducing the costs of computation and memory.\nUnfortunately, pruning a generative model has not been extensively explored,\nand all existing pruning algorithms suffer from excessive weight-training\ncosts, performance degradation, limited generalizability, or complicated\ntraining. To address these problems, we propose to find a strong lottery ticket\nvia moment-matching scores. Our experimental results show that the discovered\nsubnetwork can perform similarly or better than the trained dense model even\nwhen only 10% of the weights remain. To the best of our knowledge, we are the\nfirst to show the existence of strong lottery tickets in generative models and\nprovide an algorithm to find it stably. Our code and supplementary materials\nare publicly available.\n","authors":["Sangyeop Yeo","Yoojin Jang","Jy-yong Sohn","Dongyoon Han","Jaejun Yoo"],"pdf_url":"https://arxiv.org/pdf/2212.08311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.05833v2","updated":"2022-12-16T07:14:59Z","published":"2022-10-11T23:26:47Z","title":"Parameter estimation of the homodyned K distribution based on neural\n  networks and trainable fractional-order moments","summary":"  Homodyned K (HK) distribution has been widely used to describe the scattering\nphenomena arising in various research fields, such as ultrasound imaging or\noptics. In this work, we propose a machine learning based approach to the\nestimation of the HK distribution parameters. We develop neural networks that\ncan estimate the HK distribution parameters based on the signal-to-noise ratio,\nskewness and kurtosis calculated using fractional-order moments. Compared to\nthe previous approaches, we consider the orders of the moments as trainable\nvariables that can be optimized along with the network weights using the\nback-propagation algorithm. Networks are trained based on samples generated\nfrom the HK distribution. Obtained results demonstrate that the proposed method\ncan be used to accurately estimate the HK distribution parameters.\n","authors":["Michal Byra","Ziemowit Klimonda","Piotr Jarosik"],"pdf_url":"https://arxiv.org/pdf/2210.05833v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.08302v1","updated":"2022-12-16T06:43:16Z","published":"2022-12-16T06:43:16Z","title":"Safe Evaluation For Offline Learning: Are We Ready To Deploy?","summary":"  The world currently offers an abundance of data in multiple domains, from\nwhich we can learn reinforcement learning (RL) policies without further\ninteraction with the environment. RL agents learning offline from such data is\npossible but deploying them while learning might be dangerous in domains where\nsafety is critical. Therefore, it is essential to find a way to estimate how a\nnewly-learned agent will perform if deployed in the target environment before\nactually deploying it and without the risk of overestimating its true\nperformance. To achieve this, we introduce a framework for safe evaluation of\noffline learning using approximate high-confidence off-policy evaluation\n(HCOPE) to estimate the performance of offline policies during learning. In our\nsetting, we assume a source of data, which we split into a train-set, to learn\nan offline policy, and a test-set, to estimate a lower-bound on the offline\npolicy using off-policy evaluation with bootstrapping. A lower-bound estimate\ntells us how good a newly-learned target policy would perform before it is\ndeployed in the real environment, and therefore allows us to decide when to\ndeploy our learned policy.\n","authors":["Hager Radi","Josiah P. Hanna","Peter Stone","Matthew E. Taylor"],"pdf_url":"https://arxiv.org/pdf/2212.08302v1.pdf","comment":"NeurIPS 2021 Workshop on Deployable Decision Making in Embodied\n  Systems [Spotlight]"},{"id":"http://arxiv.org/abs/2212.08299v1","updated":"2022-12-16T06:31:18Z","published":"2022-12-16T06:31:18Z","title":"Metaheuristic for Hub-Spoke Facility Location Problem: Application to\n  Indian E-commerce Industry","summary":"  Indian e-commerce industry has evolved over the last decade and is expected\nto grow over the next few years. The focus has now shifted to turnaround time\n(TAT) due to the emergence of many third-party logistics providers and higher\ncustomer expectations. The key consideration for delivery providers is to\nbalance their overall operating costs while meeting the promised TAT to their\ncustomers. E-commerce delivery partners operate through a network of facilities\nwhose strategic locations help to run the operations efficiently. In this work,\nwe identify the locations of hubs throughout the country and their\ncorresponding mapping with the distribution centers. The objective is to\nminimize the total network costs with TAT adherence. We use Genetic Algorithm\nand leverage business constraints to reduce the solution search space and hence\nthe solution time. The results indicate an improvement of 9.73% in TAT\ncompliance compared with the current scenario.\n","authors":["Aakash Sachdeva","Bhupinder Singh","Rahul Prasad","Nakshatra Goel","Ronit Mondal","Jatin Munjal","Abhishek Bhatnagar","Manjeet Dahiya"],"pdf_url":"https://arxiv.org/pdf/2212.08299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08295v1","updated":"2022-12-16T06:20:19Z","published":"2022-12-16T06:20:19Z","title":"Learning on Persistence Diagrams as Radon Measures","summary":"  Persistence diagrams are common descriptors of the topological structure of\ndata appearing in various classification and regression tasks. They can be\ngeneralized to Radon measures supported on the birth-death plane and endowed\nwith an optimal transport distance. Examples of such measures are expectations\nof probability distributions on the space of persistence diagrams. In this\npaper, we develop methods for approximating continuous functions on the space\nof Radon measures supported on the birth-death plane, as well as their\nutilization in supervised learning tasks. Indeed, we show that any continuous\nfunction defined on a compact subset of the space of such measures (e.g., a\nclassifier or regressor) can be approximated arbitrarily well by polynomial\ncombinations of features computed using a continuous compactly supported\nfunction on the birth-death plane (a template). We provide insights into the\nstructure of relatively compact subsets of the space of Radon measures, and\ntest our approximation methodology on various data sets and supervised learning\ntasks.\n","authors":["Alex Elchesen","Iryna Hartsock","Jose A. Perea","Tatum Rask"],"pdf_url":"https://arxiv.org/pdf/2212.08295v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.08290v1","updated":"2022-12-16T05:51:52Z","published":"2022-12-16T05:51:52Z","title":"Robust Learning Protocol for Federated Tumor Segmentation Challenge","summary":"  In this work, we devise robust and efficient learning protocols for\norchestrating a Federated Learning (FL) process for the Federated Tumor\nSegmentation Challenge (FeTS 2022). Enabling FL for FeTS setup is challenging\nmainly due to data heterogeneity among collaborators and communication cost of\ntraining. To tackle these challenges, we propose Robust Learning Protocol\n(RoLePRO) which is a combination of server-side adaptive optimisation (e.g.,\nserver-side Adam) and judicious parameter (weights) aggregation schemes (e.g.,\nadaptive weighted aggregation). RoLePRO takes a two-phase approach, where the\nfirst phase consists of vanilla Federated Averaging, while the second phase\nconsists of a judicious aggregation scheme that uses a sophisticated\nreweighting, all in the presence of an adaptive optimisation algorithm at the\nserver. We draw insights from extensive experimentation to tune learning rates\nfor the two phases.\n","authors":["Ambrish Rawat","Giulio Zizzo","Swanand Kadhe","Jonathan P. Epperlein","Stefano Braghin"],"pdf_url":"https://arxiv.org/pdf/2212.08290v1.pdf","comment":"14 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2212.08279v1","updated":"2022-12-16T04:52:53Z","published":"2022-12-16T04:52:53Z","title":"Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion\n  Behaviors in Social Deduction Games","summary":"  Persuasion modeling is a key building block for conversational agents.\nExisting works in this direction are limited to analyzing textual dialogue\ncorpus. We argue that visual signals also play an important role in\nunderstanding human persuasive behaviors. In this paper, we introduce the first\nmultimodal dataset for modeling persuasion behaviors. Our dataset includes 199\ndialogue transcriptions and videos captured in a multi-player social deduction\ngame setting, 26,647 utterance level annotations of persuasion strategy, and\ngame level annotations of deduction game outcomes. We provide extensive\nexperiments to show how dialogue context and visual signals benefit persuasion\nstrategy prediction. We also explore the generalization ability of language\nmodels for persuasion modeling and the role of persuasion strategies in\npredicting social deduction game outcomes. Our dataset, code, and models can be\nfound at https://persuasion-deductiongame.socialai-data.org.\n","authors":["Bolin Lai","Hongxin Zhang","Miao Liu","Aryan Pariani","Fiona Ryan","Wenqi Jia","Shirley Anugrah Hayati","James M. Rehg","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08279v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2212.08277v1","updated":"2022-12-16T04:25:43Z","published":"2022-12-16T04:25:43Z","title":"Improving self-supervised representation learning via sequential\n  adversarial masking","summary":"  Recent methods in self-supervised learning have demonstrated that\nmasking-based pretext tasks extend beyond NLP, serving as useful pretraining\nobjectives in computer vision. However, existing approaches apply random or ad\nhoc masking strategies that limit the difficulty of the reconstruction task\nand, consequently, the strength of the learnt representations. We improve upon\ncurrent state-of-the-art work in learning adversarial masks by proposing a new\nframework that generates masks in a sequential fashion with different\nconstraints on the adversary. This leads to improvements in performance on\nvarious downstream tasks, such as classification on ImageNet100, STL10, and\nCIFAR10/100 and segmentation on Pascal VOC. Our results further demonstrate the\npromising capabilities of masking-based approaches for SSL in computer vision.\n","authors":["Dylan Sam","Min Bai","Tristan McKinney","Li Erran Li"],"pdf_url":"https://arxiv.org/pdf/2212.08277v1.pdf","comment":"9 pages, 2 figures, Presented at NeurIPS 2022 SSL: Theory and\n  Practice Workshop"},{"id":"http://arxiv.org/abs/2212.08276v1","updated":"2022-12-16T04:23:36Z","published":"2022-12-16T04:23:36Z","title":"Preventing RNN from Using Sequence Length as a Feature","summary":"  Recurrent neural networks are deep learning topologies that can be trained to\nclassify long documents. However, in our recent work, we found a critical\nproblem with these cells: they can use the length differences between texts of\ndifferent classes as a prominent classification feature. This has the effect of\nproducing models that are brittle and fragile to concept drift, can provide\nmisleading performances and are trivially explainable regardless of text\ncontent. This paper illustrates the problem using synthetic and real-world data\nand provides a simple solution using weight decay regularization.\n","authors":["Jean-Thomas Baillargeon","Hélène Cossette","Luc Lamontagne"],"pdf_url":"https://arxiv.org/pdf/2212.08276v1.pdf","comment":"6 pages, but my overleaf generrates 5 pages. I have no error, the\n  font size seems different"},{"id":"http://arxiv.org/abs/2212.08273v1","updated":"2022-12-16T04:18:47Z","published":"2022-12-16T04:18:47Z","title":"Learning for Vehicle-to-Vehicle Cooperative Perception under Lossy\n  Communication","summary":"  Deep learning has been widely used in the perception (e.g., 3D object\ndetection) of intelligent vehicle driving. Due to the beneficial\nVehicle-to-Vehicle (V2V) communication, the deep learning based features from\nother agents can be shared to the ego vehicle so as to improve the perception\nof the ego vehicle. It is named as Cooperative Perception in the V2V research,\nwhose algorithms have been dramatically advanced recently. However, all the\nexisting cooperative perception algorithms assume the ideal V2V communication\nwithout considering the possible lossy shared features because of the Lossy\nCommunication (LC) which is common in the complex real-world driving scenarios.\nIn this paper, we first study the side effect (e.g., detection performance\ndrop) by the lossy communication in the V2V Cooperative Perception, and then we\npropose a novel intermediate LC-aware feature fusion method to relieve the side\neffect of lossy communication by a LC-aware Repair Network (LCRN) and enhance\nthe interaction between the ego vehicle and other vehicles by a specially\ndesigned V2V Attention Module (V2VAM) including intra-vehicle attention of ego\nvehicle and uncertainty-aware inter-vehicle attention. The extensive experiment\non the public cooperative perception dataset OPV2V (based on digital-twin CARLA\nsimulator) demonstrates that the proposed method is quite effective for the\ncooperative point cloud based 3D object detection under lossy V2V\ncommunication.\n","authors":["Jinlong Li","Runsheng Xu","Xinyu Liu","Jin Ma","Zicheng Chi","Jiaqi Ma","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2212.08273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.02062v2","updated":"2022-12-16T03:51:02Z","published":"2022-07-05T14:13:22Z","title":"Image Amodal Completion: A Survey","summary":"  Existing computer vision systems can compete with humans in understanding the\nvisible parts of objects, but still fall far short of humans when it comes to\ndepicting the invisible parts of partially occluded objects. Image amodal\ncompletion aims to equip computers with human-like amodal completion functions\nto understand an intact object despite it being partially occluded. The main\npurpose of this survey is to provide an intuitive understanding of the research\nhotspots, key technologies and future trends in the field of image amodal\ncompletion. Firstly, we present a comprehensive review of the latest literature\nin this emerging field, exploring three key tasks in image amodal completion,\nincluding amodal shape completion, amodal appearance completion, and order\nperception. Then we examine popular datasets related to image amodal completion\nalong with their common data collection methods and evaluation metrics.\nFinally, we discuss real-world applications and future research directions for\nimage amodal completion, facilitating the reader's understanding of the\nchallenges of existing technologies and upcoming research trends.\n","authors":["Jiayang Ao","Qiuhong Ke","Krista A. Ehinger"],"pdf_url":"https://arxiv.org/pdf/2207.02062v2.pdf","comment":"The manuscript is under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2212.08262v1","updated":"2022-12-16T03:13:43Z","published":"2022-12-16T03:13:43Z","title":"Uniform Sequence Better: Time Interval Aware Data Augmentation for\n  Sequential Recommendation","summary":"  Sequential recommendation is an important task to predict the next-item to\naccess based on a sequence of interacted items. Most existing works learn user\npreference as the transition pattern from the previous item to the next one,\nignoring the time interval between these two items. However, we observe that\nthe time interval in a sequence may vary significantly different, and thus\nresult in the ineffectiveness of user modeling due to the issue of\n\\emph{preference drift}. In fact, we conducted an empirical study to validate\nthis observation, and found that a sequence with uniformly distributed time\ninterval (denoted as uniform sequence) is more beneficial for performance\nimprovement than that with greatly varying time interval. Therefore, we propose\nto augment sequence data from the perspective of time interval, which is not\nstudied in the literature. Specifically, we design five operators (Ti-Crop,\nTi-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original\nnon-uniform sequence to uniform sequence with the consideration of variance of\ntime intervals. Then, we devise a control strategy to execute data augmentation\non item sequences in different lengths. Finally, we implement these\nimprovements on a state-of-the-art model CoSeRec and validate our approach on\nfour real datasets. The experimental results show that our approach reaches\nsignificantly better performance than the other 11 competing methods. Our\nimplementation is available: https://github.com/KingGugu/TiCoSeRec.\n","authors":["Yizhou Dang","Enneng Yang","Guibing Guo","Linying Jiang","Xingwei Wang","Xiaoxiao Xu","Qinghui Sun","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2212.08262v1.pdf","comment":"9 pages, 4 figures, AAAI-2023"},{"id":"http://arxiv.org/abs/2212.08254v1","updated":"2022-12-16T02:52:37Z","published":"2022-12-16T02:52:37Z","title":"RepQ-ViT: Scale Reparameterization for Post-Training Quantization of\n  Vision Transformers","summary":"  Post-training quantization (PTQ), which only requires a tiny dataset for\ncalibration without end-to-end retraining, is a light and practical model\ncompression technique. Recently, several PTQ schemes for vision transformers\n(ViTs) have been presented; unfortunately, they typically suffer from\nnon-trivial accuracy degradation, especially in low-bit cases. In this paper,\nwe propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale\nreparameterization, to address the above issues. RepQ-ViT decouples the\nquantization and inference processes, where the former employs complex\nquantizers and the latter employs scale-reparameterized simplified quantizers.\nThis ensures both accurate quantization and efficient inference, which\ndistinguishes it from existing approaches that sacrifice quantization\nperformance to meet the target hardware. More specifically, we focus on two\ncomponents with extreme distributions: post-LayerNorm activations with severe\ninter-channel variation and post-Softmax activations with power-law features,\nand initially apply channel-wise quantization and log$\\sqrt{2}$ quantization,\nrespectively. Then, we reparameterize the scales to hardware-friendly\nlayer-wise quantization and log2 quantization for inference, with only slight\naccuracy or computational costs. Extensive experiments are conducted on\nmultiple vision tasks with different model variants, proving that RepQ-ViT,\nwithout hyperparameters and expensive reconstruction procedures, can outperform\nexisting strong baselines and encouragingly improve the accuracy of 4-bit PTQ\nof ViTs to a usable level.\n","authors":["Zhikai Li","Junrui Xiao","Lianwei Yang","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2212.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12134v3","updated":"2022-12-16T02:31:25Z","published":"2022-05-24T15:10:50Z","title":"Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box\n  Score-Based Query Attacks","summary":"  The score-based query attacks (SQAs) pose practical threats to deep neural\nnetworks by crafting adversarial perturbations within dozens of queries, only\nusing the model's output scores. Nonetheless, we note that if the loss trend of\nthe outputs is slightly perturbed, SQAs could be easily misled and thereby\nbecome much less effective. Following this idea, we propose a novel defense,\nnamely Adversarial Attack on Attackers (AAA), to confound SQAs towards\nincorrect attack directions by slightly modifying the output logits. In this\nway, (1) SQAs are prevented regardless of the model's worst-case robustness;\n(2) the original model predictions are hardly changed, i.e., no degradation on\nclean accuracy; (3) the calibration of confidence scores can be improved\nsimultaneously. Extensive experiments are provided to verify the above\nadvantages. For example, by setting $\\ell_\\infty=8/255$ on CIFAR-10, our\nproposed AAA helps WideResNet-28 secure 80.59% accuracy under Square attack\n(2500 queries), while the best prior defense (i.e., adversarial training) only\nattains 67.44%. Since AAA attacks SQA's general greedy strategy, such\nadvantages of AAA over 8 defenses can be consistently observed on 8\nCIFAR-10/ImageNet models under 6 SQAs, using different attack targets, bounds,\nnorms, losses, and strategies. Moreover, AAA calibrates better without hurting\nthe accuracy. Our code is available at https://github.com/Sizhe-Chen/AAA.\n","authors":["Sizhe Chen","Zhehao Huang","Qinghua Tao","Yingwen Wu","Cihang Xie","Xiaolin Huang"],"pdf_url":"https://arxiv.org/pdf/2205.12134v3.pdf","comment":"accepted by NeurIPS 2022"},{"id":"http://arxiv.org/abs/2212.08244v1","updated":"2022-12-16T02:23:50Z","published":"2022-12-16T02:23:50Z","title":"Offline Reinforcement Learning for Visual Navigation","summary":"  Reinforcement learning can enable robots to navigate to distant goals while\noptimizing user-specified reward functions, including preferences for following\nlanes, staying on paved paths, or avoiding freshly mowed grass. However, online\nlearning from trial-and-error for real-world robots is logistically\nchallenging, and methods that instead can utilize existing datasets of robotic\nnavigation data could be significantly more scalable and enable broader\ngeneralization. In this paper, we present ReViND, the first offline RL system\nfor robotic navigation that can leverage previously collected data to optimize\nuser-specified reward functions in the real-world. We evaluate our system for\noff-road navigation without any additional data collection or fine-tuning, and\nshow that it can navigate to distant goals using only offline training from\nthis dataset, and exhibit behaviors that qualitatively differ based on the\nuser-specified reward function.\n","authors":["Dhruv Shah","Arjun Bhorkar","Hrish Leen","Ilya Kostrikov","Nick Rhinehart","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2212.08244v1.pdf","comment":"Project page https://sites.google.com/view/revind/home"},{"id":"http://arxiv.org/abs/2106.02735v3","updated":"2022-12-16T02:05:34Z","published":"2021-06-04T22:00:53Z","title":"Learning particle swarming models from data with Gaussian processes","summary":"  Interacting particle or agent systems that display a rich variety of swarming\nbehaviours are ubiquitous in science and engineering. A fundamental and\nchallenging goal is to understand the link between individual interaction rules\nand swarming. In this paper, we study the data-driven discovery of a\nsecond-order particle swarming model that describes the evolution of $N$\nparticles in $\\mathbb{R}^d$ under radial interactions. We propose a learning\napproach that models the latent radial interaction function as Gaussian\nprocesses, which can simultaneously fulfill two inference goals: one is the\nnonparametric inference of {the} interaction function with pointwise\nuncertainty quantification, and the other one is the inference of unknown\nscalar parameters in the non-collective friction forces of the system. We\nformulate the learning problem as a statistical inverse problem and provide a\ndetailed analysis of recoverability conditions, establishing that a coercivity\ncondition is sufficient for recoverability. Given data collected from $M$ i.i.d\ntrajectories with independent Gaussian observational noise, we provide a\nfinite-sample analysis, showing that our posterior mean estimator converges in\na Reproducing kernel Hilbert space norm, at an optimal rate in $M$ equal to the\none in the classical 1-dimensional Kernel Ridge regression. As a byproduct, we\nshow we can obtain a parametric learning rate in $M$ for the posterior marginal\nvariance using $L^{\\infty}$ norm, and the rate could also involve $N$ and $L$\n(the number of observation time instances for each trajectory), depending on\nthe condition number of the inverse problem. Numerical results on systems that\nexhibit different swarming behaviors demonstrate efficient learning of our\napproach from scarce noisy trajectory data.\n","authors":["Jinchao Feng","Charles Kulick","Yunxiang Ren","Sui Tang"],"pdf_url":"https://arxiv.org/pdf/2106.02735v3.pdf","comment":"44 pages; Appendix 5 pages"},{"id":"http://arxiv.org/abs/2212.08235v1","updated":"2022-12-16T02:00:55Z","published":"2022-12-16T02:00:55Z","title":"A Simple Decentralized Cross-Entropy Method","summary":"  Cross-Entropy Method (CEM) is commonly used for planning in model-based\nreinforcement learning (MBRL) where a centralized approach is typically\nutilized to update the sampling distribution based on only the top-$k$\noperation's results on samples. In this paper, we show that such a centralized\napproach makes CEM vulnerable to local optima, thus impairing its sample\nefficiency. To tackle this issue, we propose Decentralized CEM (DecentCEM), a\nsimple but effective improvement over classical CEM, by using an ensemble of\nCEM instances running independently from one another, and each performing a\nlocal improvement of its own sampling distribution. We provide both theoretical\nand empirical analysis to demonstrate the effectiveness of this simple\ndecentralized approach. We empirically show that, compared to the classical\ncentralized approach using either a single or even a mixture of Gaussian\ndistributions, our DecentCEM finds the global optimum much more consistently\nthus improves the sample efficiency. Furthermore, we plug in our DecentCEM in\nthe planning problem of MBRL, and evaluate our approach in several continuous\ncontrol environments, with comparison to the state-of-art CEM based MBRL\napproaches (PETS and POPLIN). Results show sample efficiency improvement by\nsimply replacing the classical CEM module with our DecentCEM module, while only\nsacrificing a reasonable amount of computational cost. Lastly, we conduct\nablation studies for more in-depth analysis. Code is available at\nhttps://github.com/vincentzhang/decentCEM\n","authors":["Zichen Zhang","Jun Jin","Martin Jagersand","Jun Luo","Dale Schuurmans"],"pdf_url":"https://arxiv.org/pdf/2212.08235v1.pdf","comment":"NeurIPS 2022. The last two authors advised equally"},{"id":"http://arxiv.org/abs/2212.08233v1","updated":"2022-12-16T01:45:17Z","published":"2022-12-16T01:45:17Z","title":"Geometry-aware Autoregressive Models for Calorimeter Shower Simulations","summary":"  Calorimeter shower simulations are often the bottleneck in simulation time\nfor particle physics detectors. A lot of effort is currently spent on\noptimizing generative architectures for specific detector geometries, which\ngeneralize poorly. We develop a geometry-aware autoregressive model on a range\nof calorimeter geometries such that the model learns to adapt its energy\ndeposition depending on the size and position of the cells. This is a key\nproof-of-concept step towards building a model that can generalize to new\nunseen calorimeter geometries with little to no additional training. Such a\nmodel can replace the hundreds of generative models used for calorimeter\nsimulation in a Large Hadron Collider experiment. For the study of future\ndetectors, such a model will dramatically reduce the large upfront investment\nusually needed to generate simulations.\n","authors":["Junze Liu","Aishik Ghosh","Dylan Smith","Pierre Baldi","Daniel Whiteson"],"pdf_url":"https://arxiv.org/pdf/2212.08233v1.pdf","comment":"This paper was submitted to NeurIPS Machine Learning and the Physical\n  Sciences Workshop 2022"},{"id":"http://arxiv.org/abs/2212.08232v1","updated":"2022-12-16T01:41:59Z","published":"2022-12-16T01:41:59Z","title":"Offline Robot Reinforcement Learning with Uncertainty-Guided Human\n  Expert Sampling","summary":"  Recent advances in batch (offline) reinforcement learning have shown\npromising results in learning from available offline data and proved offline\nreinforcement learning to be an essential toolkit in learning control policies\nin a model-free setting. An offline reinforcement learning algorithm applied to\na dataset collected by a suboptimal non-learning-based algorithm can result in\na policy that outperforms the behavior agent used to collect the data. Such a\nscenario is frequent in robotics, where existing automation is collecting\noperational data. Although offline learning techniques can learn from data\ngenerated by a sub-optimal behavior agent, there is still an opportunity to\nimprove the sample complexity of existing offline reinforcement learning\nalgorithms by strategically introducing human demonstration data into the\ntraining process. To this end, we propose a novel approach that uses\nuncertainty estimation to trigger the injection of human demonstration data and\nguide policy training towards optimal behavior while reducing overall sample\ncomplexity. Our experiments show that this approach is more sample efficient\nwhen compared to a naive way of combining expert data with data collected from\na sub-optimal agent. We augmented an existing offline reinforcement learning\nalgorithm Conservative Q-Learning with our approach and performed experiments\non data collected from MuJoCo and OffWorld Gym learning environments.\n","authors":["Ashish Kumar","Ilya Kuzovkin"],"pdf_url":"https://arxiv.org/pdf/2212.08232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08230v1","updated":"2022-12-16T01:38:35Z","published":"2022-12-16T01:38:35Z","title":"Multi-Agent Patrolling with Battery Constraints through Deep\n  Reinforcement Learning","summary":"  Autonomous vehicles are suited for continuous area patrolling problems.\nHowever, finding an optimal patrolling strategy can be challenging for many\nreasons. Firstly, patrolling environments are often complex and can include\nunknown and evolving environmental factors. Secondly, autonomous vehicles can\nhave failures or hardware constraints such as limited battery lives.\nImportantly, patrolling large areas often requires multiple agents that need to\ncollectively coordinate their actions. In this work, we consider these\nlimitations and propose an approach based on a distributed, model-free deep\nreinforcement learning based multi-agent patrolling strategy. In this approach,\nagents make decisions locally based on their own environmental observations and\non shared information. In addition, agents are trained to automatically\nrecharge themselves when required to support continuous collective patrolling.\nA homogeneous multi-agent architecture is proposed, where all patrolling agents\nhave an identical policy. This architecture provides a robust patrolling system\nthat can tolerate agent failures and allow supplementary agents to be added to\nreplace failed agents or to increase the overall patrol performance. This\nperformance is validated through experiments from multiple perspectives,\nincluding the overall patrol performance, the efficiency of the battery\nrecharging strategy, the overall robustness of the system, and the agents'\nability to adapt to environment dynamics.\n","authors":["Chenhao Tong","Aaron Harwood","Maria A. Rodriguez","Richard O. Sinnott"],"pdf_url":"https://arxiv.org/pdf/2212.08230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08228v1","updated":"2022-12-16T01:35:27Z","published":"2022-12-16T01:35:27Z","title":"SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image\n  Generation","summary":"  Human organs constantly undergo anatomical changes due to a complex mix of\nshort-term (e.g., heartbeat) and long-term (e.g., aging) factors. Evidently,\nprior knowledge of these factors will be beneficial when modeling their future\nstate, i.e., via image generation. However, most of the medical image\ngeneration tasks only rely on the input from a single image, thus ignoring the\nsequential dependency even when longitudinal data is available. Sequence-aware\ndeep generative models, where model input is a sequence of ordered and\ntimestamped images, are still underexplored in the medical imaging domain that\nis featured by several unique challenges: 1) Sequences with various lengths; 2)\nMissing data or frame, and 3) High dimensionality. To this end, we propose a\nsequence-aware diffusion model (SADM) for the generation of longitudinal\nmedical images. Recently, diffusion models have shown promising results on\nhigh-fidelity image generation. Our method extends this new technique by\nintroducing a sequence-aware transformer as the conditional module in a\ndiffusion model. The novel design enables learning longitudinal dependency even\nwith missing data during training and allows autoregressive generation of a\nsequence of images during inference. Our extensive experiments on 3D\nlongitudinal medical images demonstrate the effectiveness of SADM compared with\nbaselines and alternative methods.\n","authors":["Jee Seok Yoon","Chenghao Zhang","Heung-Il Suk","Jia Guo","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2212.08228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08225v1","updated":"2022-12-16T01:27:42Z","published":"2022-12-16T01:27:42Z","title":"Materials Discovery using Max K-Armed Bandit","summary":"  Search algorithms for the bandit problems are applicable in materials\ndiscovery. However, the objectives of the conventional bandit problem are\ndifferent from those of materials discovery. The conventional bandit problem\naims to maximize the total rewards, whereas materials discovery aims to achieve\nbreakthroughs in material properties. The max K-armed bandit (MKB) problem,\nwhich aims to acquire the single best reward, matches with the discovery tasks\nbetter than the conventional bandit. Thus, here, we propose a search algorithm\nfor materials discovery based on the MKB problem using a pseudo-value of the\nupper confidence bound of expected improvement of the best reward. This\napproach is pseudo-guaranteed to be asymptotic oracles that do not depends on\nthe time horizon. In addition, compared with other MKB algorithms, the proposed\nalgorithm has only one hyperparameter, which is advantageous in materials\ndiscovery. We applied the proposed algorithm to synthetic problems and\nmolecular-design demonstrations using a Monte Carlo tree search. According to\nthe results, the proposed algorithm stably outperformed other bandit algorithms\nin the late stage of the search process when the optimal arm of the MKB could\nnot be determined based on its expectation reward.\n","authors":["Nobuaki Kikkawa","Hiroshi Ohno"],"pdf_url":"https://arxiv.org/pdf/2212.08225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08217v1","updated":"2022-12-16T01:10:49Z","published":"2022-12-16T01:10:49Z","title":"Toward Improved Generalization: Meta Transfer of Self-supervised\n  Knowledge on Graphs","summary":"  Despite the remarkable success achieved by graph convolutional networks for\nfunctional brain activity analysis, the heterogeneity of functional patterns\nand the scarcity of imaging data still pose challenges in many tasks.\nTransferring knowledge from a source domain with abundant training data to a\ntarget domain is effective for improving representation learning on scarce\ntraining data. However, traditional transfer learning methods often fail to\ngeneralize the pre-trained knowledge to the target task due to domain\ndiscrepancy. Self-supervised learning on graphs can increase the\ngeneralizability of graph features since self-supervision concentrates on\ninherent graph properties that are not limited to a particular supervised task.\nWe propose a novel knowledge transfer strategy by integrating meta-learning\nwith self-supervised learning to deal with the heterogeneity and scarcity of\nfMRI data. Specifically, we perform a self-supervised task on the source domain\nand apply meta-learning, which strongly improves the generalizability of the\nmodel using the bi-level optimization, to transfer the self-supervised\nknowledge to the target domain. Through experiments on a neurological disorder\nclassification task, we demonstrate that the proposed strategy significantly\nimproves target task performance by increasing the generalizability and\ntransferability of graph-based knowledge.\n","authors":["Wenhui Cui","Haleh Akrami","Anand A. Joshi","Richard M. Leahy"],"pdf_url":"https://arxiv.org/pdf/2212.08217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08216v1","updated":"2022-12-16T01:10:41Z","published":"2022-12-16T01:10:41Z","title":"Azimuth: Systematic Error Analysis for Text Classification","summary":"  We present Azimuth, an open-source and easy-to-use tool to perform error\nanalysis for text classification. Compared to other stages of the ML\ndevelopment cycle, such as model training and hyper-parameter tuning, the\nprocess and tooling for the error analysis stage are less mature. However, this\nstage is critical for the development of reliable and trustworthy AI systems.\nTo make error analysis more systematic, we propose an approach comprising\ndataset analysis and model quality assessment, which Azimuth facilitates. We\naim to help AI practitioners discover and address areas where the model does\nnot generalize by leveraging and integrating a range of ML techniques, such as\nsaliency maps, similarity, uncertainty, and behavioral analyses, all in one\ntool. Our code and documentation are available at\ngithub.com/servicenow/azimuth.\n","authors":["Gabrielle Gauthier-Melançon","Orlando Marquez Ayala","Lindsay Brin","Chris Tyler","Frédéric Branchaud-Charron","Joseph Marinier","Karine Grande","Di Le"],"pdf_url":"https://arxiv.org/pdf/2212.08216v1.pdf","comment":"To be published in Proceedings of the 2022 Conference on Empirical\n  Methods in Natural Language Processing: System Demonstrations. 13 pages and\n  14 figures"},{"id":"http://arxiv.org/abs/2208.02879v2","updated":"2022-12-16T00:31:44Z","published":"2022-08-04T20:31:46Z","title":"PointConvFormer: Revenge of the Point-based Convolution","summary":"  We introduce PointConvFormer, a novel building block for point cloud based\ndeep network architectures. Inspired by generalization theory, PointConvFormer\ncombines ideas from point convolution, where filter weights are only based on\nrelative position, and Transformers which utilize feature-based attention. In\nPointConvFormer, attention computed from feature difference between neighboring\npoints is used to modify the convolutional weights at each point. Hence,\ninvariances from point convolution are preserved, whereas attention helps to\nselect relevant points in the neighborhood. PointConvFormer is suitable for\nmultiple tasks that require details at the point level, such as segmentation\nand scene flow estimation tasks. We experiment on both tasks with multiple\ndatasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our\nresults show that PointConvFormer substantially outperforms classic\nconvolutions, regular transformers, and voxelized sparse convolution approaches\nwith much smaller and faster networks. Visualizations show that PointConvFormer\nperforms similarly to convolution on flat areas, whereas the neighborhood\nselection effect is stronger on object boundaries, showing that it has got the\nbest of both worlds.\n","authors":["Wenxuan Wu","Qi Shan","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2208.02879v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2212.08281v1","updated":"2022-12-16T05:08:52Z","published":"2022-12-16T05:08:52Z","title":"HGAN: Hierarchical Graph Alignment Network for Image-Text Retrieval","summary":"  Image-text retrieval (ITR) is a challenging task in the field of multimodal\ninformation processing due to the semantic gap between different modalities. In\nrecent years, researchers have made great progress in exploring the accurate\nalignment between image and text. However, existing works mainly focus on the\nfine-grained alignment between image regions and sentence fragments, which\nignores the guiding significance of context background information. Actually,\nintegrating the local fine-grained information and global context background\ninformation can provide more semantic clues for retrieval. In this paper, we\npropose a novel Hierarchical Graph Alignment Network (HGAN) for image-text\nretrieval. First, to capture the comprehensive multimodal features, we\nconstruct the feature graphs for the image and text modality respectively.\nThen, a multi-granularity shared space is established with a designed\nMulti-granularity Feature Aggregation and Rearrangement (MFAR) module, which\nenhances the semantic corresponding relations between the local and global\ninformation, and obtains more accurate feature representations for the image\nand text modalities. Finally, the ultimate image and text features are further\nrefined through three-level similarity functions to achieve the hierarchical\nalignment. To justify the proposed model, we perform extensive experiments on\nMS-COCO and Flickr30K datasets. Experimental results show that the proposed\nHGAN outperforms the state-of-the-art methods on both datasets, which\ndemonstrates the effectiveness and superiority of our model.\n","authors":["Jie Guo","Meiting Wang","Yan Zhou","Bin Song","Yuhao Chi","Wei Fan","Jianglong Chang"],"pdf_url":"https://arxiv.org/pdf/2212.08281v1.pdf","comment":null}]}}